Studies in Systems, Decision and Control 504
Julio B. Clempner
Alexander Poznyak
Optimization 
and Games 
for Controllable 
Markov Chains
Numerical Methods with Application 
to Finance and EngineeringStudies in Systems, Decision and Control 
Volume 504 
Series Editor 
Janusz Kacprzyk, Systems Research Institute, Polish Academy of Sciences, 
Warsaw, PolandThe series “Studies in Systems, Decision and Control” (SSDC) covers both new 
developments and advances, as well as the state of the art, in the various areas of 
broadly perceived systems, decision making and control–quickly, up to date and 
with a high quality. The intent is to cover the theory, applications, and perspectives 
on the state of the art and future developments relevant to systems, decision 
making, control, complex processes and related areas, as embedded in the fields of 
engineering, computer science, physics, economics, social and life sciences, as well 
as the paradigms and methodologies behind them. The series contains monographs, 
textbooks, lecture notes and edited volumes in systems, decision making and 
control spanning the areas of Cyber-Physical Systems, Autonomous Systems, 
Sensor Networks, Control Systems, Energy Systems, Automotive Systems, 
Biological Systems, Vehicular Networking and Connected Vehicles, Aerospace 
Systems, Automation, Manufacturing, Smart Grids, Nonlinear Systems, Power 
Systems, Robotics, Social Systems, Economic Systems and other. Of particular 
value to both the contributors and the readership are the short publication timeframe 
and the world-wide distribution and exposure which enable both a wide and rapid 
dissemination of research output. 
Indexed by SCOPUS, DBLP, WTI Frankfurt eG, zbMATH, SCImago. 
All books published in the series are submitted for consideration in Web of Science.Julio B. Clempner · Alexander Poznyak 
Optimization and Games 
for Controllable Markov 
Chains 
Numerical Methods with Application 
to Finance and EngineeringJulio B. Clempner 
Escuela Superior de Física y Matemáticas 
(ESFM) 
Instituto Politécnico Nacional 
Mexico City, Mexico 
Alexander Poznyak 
Department of Automatic Control 
Center for Research and Advanced Studies 
Mexico City, Mexico 
ISSN 2198-4182 ISSN 2198-4190 (electronic) 
Studies in Systems, Decision and Control 
ISBN 978-3-031-43574-4 ISBN 978-3-031-43575-1 (eBook) 
https://doi.org/10.1007/978-3-031-43575-1 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Paper in this product is recyclable.This book is dedicated to my beloved wife 
Erika and our three children, Michelle, 
Alexander and Erick who took delight in my 
triumphs and express great joy in my success. 
Julio B. Clempner 
This book is dedicated to my teachers and my 
students. 
Alexander PoznyakPreface 
Markov systems have a wide range of applications, including modeling the behavior 
of macroeconomic systems, computer communication networks, manufacturing 
processes, and computer operating systems. These models have logically inferred 
the possibility of the Markov property. As a result, stochastic dynamic programming 
has been the focus of several journal articles and books as an optimization strategy 
that may be used in the Markov situation. 
Here we consider the class of ergodic finite controllable Markov’s chains. The core 
idea behind the method, described in this book, is to “immerse” the original discrete 
optimization problems (or game models) in the space of randomized formulations, 
where the variables stand in for the distributions (mixed strategies or preferences) 
of the original discrete (pure) strategies in the use. The following assumptions are 
made: a finite state space, a limited action space, continuity of the probabilities 
and rewards associated with the actions, and a necessity for accessibility. These 
hypotheses lead to the existence of an optimal policy. The best course of action is 
always stationary. It is either simple (i.e., non-randomized stationary) or composed of 
two non-randomized policies, which is equivalent to randomly selecting one of two 
simple policies throughout each epoch by tossing a biased coin. As an added bonus, 
the optimization procedure just has to repeatedly solve the time-average dynamic 
programming equation, making it theoretically feasible to choose the optimum course 
of action under the global restriction. In the ergodic cases the state distributions, 
generated by the corresponding transition equations, exponentially quickly converge 
to their stationary (final) values. This makes it possible to employ all widely used 
optimization methods (such as Gradient-like procedures, Lagrange’s multipliers, and 
Tikhonov’s regularization), including the related numerical techniques. 
This book consists of 12 chapters: 
– Chapter 1 describes a class of discrete-time, controllable, and ergodic Markov 
chains and to convert the nonlinear optimization Markov problem into a linear 
one and make the problem tractable, we propose to use an auxiliary c-variable. 
– Chapter 2 presents a multi-objective Pareto front solution for a particular type 
of discrete-time ergodic controllable Markov chains considering the regularized
viiviii Preface
penalty method to identify the Pareto policies along the Pareto frontier based on 
Tikhonov’s regularization method. 
– Chapter 3 focuses on the design of an observer for a class of partially observ￾able ergodic homogeneous finite Markov chains, where the goal is to derive 
the formulas for computing an observer and, as a consequence, the best control 
strategy. 
– Chapter 4 extends the c-variable approach by adding a new linear constraint for 
continuous-time Markov chains, where this method’s benefit is that it trans￾forms the continuous-time Markov decision process problem into a discrete￾time Markov decision process, such that the linear constraints make the problem 
computationally tractable. 
– Chapter 5 provides an approach to locating the strong Nash and Stackelberg 
equilibrium based on a method that depends on identifying a scalar λ∗ and the 
associated strategies d∗(λ∗) fixing particular boundaries (min and max) that belong 
to the Pareto front, which refer to limits placed by the player over the Pareto front 
that form a specific decision region where the strategies can be selected. 
– Chapter 6 demonstrates that the best-reply actions surely lead to an equilibrium 
point for a type of finite controlled Markov Chains dynamic games, providing a 
technique for creating a Lyapunov-like function that describes how players behave 
in a recurrent Markov-Lyapunov game that replaces the components of the ergodic 
system, which simulate players’ anticipated behavior in one-shot games, for the 
recursive process. 
– Chapter 7 suggests an analytical method for computing Bayesian incentive￾compatible mechanisms where the private information is revealed following a 
class of controllable Markov games, including a new variable that denotes the 
outcome of the distribution vector, the strategies, and the mechanism design using 
a Reinforcement Learning methodology to calculate a mechanism that is nearly 
optimum and in equilibrium with the game’s winning strategy. 
– Chapter 8 offers a solution for Bayesian Partially Observable Markov Games 
(BPOMGs) supported by an AI strategy, based on a nucleus structure governed 
by three essential concepts: game theory, learning, and inference. 
– Chapter 9 explores the theory behind bargaining games and offers a way for 
solving the game-theoretic models of bargaining put out by Nash and Kalai￾Smorodinsky, which suggest a beautiful axiomatic solution to the problem based 
on several fairness standards. 
– Chapter 10 provides a brand-new paradigm for combining game theory and the 
extraproximal approach to represent the multi-traffic signal-control synchroniza￾tion problem, where the intersection’s goal is to reduce queuing time, and finding 
the best signal timing strategy, or assigning a green period to each signal phase, 
which is a challenge for signal controllers. 
– Chapter 11 presents a game that helps myopic participants achieve equilibrium 
as if they were forward-thinking agents. One of the game’s main mechanics is 
that players are penalized for deviating from their prior best-reply plan as well as 
for the amount of time they spend to make decisions at each stage of play. Our 
chapter adds to existing research on typical myopic agent bargaining while alsoPreface ix
broadening the class of processes and functions that may be used to define and 
apply Rubinstein’s non-cooperative bargaining solutions. 
– Chapter 12 suggests a solution to the transfer pricing problem. It analyzes a 
company with sequential transfers among its several divisions, where central 
management decides on the transfer price to maximize operational profitability. 
Throughout the negotiation process, the price shifting between divisions is a tool 
for bargaining. 
This book is aimed at graduate students (Masters and Doctorate), who wish to 
learn more about how Markov chains theory solves different problems that arise in 
the real world. 
Mexico City, Mexico Julio B. Clempner 
Alexander PoznyakAcknowledgements 
The presented material is based on more than 15 years of collaboration and teaching 
experience of the authors in Mexico Center for Research and Advanced Studies of 
the IPN (CINVESTAV), Automatic Control Department and School of Physics and 
Mathematics, National Polytechnic Institute, Mexico City, Mexico. The fundamental 
concepts of this course were created by world-renowned scientists such as John 
Nash, Harry Markowitz, John G. Kemeny and J. Laurie Snell, Ehud Kalai and Meir 
Smorodinsky, Andrey Tikhonov, and others. 
The authors would like to express their wide thanks to the colleagues from the 
CINVESTAV and from the National Polytechnic Institute as well as their Mexican ex￾Ph.D. students for their kind collaboration in the development of this book. Of course, 
we feel deep gratitude to our students over the years, without whose evident enjoy￾ment and expressed appreciation the current book would not have been undertaken. 
Finally, we wish to acknowledge the editors at Springer for being so cooperative 
during the production process. 
Mexico City, Mexico Julio B. Clempner 
Alexander Poznyak
xiContents 
1 Controllable Markov Chains ................................... 1 
1.1 Finite Markov Chains ..................................... 1 
1.1.1 General Properties ................................ 1 
1.1.2 Ergodic Markov Chains ............................ 4 
1.1.3 Transition Equation and Invariant State 
Distribution ...................................... 7 
1.2 Controllable Markov Chain ................................ 7 
1.2.1 Control Policy .................................... 7 
1.2.2 Main Definition .................................. 8 
1.3 Cost Functions for Markov Chains .......................... 9 
1.3.1 Average Cost Function ............................ 9 
1.3.2 Auxiliary c-Variables .............................. 9 
1.4 Markov Decision Process as Linear a Programming 
Problem ................................................ 10 
1.4.1 Lineal Programming Formulation ................... 10 
1.4.2 Realization of Stationary Strategies in the State 
Space ........................................... 12 
1.4.3 Numerical Example ............................... 15 
References .................................................... 15 
2 Multiobjective Control ......................................... 17 
2.1 Introduction ............................................. 17 
2.1.1 Pareto Set ....................................... 17 
2.1.2 Parametrization of All Pareto Points ................. 19 
2.1.3 Utopia Point ..................................... 21 
2.2 Regularized Penalty Function Optimization Method 
(RPFOM) ............................................... 23 
2.2.1 Poly-Linear Optimization Problem Formulation ....... 23 
2.2.2 Penalty Functions Approach ........................ 23 
2.2.3 Expected Property of RPFA ........................ 24
xiiixiv Contents
2.2.4 The Main Result on the Extremal Points 
of the Penalty Functions ........................... 24 
2.2.5 Recurrent Algorithm .............................. 25 
2.2.6 Special Selection of the Parameters .................. 26 
2.2.7 Numerical Example: Pareto Front Calculation ......... 27 
2.3 Portfolio Optimization Problem ............................ 29 
2.3.1 Problem Formulation .............................. 29 
2.3.2 Markowitz Function ............................... 31 
2.3.3 Numerical Example: A Rational Investor ............. 32 
2.4 Appendix ............................................... 33 
References .................................................... 44 
3 Partially Observable Markov Chains ............................ 47 
3.1 Introduction ............................................. 47 
3.1.1 Brief Review ..................................... 47 
3.2 Partially Observable Markov Chains ........................ 49 
3.3 Formulation of the Problem ................................ 51 
3.4 Description in the c-Variables .............................. 52 
3.5 Computation of the Estimated Value by Measurable 
Realization: Projection Stochastic Approximation 
Procedure ............................................... 55 
3.5.1 Adaptive Algorithm ............................... 57 
3.6 Numerical Example: A Partially Observable Markowitz 
Portfolio ................................................ 57 
References .................................................... 62 
4 Continuous-Time Markov Chains ............................... 65 
4.1 Introduction ............................................. 65 
4.1.1 Related Work .................................... 65 
4.2 Continuous-Time Markov Chains ........................... 67 
4.3 Programming Solver for CTMDP ........................... 71 
4.3.1 The c-Variable Method ............................ 71 
4.3.2 Linear Programming Solver ........................ 72 
4.4 Chemical Reaction Markov Models ......................... 74 
4.4.1 Example 1. Formation of the Amidogen Radical ....... 74 
4.4.2 Example 2. Proton Transfer, Hydration 
and Tautomeric Reaction of Anthocyanin Pigments .... 78 
References .................................................... 83 
5 Nash and Stackelberg Equilibrium .............................. 85 
5.1 Optimization and Equilibrium .............................. 85 
5.2 ε-Nash Equilibrium and Tanaka’s Function ................... 87 
5.2.1 Individual Cost Function ........................... 87 
5.2.2 Regularized Lagrange Function ..................... 89 
5.2.3 Tanaka’s Function ................................ 90 
5.2.4 ASG Continuous-Time Algorithm ................... 91Contents xv
5.3 Extraproximal Method .................................... 93 
5.3.1 Proximal Format .................................. 93 
5.4 Numerical Example: Strong Nash Equilibrium in Pareto 
Front ................................................... 94 
5.4.1 Euler Approach ................................... 94 
5.4.2 Numerical Data ................................... 96 
5.5 The Stackelberg-Nash Equilibrium Concept .................. 100 
5.5.1 Specific Features ................................. 100 
5.5.2 Individual Aims and Tanaka’s Representation ......... 101 
5.5.3 Extraproximal Procedure .......................... 102 
5.6 Convergence Analysis .................................... 104 
5.6.1 Auxiliary Results ................................. 104 
5.6.2 Main Convergence Theorem ........................ 105 
5.7 Application Example: Four Supermarkets Chain .............. 108 
References .................................................... 113 
6 Best-Reply Strategies in Repeated Games ....................... 115 
6.1 Introduction ............................................. 115 
6.2 Preliminaries ............................................ 117 
6.2.1 Controllable Markov Decision Process ............... 117 
6.2.2 Game Description ................................ 119 
6.3 Problem Formulation ..................................... 120 
6.3.1 The State-Value Function .......................... 120 
6.3.2 The Recursive Matrix Form ........................ 122 
6.4 Construction of a Lyapunov-Like Function ................... 124 
6.4.1 Recurrent Form for the Cost Function ................ 125 
6.4.2 The Lyapunov Function Design ..................... 125 
6.5 Examples ............................................... 127 
6.5.1 Example 1 (Banks Marketing Planning 
as Prisoner’s Dilemma) ............................ 127 
6.5.2 Example 2 (Duel Game) ........................... 129 
References .................................................... 134 
7 Mechanism Design ............................................ 137 
7.1 Introduction ............................................. 137 
7.1.1 Brief Review ..................................... 137 
7.2 Description of the Model .................................. 140 
7.3 Mechanism and Equilibrium ............................... 142 
7.4 Reinforcement Learning Approach .......................... 146 
7.5 Risk-Averse Agents Strategies in Contracting Problem ......... 150 
References .................................................... 153xvi Contents
8 Joint Observer and Mechanism Design .......................... 155 
8.1 Introduction ............................................. 156 
8.1.1 Brief Problem Analysis ............................ 156 
8.1.2 Related Work .................................... 157 
8.2 Markov Games .......................................... 159 
8.3 Problem Formulation ..................................... 162 
8.3.1 Initial Problem ................................... 162 
8.3.2 Auxiliary Problem ................................ 162 
8.4 Relation of Solutions for Initial and Auxiliary Problems ....... 163 
8.4.1 Ergodicity Condition .............................. 165 
8.5 Reinforcement Learning Approach .......................... 167 
8.5.1 Iterative Procedure ................................ 167 
8.5.2 Learning Algorithm ............................... 169 
8.6 Nash Equilibrium as a Solution of a Max-Min Problem ........ 170 
8.7 Application: Patrolling .................................... 172 
8.7.1 Description of the Patrolling Problem ................ 172 
8.7.2 Solver ........................................... 173 
8.7.3 Random Walk Problem Formulation ................. 174 
8.7.4 Resulting Values .................................. 175 
References .................................................... 181 
9 Bargaining Games or How to Negotiate ......................... 185 
9.1 Introduction ............................................. 185 
9.1.1 Brief Review ..................................... 185 
9.1.2 Related Work .................................... 186 
9.1.3 Nash Versus Kalai-Smorodinsky .................... 189 
9.2 Motivation .............................................. 190 
9.3 Preliminaries ............................................ 192 
9.4 The Nash Bargaining Model ............................... 196 
9.5 The Kalai-Smorodinsky Bargaining Model ................... 198 
9.5.1 The n-Person Kalai-Smorodinsky Solution ........... 199 
9.6 The Bargaining Solver .................................... 201 
9.6.1 The Nash Bargaining Solver ........................ 201 
9.6.2 Kalai-Smorodinsky Solver ......................... 202 
9.6.3 The Extraproximal Solver Method .................. 204 
9.7 The Model for the Disagreement Point ...................... 208 
9.7.1 The Extraproximal Solver Method .................. 210 
9.8 Numerical Illustration ..................................... 211 
9.8.1 Computing the Disagreement Point .................. 213 
9.8.2 Computing the Nash Bargaining Solution ............ 214 
9.8.3 Computing the Kalai-Smorodinsky Bargaining 
Solution ......................................... 216 
References .................................................... 218Contents xvii
10 Multi-traffic Signal-Control Synchronization .................... 221 
10.1 Introduction ............................................. 222 
10.1.1 Brief Review ..................................... 222 
10.1.2 Related Work on Traffic Control .................... 222 
10.2 Preliminaries ............................................ 225 
10.3 Nash Equilibrium ........................................ 226 
10.3.1 The Regularized Lagrange Principle Application ...... 228 
10.3.2 The Proximal Format .............................. 229 
10.3.3 The Extraproximal Method ......................... 229 
10.4 Traffic-Signal-Control Problem Formulation ................. 230 
10.4.1 Transition Matrix ................................. 231 
10.4.2 Ergodicity ....................................... 237 
10.4.3 Cost Function .................................... 237 
10.5 Gradient Solver .......................................... 238 
10.6 Application Example ..................................... 240 
References .................................................... 245 
11 Non-cooperative Bargaining with Unsophisticated Agents ......... 249 
11.1 Introduction ............................................. 249 
11.1.1 Related Literature ................................. 252 
11.2 The Rubinstein’s Alternating-Offers Model .................. 254 
11.3 Bargaining with Unsophisticated Players .................... 256 
11.4 An Extension to Continuous-Time Markov Chains ............ 259 
11.4.1 Solution Method .................................. 261 
11.4.2 The Pareto Optimal Solution of the Bargaining 
Problem ......................................... 262 
11.4.3 The Non-cooperative Bargaining Solution ............ 264 
11.5 Numeric Simulations ..................................... 266 
11.5.1 Division of a Fix Resource ......................... 266 
11.6 Extensions .............................................. 268 
11.6.1 Bargaining Under Different Discounting ............. 268 
11.6.2 Bargaining with Collusive Behavior ................. 270 
11.7 Appendix: Proofs ........................................ 274 
11.7.1 The Non-cooperative Bargaining Game .............. 274 
11.7.2 Formulation of the Problem ........................ 276 
11.7.3 Convergence Analysis ............................. 277 
11.7.4 Convergence Conditions of δ and α ................. 285 
References .................................................... 287 
12 Transfer Pricing as Bargaining ................................. 289 
12.1 Introduction ............................................. 289 
12.1.1 Transfer Pricing Process ........................... 289 
12.1.2 Brief Review ..................................... 290 
12.2 Preliminaries ............................................ 293 
12.2.1 Nash’s Bargaining ................................ 293 
12.2.2 Continuous-Time Bargaining ....................... 294xviii Contents
12.3 Transfer Pricing .......................................... 296 
12.4 The Transfer Pricing Nash Bargaining Solution ............... 300 
12.5 Transfer Price Bargaining Solver with Additional 
Constraints .............................................. 303 
12.5.1 Numerical Example for Nash’s Bargaining 
Transfer Pricing .................................. 305 
12.6 Continuous-Time Transfer Pricing .......................... 310 
12.6.1 Revenue of a Passenger Between Members 
of an Airline Alliance ............................. 310 
12.7 Extensions .............................................. 314 
12.7.1 Bargaining Under Different Discounting ............. 314 
12.7.2 Bargaining with Collusive Behavior ................. 318 
References .................................................... 325 
Index ............................................................. 329Chapter 1 
Controllable Markov Chains 
Abstract In this chapter, we describe a class of discrete-time, controllable, and 
ergodic Markov chains. In these concepts, time and space are discrete. We start 
by outlining the fundamental model. Its single-step transition probabilities is then 
utilized to determine if a Markov chain is ergodic. In order to convert the nonlinear 
optimization Markov problem into a linear one and make the problem tractable, we 
propose to use an auxiliary .c-variable. With such a setup, a discrete time Markov 
chain simulation is shown. A piece of the illustrative Markov chain laws for discrete 
time is constructed in the end. 
1.1 Finite Markov Chains 
1.1.1 General Properties 
Let .N .= {0, 1,...} and let .Rn be an .n-dimensional Euclidean space, .R = R1. Let 
us define a probability space .(Ω, F, P) where .Ω is a set of elementary events, . F
is the minimal.σ-algebra of the subsets of. Ω, and.P is a given probability measure 
defined on.A ∈ F. Let us also consider the natural sequence.t = 0, 1, 2,... (.t ∈ N) 
as a time argument. Let . S be a finite set consisting of states .{s1,...,sN }, .N ∈ N, 
called the state space. 
Definition 1.1 A Markov chain 1 [ 5, 6, 9] is a sequence of .N-valued random vari￾ables.s(t),.t ∈ N, satisfying the Markov condition: 
.
P(s(t + 1) = sj|s(t) = si,s(t − 1) = sit−1 ,...,s(0) = si0 ) =
P (
s(t + 1) = sj|s(t) = si
)
:= πj|i(t), (1.1.1) 
1 Markov chains are named in honor of the Russian mathematician, Alexander Markov (1856–1922), 
who did pioneering work in the definition and analysis of this class of processes. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_1 
12 1 Controllable Markov Chains
namely, only the current state determines the conditional probability that the system 
will be in the state .sj at some future time under the specified prehistory. This phe￾nomenon may be demonstrated with a physical example in which a particle’s future 
dynamics depend only on this state and not on how it came to be in its current state. 
We call (1.1.1) the Markov chain condition. The whole theory of Markov chains 
flows out of this condition. 
Remark 1.1 In particular, (1.1.1) a general property called the Markov property, 
which implies that at any time . t the past states .s(t − 1), . . . ,s(0) and future state 
.s(t + 1) of the process are conditionally independent, given the present.s(t). 
The (1.1.1) is a consequence of the simple identity given by . P(A ∩ B) =
P(A|B)P(B), such that .A = {s(t + 1) = sj} and .B = {s(0) = si0 ,...,s(t) = si}. 
If. t is thought of as the present time, the expression 
. P(s(t + 1) = sj|s(t) = si,s(t − 1) = sit−1 ,...,s(0) = si0 ) for t ≥ 0,
in (1.1.1) is a conditional probability for the value of the process one step ahead into 
the future, given its entire past history. We call it a one-step-ahead conditional prob￾ability. As we see, from (1.1.1), these conditional probabilities link the joint distribu￾tion of .(s(0) = si0 ,...,s(t) = si,s(t + 1) = sj) to that of . (s(0) = si0 ,...,s(t) =
si), and thus they determine how joint distributions evolve as the time goes forward. 
Then, we can model a stochastic process by specifying its one-step-ahead conditional 
probabilities. In practice, they can often be deduced directly from assumptions on 
the physical nature of the process. 
Definition 1.2 If the probability .πj|i(t) = P (
s(t + 1) = sj|s(t) = si
) does not 
depend on. t, i.e..πj|i, then the Markov chain is said to be time-homogeneous. 
The random variables.s(t) are defined on the probability space.(Ω, F, P) and take 
values in . S. The stochastic process .{s(t), t ∈ N} is assumed to be a Markov chain. 
The Markov chain can be represented by a complete graph (see Fig. 1.1) whose nodes 
are the states, where each edge.(si,sj) ∈ S2 is labeled by the transition probability. 
The sum of the probabilities of.πj|i over all states equals 1, that is, 
.
∑
N
j=1
πj|i(t) = ∑
N
j=1
P(s(t + 1) = sj|s(t) = si) = 1. (1.1.2) 
A stochastic matrix is any square matrix of non-negative entries each of whose 
rows sums to 1. Then, state transition matrices are stochastic matrices. Conversely, 
any stochastic matrix is a valid model for the transition probabilities of a Markov 
chain. 
The matrix.Π = (πj|i)(si,sj)∈S ∈ [0, 1]
N×N determines the evolution of the time￾homogeneous chain: for each .t ∈ N, the power .Πt has in each entry .(si,sj) the 
probability of going from state. si to state.sj in exactly. t steps. In particular, we have1.1 Finite Markov Chains 3
s 
s3 
0.4887 
0.1813 
0.2190 
0.1110 
s1 4 
s2 
s4 
0.2656 s3 s3 
0.0209 
0.7086 
s1 
0.8539 
s1 
s4 
0.0156 
0.0049 s2 0.1203 s2 
0.0103 
s3 
0.1778 
s1 0.6334 0.0142 
s4 
s2 0.1746 s1 s2 s3 s4 
Fig. 1.1 Markov chain graph 
that .Π1 = Π. The probability, starting in state . i, of going to state . j in two steps is 
the sum over. l of the probability of going first to. l and then to. j. Using the Markov 
condition [ 7] we have 
. πj|i = ∑
N
l=1
(
πj|l
) (πl|i
)
.
It can be seen that this is just the.i j term of the product of the matrix.Π with itself, 
this means that.
(
πj|i
)2 is the.(i, j) element of the matrix.Π2. 
Proposition 1.1 The Chapman-Kolmogorov equation is given by 
. Πt+n = Πt
Πn, (1.1.3) 
where.Πt is the.t-step transition probability matrix and.Π is the one-step transition 
matrix. 
Proof By induction, for .t = 0,we have that .Π0Π = IΠ. Let us suppose that the 
proposition is true for time. t. Then, for time.t + 1 we have 
. Πt+1 = ∑
N
l=1
(
πj|l
)t (
πl|i
)1 = ∑
N
l=1
(
πj|l
)t
πl|i = Πt
Π,
and for. n we have that 
. (
πj|l
)t+n = ∑
N
l=1
(
πj|l
)t (
πl|i
)n ,
or in matrix form we have 
.Πt+n = Πt
Πn.
□4 1 Controllable Markov Chains
1.1.2 Ergodic Markov Chains 
Our results are based on the following Theorems and Lemmas (for the proof see [ 9]). 
Theorem 1.1 (the ergodic theorem) Let for some state . j ∈ (1,..., N) of a homo￾geneous (stationary) Markov chain with the transition matrix.Π and some.t > 0,. ξ
.∈.(0, 1) for all. i ∈ G
.π˜ j|i(t) := P(s(t) = sj0 |s(0) = si) = ⊓t
τ=1
πs(τ )|s(τ−1) ≥ ξ > 0. (1.1.4) 
Then for any initial state distribution.P {s(0) = si} and for any.i, j.= 1,..., N there 
exists the limit 
.p∗
j := lim
t→∞π˜ j|i(t) > 0, (1.1.5) 
such that for any.t ≥ 0 this limit is reachable with an exponential rate, namely, 
.
|
|π˜ j|i(t) − p∗
j
|
| ≤ (1 − ξ )
t = e−αt
, (1.1.6) 
where.α := |ln (1 − ξ )|. 
Proof (a) For any.t ≥ 0 define 
. qj(t) := inf
i=1,N
πi,j(t) and Q j(t) := sup
i=1,N
πi,j(t),
which evidently satisfy 
. qj(t) ≤ πi,j(t) ≤ Q j(t)
for any.i, j .= 1,..., N and any.t ≥ 0. Show that.qj(t) monotonically increases and 
.Q j(t) monotonically decreases such that 
.Q j(t) − qj(t) →t→∞ 0 (1.1.7) 
since, having (1.1.7), we obtain (1.1.5). Using the property of Markov chain we have 
. 
qj(r + t) := inf
i=1,N
∑
N
k=1
πi,k (r)πk,j(t) ≥ qj(t) inf
i=1,N
∑
N
k=1
πi,k (t) = qj(t),
Q j(r + t) := sup
i=1,N
∑
N
k=1
πi,k (r)πk,j(t) ≤ Q j(t) sup
i=1,N
∑
N
k=1
πi,k (r) = Q j(t).
Next, for.0 ≤ n ≤ t1.1 Finite Markov Chains 5
. 
Q j (t) − q j (t) = sup
i=1,N
πi,j (t) + sup
l=1,N
[
−πl,j (t)
]
= sup
i,l=1,N
[
πi,j (t) − πl,j (t)
]
=
sup
i,l=1,N
∑N
k=1
[
πi,k (h) − πl,k (h)
]
πk,j (t − h) = sup
i,l=1,N
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∑
k∈G+
[
πi,k (h) − πl,k (h)
]
πk,j (t − h) +
∑
k∈G−
[
πi,k (h) − πl,k (h)
]
πk,j (t − h)
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
≤
sup
i,l=1,N
{
Q j (t − h) ∑
k∈G+
[
πi,k (h) − πl,k (h)
]
+ q j (t − h) ∑
k∈G−
[
πi,k (h) − πl,k (h)
] }
.
Here 
. 
G+ := {
k = 1,..., N : πi,k (h) − πl,k (h) ≥ 0
}
,
G− := {
k = 1,..., N : πi,k (h) − πl,k (h) < 0
}
.
So, evidently that 
. 
∑
k∈G+
[
πi,k (h) − πl,k (h)
]
+ ∑
k∈G−
[
πi,k (h) − πl,k (h)
]
=
∑
k∈G+
πi,k (h) + ∑
k∈G−
πi,k (h) − ∑
k∈G+
πl,k (h) − ∑
k∈G−
πl,k (h) =
∑
k∈G
πi,k (h) − ∑
k∈G
πl,k (h) = 1 − 1 = 0,
and therefore 
.
Q j (t) − qj (t) ≤ [
Q j (t − h) − qj (t − h)
] ∑
k∈G+
[
πi,k (h) − πl,k (h)
]
. (1.1.8) 
Now notice that if. j0 ∈/ G+ then 
. ∑
k∈G+
[
πi,k (h) − πl,k (h)
]
≤ ∑
k∈G+
πi,k (h) ≤ 1 − πi,j (h0) ≤ 1 − δ,
and if. j0 ∈ G+ then 
. ∑
k∈G+
[
πi,k (h) − πl,k (h)
]
≤ ∑
k∈G+
πi,k (h) − πi,j0 (h) ≤ 1 − δ,
For h.= 1 (1.1.8) leads to 
. Q j (t) − qj (t) ≤ (1 − δ)
[
Q j (t − 1) − qj (t − 1)
]
.
Iterating back this inequality.t-times and using the estimate we get 
.Q j (t) − qj (t) ≤ (1 − δ)
t , (1.1.9)6 1 Controllable Markov Chains
which proves (1.1.7) and, consequently, (1.1.5). 
(b) In view of the inequality 
. 
|
|πi,j (t) − p∗
j
|
| ≤ Q j (t) − qj (t),
and using (1.1.9) we obtain (1.1.6). Theorem is proven. □
Corollary 1.1 (on a stationary state distribution) Suppose that (1.1.4) holds. Then 
for any. j = 1,..., N and for any 
.pj (t) := P {
s(t) = sj
}
, (1.1.10) 
the following property holds 
.
|
|pj(t) − p∗
j
|
| ≤ (1 − δ)
t , (1.1.11) 
where.p∗
j as in (1.1.5). 
Proof The existence of.p∗ follows from Theorem 1.1, and the formula (1.1.11) results 
from 
. 
|
|
|pt(t) − p∗
j
|
|
| =
|
|
|
|
|
∑
i∈G
πi,j (t) pi (0) − p∗
j
|
|
|
|
|
=
|
|
|
|
∑
N
i=1
[
πi,j (t) − p∗
j
]
pi (0)
|
|
|
|
≤
∑
N
i=1
|
|
|
πi,j (t) − p∗
j
|
|
| pi (0) ≤ (1 − δ)
t ∑
N
i=1
pi (0) = (1 − δ)
t .
which proves the corollary. □
Corollary 1.2 Since.π˜ j0|i (t) = (
Πt
)T |i j0 , to verify the property (1.1.4) it is sufficient 
to multiply .Π by itself . t times up to the moment when all elements of at least one 
row will be positive. 
Definition 1.3 For a homogeneous finite Markov chain with transition matrix. Π = [
πj|i
]
i,j=1,...,N the parameter.kerg(t0) defined by 
. kerg(t0) := 1 − 1
2
max
i,j=1,...,N
∑
N
m=1
|
|(π˜im(t0)) − (
π˜ jm(t0)
)|
| ∈ [0, 1). (1.1.12) 
is said to be coefficient of ergodicity of this Markov chain at time. t0 , where 
. (π˜im(t0)) = P {s(t0) = sm|s(1) = si},
is the probability to evolve from the initial state.s1 = si to the state.st0 = sm after. t0
transitions.1.2 Controllable Markov Chain 7
Lemma 1.1 The coefficient of ergodicity.kerg(t0) can be estimated from below as 
. kerg(t0) ≥ min
i=1,...,N
max
j=1,...,N
π˜ j|i (t0).
Remark 1.2 If all the elements.π˜ j|i (t0) of the transition matrix.Πt0 are positive, then 
the coefficient of ergodicity .kerg(t0) is also positive. Notice that there exist ergodic 
Markov chains with elements .π˜ j|i (t0) equal to zero, but with a positive coefficient 
of ergodicity .kerg(t0). So, the coefficient of ergodicity is strictly positive, then the 
corresponding Markov’s chain is said to be ergodic. 
1.1.3 Transition Equation and Invariant State Distribution 
The state probabilities.pj(t) (1.1.10) satisfies the following transition equation 
. pj(t + 1) = ∑
N
i=1
πi j(t)pi(t),
which in matrix form is 
. p(t + 1) = ΠT(t)p(t), (1.1.13) 
where 
. p(t) = (p1(t), . . . , pN (t))T.
For ergodic homogeneous (stationary) Markov chain the final distribution . p∗ =
lim
t→∞.p(t) (1.1.5) and satisfies the relation 
. p∗ = ΠT p∗. (1.1.14) 
Remark 1.3 For ergodic homogeneous (stationary) Markov chain the final distri￾bution.p∗ is unique. 
Remark 1.4 Theorem 1.1 ensures that.∏∗ has a unique everywhere positive invari￾ant distribution .p∗ and, it is equivalent to the existence of some . t0, such that 
.π∗
i j (t0) > 0. 
1.2 Controllable Markov Chain 
1.2.1 Control Policy 
Let .A be a finite set consisting of actions .{a1,..., aM }, .M ∈ N, called the action 
space. A trajectory of a Markov chain is a sequence.s(0), a(0),s(1), a(1), . . . .8 1 Controllable Markov Chains
Definition 1.4 • A.policy.{φ(t)}t∈N is a sequence of finite collections 
. φ(t) = (s(0), a(0), . . . ,s(t − 1), a(t − 1),s(t)). (1.2.1) 
• A policy .{d(t)}t∈N is referred to as a randomized control policy if the matrix 
.d(t) := [dk|i(t)] ∈ RM×N has elements 
. dk|i(t) := P(a(t) = ak |s(t) = si), (1.2.2) 
which means the probability to apply the action .a(t) = ak in the state .s(t) = si . 
Evidently, the elements.dk|i(t) satisfy for.t ∈ N the condition 
. d(t) ∈ D =
{
d(t) ∈ RM×N
|
|
|
|
|
dk|i(t) ≥ 0,
∑
M
k=1
dk|i(t) = 1
}
. (1.2.3) 
• If.d(t) = d = Const, then.d ∈ RM×N is said to be a stationary randomize control 
policy . 
If each row of matrix. d contains only one element equal to 1 and others elements 
equal to 0, then such policy becomes to be a non-randomized policy coinciding with 
(1.2.1). 
1.2.2 Main Definition 
Definition 1.5 A controllable Markov chain [ 1– 4, 7, 8, 10] is a four-tuple 
.MC = (S, A,{d(t)}t∈N ,{∏(t)}t∈N), (1.2.4) 
where. S is a finite set of states,.S ⊂ N,. A is a finite set of actions,.{d(t)}t∈N is a ran￾domize control policy, and.∏(t) = [
πj|ik (t)
]
i,j=1,N,k=1,M is a controlled transition 
matrix (3 dimensional) where 
. πj|ik (t) := P(s(t + 1) = sj|s(t) = si, a(t) = ak ), (1.2.5) 
.∀t ∈ N represents the probability associated with the transition from state. si to state 
.sj .
(
i, j = 1, N
)
under the action.ak .
(
k = 1, K)
. 
Obviously, for all. i, j = 1, N, k = 1, M
. πj|ik (t) ≥ 0,
∑
N
j=1
πj|ik (t) = 1. (1.2.6)1.3 Cost Functions for Markov Chains 9
In view of the definitions above, we have 
. πj|i(t|d(t)) = ∑
M
k=1
πj|ik (t)dk|i(t). (1.2.7) 
The transition equation (1.1.13) in this case becomes 
. p(t + 1) = ΠT(t|d(t))p(t), (1.2.8) 
where 
.ΠT(t|d(t)) = [πj|i(t|d(t))]. (1.2.9) 
1.3 Cost Functions for Markov Chains 
1.3.1 Average Cost Function 
Let us define.vijk as the cost for changing from state. i to state. j applying action. k. 
Definition 1.6 Let us define .J (d) the average cost function for homogeneous 
Markov chain with transition matrix .[πj|ik ] and final distribution .pi(d) under sta￾tionary randomize control policy.d = [dk|i] as follows 
. J (d) = ∑
N
i=1
∑
N
j=1
∑
M
k=1
vijkπj|ikdk|i pi(d), (1.3.1) 
where.pi(d) satisfies for all. i = 1, N
. pi(d) = ∑
N
l=1
∑
M
k=1
πi|lkdk|l pl(d). (1.3.2) 
Remark 1.5 As it follows from (1.3.1) the average cost function .J (d) is an 
extremely nonlinear function of the elements.dk|i of matrix. d. 
1.3.2 Auxiliary c-Variables 
Let us denote by.c ∈ RN×M the matrix with elements.cik defined as 
. cik = dk|i pi(d). (1.3.3)10 1 Controllable Markov Chains
The average cost function in terms of c-variable is 
. J (c) = ∑
N
i=1
∑
M
k=1
∑
N
j=1
vijkπj|ik cik = ∑
N
i=1
∑
M
k=1
wik cik , (1.3.4) 
where 
.wik = ∑
N
j=1
vijkπj|ik . (1.3.5) 
Remark 1.6 The average cost function.J (c) is now a linear function of. c. 
Notice that in view of (1.2.3) the variables .pi and .dk|i can be recovered by the 
following relations 
. pi (d) = ∑
M
k=1
cik and dik = cik/
∑
M
l=1
cil . (1.3.6) 
In the ergodic case.
∑
M
k=1
cik > 0 and 
. c ∈ Cadm=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∑
N
i=1
∑
M
k=1
cik = 1, cik > 0,
∑
M
k=1
c jk = ∑
N
i=1
∑
M
k=1
πj|ik cik .
(1.3.7) 
1.4 Markov Decision Process as Linear a Programming 
Problem 
1.4.1 Lineal Programming Formulation 
Definition 1.7 A Markov Decision Process (MDP) for homogeneous Markov chain 
models with stationary randomize control policy. d (1.2.3) is a pair 
. MDP = (MC, J ),
where.MC is a controllable Markov chain (1.2.4) and. J is a cost function (1.3.4).1.4 Markov Decision Process as Linear a Programming Problem 11
Definition 1.8 We say that a MDP is optimal if it is a solution of the following 
optimization problem 
. J (c) → min
c∈Cadm
, (1.4.1) 
under possible additional constraints 
.
Φeq,l(c) = ∑
N
i=1
∑
N
j=1
∑
M
k=1
v˜
eq
ijk,lπj|ikdk|i pi(d) =
∑
N
i=1
∑
M
k=1
w˜ eq
ik,lcik = beq,l,l = 1, Leq ,
Φineq,m(c) = ∑
N
i=1
∑
N
j=1
∑
M
k=1
v˜
ineq
ijk,mπj|ikdk|i pi(d) =
∑
N
i=1
∑
M
k=1
w˜
ineq
ik,mcik ≤ bineq,m, m = 1, Lineq .
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(1.4.2) 
The optimization problem given by (1.4.1) and (1.4.2) may be represented in a 
Lineal Programming (LP) format 
.minx f Tx subject to
⎧
⎨
⎩
Aeq x = beq
Aineq x ≤ bineq
lb ≤ x ≤ ub,
(1.4.3) 
where 
. x = col[cik ] := (c11, c12 ..., c1M ; c21, c22 ..., c2M ;... cN1, cN2 ..., cN M )
T ,
(1.4.4) 
. f = col[wik ] := (w11, w12 ...,w1M ; w21, w22 ...,w2M ;...wN1, wN2 ...,wN M )T ,
(1.4.5) 
.Aeq =
⎡
⎢
⎣
B L(1)
eq
B L(2)
eq
B L(3)
eq
⎤
⎥
⎦ , (1.4.6) 
. B L(1)
eq = [
δ(j,i) − πj|ik ]
j=1,Ni=1,N =
⎡
⎣
1 − π(1,11) ... −π(N,1,M)
... ... ...
−π(N|11) ... 1 − π(N|N,M)
⎤
⎦ ∈ RN×(N M)
,
(1.4.7) 
.B L(2)
eq = [
1 ... 1
]
∈ R1×(N M)
, (1.4.8)12 1 Controllable Markov Chains
. B L(3)
eq =
⎡
⎣
w˜
eq
11,1, w˜
eq
12,1 ..., w˜
eq
1M,1; ... w˜
eq
N1,1, w˜
eq
N2,1 ..., w˜
eq
N M,1
... ... ...
w˜
eq
11,Leq , w˜
eq
12,Leq ..., w˜
eq
1M,Leq ; ... w˜
eq
N1,Leq , w˜
eq
N2,Leq ..., w˜
eq
N M,Leq
⎤
⎦ ∈ RLeq×(N M)
,
(1.4.9) 
. 
Aineq =
⎡
⎢
⎣
w˜
ineq
11,1 , w˜
ineq
12,1 ..., w˜
ineq
1M,1; ... w˜
ineq
N1,1, w˜
ineq
N2,1 ..., w˜
ineq
N M,1
... ... ...
w˜
ineq
11,Lineq , w˜
ineq
12,Lineq ..., w˜
ineq
1M,Lineq ; ... w˜
ineq
N1,Lineq , w˜
ineq
N2,Lineq ..., w˜
ineq
N M,Lineq
⎤
⎥
⎦ ∈ RLineq×(N M)
,
(1.4.10) 
.beq = [
0,... 0; 1; beq,N+2 ... beq,N+Leq ]T ∈ RN+1+Leq , (1.4.11) 
.bineq ∈ RLineq . (1.4.12) 
The matrix block .B L(1)
eq (1.4.7) corresponds to the constraints defined in (1.3.7) 
rewritten as 
. ∑
M
k=1
∑
N
i=1
(
c jk − πj|ik cik )
= 0.
The matrix block.B L(2)
eq (1.4.8) corresponds to the relation defined in (1.3.7) 
. ∑
N
i=1
∑
M
k=1
cik = 1.
The lower bound and the upper bound for c-variables are given by 
.lb = 0 ≤ cik ≤ ub = 1. (1.4.13) 
Remark 1.7 All the constraint in LP problem (1.4.3) are consistent, that is, the set 
of arguments satisfying this constraints is not empty. 
1.4.2 Realization of Stationary Strategies in the State Space 
To realize a random process .s(t) that corresponds, for example, to the transition 
matrix 
.πj|i =
⎡
⎣
0 0.6 0.4
10 0
0.5 0.25 0.25
⎤
⎦1.4 Markov Decision Process as Linear a Programming Problem 13
0.55 Probability vector 
0.5 
0.45 
0.4 
0.35 
0.3 
0.25 
0.2 
0 5 10 15 20 25 30 35 40 45 50 
Fig. 1.2 Probability vector 
with initial state distribution 
. p = [
0.3333 0.3333 0.3333]
and recurrent equation given by (1.1.13) 
. p(t + 1) = ΠT(t)p(t),
we need to compute 
. p∗ = lim
t→∞p(t) = [0.4412; 0.3235; 0.2353].
The convergence of vector.p(t) is given in Fig. 1.2 At time.t = 1,... to generate the 
state vector .s(t) we use the generator of uniform distribution in the interval .[0, 1]. 
Then, the output of the generator is projected in the interval.[0, 1] with subintervals 
proportional to the components of the .p(t) (see Fig. 1.3). The state . si is associated 
with the number of the subinterval. 
The corresponding Markov chain state process s(t) is presented in Fig. 1.4.14 1 Controllable Markov Chains
1/3 
1 
Uniform distributon 
0 
p1(t) p2(t) p3(t) 
Fig. 1.3 State generator 
3.5 States s(t) 
3 
2.5 
2 
1.5 
1 
0.5 
0 
0 5 10 15 20 25 30 35 
t 
40 45 50 
Fig. 1.4 States of the Markov chainReferences 15
1.4.3 Numerical Example 
Consider an environment where total number of states is.N = 3 and the total number 
of actions is.M = 2 for which the transition matrices are given by 
. πj|i1 =
⎡
⎣
0.7094 0.4984 0.0060
0.7547 0.1597 0.6991
0.2760 0.3404 0.0909
⎤
⎦ , πj|i2 =
⎡
⎣
0.4694 0.1656 0.0838
0.4119 0.6020 0.2290
0.3371 0.2630 0.9133
⎤
⎦ ,
and the cost matrices are given by 
. vi j1 =
⎡
⎣
2.9411 7.8237 9.9358
2.9411 7.8237 9.9358
2.9411 7.8237 9.9358
⎤
⎦ , vi j2 =
⎡
⎣
4.3204 6.5513 8.5479
4.3204 6.5513 8.5479
4.3204 6.5513 8.5479
⎤
⎦ ,
We get for optimal policy 
. p∗
i =
⎡
⎣
0.4514
0.4508
0.0979
⎤
⎦ , c∗
ik =
⎡
⎣
0.4514 0.0000
0.0000 0.4508
0.0979 0.0000
⎤
⎦ , d∗
k|i =
⎡
⎣
1.0000 0.0000
0.0000 1.0000
1.0000 0.0000
⎤
⎦ ,
and.J (c) = 5.6395. 
References 
1. Altman, E.: Constrained Markov Decision Processes: Stochastic Modeling. Routledge (1999) 
2. Clempner, J.B.: A lyapunov approach for stable reinforcement learning. Comput. Appl. Math. 
41, 279 (2022) 
3. Clempner, J.B., Poznyak, A.S.: Analysis of best-reply strategies in repeated finite markov chains 
games. In: 52nd IEEE Conference on Decision and Control (CDC), pp. 568–573. Firenze, Italy 
(2013) 
4. Clempner, J.B., Poznyak, A.S.: Simple computing of the customer lifetime value: a fixed local￾optimal policy approach. J. Syst. Sci. Syst. Eng. 23(4), 439–459 (2014) 
5. Howard, R.A.: Dynamic Programming and Markov Processes (1960) 
6. Kemeny, J.G., Snell, J.L.: Finite Markov Chains: With a New Appendix “Generalization of a 
Fundamental Matrix”. Springer (1983) 
7. Poznyak, A.S., Najim, K., Gomez-Ramirez, E.: Self-learning Control of Finite Markov Chains. 
Marcel Dekker, New York (2000) 
8. Puterman, M.L.: Markov decision processes. Handbooks in Operations Research and Manage￾ment Science, vol. 2, pp. 331–434 (1990) 
9. Rozanov, Y.: Probability Theory: A Concise Course (1977) 
10. Sragovich, V.G.: Mathematical Theory of Adaptive Control, vol. 4. World Scientific (2005)Chapter 2 
Multiobjective Control 
Abstract A multi-objective Pareto front solution is presented in this chapter for 
a particular type of discrete-time ergodic controllable Markov chains. We offer a 
technique that, given specific boundaries, chooses the best multi-objective option for 
the Pareto frontier as a decision support system. We only consider a class of finite, 
ergodic, and controllable Markov chains while addressing this issue. The regularized 
penalty method utilizes a projection-gradient strategy to identify the Pareto policies 
along the Pareto frontier and is based on Tikhonov’s regularization method. The goal 
is to make the parameters as efficient as possible while still maintaining the original 
form of the functional. After setting the initial value, we gradually reduce it until 
each policy closely resembles the Pareto policy. In this sense, we specify the precise 
direction of the parameter tendencies toward zero and establish the convergence of the 
gradient regularized penalty algorithm. The matching picture in the objective space 
receives a Pareto frontier of only Pareto policies thanks to our policy-gradient multi￾objective algorithms, which, on the other hand, use a gradient-based strategy. In order 
to improve security when transporting cash and valuables, we empirically validate the 
technique by providing a numerical example of a genuine alternative solution to the 
vehicle routing planning problem. In addition, we describe a portfolio optimization 
and represent the Pareto frontier. The decision-making techniques investigated in 
this paper are consistent with the most widely used computational intelligent models 
in the Artificial Intelligence research field. 
2.1 Introduction 
2.1.1 Pareto Set 
In practical areas arises the problem that several objective functions (outcomes) have 
to be optimized concurrently. For example, companies may take different approaches 
to maximize profit, minimize loss, minimize cost and maximize quality, etc. This 
example presents a dilemma: different objectives contradict each other and therefore 
do not have the same optima. The problem that comes up is how to compute all the 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_2 
1718 2 Multiobjective Control
optimal compromises of this Multi-objective Optimization Problem (MOP) [ 4, 6, 
14, 19, 20]. 
We will consider controllable homogeneous Markov chains under stationary pol￾icy. In a MOP there are given . n objective functions .J 1,..., J n which have to be 
minimized: 
.J (d) = min
d∈D
(J 1
(d), . . . , J n(d)) (2.1.1) 
over the class of all admissible policies (strategies) .d ∈ D (see Chap. 1). There is 
no single solution for a nontrivial multi-objective optimization problem that concur￾rently optimizes all of the objectives. 
Definition 2.1 The goal functions are said to be at conflict in that situation. If none 
of the objective functions can have their value decreased without increasing some of 
the other objective values, the solution is said to be nondominated, Pareto optimum, 
Pareto efficient, or noninferior. There may be a (potentially infinite) number of Pareto 
optimum solutions, all of which are regarded as equally acceptable, without extra 
subjective preference information. If.d∗ minimizes.J (d) in the sense of Pareto, then 
.d∗ is said to be a Pareto policy. 
A two dimensional case is represented in Fig. 2.1. If the internal part of the cone 
.K with vertice .P those not contains any points from set.Ω of all possible functions 
value, then this vertice is a Pareto optimum point. For instance, it is.K2 with vertice 
.P2. If it is not the case (.K1 with vertice .P1), then this vertice point is not Pareto 
optimal. 
In multi-objective optimization (2.1.1), there does not typically exist a feasible 
solution that minimizes all objective functions simultaneously. Therefore, attention 
is paid to Pareto optimal solutions, that is, solutions that cannot be improved in any of 
the objectives without degrading at least one of the other objectives. In mathematical 
terms, a feasible solution.d1 ∈ D is said to (Pareto) dominate another solution.d2 ∈ D, 
if 
.
∀i ∈ {1,..., n}, J i
(d1) ≤ J i
(d2), and
∃i ∈ {1,..., n}, J i
(d1) < J i
(d2). (2.1.2) 
A local Pareto optimal policy is a policy such that no improvement in all the 
objectives can be achieved by moving to a neighboring feasible point. The MOP 
is different from single-objective nonlinear programming because the set of Pareto 
optimal points is usually a continuum that may have disjoint components. 
Definition 2.2 A solution.d∗ ∈ D (and the corresponding outcome.J (d∗)) is called 
Pareto optimal if there does not exist another solution that dominates it. The set 
of Pareto optimal outcomes, denoted .D∗, is often called the Pareto front, Pareto 
frontier, or Pareto boundary. 
In this chapter we study discrete-time multi-objective Markov chains problem. 
– Presents a formulation of the problem in terms of a nonlinear programming prob￾lem implementing the Lagrange principle.2.1 Introduction 19
Fig. 2.1 Pareto front J2 
Pareto front 
Ω
p1 
K1 p2 
K2
The set of all posible 
functons' value 
J1 
– Highlights a fundamental feature of the Pareto set where the search space is in 
most of the cases nonconvex (strict convex) and a complicated set because there 
is not a unique solution. For solving the existence and characterization of strong 
Pareto policies we employ the Tikhonov’s regularization method [ 30, 31]. 
– Formulates the original problem considering several constraints: 
(a) we employ the.c-variable method for the introduction of linear constraints over 
the nonlinear problem and, 
(b) we restrict the cost-functions allowing points in the Pareto front to have a small 
distance from one another. 
– Solves the optimization problem using the projected gradient method. 
– Suggests the convergence conditions and compute the estimate rate of conver￾gence of variables . θ and . δ corresponding to the Lagrange principle and the 
Tikhonov’s regularization respectively. We describe the dependence for the reg￾ularized Lagrange function on the regularizing parameters . θ and . δ, and analyses 
the asymptotic behavior when.θn ↓ 0 and the fact that.
θn
δn
↓ 0 also holds. 
– Provides the details needed to implement the proposed method in an efficient way. 
2.1.2 Parametrization of All Pareto Points 
A different important problem arises when the individual cost-functions 
.J 1(d), . . . , J n(d) are ranked in a hierarchical order: an optimal policy became a 
particular Pareto policy. 
One of the fundamental problems are the existence and characterization of Pareto 
policies. This can be obtained via the usual linear scalarization 1 approach, in which
1 By far most of the methods for the computation of single Pareto points or the entire Pareto set are 
based on a “scalarization” of the MOP (see e.g. [ 9, 10, 27, 29]). 20 2 Multiobjective Control
Fig. 2.2 Simplex 
the multi-objective Markov chain is reduced to a single-objective with a “ weighted” 
objective function of the form 
. Φ(λ, d) := λ J (d) = λ1 J 1
(d) +···+ λn J n(d), (2.1.3) 
where the vector. λ is from.n-dimensional simplex, that is, 
. λ ∈ Sn = {λi ≥ 0|
∑n
i=1
λi = 1} (2.1.4) 
(see Fig. 2.2 for.n = 3). Scalarizing a multi-objective optimization problem (2.1.1) is 
an a priori method, which means formulating a single-objective optimization problem 
such that optimal solutions to the single-objective optimization problem are Pareto 
optimal solutions to the multi-objective optimization problem. 
Theorem 2.1 ([ 2, 14, 19]) For any weights combination.λ ∈ Sn the point 
.d∗(λ) ∈ Arg min
d∈D
Φ(λ, d) (2.1.5) 
is a Pareto optimal, i.e.,.d∗(λ) ∈ D∗. 
Remark 2.1 If in the optimization problem (2.1.5) there exists a unique solution 
then 
.d∗(λ) = arg min
d∈D
Φ(λ, d) ∈ D∗. (2.1.6)2.1 Introduction 21
Fig. 2.3 Utopia point 
J1 
J2 
Pareto front 
Ω
min J2 
min J1 
Utopía point 
2.1.3 Utopia Point 
If we let 
. J i∗ = inf
d
J i
(d)
and define the utopia minimum as.J ∗(d) = (J 1∗(d), . . . , J n∗(d)) (infeasible in gen￾eral) the resulting problem is to find the Pareto policies.d∗ whose cost vector. J (d∗)
is the “closest” to.J ∗(d) in the usual Euclidean norm (see Fig. 2.3). 
Rigorously, this problem can be formulated as the following optimization problem 
. Φ(λ) := ∑n
i=1
[
J i
(d∗(λ)) − J i∗]2 → min
λ∈Sn
, (2.1.7) 
where.d∗(λ)is defined by (2.1.6). The solution of the problem (2.1.7) can be obtained 
by applying the numerical Kiefer–Wolfowitz procedure [ 15] for the case when there 
is no noise in function observations: 
.λ(k) = PSn
⎧
⎨
⎩
λ(k−1) − γ
α
∑n
j=1
[
Φ(λ(k−1) + αe j) − Φ(λ(k−1)
)
]
⎫
⎬
⎭ , (2.1.8) 
where,.γ, α are small positive parameters,.e j = [0,..., 0, 1 j
, 0,..., 0]
 are test vec￾tors and.k = 1, 2,... . In (2.1.8) the operator.PSn is the projector to the simplex.Sn, 
which is a finite step procedure realized by the following procedure [ 24]:22 2 Multiobjective Control
Algorithm 2.1 Projector 
f unction[λ] = Pr ojector(λ)
n ← length(λ); 
e ← 0.0001; 
R ← e ∗ n; 
AR ← 1 − R; 
AK ← n; 
SQ1 ← 1 − sum(λ); 
Mon ← zer os(1, n); 
Mon ← zer os(1, n); 
for i = 1 : n do 
λ(i ) ← (λ(i ) − e)/AR; 
end for 
SQ1 ← SQ1/AR; 
λ ← Calcλ(n, λ, AK, SQ1, Mon); 
for i = 1 : n do 
λ(i) ← e + AR ∗ λ(i ); 
end for 
end 
Algorithm 2.2 Calc 
f unction[λ] = Calcλ(n, λ, AK , SQ1, Mon)
λ = CalcN omali zationλ(n, λ, AK , SQ1, Mon); 
for i = 1 : n do 
if Mon(i) == 0&&λ(i ) < 0.0000 then 
SQ1 ← λ(i ); 
λ(i ) ← 0; 
Mon(i ) ← 1; 
AK ← AK − 1; 
λ ← Calcλ(n, λ, AK , SQ1, Mon); 
end if 
end for 
end 
Algorithm 2.3 CalcNomalization 
f unction[λ] = CalcN omali zationλ(n, λ, AK , SQ1, Mon)
for i = 1 : n do 
if Mon(i ) == 0 then 
λ(i ) = λ(i) + SQ1/AK ; 
end if 
end for 
end2.2 Regularized Penalty Function Optimization Method (RPFOM) 23
2.2 Regularized Penalty Function Optimization Method 
(RPFOM) 
2.2.1 Poly-Linear Optimization Problem Formulation 
Consider the following poly-linear programming problem 
.
f (x) = α1
∑
N
j1=1
c j1 x j1 + α2
∑
N
j1=1
∑
N
j2=1
c j1,j2 x j1 x j2+
α3
∑
N
j1=1
∑
N
j2=1
∑
N
j3=1
c j1,j2,j3 x j1 x j2 x j3 +···+
αN−1
∑
N
j1=1
∑
N
j2=1
··· ∑
N
jN−1=1
c j1,···,jN−1 x j1 ··· x jN−1+
αN
∑
N
j1=1
∑
N
j2=1
··· ∑
N
jN=1
c j1,···,jN x j1 ··· x jN → min
x∈Xadm
(2.2.1) 
. 
αj = {0; 1}(j = 1,..., N) are binary variables
Xadm := {
x ∈ RN : x ≥ 0, V0x = b0 ∈ RM0 , V1x ≤ b1 ∈ RM1
}
is a bounded set.
Introducing the “slack” vectors.u ∈ RM1 with nonnegative components, that is,. u j ≥
0 for all. j = 1,..., M1, the original problem (2.2.1) can be rewritten as 
.
min x∈Xadm ,u≥0
f (x)
subject to
Xadm := {
x ∈ RN : x ≥ 0, V0x = b0, V1x − b1 + u = 0}.
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(2.2.2) 
Note that this problem may have non-unique solution and.det (
V 
0 V0
)
= 0. Define 
by.X∗ ⊆ Xadm the set of all solutions of the problem (2.2.2). 
2.2.2 Penalty Functions Approach 
Following [ 11, 35] consider the penalty function [ 7, 8] 
.
F˜
k,δ (x, u):= f (x)+k
[ 1
2 |V0x − b0|2 +
1
2 |V1x − b1+u|2 +δ1
2
(
|x|2 + |u|2)]
(2.2.3)24 2 Multiobjective Control
where the parameters . k and .δ1 are positive. Obviously, the unconstraint on . x the 
optimization problem 
. min x∈Xadm ,u≥0
F˜
k,δ (x, u) (2.2.4) 
has a unique solution since the optimized function (2.2.3) is strongly convex if.δ > 0. 
Note also that 
. arg min x∈Xadm ,u≥0
F˜
k,δ (x, u) = arg min x∈Xadm ,u≥0
Fμ,δ (x, u),
where.μ := k−1 > 0 and 
.
Fμ,δ (x, u) := μ f (x) + 1
2 |V0x − b0|2 +
1
2 |V1x − b1 + u|2 + δ
2
(
|x|2 + |u|2)
.
(2.2.5) 
2.2.3 Expected Property of RPFA 
Proposition 2.1 If the penalty parameter .μ and . δ tends to zero by a particular 
manner, then we may expect that .x∗ (μ, δ) and .u∗ (μ, δ), which are the solutions of 
the optimization problem 
. min x∈Xadm ,u≥0
Fμ,δ (x, u)
tend to the set .X∗ of all solutions of the original optimization problem (2.2.2), that 
is, 
.ρ
{
x∗ (μ, δ), u∗ (μ, δ); X∗} →μ↓0
0 (2.2.6) 
where.ρ {a; X∗} is the Hausdorff distance defined as 
. ρ
{
a; X∗}
= min
x∗∈X∗
|
|a − x∗
|
|
2
.
Below we define exactly how the parameters. μ and. δ should tend to zero to provide 
the property (2.2.6). 
2.2.4 The Main Result on the Extremal Points of the Penalty 
Functions 
Theorem 2.2 Let us assume that 
(1) the bounded set.X∗ of all solutions of the original optimization problem (2.2.2) is 
not empty and the Slater’s condition holds, that is, there exists a point. x˚ ∈ Xadm
such that2.2 Regularized Penalty Function Optimization Method (RPFOM) 25
.V1x˚ < b1. (2.2.7) 
(2) The parameters. μ and. δ are time-varying, i.e., 
. μ = μn, δ = δn (n = 0, 1, 2,....)
such that 
.0 < μn ↓ 0,
μn
δn
↓ 0 when n → ∞. (2.2.8) 
Then 
.
x∗
n := x∗ (μn, δn) →n→∞ x∗∗
u∗
n := u∗ (μn, δn) →n→∞ u∗∗ 
(2.2.9) 
where .x∗∗ ∈ X∗ is the solution of the original problem (2.2.2) with the minimal 
weighted norm which is unique, i.e., 
.
|
|x∗∗|
| ≤ |
|x∗
|
| for all x∗ ∈ X∗} (2.2.10) 
and 
.u∗∗ = b1 − V1x∗∗. (2.2.11) 
Proof See Appendix 2.4. ◻
We also need the following lemma. 
Lemma 2.1 Under the assumptions of the Theorem above there exist positive con￾stants.Cμ and.Cδ such that 
.
|
|x∗
n − x∗
m
|
| + |
|u∗
n − u∗
m
|
| ≤ Cμ |μn − μm| + Cδ |δn − δm| . (2.2.12) 
Proof See Appendix 2.4. ◻
2.2.5 Recurrent Algorithm 
General case 
Consider the following recurrent procedure for finding the extremal point .z∗∗ . =
.
 x∗∗
u∗∗ 
:26 2 Multiobjective Control
.
zn =
 
zn−1 − γn
∂
∂z
Pμn ,δn (zn−1)
 
+
, z := x
u
 
,
∂
∂z
Pμn ,δn (zn−1) =
⎛
⎜
⎝
∂
∂x
Pμn ,δn (xn−1, un−1)
∂
∂u
Pμn ,δn (xn−1, un−1)
⎞
⎟
⎠ =
⎛
⎜
⎜
⎜
⎝
μn
∂
∂x f (xn−1) + V 
0
[
V0xn−1 − b0
]
+
V 
1
[
V1xn−1 − b1 + un−1
]
+ δn xn−1
V1xn−1 − b1 + (1 + δn) un−1
⎞
⎟
⎟
⎟
⎠ .
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(2.2.13) 
Theorem 2.3 (on the convergence of the projection gradient method) If 
.
∑∞
n=0
γnδn = ∞,
γn
δn
→n→∞ 0,
|μn − μn−1| + |δn − δn−1|
γnδn
→n→∞ 0,
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(2.2.14) 
then 
.Wn := |
|zn − z∗
n
|
|
2 →n→∞ 0. (2.2.15) 
Proof See the Appendix 2.4. ◻
2.2.6 Special Selection of the Parameters 
Let us select the parameters of the algorithm (2.2.13) as follows: 
.
δn =
 δ0 if n ≤ n0
δ0
[1+ln(n−n0)]
(1+n−n0)
δ if n > n0 , μn =
 μ0 if n < n0
μ0
(1+n−n0)
μ if n ≥ n0 ,
γn =
 γ0 if n < n0
γ0
(1+n−n0)
γ if n ≥ n0 , δ, μ, γ > 0, δ0, μ0, γ0 > 0.
(2.2.16) 
To guarantee the convergence of the suggested procedure, by the property. 
μn
δn
→n→∞ 0
and by the conditions (2.2.14) we should have 
.δ ≤ μ, γ ≥ δ, γ + δ ≤ 1. (2.2.17)2.2 Regularized Penalty Function Optimization Method (RPFOM) 27
2.2.7 Numerical Example: Pareto Front Calculation 
This example presents a real alternative solution of the vehicle routing planning 
problem to increase security in transportation of cash and valuables (see, [ 1, 3, 5] 
for multi-objective Markov decision in urban modeling). Companies in the arena are 
focused in the physical transfer of cash, jewels, coins, etc. As a result of the type of 
the transported goods the security vehicles are frequently exposed to attacks along 
their routes. Crime is a significant challenge. In addition, the risk rates and the losses 
are different from sector to sector. A conflict arises because higher risk exposures 
allow a reduction of the travel cost. We suggest a bi-objective formulation using the 
proposed method with the goal of reducing both the risk and the travel cost. The 
problem is weighted according to which a maximum amount of valuables can be 
transported the security vehicle. 
It is important to note that each customer must be assigned to exactly one of the 
routes and the vehicle capacity must not be exceeded. Assuming that a vehicle picks 
up cash and valuables at certain places. i visited along route, a risk index. ϕ for each 
criminal. l can be defined as follows 
. ∑
k
ϕ−l
ik c−l∗
ik (λ) = Φ−l
i ,
where.ϕ−l
ik is the risk of criminal.−l to attack place. i and action. k along a given route 
and, .Φ−l
i is the maximum risk able to undertaken by a criminal .−l to attack place 
. i. The risk method assesses the probabilities for the actions of the attackers and it is 
defined as follows 
.
arg min
λ∈∆N
Jl
(cl∗(λ))
Φ−l
prev ≤ J −l
(c−l∗(λ)) and
J −l
(c−l∗(λ)) ≤ Φ−l for − l = {1, q}\{l}.
(2.2.18) 
It is interesting to remark that in our case we used. i and. j as the sequence between 
two consecutive places (states) that measure the probability of an attack on a specific 
roadway segment. In terms of Markov chains we observe the current state .i ∈ S. 
Then, by optimizing the risk using Eq. (2.2.18) is selected an optimal action. a(n) =
a(k) ∈ A(s). Then two things happen: a cost.Jijk is incurred and, the system at time 
.n + 1 moves to a new state. j ∈ S with probability.πj|ik . 
Let us consider.N = 6 and.M = 2. Then, fixing.δ = 0.1 and.μ = 0.001 the Pareto 
front for 1000 points is shown in Fig. 2.4 which represents the routing plans that 
should both be safe and efficient. The Pareto front allows the decision-maker to deal 
with two critical problems: 
(a) the minimization of the traveled cost-time of a security vehicle and, 
(b) the reduction of the expected exposure of the transported goods to robberies.28 2 Multiobjective Control
Fig. 2.4 Regularized Pareto 
front: security routing 
against vehicle attack 
This is not a simple assignment. It involves the conflict between objectives and the 
difficulty of selecting the routes which implicate to visit and to collect valuables 
along several places every day. The corresponding value of the vector. λ and the joint 
strategy.c
(l)
ik for.l = 1, 2 is as follows: 
Route 1 (color cyan) 
. λ1 = 0.8120, λ2 = 0.1880,
. c
(1)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1164 0.1059
0.1115 0.0904
0.1356 0.0122
0.1487 0.0070
0.0711 0.1039
0.0957 0.0000
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, c
(2)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1901 0.0198
0.0756 0.1177
0.1215 0.0005
0.0985 0.0020
0.0582 0.0659
0.2046 0.0437
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Route 2 (color blue) 
. λ1 = 0.5770, λ2 = 0.4230
. c
(1)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1305 0.0949
0.1221 0.0697
0.1176 0.0308
0.1469 0.0125
0.0677 0.1082
0.0969 0.0000
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, c
(2)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1888 0.0264
0.1036 0.0950
0.1168 0.0028
0.0982 0.0055
0.0631 0.0660
0.1766 0.0551
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Route 3 (color red) 
. λ1 = 0.3580, λ2 = 0.6420,2.3 Portfolio Optimization Problem 29
. c
(1)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1220 0.0986
0.1138 0.0686
0.0993 0.0527
0.1138 0.0463
0.0451 0.1328
0.1006 0.0033
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, c
(2)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1673 0.0579
0.1170 0.0819
0.0957 0.0237
0.0904 0.0268
0.0603 0.0730
0.1209 0.0826
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Route 4 (color green) 
. λ1 = 0.1650, λ2 = 0.8350,
. c
(1)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1167 0.0937
0.1074 0.0572
0.0852 0.0588
0.0814 0.0635
0.0057 0.1660
0.0389 0.1212
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, c
(2)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1326 0.1028
0.1181 0.0603
0.0637 0.0605
0.0757 0.0473
0.0016 0.1480
0.0667 0.1187
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Route 5 (color magenta) 
. λ1 = 0.0520 λ2 = 0.9480
. c
(1)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1147 0.0895
0.1079 0.0454
0.0789 0.0615
0.0643 0.0755
0.0021 0.1677
0.0030 0.1847
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
c
(2)
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
, 0.1279 0.1149
0.1351 0.0467
0.0385 0.0816
0.0691 0.0569
0.0000 0.1563
0.0308 0.1370
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
In Fig. 2.5 we show the security routing bounds for decision making over the 
Pareto front. Bounds correspond to restrictions imposed by the decision maker over 
the Pareto front that establish a specific decision area where the strategies can be 
selected. By computing the minimum distance to the utopian point we have that 
Route 3 (color red) fulfill the requirements. 
2.3 Portfolio Optimization Problem 
2.3.1 Problem Formulation 
The portfolio optimization issue compares the expected (mean) return of a portfolio 
with the standard deviation of the same portfolio using the mean-variance theory [ 12, 
13, 21, 22, 25, 26, 32]. The expected return is plotted on the vertical axis in Fig. 2.6,30 2 Multiobjective Control
Fig. 2.5 Security routing bounds for decision making over the Pareto front 
Fig. 2.6 Efficient Frontier. No risk-free asset is available 
while the standard deviation is plotted on the horizontal axis (volatility). Standard 
deviation serves as a measure of risk and describes volatility [ 16, 18]. 
The space of “expected return vs risk” is another name for the return-standard 
deviation relationship. This risk-expected return space allows for the plotting of any 
conceivable combination of risky assets, and the area in this space defined by the 
collection of all such potential portfolios. In the absence of a risk-free asset, the 
higher portion of the region’s left parabolic border serves as the efficient frontier 
(sometimes called “the Markowitz bullet” ). Combinations near this top edge reflect2.3 Portfolio Optimization Problem 31
portfolios with the lowest risk for a specific level of projected return (including no 
holdings of the risk-free asset). The combination that offers the best feasible expected 
return for a given risk level is equivalently represented by a portfolio that is located 
on the efficient frontier. The capital allocation line is tangent to the top portion of the 
parabolic boundary. 
• The expected return function.U(c) is 
.
U(c) = ∑
N
i=1
∑
M
k=1
∑
N
j=1
uijkπj|ikdk|i pi(d) = ∑
N
i=1
∑
M
k=1
wik cik ,
cik = dk|i pi(d), wik = ∑
N
j=1
uijkπj|ik ,
(2.3.1) 
where.uijk are the elements of the utility tensor, which mean the utility of transfer 
from state (asset of portfolio). i to state. j applying action (one of possible versions 
of scenario assets distributions). k under the applied strategy distribution.cik . The 
variables.pi and.dk|i are defined in Chap. 1. 
• The portfolio return variance.Var(U(c)) is: 
. Var(U(c)) := ∑
N
i=1
∑
M
k=1
[wik − U(c)]
2 cik = ∑
N
i=1
∑
M
k=1
w2
ik cik − U2(c). (2.3.2) 
So, the portfolio optimization problem may be formulated as designing the strate￾gies .cik (defined in Chap. 1), which solve the following two-criteria optimization 
problem: 
.
U(c) → max
c∈Cadm
or [−U(c)] → min
c∈Cadm
,
Var(U(c)) → min
c∈Cadm
.
(2.3.3) 
2.3.2 Markowitz Function 
There are known two possible approaches for the solution of the problem (2.3.3): 
– transformation of the initial problem into a quadratic optimization with linear 
constraints, 
– representation of the initial problem as a Pareto front optimization. 
1. Let us introduce the additional requirement that the expected return would be no 
less than some required label.U0, that is, 
.U(c) → max
c∈Cadm
≥ U0 or [−U(c)] → min
c∈Cadm
≤ [−U0] . (2.3.4)32 2 Multiobjective Control
Then, reformulate the problem (2.3.3) as 
.
Var(U(c)) → min
c∈Cadm
,
[−U(c)] → min
c∈Cadm
≤ [−U0] .
(2.3.5) 
Notice that the problem (2.3.5) is a quadratic optimization problem where 
.Var(U(c)) is a quadratic function and .U(c) as well as .c ∈ Cadm are linear con￾straints, which can be resolved as in [ 17]. 
2. As it follows from Theorem 2.1, the set of all simultaneously not improvable 
policies (Pareto front points) can be represented as 
. c∗(λ) = arg min c∈Cadm
[λ1Var(U(c)) − λ2U(c)] ∈ Cadm (2.3.6) 
With fixed weights .λ1 and .λ2 = 1 − λ1 the problem (2.3.6) is again a quadratic 
optimization problem subject to linear constraints. 
To select desirable weights.λ1, λ2 may be applied the following methods: 
• Projection of the utopia point to Pareto front (2.1.7) and (2.1.8); 
• Lexicographic Goal Programming method [ 33]; 
• Wierzbicki’s achievement scalarization [ 34]; 
• Sen’s Multi-Objective Programming [ 28]; 
• Hypervolume/Chebyshev Scalarization [ 36]. 
2.3.3 Numerical Example: A Rational Investor 
Gaining a specific return is the investor’s principal objective. A rational investor 
makes an effort to locate the portfolio with the lowest risk that meets this objective. 
We provide every potential portfolio for achieving this goal displaying the expected 
returns and risk (variance) of a portfolio’s hazardous assets as a mean-variance dia￾gram, where the points stand for the expected returns (. U) and risk (.Var). Also, we 
refer to the collection of all points that have the shape of a hyperbola (Pareto typeface) 
as the efficient frontier. For more information, see Fig. 2.7. 
If for a given volatility there is no portfolio with a higher return such that. U(c∗) ≤
U(c) and .Var(c∗) ≥ Var(c). In the mean-variance diagram from Fig. 2.7, it is the 
top boundary of every portfolio. According to our paradigm, the rational investor is 
specifically seeking the following set of portfolios: They both increase the expected 
return while minimizing the risk for a given return.2.4 Appendix 33
Considering 
. πj|i,1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
[0.3145 0.0936 0.0402 0.2803 0.2953 0.1823
0.3496 0.2510 0.4819 0.1980 0.1121 0.0547
0.0490 0.2545 0.0002 0.2320 0.3233 0.4762
0.0052 0.2906 0.0998 0.0126 0.1706 0.0794
0.2441 0.0530 0.2966 0.0174 0.0241 0.1670
0.0376 0.0573 0.0813 0.2597 0.0745 0.0404]
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
. πj|i,2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
[0.0698 0.0300 0.1055 0.2260 0.0665 0.0686
0.1494 0.0436 0.3801 0.1869 0.3306 0.0561
0.1612 0.3607 0.1894 0.1134 0.1374 0.2171
0.2543 0.2750 0.0401 0.0767 0.1414 0.3745
0.2645 0.0667 0.1788 0.3519 0.1178 0.0592
0.1008 0.2239 0.1061 0.0452 0.2064 0.2244]
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
the optimal portfolio.π∗
k|i and the distribution vector.P∗
i are given by 
. d∗
k|i =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.5832 0.4168
0.4168 0.5832
0.5832 0.4168
0.6663 0.3337
0.5000 0.5000
0.5012 0.4988
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, P∗
i =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1358
0.0984
0.1150
0.2565
0.0012
0.3932
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
The mean-variance portfolio is the one on the efficient frontier with the lowest 
volatility (red circle in Fig. 2.7). The set of mean-variance efficient portfolios created 
by the risk-free and risky assets is the tangency point on the efficient frontier (blue 
circle in Fig. 2.7) if there is a risk-free asset (asset with zero volatility). Figure 2.8 
plots the convergence of the functional. 
2.4 Appendix 
Proof (Theorem 2.2) 
(a) First, let us prove that the Hessian matrix.H associated with the penalty function 
(2.2.5) is strictly positive definite for any positive. μ and. δ, i.e., we prove that for 
all.x ∈ RN and. u ∈ RM1
.H =
[ ∂2
∂x2 Fμ,δ (x, u) ∂2
∂u∂x Fμ,δ (x, u)
∂2
∂x∂u Fμ,δ (x, u) ∂2
∂u2 Fμ,δ (x, u)
]
> 0. (2.4.1) 
To prove that, by the Schur lemma, it is necessary and sufficient to prove that34 2 Multiobjective Control
Eficient Frontier 
70 
60 
50 
40 
30 
20 
10 
0 
0 500 1000 1500 2000 2500 3000 3500 4000 
Variance 
Var vs U 
Utility 
Fig. 2.7 Efficient Frontier 
.
∂2
∂x2 Fμ,δ (x, u) > 0, ∂2
∂u2 Fμ,δ (x, u) > 0
∂2
∂x2 Fμ,δ (x, u) > ∂2
∂u∂x Fμ,δ (x, u)
[ ∂2
∂u2 Fμ,δ (x, u)
]−1 ∂2
∂x∂u Fμ,δ (x, u) (2.4.2) 
We have 
. 
∂2
∂x2 Fμ,δ (x, u) = μ ∂2
∂x2 f (x) + V 
0 V0 + V 
1 V1 + δIN×N
≥ μ ∂2
∂x2 f (x) + δIN×N ≥ δ
(
1 + μ
δ
λ−
)
IN×N > 0,
∀δn > 0
λ− := min
x∈Xadm
λmin ( ∂2
∂x2 f (x)
)
,
∂2
∂u2 Fμ,δ (x, u) = IM1×M1 > 0.
By the Schur lemma (see for example [ 23]) 
.
∂2
∂x2 Fμ,δ (x, u) = μ ∂2
∂x2 f (x) + V 
0 V0 + V 
1 V1 + δIN×N >
∂2
∂u∂x Fμ,δ (x, u)
[ ∂2
∂u2 Fμ,δ (x, u)
]−1 ∂2
∂x∂u Fμ,δ (x, u) = (1 + δ)
−1 V 
1 V12.4 Appendix 35
Functional 
55 
50 
45 
40 
35 
30 
25 
20 
15 
0 20 40 60 80 100 120 140 160 180 200 
Iteraciones 
F 
Fig. 2.8 Functional 
implying .μ ∂2
∂x2 f (x) + V 
0 V0 + δ
1+δ V 
1 V1 + δIN×N > 0, which holds for any 
.δ > 0 by the condition (2.2.8) since 
. 
(
μλ− + δ
)
IN×N + V 
0 V0 +
δ
1 + δ
V 
1 V1 ≥
δ
(
1 + μ
δ λ−)
IN×N = δ (1 + o(1)) IN×N > 0.
So,.H > 0 which means that the penalty function (2.2.5) is strongly convex and, 
hence, has a unique minimal point defined below as.x∗ (μ, δ) and.u∗ (μ, δ). 
(b) By the strictly convexity property (2.4.1) for any.z := x
u
 
and any vector 
.z∗
n .:=.
 x∗
n = x∗ (μn, δn)
u∗
n = u∗ (μn, δn)
 
for the function.Fμ,δ (x, u) = Fμ,δ (z) we have36 2 Multiobjective Control
. 
0 ≥ (
z∗
n − z
) ∂
∂z
Fμn ,δn
(
z∗
n
)
= μn
(
x∗
n − x
) ∂
∂x f
(
x∗
n
)
+
[
V0
(
x∗
n − x
)] [
V0x∗
n − b0
]
+ [
V1
(
x∗
n − x
)] [
V1x∗
n − b1 + u∗
n
]
+
δn
(
x∗
n − x
) x∗
n + (
u∗
n − u
) [
V1x∗
n − b1 + (1 + δ) u∗
n
]
.
⎫
⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎭
(2.4.3) 
Selecting in (2.4.3) .x := x∗ ∈ X∗ (.x∗ is one of admissible solutions such that 
.V0x∗ = b0) and.u := b1 − V1x∗
n we obtain 
. 
0 ≥ μn
(
x∗
n − x∗
) ∂
∂x f
(
x∗
n
)
+ |
|V0
(
x∗
n − x∗
)|
|
2 + |
|V1
(
x∗
n − x∗
)|
|
2
+
δn
(
x∗
n − x∗
) x∗
n + (1 + δ)
−1 |
|V1x∗
n − b1 + (1 + δ) u∗
n
|
|
2
+
δn
(
u∗
n − b1 − V1x∗
n
) u∗
n.
Dividing both sides of this inequality by.δn we get 
.
0 ≥ μn
δn
(
x∗
n − x∗
) ∂
∂x f
(
x∗
n
)
+
1
δn
(|
|V0x∗
n − b0
|
|
2 + |
|V1
(
x∗
n − x∗
)|
|
2
+ |
|V1x∗
n − b1 + (1 + δ) u∗
n
|
|
2
)
+
(
x∗
n − x∗
) x∗
n + (
u∗
n − b1 − V1x∗
n
) u∗
n.
(2.4.4) 
Notice also that from (2.4.3), taking.x = x∗
n and.u = 0, it follows 
. 0 ≥
⎡
⎣
|
|
|
|
|
√
1 + δu∗
n +
(
V1x∗
n − b1
)
2
√1 + δ
|
|
|
|
|
2
−
|
|
|
|
|
(
V1x∗
n − b1
)
2
√1 + δ
|
|
|
|
|
2
⎤
⎦ ,
implying 
. 1 ≥
|
|
|e + 2 (1 + δ) u∗
n
|
|
(
V1x∗
n − b1
)|
|−1
|
|
|
2
, |e| = 1.
which means that the sequence.
{
u∗
n
}
is bounded. In view of this and taking into 
account that by the supposition (2.2.8).
μn
δn
→n→∞ 0, from (2.4.4) it follows 
.
Const = lim sup
n→∞
(|
|
(
x∗
n − x∗
) x∗
n
|
| + |
|
(
u∗
n − b1 − V1x∗
n
) u∗
n
|
|
)
≥
lim sup
n→∞
1
δn
(|
|V0x∗
n − b0
|
|
2
+
|
|V1
(
x∗
n − x∗
)|
|
2 + (1 + δn)
−1 |
|V1x∗
n − b1 + (1 + δn) u∗
n
|
|
2
)
.
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(2.4.5) 
From (2.4.5) we may conclude that2.4 Appendix 37
.
|
|V0x∗
n − b0
|
|
2 + |
|V1
(
x∗
n − x∗
)|
|
2
+
(1 + δn)
−1 |
|V1x∗
n − b1 + (1 + δn) u∗
n
|
|
2 = O (δn)
 
(2.4.6) 
and 
. 
V0x∗
∞ − b0 = 0,
V1x∗
∞ − V1x∗ = V1x∗
∞ − b1 + u∗
∞ = 0,
where.x∗
∞ ∈ X∗ is a partial limit of the sequence.
{
x∗
n
}
which, obviously, may be 
not unique. The vector.u∗
∞ is also a partial limit of the sequence.
{
u∗
n
}
. 
(c) Denote by.xˆn the projection of.x∗
n to the set.Xadm, namely, 
.xˆn = PrXadm (
x∗
n
)
, (2.4.7) 
and show that 
.
|
|x∗
n − ˆxn
|
| ≤ C
√
δn,C = const > 0. (2.4.8) 
Developing, we have 
. 
|
|V1x∗
n − b1 + u∗
n
|
| ≤ C1
√
δn,C1 = const > 0,
implying 
. V1x∗
n − b1 ≤ C1
√
δne − u∗
n ≤ C1
√
δne, |e| = 1.
where the vector inequality is treated in component-wise sense. Therefore 
. 
|
|x∗
n − ˆxn
|
|
2
≤ max
V1 x−b1≤C1
√δn e, x∈Xadm
min
y∈Xadm
|x − y|2 := d (δn).
Introduce the new variable 
.x˜ := (1 − νn) x + νn x˚ ∈ Xadm (2.4.9) 
where by the Slater condition (2.2.7). 0.<.νn :=. C1
√δn
C1
√δn+ min j=1,...,M1
|
|
|(V1 x˚−b1)j
|
|
|
.<. 1. For 
new variable.x = x˜ − νn x˚
1 − νn
we have 
. 
V1x˜ − b1 ≤ C1
√δn
C1
√δn+ min j=1,...,M1
|
|
|(V1 x˚−b1)j
|
|
|
·
 
min
j=1,...,M1
|
|
|
(
V1x˚ − b1
)
j
|
|
| e + (
V1x˚ − b1
)
 
≤ 0.
and therefore38 2 Multiobjective Control
. 
d (δn) ≤ max
V1 x˜−b1≤0,x˜∈Xadm
|
|
|
|
x˜ − νn x˚
1 − νn
− ˜x
|
|
|
|
2
=
ν2
n
(1 − νn)
2 max
V1 x˜−b1≤0,x˜∈Xadm
|
|x˜ − x˚
|
|
2
≤ C2δn,
0 < C2 < ∞.
In view of that.
|
|x∗
n − ˆxn
|
|.≤.
√d (δn).≤.
√C2
√δn which proves (2.4.8). 
(d) The last step is to prove the inequality 
.0 ≥ (
x∗
∞ − x∗) x∗
∞ for any x∗
∞ ≤ X∗. (2.4.10) 
From (2.4.4) we get 
.
0 ≥ (
x∗
n − x∗
) ∂
∂x f
(
x∗
n
)
+
1
μn
(|
|V0x∗
n − b0
|
|
2 + |
|V1
(
x∗
n − x∗
)|
|
2
)
+
δn
μn
(
x∗
n − x∗
) x∗
n+
1
μn
|
|V1x∗
n − b1 + u∗
n
|
|
2
≥
(
x∗
n − x∗
) ∂
∂x f
(
x∗
n
)
+
δn
μn
(
x∗
n − x∗
) x∗
n .
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(2.4.11) 
By the strong convexity property we have 
.(x − y)
 
 ∂
∂x f (x) − ∂
∂x f (y)
 
.≥. 0 for any.x, y ∈ RN which, in view of the 
property (2.4.8), implies 
. 
(
x∗
n − ˆxn
) ∂
∂x f
(
x∗
n
)
= O (√δn
)
,
(
xˆn − x∗
) ∂
∂x f
(
xˆn
)
≥ (
xˆn − x∗
) ∂
∂x f (x∗) ≥ 0,
and 
. 
(
x∗
n − x∗
) ∂
∂x f
(
x∗
n
)
= (
x∗
n − ˆxn
) ∂
∂x f
(
x∗
n
)
+
(
xˆn − x∗
) ∂
∂x f
(
x∗
n
)
= O (√δn
)
+ (
xˆn − x∗
) 
 ∂
∂x f
(
x∗
n
)
− ∂
∂x f
(
xˆn
)
 
+
(
xˆn − x∗
) ∂
∂x f
(
xˆn
)
≥ O (√δn
)
− |
|xˆn − x∗
|
|
|
|
|
|
∂
∂x f
(
x∗
n
)
− ∂
∂x f
(
xˆn
)
|
|
|
| .
Since any polynomial function is Lipschitz continuous on any bounded compact 
set, we can conclude that 
.
|
|
|
|
∂
∂x f
(
x∗
n
)
− ∂
∂x f
(
xˆn
)
|
|
|
|
≤ Const |
|x∗
n − ˆxn
|
| = O
(√
δn
)
,2.4 Appendix 39
which gives.
(
x∗
n − ˆx∗
) ∂
∂x f
(
x∗
n
)
.=.O (√δn
)
, which by (2.4.11) leads to 
.
0 ≥ (
x∗
n − ˆxn
) ∂
∂x f
(
x∗
n
)
+
δn
μn
(
x∗
n − x∗
) x∗
n =
O (√δn
)
+
δn
μn
(
x∗
n − x∗
) x∗
n .
⎫
⎪⎬
⎪⎭
(2.4.12) 
Dividing both side of the inequality (2.4.12) by.
μn
δn
and in view (2.4.8) we finally 
obtain 
. 0 ≥ O
 μn √δn
 
+ (
x∗
n − x∗
) x∗
n = o (1)
√δn + (
x∗
n − x∗
) x∗
n . (2.4.13) 
This, by (2.2.8), for.n → ∞ leads to (2.4.10). Finally, for any.x∗ ≤ X∗ it implies 
. 0 ≥ (
x∗
∞ − x∗
) x∗
∞ = |
|x∗
∞ − x∗
|
|
2 + (
x∗
∞ − x∗
) x∗ ≥ (
x∗
∞ − x∗
) x∗.
This inequality exactly represents the necessary and sufficient condition that the 
point.x∗ is the minimum point of the function.
|
|x∗
∞
|
|
2 on the set.X∗. Obliviously, 
this point is unique and has a minimal norm among all possible partial limits 
.x∗
∞. 
Theorem is proven. ◻
Proof (Lemma 2.1) The necessary and sufficient conditions for the points . x∗
n =
x∗ (μn, δn), .u∗
n = u∗ (μn, δn) to the extremal points of the function .Fμn ,δn (x, u) are 
as follows: 
. 
0 = ∂
∂x
Lμn ,δn
(
x∗
n , u∗
n | λ∗
n,x , λ∗
n,u
)
=
μn
∂
∂x f (x∗
n ) + V 
0
(
V0x∗
n − b0
)
+ V 
1
(
V1x∗
n − b1 + u∗
n
)
+ δn x∗
n − λ∗
n,x ,
0 = ∂
∂u
Lμn ,δn
(
x∗
n , u∗
n | λ∗
n,x , λ∗
u,x
)
= V1x∗
n − b1 + (1 + δ) u∗
n − λ∗
n,u,
λ∗
n,x (i) x∗
n (i) = λ∗
n,u (j) u∗
n (j) = 0 for all i, j -
complementary slackness condition,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(2.4.14) 
where .Lμn ,δn
(
x, u | λn,x , λu,x
) is the Lagrange function for the problem (2.2.4), 
defined for.λn,x (i).≥.0,.λu,x (j).≥. 0 as 
.Lμn ,δn
(
x, u | λn,x , λn,u
)
:= Fμn ,δn (x, u) − ∑
N
i=1
λn,x .40 2 Multiobjective Control
Multiplying the first equation in (2.4.14) by.x∗
n and the second one by. u∗
n, in view of 
the complementary slackness conditions we derive 
. 
0 = μn
(
x∗
n
) ∂
∂x f (x∗
n ) + (
V0x∗
n
) (
V0x∗
n − b0
)
+
(
V1x∗
n
) (
V1x∗
n − b1 + u∗
n
)
+ δn
|
|x∗
n
|
|
2
,
0 = (
u∗
n
) (
V1x∗
n − b1 + (1 + δ) u∗
n
)
,
implying 
.
0 = μn
(
x∗
n
) ∂
∂x f (x∗
n ) + (
V0x∗
n
) (
V0x∗
n − b0
)
+
|
|V1x∗
n − b1 + u∗
n
|
|
2 + b 
1
(
V1x∗
n − b1 + u∗
n
)
+ δn
(|
|x∗
n
|
|
2 + |
|u∗
n
|
|
2
)
.
⎫
⎬
⎭
(2.4.15) 
By the construction of the regularized penalty function it follows that 
.V1x∗
n − b1 + u∗
n = 0. (2.4.16) 
Indeed, if it is not the case, then .u∗
n can not be the optimal point since the function 
.Fμ,δ
(
x∗
n , u∗
n
)
is more than its value when 
. u∗
n = − (
V1x∗
n − b1
)
.
In view of this, the identity (2.4.15) is equal to 
. 0 = μn
(
x∗
n
) ∂
∂x f (x∗
n ) + (
V0x∗
n
) (
V0x∗
n − b0
)
+ δn
(|
|x∗
n
|
|
2 + |
|u∗
n
|
|
2
)
,
(2.4.17) 
implying 
. 
0 = μn
(
x∗
n
) ∂
∂x f (x∗
n )+
|
|
|
(
V 
0 V0 + δn IN×N
)1/2 x∗
n−
[(
V 
0 V0 + δn IN×N
)−1/2 V 
0 b0/2
]|
|
|
2
− |
|
|
(
V 
0 V0 + δn IN×N
)−1/2 V 
0 b0/2
|
|
|
2
+ δn
|
|u∗
n
|
|
2
and 
. 
μn
 
(
x∗
n
) ∂
∂x f (x∗
n )
 
+
|
|
|
(
V 
0 V0 + δn IN×N
)−1/2 V 
0 b0/2
|
|
|
2
=
|
|
|
(
V 
0 V0 + δn IN×N
)1/2 x∗
n−
[(
V 
0 V0 + δn IN×N
)−1/2 V 
0 b0/2
]|
|
|
2
.
The last identity can be represented as2.4 Appendix 41
.
x∗
n = (
V 
0 V0 + δn IN×N
)−1 V 
0 b0/2+
ρn
(
x∗
n
) (V 
0 V0 + δn IN×N
)−1/2
en
(
x∗
n
)
. (2.4.18) 
where.
|
|en
(
x∗
n
)|
|.=. 1 and 
. 
ρn
(
x∗
n
)
:= 
μn
 
(
x∗
n
) ∂
∂x f (x∗
n )
 
+
|
|
|
(
V 
0 V0 + δn IN×N
)−1/2 V 
0 b0/2
|
|
|
2
 1/2
.
Notice that in the last identity, by the boundedness property of.Xadm,. 
|
|
|
|
(
x∗
n
) ∂
∂x f (x∗
n )
|
|
|
|
.≤. c .= const and the matrix.
(
V 
0 V0 + δn IN×N
)−1
have following structure 
. 
(
V 
0 V0 + δn IN×N
)−1 =
adj[
V 
0 V0 + δn IN×N
]
det [
V 
0 V0 + δn IN×N
] =
⎡
⎢
⎢
⎢
⎣
⎛
⎜
⎜
⎜
⎝
N
∑−1
k=0
bi j|k δk
n
∑
N
k=0
ai j|k δk
n
⎞
⎟
⎟
⎟
⎠
i,j
⎤
⎥
⎥
⎥
⎦
i,j=1,N
,
where adj.A = [
Aji] is the matrix adjoined to .A and .Aji is the cofactor to the 
element .ai j . Since this matrix is nonsingular, it follows that .ai j|k /= 0. The matrix 
.
(
V 
0 V0 + δn IN×N
)−1/2 has the same structure, namely, 
. (
V 
0 V0 + δn IN×N
)−1/2 =
⎡
⎢
⎢
⎢
⎣
⎛
⎜
⎜
⎜
⎝
N
∑−1
k=0
b¯
i j|k δk
n
∑
N
k=0
a¯i j|k δk
n
⎞
⎟
⎟
⎟
⎠
i,j
⎤
⎥
⎥
⎥
⎦
i,j=1,N
, a¯i j|k /= 0.
In view of that the vector.x∗
n (2.4.18) has the following structure 
.
(
x∗
n
)
i = ∑
N
j=1
dj
N
∑−1
k=0
bi j|k δk
n
∑
N
k=0
ai j|k δk
n
+
(/
μnc
(
x∗
n
)
+ c0
) ∑
N
j=1
d¯j
(
x∗
n
)
N
∑−1
k=0
b¯
i j|k δk
n
∑
N
k=0
a¯i j|k δk
n
(2.4.19) 
where .
|
|c
(
x∗
n
)|
| ≤ c < ∞ and .
|
|d¯j
(
x∗
n
)|
| ≤ c1 < ∞. The structure (2.4.19) together 
with the equality (2.4.16) directly implies (2.2.12). ◻42 2 Multiobjective Control
Proof (Theorem 2.3) In view of (2.2.13) it follows 
.
Wn =
|
|
|
|
 
zn−1 − γn
∂
∂z
Fμn ,δn (zn−1)
 
+
− z∗
n
|
|
|
|
2
≤
|
|
|
|
(
zn−1 − z∗
n−1
)
− γn
∂
∂z
Fμn ,δn (zn−1) + (
z∗
n−1 − z∗
n
)|
|
2 =
Wn−1 + γ2
n
|
|
|
|
∂
∂z
Fμn ,δn (zn−1)
|
|
|
|
2
+ |
|
(
z∗
n−1 − z∗
n
)|
|
2
−
2γn
(
zn−1 − z∗
n−1
) ∂
∂z
Fμn ,δn (zn−1) +
2
(
zn−1 − z∗
n−1
) (
z∗
n−1 − z∗
n
)
− 2γn
(
z∗
n−1 − z∗
n
) ∂
∂z
Fμn ,δn (zn−1).
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(2.4.20) 
By the inequalities (see the inequalities (21.17) and (21.36) in [ 23]) we can conclude 
that 
.
|
|
|
|
∂
∂z
Fμn ,δn (zn−1)
|
|
|
|
2
≤ (1 + ϑn) L∇Wn−1 + (
1 + ϑ−1
n
)
d, (2.4.21) 
where 
.
|
|
|
|
∂
∂z
Fμn ,δn
(
z∗
n−1
)
|
|
|
|
2
≤ d. (2.4.22) 
We also have 
. 
(
zn−1 − z∗
n−1
) ∂
∂z
Fμn ,δn (zn−1) ≥ lnWn−1,ln = (
μnλ− + δn
)
|
|
(
zn−1 − z∗
n−1
) (
z∗
n−1 − z∗
n
)|
| ≤ |
|z∗
n−1 − z∗
n
|
| ,
√Wn−1
|
|
|
|
(
z∗
n−1 − z∗
n
) ∂
∂z
Fμn ,δn (zn−1)
|
|
|
|
ϑ>0
≤
|
|z∗
n−1 − z∗
n
|
|
[(
1 + ϑ1/2
) √L∇
√Wn−1 + (
1 + ϑ−1/2
) √d
]
.
Then, in view of Lemma 2.1, from (2.4.20) for.m = n − 1 we obtain 
.
Wn ≤ Wn−1 + γ2
n
[
(1 + ϑ) L∇Wn−1 + (
1 + ϑ−1
)
d
]
+
2
(
C2
1 |μn − μn−1|
2 + C2
2 |δn − δn−1|
2)
− 2γn
(
μnλ− + δn
)
Wn−1+
2 (C1 |μn − μn−1| + C2 |δn − δn−1|)
√Wn−1+
2γn (C1 |μn − μn−1| + C2 |δn − δn−1|)·
[(
1 + ϑ1/2
) √L∇
√Wn−1 + (
1 + ϑ−1/2
) √d
]
,2.4 Appendix 43
or, equivalently, 
.Wn ≤ Wn−1 (1 − αn−1) + ¯
δn−1
√
Wn−1 + βn−1, (2.4.23) 
where 
.
αn−1 = 2γn
(
μnλ− + δn
)
− γ2
n (1 + ϑ) L∇ =
2γn
(
μnλ− + δn
)
 
1 − γn (1 + ϑ) L∇
2 (μnλ− + δn)
 
≥
γnδn2 (1 + o (1)) 
1 − γn (1 + ϑ) L∇
2δn (o (1) + 1)
 
≥ Cαγnδn,
¯
δn−1 = 2 (C1 |μn − μn−1| + C2 |δn − δn−1|)
[
1 + γn
(
1 + ϑ1/2
) √L∇
]
≤ Cδ (|μn − μn−1| + |δn − δn−1|),
βn−1 = γ2
n
(
1 + ϑ−1
)
d + (
C2
1 |μn − μn−1|
2 + C2
2 |δn − δn−1|
2)
+
2γn (C1 |μn − μn−1| + C2 |δn − δn−1|)
(
1 + ϑ−1/2
) √d ≤ γ2
nCβ,1+
γn (|μn − μn−1| + |δn − δn−1|)Cβ,2+
(
|μn − μn−1|
2 + |δn − δn−1|
2)
Cβ,3.
(2.4.24) 
Using the inequality 
.Wr
n ≤ (1 − r) θr
n + r
θ1−r n
Wn,r ∈ (0, 1), θn > 0, (2.4.25) 
for .r = 1/2 and .
√θn = ¯
δn−1
2αn−1 (1 − ρ)
, .ρ ∈ (0, 1), the inequality (2.4.23) can be 
reduced to the following one 
.
Wn ≤ Wn−1
(
1 − αn−1
[
1 − ¯
δn−1
2αn−1
√θn
])
+ [
βn−1 + 1
2 ¯
δn−1
√θn
]
= Wn−1 (1 − αn−1ρ) +
[
βn−1 + ¯
δ2
n−1
4 (1 − ρ) αn−1
]
.
(2.4.26) 
By Theorem 16.14 in [ 23],.Wn →n→∞ 0 if 
. ∑∞
n=0
αn = ∞,
βn−1
αn−1
+ ¯
δ2
n−1
α2
n−1
→n→∞ 0,
which is equivalent to (2.2.15). Theorem is proven. ◻44 2 Multiobjective Control
References 
1. Beltrami, E., Katehakis, M., Durinovic, S.: Multiobjective markov decisions in urban mod￾elling. Math. Model. 6(4), 333–338 (1995) 
2. Benson, H.P., et al.: Matthias ehrgott, multicriteria optimization. Springer (2005) ISBN 3-540-
21398-8. 323 p. Eur. J. Oper. Res. 176(3), 1961–1964 (2007) 
3. Clempner, J.B.: Necessary and sufficient karush-kuhn-tucker conditions for multiobjective 
markov chains optimality. Automatica 71, 135–142 (2016) 
4. Clempner, J.B.: Computing multiobjective markov chains handled by the extraproximal 
method. Ann. Oper. Res. 271, 469–486 (2018) 
5. Clempner, J.B.: A team formation method based on a markov chains games approach. Cybern. 
Syst. 50(5), 417–443 (2019) 
6. Clempner, J.B., Poznyak, A.S.: Using the manhattan distance for computing the multiobjective 
markov chains problem. Int. J. Comput. Math. 95(11), 2269–2286 (2017) 
7. Clempner, J.B., Poznyak, A.S.: A tikhonov regularization parameter approach for solving 
lagrange constrained optimization problems. Eng. Optim. 50(11), 1996–2012 (2018) 
8. Clempner, J.B., Poznyak, A.S.: A tikhonov regularized penalty function approach for solving 
polylinear programming problems. J. Comput. Appl. Math. 328, 267–286 (2018) 
9. Das, I., Dennis, J.E.: Normal-boundary intersection: an alternate approach for generating 
pareto-optimal points in multicriteria optimization problems. SIAM J. Optim. 8, 631–657 
(1998) 
10. Fliege, J., Heseler, A.: Constructing approximations to the efficient set of convex quadratic 
multi-objective problems. University of Dortmund, Germany, Technical report (2003) 
11. Garcia, C.B., Zangwill, W.I.: Pathways to Solutions, Fixed Points and Equilibria. Prentice-Hall, 
Englewood Cliffs (1981) 
12. Garcia-Galicia, M., Carsteanu, A.A., Clempner, J.: Continuous-time learning method for cus￾tomer portfolio with time penalization. Expert Syst. Appl. 129, 27–36 (2019) 
13. Garcia-Galicia, M., Carsteanu, A.A., Clempner, J.: Continuous-time mean variance portfolio 
with transaction costs: a proximal approach involving time penalization. Int. J. Gen Syst 48(2), 
91–111 (2019) 
14. Germeyer, Y.: Introduction to the Theory of Operations Research. Nauka, Moscow (1971) 
15. Kiefer, J., Wolfowitz, J.: Stochastic estimation of the maximum of a regression function. Ann. 
Math. Stat. 23(2), 462–466 (1952) 
16. Markowitz, H.: Portfolio selection. J. Finance 7, 77–98 (1952) 
17. Markowitz, H.: The optimization of a quadratic function subject to linear constraints. Nav. Res. 
Logist. Q. 3, 111–133 (1956) 
18. Markowitz, H.M.: Mean-variance analysis. In: Finance, pp. 194–198. Springer (1989) 
19. Miettinen, K.: Nonlinear multiobjective optimization, vol. 12. Springer Science & Business 
Media (2012) 
20. Novák, J.: Linear programming in tector criterion markov and semi-markov decision processes. 
Optim. 20(5), 651–670 (1989) 
21. Ortiz-Cerezo, L., Carsteanu, A., Clempner, J.B.: Optimal constrained portfolio analysis for 
incomplete information and transaction costs. Econ. Comput. Econ. Cybern. Stud. Res. 4(56), 
107–121 (2022) 
22. Ortiz-Cerezo, L., Carsteanu, A., Clempner, J.B.: Sharpe-ratio portfolio in controllable markov 
chains: analytic and algorithmic approach for second order cone programming. Mathematics 
10(18), 3221 (2022) 
23. Poznyak, A.S.: Advanced Mathematical Tools for Automatic Control Engineers. Deterministic 
Technique, vol. 1. Elsevier, Amsterdam, Oxford (2008) 
24. Poznyak, A.S., Najim, K., Gómez-Ramírez, E.: Self-learning Control of Finite Markov Chains. 
Marcel Dekker, Inc. (2000) 
25. Sánchez, E.M., Clempner, J.B., Poznyak, A.S.: A priori-knowledge/actor-critic reinforcement 
learning architecture for computing the mean-variance customer portfolio: the case of bank 
marketing campaigns. Eng. Appl. Artif. Intell. 46, Part A, 82–92 (2015)References 45
26. Sánchez, E.M., Clempner, J.B., Poznyak, A.S.: Solving the mean-variance customer portfolio 
in markov chains using iterated quadratic/lagrange programming: a credit-card customer-credit 
limits approach. Expert Syst. Appl. 42(12), 5315–5327 (2015) 
27. Schittkowski, K.: Easy-opt: an interactive optimization system with automatic differentiation
- user’s guide. Department of Mathematics, University of Bayreuth, Technical report (1999) 
28. Sen, C.: A new approach for multi-objective rural development planning. Indian Econ. J. 30(4), 
91–96 (1983) 
29. Steuer, R.E.: The Tchebycheff procedure of interactive multiple objective programming. In: 
Multiple Criteria Decision Making and Risk Analysis Using Microcomputers, pp. 235–249. 
Springer, Berlin (1989) 
30. Tikhonov, A., Goncharsky, A., Stepanov, V., Yagola, A.G.: Numerical Methods for the Solution 
of Ill-Posed Problems. Kluwer Academic Publishers (1995) 
31. Tikhonov, A.N., Arsenin, V.Y.: Solution of Ill-posed Problems. Winston & Sons, Washington 
(1977) 
32. Vazquez, E., Clempner, J.B.: Customer portfolio model driven by continuous-time markov 
chains: an l2 lagrangian regularization method. Econ. Comput. Econ. Cybern. Stud. Res. 2, 
23–40 (2020) 
33. Wang, Y.M.: On lexicographic goal programming method for generating weights from incon￾sistent interval comparison matrices. Appl. Math. Comput. 173(2), 985–991 (2006) 
34. Wierzbicki, A.P.: A mathematical basis for satisficing decision making. Math. Model. 3(5), 
391–405 (1982) 
35. Zangwill, W.I.: Nonlinear Programming: A Unified Approach. Prentice-Halt, Englewood Cliffs 
(1969) 
36. Zhang, R., Golovin, D.: Random hypervolume scalarizations for provable multi-objective black 
box optimization. In: International Conference on Machine Learning, pp. 11096–11105. PMLR 
(2020)Chapter 3 
Partially Observable Markov Chains 
Abstract The controlled Partly Observable Markov Decision Process (POMDP) 
architecture has shown to be effective in a variety of fields where one is required to 
disclose only partial knowledge about the problem’s structure and parameters. Since 
some state variables are difficult to track and measure correctly, it may be more 
beneficial to base judgments on less accurate information. This chapter focuses on 
the design of an observer for a class of partially observable ergodic homogeneous 
finite Markov chains. The major objective of the suggested approach is to derive the 
formulas for computing an observer and, as a consequence, the best control strategy. 
We create a new variable that combines the policy, the observation kernel, and the 
distribution vector in order to solve the issue. To retrieve the important variables, 
we derive the formulas. The POMDP model’s parameters are being learned in a 
dynamic context in this work. The development of the adaptive policies is based on an 
identification method, in which we count the number of unobserved events to estimate 
the components of the utility and transition matrices. The practical applications of the 
theoretical concerns addressed to a portfolio optimization problem are demonstrated 
through the use of a numerical example. 
3.1 Introduction 
3.1.1 Brief Review 
The inaccessibility of the observer to the dynamics of the states. s poses a challenge 
for engineering and economics applications. Some state variables might be hard or 
impossible to measure precisely at times. This occurs when the accessible devices 
only capture a portion of the object’s states. Making judgments based on imperfect 
knowledge about the system state, which may be derived intuitively, may be more 
successful in certain sorts of challenges. This understanding relates to the observation 
kernel.P(s|·) in the context of partially observable Markov chains . The information 
about the states . s and actions . a of the unobserved Markov process is supplied in 
the form of a functional given. If the choice of the acceptable policies’ actions . d
does not depend on the current or past states, a Markov chain is said to be partly 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_3 
4748 3 Partially Observable Markov Chains
observable. We analyze an observation. y belongs to a given observation state space 
.Y in the evolution of a partly observable dynamical system that is defined by the 
probability measures.P(s|·) (observation kernel) (see [ 3, 12, 24, 26]). We know that 
the estimated. y appears with probability.P(s|·) if the state. s is feasible. 
Cassandra et al. [ 3] considered that the belief space is continuous in Partially 
Observable Markov Decision Process (POMDP) , but the existing techniques for 
determining optimum policies in Markov decision processes (MDP’s) that function 
exclusively in finite state spaces. Using the minimal principle, a required condition 
that an optimum control must satisfy, Lai and Elliott [ 12] studied the optimal control 
of a finite state, continuous time Markov chain seen in Gaussian noise. Whiting and 
Pickett [ 24] studied the estimate of a POMDP’s model order, which is the total number 
of independent model parameters. Based on the problem’s robust geometric features 
and the belief vectors, Zhang [ 26] offered a new perspective and methodology for 
understanding the POMDP problem. 
Different approaches to handling the unknown model parameters have been sug￾gested in the literature for the context of dynamical processes; see, for example, [ 2, 
9, 13, 14, 22, 25]. All of these methods put an emphasis on learning the dynamics 
of transition and observation for each condition. Since it is difficult to recover the 
probability measures of the observation kernel .P(s|·) with unknown parameters, a 
significant portion of the model will be very uncertain. The process of retrieving data 
frequently calls for a lot of time, physical resources, and processing power, or it relies 
on intuition, which might be inaccurate if the expert’s assumptions and expectations 
don’t fit the actual model well. In order to handle partially observed inventory issues, 
Bensoussan et al. [ 2] suggested an approach that includes linearizing state transitions 
using unnormalized probabilities. When transition, observation, and reward models 
are unknown, Doshi et al. [ 9] developed an estimate based on reducing the imme￾diate Bayes risk for making decisions. When determining a suitable statistic that 
reduces the issue to one of perfect state knowledge, Lesser and Oishi [ 13] suggested 
a way to maintain the multiplicative cost structure. Littman et al. [ 14] examined a 
number of straightforward problem-solving techniques and shown that they can all 
produce nearly optimum solutions for a number of very tiny POMDP’s drawn from 
the learning literature. Prediction profile models are non-generative partial models 
for partially observable systems that only provide a certain set of predictions and 
are, in some situations, far simpler than generative models, according to Talvitie 
and Singh [ 22]. For time-varying uncertain discrete-time, homogeneous, first-order, 
finite-state, finite-alphabet hidden Markov models, Xie et al. [ 25] explored a robust 
state estimation issue. 
The control objective of the earlier studies is, in general, the same as in the 
case of the total observability method but involves an observation kernel and takes 
into account that the sets of the acceptable policies rely on estimated state . y. The 
difficulty, therefore, is in creating an observer that can keep the best control features 
in a dynamic environment while still being computationally tractable.3.2 Partially Observable Markov Chains 49
In this chapter we follow [ 8]: 
– a technique for creating a joint observer .o ∈ O with incomplete information is 
addressed. Here, .o = P(s|y)P(s) is defined as the product of the distribution 
vector .P(s) and the observation kernel .P(s|y), which specifies the connection 
between the real and the estimated state, and.O is the set of joint observers. 
– the joint observer .o(a|y) and the policy .d(a|y) are multiplied together to create 
the new variable.c = P(s|y)d(a|y)P(s), which is used to solve the problem. The 
optimal policy . d∗, the estimated stochastic kernel .P(s|y), and the distribution 
vector .P(s), which are our variables of interest, may all be recovered after the 
optimal.c∗ has been calculated. The policy. d∗, the observation kernel.P∗(s|y), the 
distribution vector.P∗, and the joint observer. o are computed using formulae that 
are derived as a consequence of the suggested technique. 
– a dynamic environment is taken into account during learning the POMDP model’s 
parameters. The development of adaptive policies is based on an identification 
strategy in which we calculate the utility matrices and transition matrices by com￾puting the total number of unobserved events. The desired optimum adaptive strat￾egy is then computed. 
3.2 Partially Observable Markov Chains 
Let us associate with the finite state space. S the observation set. Y , which takes values 
in a finite space.{1,..., M},.M ∈ N. The stochastic process.{y(n), n ∈ N} is called 
the observation process . By observing.y(n) at time. n information regarding the true 
value of .s(n) is obtained. If an observation .y(n) = ym and .a(n) = ak , then the real 
.s(n) = si will have a probability 
.qi|mk := P(s(n) = si|y(n) = ym, a(n) = ak ), (3.2.1) 
where.m = 1, M,.i = 1, N,.k = 1, K that denotes the relationship between the state 
and the observation when an action.ak ∈ A(si) is chosen at time. n. The observation 
kernel is a stochastic kernel on. Y given by.Q = [qi|mk ]m=1,M,i=1,N,k=1,K . Formally, 
we have that if at time . n an action .a(n) = ak is chosen and the state .s(n) = si is 
possible then with probability.qi|ma the estimated.ym appears. 
Definition 3.1 A controllable POMDP is an 8-tuple 
.POMDP = {S, A,Π, Y, Q, Q0, P0, u}, (3.2.2) 
where 
–. S is a finite state space; 
–. A is a finite control space;50 3 Partially Observable Markov Chains
– .Π = [
πj|ik ]
is a stochastic kernel representing a stationary controlled transi￾tion matrix, where.πj|ik ≡ P(s(n + 1) = sj|s(n) = si, a(n) = ak ) is the probability 
associated with the transition from state .si to state .sj under an action . ak ∈ A(si),
.k = 1,..., K,.K ∈ N; 
–. Y is the observation set, which takes values in a finite space.{1,..., M},.M ∈ N, 
–.Q = [qi|m]m=1,M,i=1,N denotes the observation kernel is a stochastic kernel on 
. Y such that 
.
∑
m
qi|m = 1 for alli = 1, N, (3.2.3) 
which means that each state. i is observable with probability one, 
– .Q0 = [qi|m]m=1,M,i=1,N denotes the initial observation kernel, which is a 
stochastic kernel on. Y given. S, 
–.P0 is the (a prior) initial distribution; 
–.uijmk : S × S × Y × A → Ris the one-step reward function given the real unob￾servable state.si ∈ S, the next state.sj ∈ S and the estimated (measured) state.ym ∈ Y , 
when the action.ak ∈ A(si, ym) is taken. 
A realization of the partially observable system at time. n is given by 
.(s(0), y(0), a(0),s(1), y(1), a(1), . . .) ∈ H := (SY A)
∞ , (3.2.4) 
where .s(0) has a given distribution .P (s(0) = si) and .{an} is a control sequence 
in .A determined by a control policy. To define a policy, we cannot use the states 
.s(0),s(1), . . .. There, we introduce the observable histories . h0 := (y(0)) ∈ H0
and .hn := (s(0), y(0), a(0), . . . , y(n − 1), a(n − 1), y(n)) ∈ Hn for all .n ≥ 1 and 
.Hn := Hn−1AY if .n ≥ 1. Then a policy for the POMDP are defined as a sequence 
.
{
dk|m(n)
}
such that, for each . n, .dk|m(n) is a stochastic kernel on .A given .Hn. The 
set of all feasible policies is denoted by .Dadm. A policy .dk|m(n) ∈ D and an initial 
distribution .P (s0), denoted also as .P0, together with the stochastic kernels . Π, . Q
and.Q0 determine on the space.H all possible realizations of the POMDP. 
Definition 3.2 A sequence of random stochastic matrices . D(n) = {
dk|m(t)
}
t=0,n,k=1,K,m=1,M is said to be a randomized control strategy such that 
for any random action.dk|m(n) at time. n
.
∑
K
k=1
dk|m(n) = 1, dk|m ≥ 0, m = 1,..., M. (3.2.5) 
For any random action.
{
dk|m(n)
}
k=1,K,m=1,M the conditional transition probability 
matrix.Π ({
dk|m(n)
}
) = [
πj|i(
{
dk|m(n)
}
)
]
i,j=1,N can be defined as follows3.3 Formulation of the Problem 51
.
πj|i(
{
dk|m(n)
}
) = ∑
K
k=1
∑
M
m=1
P (
s(n + 1)=sj|s(n)=si, a(n)=ak
)
·
P(s(n) = si|y(n) = ym)dk|m(n) = ∑
K
k=1
∑
M
m=1
πj|ikqi|mdk|m(n),
⎫
⎪⎪⎬
⎪⎪⎭
(3.2.6) 
which represents the probability to move from the states.sj to the state . si under the 
applied random action.
{
dk|m(n)
}
k=1,K,m=1,M . 
3.3 Formulation of the Problem 
The dynamics of the POMC are described in terms of information that cannot be 
observed directly. At time .n = 0, the initial (observed) state .s0 has a given a prior 
distribution.P0, and the initial observation.y(0) is generated according to the initial 
observation kernel .Q0(y(0)|s(0)). If at time . n the state of the system is .s(n) and 
the control.a(n) ∈ A is applied, then each of strategy is allowed to randomize, with 
distribution .dk|m(n), over the pure action choices .a(n).∈ .A (s(n)),.m = 1, M and 
.k = 1, K. These choices induce immediately utility.uijmk . The system tries to min￾imize the corresponding one-step reward. Next, the system moves to the new state 
.s(n) = si according to the transition probabilities .Π ({
dk|m(n)
}
k=1,K,m=1,M ). Then, 
the observation.y(n) is generated by the observation kernel.Q(y(n)|s(n)). Based on 
the obtained reward, the systems adapt a mixed strategy computing.dk|m(n + 1) for 
the next selection of the control actions. 
Within the class of all stationary strategies and considering ergodic Markov chains 
[ 6], for any fixed collection of stationary strategies.dk|m(n) = dk|m we have 
.P (
s(n + 1) = sj
)
→ Pj, n → ∞. (3.3.1) 
That is, in the case when the Markov chain is ergodic for any stationary strategy. dk|m
the distributions.Pj exponentially fast converge to their limits.P (si) = Pi satisfying 
.Pj = ∑
N
i=1
(
∑
M
m=1
∑
K
k=1
πj|ikqi|mdk|m
)
Pi . (3.3.2) 
Denote also by.P¯
m the probability to observe the estimated state. m, which can be 
calculated as 
.P¯
m = ∑
N
i=1
(
∑
K
k=1
qi|mdk|m
)
Pi . (3.3.3) 
The reward function is given by the values .Wimk , so that the “average reward 
function”.U is given by52 3 Partially Observable Markov Chains
.U(d, o) := ∑
M
m=1
∑
N
i=1
∑
K
k=1
Wimkdk|moi|m, (3.3.4) 
where 
.Wimk = ∑
j
uijmkπj|ik , (3.3.5) 
and the joint observer [ 5, 8] is defined as.oi|m = qi|m Pi such that 
.
∑
M
m=1
∑
N
i=1
oi|m = 1. (3.3.6) 
Let us denote by .O the set of “feasible joint observers”. A feasible joint observer 
.o∗ =
{
o∗
k|m
}
will be referred to as optimal, if it realizes the optimization rule 
.(d∗, o∗) ∈ Arg max d∈Dadm ,o∈Oadm
U(d, o). (3.3.7) 
Without losing generality, we have assumed that the reward is a function of the 
action, the estimated state, and the current state. Then, taking into account the cur￾rent state and action, we calculate the conditional mean of this reward with respect 
to the estimated state .ym (the conditional distribution of the estimated state is pro￾vided by the stochastic kernel .qi|m. The original reward may be replaced with its 
conditional mean in the manner previously stated without losing generality since 
the overall reward function includes the expectation. In fact, the objective function 
value (including the estimated state.ym) has the same value as the objective function 
involving observable states. 
3.4 Description in the c-Variables 
Solutions to Eq. (3.3.7) can be found using dynamic programming tools. To see this, 
let us introduce [ 6, 18, 21] the variable.c = [cimk ] as follows 
. cimk = dk|mqi|m Pi . (3.4.1) 
In terms of the .c-variable the optimization problem given in Eq. (3.3.7) is satisfied 
by the following theorem. 
Theorem 3.1 The joint observer.o ∈ O and the strategy. d are optimal if and only if 
the variable.cimk represents the solution of the following linear programming problem3.4 Description in the c-Variables 53
.U˜ (c) = ∑
N
i=1
∑
M
m=1
∑
K
k=1
Wimk cimk → max
c∈Cadm
, (3.4.2) 
subject to constraints 
. cimk ≥ 0,
∑
N
i=1
∑
M
m=1
∑
K
k=1
cimk = 1,
∑
N
i=1
∑
K
k=1
cilk = Pi > 0,
. ∑
M
m=1
∑
K
k=1
[δi j − πj|ik ]cimk = 0, j = 1,..., N,
. ∑
K
k=1
cimk = qi|m
∑
M
l=1
∑
K
k=1
cilk ,
or 
. ∑
M
l=1
∑
K
k=1
[δlm − qi|m]cilk = 0, m = 1,..., M.
Proof We have that 
. ∑
K
k=1
dk|m = 1,
∑
M
m=1
qi|m = 1,
∑
N
i=1
Pi = 1,
. Pj = ∑
imk
πj|ikqi|mdk|m Pi,
and for.cimk = dk|moi|m it follows 
. ∑
K
k=1
cimk = qi|m Pi,
∑
K
k=1
cimk = qi|m
∑
M
l=1
∑
K
k=1
cilk ,
and 
. ∑
M
l=1
∑
K
k=1
cilk = Pi > 0,
∑
N
i=1
Pi = 1,
then, 
. ∑
N
i=1
∑
M
l=1
∑
K
k=1
cilk = 1.
It follows also that54 3 Partially Observable Markov Chains
. 
U(d) := ∑
M
m=1
∑
N
i=1
∑
K
k=1
(uijmkπj|ik )dk|moi|m =
∑
M
m=1
∑
N
i=1
∑
N
j=1
∑
K
k=1
Wimkdk|mqi|m Pi = ∑
M
m=1
∑
N
i=1
∑
K
k=1
Wimk cimk = U˜ (cimk ).
Notice that Eq. (3.3.2) is equivalent to 
. 
∑
M
m=1
∑
K
k=1
c jmk = ∑
N
i=1
πj|ik cimk ,
or
∑
M
m=1
∑
K
k=1
[δi j − πj|ik ]cimk = 0.
We define the solution of the problem (3.4.2) as .c∗. Now, for obtaining .q∗ we 
have to calculate 
. d∗
kim =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
c∗
imk
∑
K
h=1
c∗
imh
if ∑
K
h=1
c∗
imh > 0,
0 if ∑
K
h=1
c∗
imh = 0,
(3.4.3) 
. P∗
i = ∑
M
l=1
∑
K
k=1
c∗
ilk , (3.4.4) 
and 
. q∗
i|m =
∑
K
k=1
c∗
imk
∑
M
l=1
∑
K
k=1
c∗
ilk
. (3.4.5) 
So, finally to recover the optimal joint observer we need to use the formula 
. o∗
i|m = q∗
i|m P∗
i . (3.4.6) 
Remark 3.1 The equation .
∑
N
i=1
∑
K
k=1
cilk = Pi > 0 holds because of the ergodicity 
property. 
The expression (3.4.5) is not trivial. The resulting stochastic matrix .
[
q∗
i|m
]
rep￾resents the imprecise information concerning the system state, which is usually 
obtained intuitively.3.5 Computation of the Estimated Value by Measurable Realization … 55
Corollary 3.1 The strategy.d∗
k|m, constructed from.d∗
kim (3.4.3), is given by 
. d∗
k|m = 1
N
∑
N
i=1
d∗
kim.
Corollary 3.2 The obtained distribution of unobservable states is given by 
. P¯ ∗
m = ∑
N
i=1
∑
K
k=1
c∗
imk .
We have hereby derived the formulas of the optimal rules to recover the joint observer 
. o∗, the optimal policy.d∗
k|m, the observation kernel.q∗ and the distribution vector.P¯ ∗
m, 
which maximize Eq. (3.4.2) based on the optimal variables.c∗
imk . 
3.5 Computation of the Estimated Value by Measurable 
Realization: Projection Stochastic Approximation 
Procedure 
The solution of the adaptive problem consists of a maximization optimization prob￾lem 
. U(c) = ∑
N
i=1
∑
M
m=1
∑
K
k=1
Wimk cimk → max
c∈Cadm
given in Theorem 3.1. For computing the estimated value of the strategy .cimk we 
employ the projected gradient method given by 
.c(t + 1) = Ψ
(
c(t) + γt
∂Ut(c)
∂c
)
(3.5.1) 
where the step parameter.γt > 0 is a numerical sequence,.Ψ is the projection operator 
to the simplex.S NMK
ε and.cimk (t) ≥ ∈tn such that 
. 
S NMK
ε := {
c ∈ RNMK : cimk ≥ ε where,
∑
M
m=1
∑
K
k=1
c jmk = ∑
N
i=1
πj|ik (t)cimk ,
∑
N
i=1
∑
M
m=1
∑
K
k=1
cimk = 1
)
.
The integers . tn, .n ≥ 0 characterizing the given simplex are the moments that 
the values.cimk (t) are calculated by using information obtained on the time interval 
.
[
tn−1, tn
]
. These values characterize the laws forming the optimal adaptive policy. 
The projection.Ψ is defined by the condition given by56 3 Partially Observable Markov Chains
. |Ψ (z) − z| = min
x∈SNMK
|x − z|
for any vector. z. The elements of the gradient are given by 
. 
∂Ut(ct)
∂cimk = E
{ξU (t) χ(y(t)=ym ,s(t)=sıˆ,a(t)=akˆ)
dk|m (t)qi|m (t)Pi(t)
}
= E
{ξU (t) χ(y(t)=ym ,s(t)=sıˆ,a(t)=akˆ) cimk (t)
}
where.ξU (t) is the realization of the utility function (3.3.4) at time. t. 
The stochastic approximation of.
∂Ut(c)
∂cimk [ 18] is given by matrix 
. V(tn+1) = 1
tn+1 − tn
tn
∑+1−1
t=tn
ξU eT(t)
eT(t)c(t)
,
where .eT(t) = (1,..., 1) , ,, , NMK elements
and .c(t) = col(cimk (t)). The elements of .Vtn+1 are com￾puted according to the recurrence 
. Vmik (tn+1) = Vmik (t) − 1
tn+1 − tn
(
Vmik (t) − ξU (t) χ(y(t)=ym ,s(t)=sıˆ,a(t)=akˆ) cmik (t)
)
,
where .ξU := uijmk + μU rand[
− 1, 1
]
, μU ≤ uıˆjˆmˆ kˆ. If the parameters . {γt},{tn}
and.{∈t} satisfies the following constraints. γt > 0, tn+1t−1 n → 0, Δtn = tn+1 − tn →
∞, when t → ∞ and n → ∞, and 
.
tn+1 − tn
γt+1
≥
tn − tn−1
γt
,
∑∞
n=1
γtΔtn∈−1
n t
−1
n +
(Δtn
tn
)2
< 0, 0 < ∈n → 0,
lim
t→∞
1
γt
[
Δtnt−1 n + |∈t+1-∈t| +Δtn
(∑t
l=1
∈lΔtl
)−1
]
= 0,
0 < ∈t → 0,
(3.5.2) 
then, the sequence.{c˜(t)}t=0,1,2..., generated by 
.c˜(t + 1) = Ψ {c˜(t) + γtVmik (t + 1)} (3.5.3) 
converges in mean-square to.c∗ with minimum norm [ 17], 
.c∗
imk (t) := min cimk (t)∈Cadm
∑t
n=1
∑
N
i=1
∑
M
m=1
∑
K
k=1
|
|c∗
imk (n)
|
|
2
,3.6 Numerical Example: A Partially Observable Markowitz Portfolio 57
(since solution of problem (3.4.2) may be not unique). Within the class of numerical 
sequences 
. γt = γ t−α, ∈t = ∈t−β, tn = nτ
the conditions of the projection-gradient algorithm given in Eq. (3.5.1) are satisfied 
if 
. 0 <β<α< 1 − α, τ > 1.
The maximal rate . τ of convergence .E
|
|cimk (n) − c∗
imk
|
|
2 = O( 1
nτ ) is attained for 
.α = 1/2, β = 1/4, τ = 5/4. 
3.5.1 Adaptive Algorithm 
In this section, we will develop an algorithm for computing the optimal adaptive 
rules. 
The system chooses randomly an estimated state.y(t) = ym form.qi|m(t) and also 
selects randomly an action.a(t) = akˆ (given. kˆ) from.πk|m(t) (for a fixed. mˆ ). Next, it 
employs the transition matrix.P =
[
pj|ˆıkˆ
]
to choose randomly the consecutive state 
.s(t + 1) = sjˆ from .pˆjˆ|ˆıkˆ(t − 1) (given . jˆ for a fixed . ıˆ and . kˆ), and choose randomly 
a state .y(t + 1) = yu from .Qu| j . Then, the environment moves to a new state . sjˆ
given .akˆ and .sıˆ. The estimated values are updated employing the learning rules 
for computing .pˆjˆ|ˆıkˆ(t) and .uˆıˆmˆ jˆkˆ(t). The value-maximizing action at each state are 
taken. If the condition of estimated error . e is not satisfied, then the selection of the 
random variables . si , .sj and .ak is carried out again. On the other hand, the policy 
.πk|m and the observation kernel .qi|m are computed again applying Theorem 3.1 and 
the projected gradient method given in Eq. (3.5.1). The process is described in the 
Algorithm presented in Table 3.1. 
3.6 Numerical Example: A Partially Observable 
Markowitz Portfolio 
This example presents a Markowitz portfolio optimization problem [ 10, 11, 15, 16, 
19, 20, 23]. Stockholders can trade in securities and observe their prices delimited to 
one period. We suppose that the market does not allow short selling, for all assets the 
selling and buying prices are the same, there are no transaction costs, and all assets 
are infinitely divisible. The market is arbitrage-free. 
The mean-variance customer model of Markowitz [ 1, 4, 7] is defined as follows: 
the reward. R given by58 3 Partially Observable Markov Chains
Table 3.1 Adaptive Algorithm 
Algorithm 
(1) Let.t = 0 and so.s(0) = si be the initial state. 
(2) Let.πk|m(t) be a policy and.qi|m(t) the observation kernel computed 
applying Theorem 3.1 and the projected gradient method given in Eq. (3.5.1). 
(3) Choose randomly with probability.qi|m(t) an estimated state.ym and 
randomly with probability.πk|m(t) an action. ak . 
(4) Choose randomly with probability.pˆj|ikˆ(t) the next state.sj and choose 
randomly with probability.qr| j(t) a state. yr. 
(5) Update the values of.χmk (t) and.χmuk (t). 
(6) Compute.pˆjˆ|ˆıkˆ(t) according to the update rule and Pr.: ˆp(·|ˆıkˆ)
(t) → SN . 
(7) Compute.uˆıˆjˆmˆ kˆ(t) according to the update rule. 
(8) Compute the mean square error.e(t). 
(9) If.
(
|Pˆ(t − 1) − P∗|≤|Pˆ(t) − P∗|
)
continue else if not goto step 3. 
(10) Update the estimated values computed by the learning rules. pˆjˆ|ˆıkˆ(t)
and.uˆıˆjˆmˆ kˆ(t). 
(11) Set.si = sj and.t ← t + 1 and go to step 2. 
. 
R(c) = ∑
M
m=1
∑
N
i=1
∑
K
k=1
∑
N
j=1
uijmk pj|ikπk|mqi|m P (si)
= ∑
M
m=1
∑
N
i=1
∑
K
k=1
Wimk cimk → max
c∈Cadm
,
where.cimk = dk|m Pi and the variance.V(c) given by 
. 
V(c) := ∑
M
m=1
∑
N
i=1
∑
K
k=1
[Wimk − R(c)]
2 cimk =
∑
M
m=1
∑
N
i=1
∑
K
k=1
W2
imk cimk − R2(c) → min
c∈Cadm
.
The resulting Markowitz portfolio problem is 
. U(c) := R(c) − ζ
2V(c) → max
c∈Cadm
where. ζ is the risk-aversion parameter. 
The main goal of the Stockholder is to gain a given return. A rational Stockholder 
makes an effort to identify the portfolio with minimal risk, which satisfies this goal. 
We apply a reinforcement learning algorithm for POMDP with a discrete action space 
to asset allocation and computing the adaptive policies of the portfolio problem. The 
construction of adaptive policies is based on an identification approach where we3.6 Numerical Example: A Partially Observable Markowitz Portfolio 59
0.08 Convergence of the strategies c 
0.07 
0.06 
0.05 
0.04 
0.03 
0.02 
0.01 
0 
0 50 100 150 200 250 
Iterations 
Probability 
Fig. 3.1 Convergence of strategies. cimk
estimate the transitions matrices and the utility matrices by counting the number of 
unobserved experiences. 
Following this purpose, we fix the values of interest .ζ = 8.5 × 10−2, . γ0 = 2 ×
10−4,.N = M = 6 and.K = 2. To fulfill Stockholder’s expectations, we outline the 
portfolio of risky assets in a diagram of the functional in Fig. 3.2. We show that the 
functional .U(c) converges. Convergence of the functional is a consequence of the 
convergence of the strategies in Fig. 3.1 that represents the portfolio. The convergence 
of the error of.pˆjˆ|ˆıkˆ (.1 × 10−6) is shown in Fig. 3.3 and the error of.uˆıˆjˆmˆ kˆ (.1 × 10−1) is 
shown in Fig. 3.4. The portfolio is the result of fixing the volatility. This is exactly the 
portfolio, that the rational stockholder in our framework is looking for: it maximize 
the expected return for a given risk. Applying the equations developed in Section 4, 
we obtain that the resulting values of interest are the following: 
.π∗
k|m=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.5277 0.4723
0.8307 0.1693
0.5167 0.4833
0.8312 0.1688
0.4120 0.5880
0.2732 0.7268
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, P∗
i =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.2305
0.2408
0.1095
0.1800
0.1239
0.1153
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,60 3 Partially Observable Markov Chains
Functional 
55 
50 
45 
40 
35 
30 
25 
20 
15 
0 20 40 60 80 100 120 140 160 180 200 
Iteraciones 
F 
Fig. 3.2 Convergence of the functional. U(c)
. q∗
i|m =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0021 0.1796 0.1724 0.1670 0.2392 0.1345
0.1942 0.1191 0.1376 0.0543 0.1744 0.1558
0.0551 0.2219 0.2436 0.1757 0.1320 0.2187
0.2954 0.2178 0.0667 0.2232 0.1261 0.1837
0.2476 0.0712 0.2169 0.2214 0.1468 0.0402
0.2057 0.1903 0.1628 0.1585 0.1816 0.2672
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.d∗
i|m =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0005 0.0432 0.0189 0.0301 0.0296 0.0155
0.0448 0.0287 0.0151 0.0098 0.0216 0.0180
0.0127 0.0534 0.0267 0.0316 0.0164 0.0252
0.0681 0.0524 0.0073 0.0402 0.0156 0.0212
0.0571 0.0171 0.0238 0.0399 0.0182 0.0046
0.0474 0.0458 0.0178 0.0285 0.0225 0.0308
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.3.6 Numerical Example: A Partially Observable Markowitz Portfolio 61
6 
5 
4 
3 
2 
1 
0 
0 50 100 150 200 250 
Iterations 
10-4 
Probability 
Fig. 3.3 Convergence of the error of. pˆjˆ|ˆıkˆ
The joint observer.di|m is conceptualized as the product of the distribution vector. Pi
and observation kernel.qi|m, which denotes the relationship between the real and the 
estimated state. The resulting observation kernel .qi|m is interpreted as the intuition 
of the managers in their decision-making process, and it is a non-trivial result for the 
portfolio optimization problem. 
Remark 3.2 For POMC the goal of the actions is the same as in the case of total 
observable Markov chains, but the sets of the admissible policies are different. It is 
natural that, for this class of admissible policies, and the lack of information about 
the states of the Markov chain, the objective function presents in a reduction of its 
maximum.62 3 Partially Observable Markov Chains
4.5 
4 
3.5 
3 
2.5 
2 
1.5 
1 
0.5 
0 
0 50 100 150 200 250 
Iterations 
Value 
Fig. 3.4 Convergence of the error of. uˆˆi ˆjmˆ kˆ
References 
1. Asiain, E., Clempner, J.B., Poznyak, A.S.: A reinforcement learning approach for solving the 
mean variance customer portfolio for partially observable models. Int. J. Artif. Intell. Tools 
27(8), 1850034–1–1850034–30 (2018) 
2. Bensoussan, A., Cakanyildirim, M., Sethi, S.P., Shi, R.: Computation of approximate optimal 
policies in a partially observed inventory model with rain checks. Automatica (2011) 
3. Cassandra, A.R., Kaelbling, L.P., Littman, M.L.: Acting optimally in partially observable 
stochastic domains. In: Proceedings of Twelfth National Conference in Artificial Intelligence, 
vol. 2, pp. 1023–1028. Menlo Park, C.A., USA (1994) 
4. Clempner, J.B.: Necessary and sufficient karush-kuhn-tucker conditions for multiobjective 
markov chains optimality. Automatica 71, 135–142 (2016) 
5. Clempner, J.B.: Revealing perceived individuals’ self-interest. J. Oper. Res. Soc. 1–10 (2023). 
To be published. https://doi.org/10.1080/01605682.2023.2195878 
6. Clempner, J.B., Poznyak, A.S.: Simple computing of the customer lifetime value: a fixed local￾optimal policy approach. J. Syst. Sci. Syst. Eng. 23(4), 439–459 (2014) 
7. Clempner, J.B., Poznyak, A.S.: Sparse mean-variance customer markowitz portfolio optimiza￾tion for markov chains: a tikhonov’s regularization penalty approach. Eng. Optim. 19(2), 383– 
417 (2018). https://doi.org/10.1007/s11081-018-9374-9 
8. Clempner, J.B., Poznyak, A.S.: Observer and control design in partially observable finite 
markov chains. Automatica 10, 108587 (2019)References 63
9. Doshi, F., Pineau, J., Roy, N.: Reinforcement learning with limited reinforcement: using bayes 
risk for active learning in pomdps. In: Proceedings of the 25th International Conference on 
Machine Learning, vol. 301, pp. 256–263. Helsinki, Finland (2008) 
10. Garcia-Galicia, M., Carsteanu, A.A., Clempner, J.: Continuous-time learning method for cus￾tomer portfolio with time penalization. Expert Syst. Appl. 129, 27–36 (2019) 
11. Garcia-Galicia, M., Carsteanu, A.A., Clempner, J.: Continuous-time mean variance portfolio 
with transaction costs: a proximal approach involving time penalization. Int. J. Gen. Syst. 48(2), 
91–111 (2019) 
12. Lai, Y., Elliott, R.J.: The mean squared loss control problem for a partially observed markov 
chain. Int. J. Control (2017). To be published. https://doi.org/10.1080/00207179.2017.1362503 
13. Lesser, K., Oishi, M.: Reachability for partially observable discrete time stochastic hybrid 
systems. Automatica 50(8), 1989–1998 (2014) 
14. Littman, M.L., Cassandra, A.R., Kaelbling, L.P.: Learning policies for partially observable 
environments: scaling up. In: Proceedings of the Twelfth International Conference on Machine 
Learning, pp. 362–370 (1995) 
15. Ortiz-Cerezo, L., Carsteanu, A., Clempner, J.B.: Optimal constrained portfolio analysis for 
incomplete information and transaction costs. Econ. Comput. Econ. Cybern. Stud. Res. 4(56), 
107–121 (2022) 
16. Ortiz-Cerezo, L., Carsteanu, A., Clempner, J.B.: Sharpe-ratio portfolio in controllable markov 
chains: Analytic and algorithmic approach for second order cone programming. Mathematics 
10(18), 3221 (2022) 
17. Poznyak, A.S.: Advanced Mathematical tools for Automatic Control Engineers. Deterministic 
technique, vol. 1. Elsevier, Amsterdam, Oxford (2008) 
18. Poznyak, A.S., Najim, K., Gomez-Ramirez, E.: Self-learning Control of Finite Markov Chains. 
Marcel Dekker Inc, New York (2000) 
19. Sánchez, E.M., Clempner, J.B., Poznyak, A.S.: A priori-knowledge/actor-critic reinforcement 
learning architecture for computing the mean-variance customer portfolio: the case of bank 
marketing campaigns. Eng. Appl. Artif. Intell. 46, Part A, 82–92 (2015) 
20. Sánchez, E.M., Clempner, J.B., Poznyak, A.S.: Solving the mean-variance customer portfolio 
in markov chains using iterated quadratic/lagrange programming: a credit-card customer-credit 
limits approach. Expert Syst. Appl. 42(12), 5315–5327 (2015) 
21. Sragovich, V.G.: Mathematical Theory of Adaptive Control. World Scientific Publishing Com￾pany (2006) 
22. Talvitie, E., Singh, S.: Learning to make predictions in partially observable environments 
without a generative model. J. Artif. Intell. Res. 42, 353–392 (2011) 
23. Vazquez, E., Clempner, J.B.: Customer portfolio model driven by continuous-time markov 
chains: an l2 lagrangian regularization method. Econ. Comput. Econ. Cybern. Stud. Res. 2, 
23–40 (2020) 
24. Whiting, R.G., Pickett, E.E.: On model order estimation for partially observed markov chains. 
Automatica 24(4), 569–572 (1988) 
25. Xie, L., Ugrinovskii, V.A., Petersen, I.R.: Finite horizon robust state estimation for uncertain 
finite-alphabet hidden markov models with conditional relative entropy constraints. SIAM J. 
Control Optim. 47(1), 476–508 (2008) 
26. Zhang, H.: Partially observable markov decision processes: a geometric technique and analysis. 
Oper. Res. 58(1), 214–228 (2010)Chapter 4
Continuous-Time Markov Chains
Abstract The .c-variable approach is extended in this chapter by adding a new
linear constraint for continuous time. This method’s benefit is that it transforms
the continuous-time Markov decision process problem into a discrete-time Markov
decision process, where the linear constraints make the problem computation￾ally tractable. Chemical reaction networks, where the concentration dynamics is
described as a continuous-time Markov chain, serve as an example of the method’s
use. Using a state-discrete continuous-time Markov decision process, we provide a
mathematical optimization method for resolving chemical processes. The first appli￾cation is a single reversible reaction that produces the amidogen radical, and we were
able to determine the ideal temperature that reduces a linear functional of interest.
The second is a chemical reaction network that involves the proton transfer, hydra￾tion, and tautomeric reaction of anthocyanin pigments, in this case we found an
optimal strategy over a set of values of pH that minimizes the corresponding linear
functional.
4.1 Introduction
4.1.1 Related Work
In a Continuous-Time Markov Decision Process(CTMDP), which is a generalization
of a discrete-time Markov chain, a decision maker or controller chooses actions
(policies) to influence the behavior of a system as it evolves over time in order
to make the system perform as well as possible in relation to some predetermined
criterion. In order to find a policy that maximizes the average (long-run expected
average reward) or discounted reward, CTMDPs are often examined across infinite
horizons; for further information, see [5, 21].
The ideal policy for CTMDPs is obtained in two steps: first, the optimal value func￾tion for the relevant optimality criterion is defined, and then the optimality policies
are found. For this, they employ the dynamic programming method, which entails
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable
Markov Chains, Studies in Systems, Decision and Control 504,
https://doi.org/10.1007/978-3-031-43575-1_4
6566 4 Continuous-Time Markov Chains
characterizing the CTMDP optimal value function as the solution of an optimality
equation, also referred to as Bellman’s equation or the Hamilton–Jacobi–Bellman
equation. They then demonstrate that the deterministic stationary policy achieves the
supremum in the optimality equation. The CTMDP is converted into an analogous
linear programming problem [17].
The book [15] presents a CTMDP with unbounded transition rates and cost and
offers a policy iteration approach to approximate the average reward optimum policy.
The same CTMDP, i.e., transition rates and cost unbounded, is described in [14],
where the control issue in the class of Deterministic Stationary Policies is investigated
with regard to the Discounted Cost Criteria without offering a method to solve the
Optimal Policy. While applying the discounted reward optimality equation and the
value iteration technique, reports the best policy.
The authors [10, 25] report on the computation of the best policies for CTMDPs
with finite state and action spaces in terms of the long-run anticipated average reward.
There, the CTMDP is solved using either the linear programming approach or the pol￾icy iteration technique. The best course of action can be determined from a Discrete￾Time Markov Decision Process (DTMDP), which is produced from the original
CTMDP using uniformization (randomization) [3]. Under fairly general conditions,
the best course of action is stationary, meaning it depends only on the state.
A discrete-time Markov decision process that is subordinated to a Poisson process
emerges from uniformization of a CTMC. In homogeneous Markov models, algo￾rithms based on uniformization have been used with great success (see, for example,
[13, 25, 31]). This has shown to be an effective method for solving computational and
theoretical issues [9, 18]; one use of this method is to speed up policy improvement
methods by adding more decision epochs artificially (tijms 1986). The CTMDP is
restated in [25] as an average reward, and then uniformization is used to apply dis￾crete time approaches (policy iteration algorithm, value iteration algorithm or linear
programming). Several applications have been presented in the literature [1, 6, 7,
11, 29].
When the generator matrix varies at certain time intervals in a CTMC, uniformiza￾tion can be used to identify the transient solution [2]. The CTMC is not discretized
in a unique way by uniformization, which is a drawback.
In particular, the linear programming formulation described in [15, 25] presents
a solution for the optimal policy is based on the introduction of the.n-bias optimality
criteria; it is calculated as an inequality that is expressed as a primal linear program,
which is then converted into a dual linear program. This transformation reduces
the issue to the minimization of a linear functional, subject to linear and equality
constraints, and makes it straightforward to resolve. If the ergodicity property is
true, it is further demonstrated that the optimum policy’s solution is distinct. This
method is limited to actions and states with boundaries.
The main results of this chapter are the following:
– Presents an optimization approach for solving an average optimality criterion in
CTMDP, in addition the resulting approach is applied to kinetic chemical reactions
for the minimization of a average reward function of interest.4.2 Continuous-Time Markov Chains 67
– Extends the c-variable method and introduces a new special linear constraint for
continuous time. The advantage of our approach is that it reduces the CTMDP
problem to a discrete-time Markov decision processes. Then, the CTMDP is easy
to conceptualize and makes the problem computationally tractable.
– Applies the results to two problems of chemical kinetics. The first analyzes a
reversible reaction modeled as a CTMC through a model of birth-death process,
where the constants rates are function of temperature, we illustrate the method
across the minimization of the cost for the average rate production of .H and
observed that for the minimization of the functional a high temperature is required.
The second analyzes a CRN of three reversible reactions, where the pH influences
the reaction constant rates, such that the actions are a set of pH values. We illustrate
the method across the minimization the expected average number of molecules, and
we found that for the minimization of the functional a mixed strategy is required.
– Models the kinetics of reactions as a CTMDP and solve the minimization of
the long-run expected average reward that is a common optimality criterion for
CTMDP in particular for finite control models [25, 27].
It is important to mention that in both examples a proposed reaction mechanism is
necessary as well as the constant rates of the reaction. The solution presented for the
CTMDP is recommended only for solving optimization problems where the number
of molecules is small and for CRN that satisfy the ergodicity property.
4.2 Continuous-Time Markov Chains
In this section we introduce the (continuous-time, time-homogeneous) Markov pro￾cess model we are interested in. As usual,.R and.N stand for the sets of real numbers
and non-negative integers, respectively.
Let.S = {x1,..., xN : N ∈ N} be a finite set called the state space,.{X(t), t ≥ 0}
a stochastic process with state space.S, satisfies the Markov property if, letting.FX(τ )
denote all the information pertaining to the history of.X up to time.τ , and.τ ≤ t with
. j,i ∈ S
.P(X(t) = j|FX(τ )) = P(X(t) = j|X(τ ) = i)
we say that the process is time homogeneous if, .t = s + τ . By simple words, this
property means that any distribution in the future depends only on the value .X(τ )
and is independent on the past values.
Throughout the remainder
.CTMDP = (S, A,{A(s)}s∈S, Q,r) (4.2.1)
stands for a continuous-time Markov decision process (CTMDP), where the state
space .S is a finite set .
{
s(1),...,s(N)
}
, .N ∈ N, endowed with the discrete topology68 4 Continuous-Time Markov Chains
and the action set .A is the action (or control) space, a metric space endowed with
the corresponding Borel.σ-algebra.B(A).
For each.s ∈ S,.A(s) ⊂ A is the nonempty set of admissible actions at.s and we
shall suppose that it is compact. Whereas, the set.K := {(s, a) : s ∈ S, a ∈ A(s)} is
the class of admissible pairs, which is considered as a topological subspace of.S × A.
The matrix.Q = [
qj|ik ]
i,j=1,N,k=1,M denotes the transition rates which satisfy that
.qj|ik ≥ 0 for all.x ∈ S and. j /= i. The transition rates.qj|ik are conservative, i.e.,
.
 
N
j=1
qj|ik = 0 (4.2.2)
and stable, which means that
.q∗
i := sup
a∈A(i)
qi(a) < ∞∀i ∈ S
where
. qi := −qi,i ≥ 0 for all x ∈ S. (4.2.3)
Finally,.r ∈ B(K) is the (measurable) one-stage cost function.
We denote the probability transition matrix by
.Π (t) = [π(s,i,τ ,j,k)]i,j=1,N,k=1,M , τ ≥ s
such that,
.π(s,i,τ ,j,k) = π(0,i,t,j,k), t = τ − s∀i, j ∈ S, (4.2.4)
and where.
 
N
j=1
πj|ik = 1,.∀i ∈ S.
The Kolmogorov forward equations, can be written as the matrix differential
equation:
. Π'
(t) = Π (t)Q; Π (0) = I (4.2.5)
.Π (t) ∈ RN×N ,.I ∈ RN×N is the identity matrix. This system can be solved by
.Π (t) = Π (0)eQt = eQt := ∞
k=0
t k Qk
k!
and at the stationary state, the probability transition matrix is defined as
.Π∗ = lim
t→∞Π (t). (4.2.6)4.2 Continuous-Time Markov Chains 69
We also point out that given a state space .S, the infinitesimal generator.Q com￾pletely determines the CTMC. Thus, it is sufficient to characterize a chain by simply
providing a state space.S, and the generator.Q.
Definition The vector.p ∈ RN is called stationary distribution vector if
.Π ∗ p = p,
where.
 
N
i=1
pi = 1 and
.pi = P(X(t) = i).
This vector can be seen as the long run proportion of time that the process is in state
.i ∈ S.
Theorem Let.X(t) be an irreducible and recurrent CTMC, then the following state￾ments are equivalent:
• .Q p = 0,
• .Π ∗ p = p.
Remark The proof of this fact is easy in the case of a finite state space, recalling the
Kolmogorov backward equation.
A strategy is then defined as a sequence .d = {d(t), t ≥ 0} of stochastic kernels
.d(t) such that:
(a) for each time.t ≥ 0.d(k|i)(t)is a probability measure on.A such that.d(A(i)|i)(t) = 1
and,
(b) for every.E ∈ B(A).d(E|i)(t) is a Borel measurable function in.t ≥ 0.
We denoted by .D the family of all strategies. From now on, we will consider only
stationary strategies.d(k|i)(t) = d(k|i).
For each action the matrix .Q (a) := [qj|i,k ], .a ∈ A denotes the transition rates
matrix for the action.a such that
.[qi j(a)] := [qj|i,k ] =
⎧
⎨
⎩
− 
i λi j(a), if i = j
λi j(a), if i /= j
while, for each strategy.d the associated transition rate matrix is defined as:
. Q(d) := [qi j(d)] = 
M
k=1
qj|i,kdk|i, (4.2.7)70 4 Continuous-Time Markov Chains
such that on a stationary state distribution for all.dk|i and.t ≥ 0 from (4.2.6) we have
that
.Π∗(d) = lim
T→∞ eQ(d)T ,
where.Π∗ (d) is a stationary transition controlled matrix.
.Π∗(d) := [πi j(d)] = 
M
k=1
πj|ikdk|i . (4.2.8)
Moreover, let .r(a) be the vector whose components.ri(a) := rik are the reward
of the state.i ∈ S given the action.ak ∈ A, we can express the reward vector in terms
of the strategy.d as:
.r(d) := [ri(d)] = 
M
k=1
rikdk|i .
Definition Let us denote by.J (d)the long-run expected average reward, also referred
to as the gain of a policy.d, as the vector
.J(d) := lim inf
T−→∞
 
T
0
Π (t, d)r(d)dt
T
with ith component.J(i, d). At the steady state
.J ∗(d) = Π∗(d)r(d).
Pondering the long-run expected average reward over the states at steady state the
following linear functional.J (d) under the fixed strategy.dk|i(t) = dk|i can be defined
as follows
. J (d) := 
N
i=1
 
N
j=1
 
M
k=1
rikπj|ikdk|i pi = 
N
i=1
 
M
k=1
rikdk|i pi . (4.2.9)
Formulation of the problem. In an ergodic chain, the steady-state probabilities
are independent of the initial state of the system. In the long run, the average cost
per stage should be a constant regardless of the initial state. Because the controllable
ergodicMarkov chains reach a stationary situation exponentially quickly, we consider
a stationary distribution and by the property in (4.2.7), the value function (4.2.9),
under a given optimal policy is:4.3 Programming Solver for CTMDP 71
. J (d) = 
N
i=1
 
M
k=1
rikdk|i pi(d) −→ min
d . (4.2.10)
4.3 Programming Solver for CTMDP
In this section we present the extended c-variable method and the linear programming
solver for CTMDP.
4.3.1 The c-Variable Method
The model presented in Eq. (4.2.10) is not linear. Then, if we define new decision
variables the problem presented above can be reformulated as a linear programming
as follows. Let introduce the joint strategy variable .cik as
.cik = pidk|i .
The variable .cik belongs to the set of matrices .c ∈ Cadm and is restricted by the
following constraints:
1. each vector from the matrix.c := [cik ] represents a stationary mixed-strategy that
belongs to the simplex
.S N×M :=
⎧
⎪⎪⎨
⎪⎪⎩
c ∈ RN×M for cik ≥ 0,
where 
N
i=1
 
M
k=1
cik = 1;
(4.3.1)
2. the variable.cik satisfies the ergodicity constraints i.e.:
.pj(d) = 
N
i=1
 
N
k=1
πj|ik pidk|i,
that in terms of.cik takes the form:
.
 
M
k=1
c jk = 
N
i=1
 
M
k=1
πj|ik cik . (4.3.2)72 4 Continuous-Time Markov Chains
3. From Eq. (4.2.7):
.
 
N
i=1
qi j(d)pi(q) = 
N
i=1
 
M
k=1
qj|ikdk|i pi = 0.
this expression in terms of.cik takes the form:
.
 
N
i=1
 
M
k=1
qj|ik cik = 0. (4.3.3)
Once the model (ergodic Markov decision process) is solved in order to recover
the quantities of interest we have that:
• Stationary Distribution
. pi(d) = 
M
k=1
cik . (4.3.4)
• Policy
. dik = cik
 
M
k=1
cik
. (4.3.5)
Then, in terms of.c-variables the reward function.J (d) becomes:
. J (c) = 
N
i=1
 
M
k=1
rikdk|i pi(d) = 
N
i=1
 
M
k=1
rik cik . (4.3.6)
4.3.2 Linear Programming Solver
Consider the problem:
.CT X → min
X ,
such that
. Aeq X = beq ,4.3 Programming Solver for CTMDP 73
where
. 0 ≤ X ≤ 1, Aeq ∈ R(2N+1)×(N M) and beq ∈ R(2N+1)
.
The vector.CT := C(N|M) is defined as:
.C(1|1) = r11,...,C(1|M) = r1M
C(2|1) = r21,...,C(2|M) = r2M ,...,
C(N|1) = rN1,...,C(N|M) = rN M .
(1) We will construct the matrix .A ∈ RN×(N M) using the ergodicity constraints
defined in (4.3.1) as
.0 = 
M
k=1
(
 
N
i=1
πj|ik cik − c jk)
.
Then, we have that
.
j = 1 
M
k=1
( 
N
i=1
πj|ik cik − c1k
)
= 0,
.
.
. .
.
.
j = N 
M
k=1
( 
N
i=1
πj|ik cik − cN k)
= 0.
(4.3.7)
Developing the formulas of (4.3.7) we have that
.A = [
πj|i,k − δ j,i
]
j=1,N i=1,N ,
where.δ(j,i) is the Kronecker’s delta.
(2) For satisfying the time constraints defined in Eq. (4.3.2) we will construct the
matrix.B ∈ RN×(N M) as follows
.B = [
qj|ik ]
j=1,N i=1,N .
(3) Finally, for satisfying the ergodicity constraints defined in (4.3.1 ) as.ci|k ∈
S(N×M)
, we have that
.Aeq =
⎡
⎣
A
B
e 
⎤
⎦ ,
where.e = (1,..., 1) ∈ R(N M) and the vector.beq is defined as
.beq = (0,..., 0 2N times
, 1)
T ∈ R2N+1
.74 4 Continuous-Time Markov Chains
4.4 Chemical Reaction Markov Models
In this section we follow [4].
A Chemical Reaction Network (CRN) comprises a set of reactants, a set of prod￾ucts, and a set of reactions. Dynamical properties of CRNs are studied by means
of differential equations (deterministic models) [16, 23], or by a kinetic scheme,
which describes the dynamics of variables by a network of states and connections
between them (stochastic models). In our case the time-dependent concentrations of
the chemical species.
The stochastic models for CRNs are represented in terms of Poisson processes.
This random time-change representation produces a stochastic equation that can be
formulated as continuous-time Markov chains (CTMC). The states of the chemical
reaction network are a set with the number of molecules for every chemical specie
and the reactions are the possible transitions of the chain. In the CRN, the molecules
have collisions randomly and they may undertake chemical reactions, which modify
the state of the system. The rate at which the transitions occur is given by the law of
mass action.
The graph of the CTMC is as follows: the nodes are elements where the i-th
component represents a set with the number of molecules of every chemical specie.
An edge .A → B represents a positive rate of transition between nodes .A and .B,
which in turn exists if there is a reaction in the CRN that allows this transition. Then,
the graph of the stochastic model is associated with the CTMC and it inherits the
structural elements from the graph of the CRN. Different results have shown that
the discreteness and randomness of the chemical reactions need to be considered for
certain applications [24, 26, 30, 32]. Then, CTMC models have increased importance
for describing the dynamics in chemical reaction networks.
4.4.1 Example 1. Formation of the Amidogen Radical
In this example we consider the reaction
.H + N H3 H2 + N H2, (4.4.1)
performed by the flash photolysis-shock tube technique, using atomic resonance
absorption to monitor the concentration of.H over the time, namely.[H(t)]. A set of
experiments at different temperatures, over the range of 900–1620.K were performed
in [28]. The initial concentration of.[H] and.[N H2] was the same i.e.,.[H]0 = [N H2]0
and both the.[N H3] and.[H2] were maintained in large excess such that the kinetics
is simplified to a system of two opposing first-order reactions described in equation
(4.4.1). The Table 4.1 shows the experimental results reported in (Sutherland and
Michael [28]) for the reaction rate at different temperatures:4.4 Chemical Reaction Markov Models 75
Table 4.1 Kinetic data of the reaction:.H + N H3 ↔ N H2 + H2, T(K),.k1(10−13).s−1,.k−1(10−13)
.s−1
T .k1 .k−1
1260 5.01 3.58
1482 13.06 5.94
1620 20.77 9.23
b0 b4
NH2 H NH2 H NH2 2 N H HN2H H H H
NH2 NH2 H NH2 NH2 H H NH2 H NH2 NH2 NH2 NH2 
d1 d5
d2 b1 d4 b3 d6 b5
b2
NH2 H NH2 H H H
NH2 NH2 H NH2 H H
NH2 NH2 NH2 H H H
d3
2 2
Fig. 4.1 Schematic illustration of Continuous time Markov Chain for the reaction.H + N H3 
H2 + N H2,.N = 7,.[H(t)] = {0, 1, 2,..., 6}
To describe the dynamics of .[H(t)], we propose a model of CTMC based on
a birth-death process. Where the state .S = [H(t)] = {0, 1, 2,..., N − 1}, is the
feasible number of molecules of .H that are converted into new molecules of .H
(birth), or consumed (death), with rates given by the mass action law, .bn(T ) and
.dn(T ) respectively:
.bn(T ) = k−1(T )(N − n), dn(T ) = k1(T )n, ∀n ∈ S.
The Fig. 4.1, shows the graph representation for the CTMC with .N = 7, where
the.i-th state corresponds to.i number of molecules of.H under certain action.a, and
.bi ,.dj are the birth and death rates respectively, for all.i, j ∈ S.
For a birth-death process the generator matrix [22] for a fix.N takes the form:
.Q(T ) =
⎡
⎣
−b0(T ) b0(T ) 0 ... 0 0
d1(T ) −(b1(T )+d1(T )) b1(T ) ... 0 0
.
.
. .
.
. .
.
. ... .
.
. .
.
. 0 00 ... dN−1(T ) −dN−1(T )
⎤
⎦ , (4.4.2)
where .N = [H]0 + [N H2]0. The constants .k−1(T ) and .k1(T ) are function of the
temperature, this variable is used as a control for the direction of the reaction,
i.e., .A = {T1, T2,..., Tk } where .A is a set of admissible temperatures. For the76 4 Continuous-Time Markov Chains
Fig. 4.2 Comparison between CTMC for the embedded chain for.[H(t)], under a pure strategy
.T = 1260 and.Q(1260) (line black), and the ODE model (line red)
sake of simplicity we consider a chain with .7 states, .N = 7, .S = {0, 1, 2,..., 6}
and a set of .3 actions, .k = 3, such that the set of admissible temperatures is.A =
{1260, 1482, 1620}. Using the values for.k1(T ) and.k−1(T )reported in Table 4.1, and
according to the expression in (4.4.2), the following matrix generators are obtained:
.Q(T1) =
⎡
⎢
⎣
−21.48 21.48 0 0 0 0 0
5.01 −22.91 17.90 0 0 0 0
0 10.02 −24.34 14.32 0 0 0
0 0 15.03 −25.77 10.74 0 0
0 0 0 20.04 −27.20 7.16 0
0 0 0 0 25.05 −28.63 3.58
0 0 0 0 0 30.06 −30.06
⎤
⎥
⎦ ,
Q(T2) =
⎡
⎢
⎣
−35.64 35.64 0 0 0 0 0
13.06 −42.76 29.70 0 0 0 0
0 26.12 −49.88 23.76 0 0 0
0 0 39.18 −57.00 17.82 0 0
0 0 0 52.24 −64.12 11.88 0
0 0 0 0 65.30 −71.24 5.94
0 0 0 0 0 78.36 −78.36
⎤
⎥
⎦ ,
Q(T3) =
⎡
⎢
⎣
−55.38 55.38 0 0 0 0 0
20.77 −66.92 46.15 0 0 0 0
0 41.54 −78.46 36.92 0 0 0
0 0 62.31 −90.00 27.69 0 0
0 0 0 83.08 −101.54 18.46 0
0 0 0 0 103.85 −113.08 9.23
0 0 0 0 0 124.62 −12.62
⎤
⎥
⎦ .
In the following, we consider.[H]0 = 0 and .[N H2]0 = 6, the initial distribution
of the CTMC is.p0 = [1, 0, 0, 0, 0, 0, 0], using the embedded chain, generated by
the matrix .Q(T ) is possible to develop the construction of the CTMC as shown in
[20]. The Fig. 4.2 shows the comparison between the model for the CTMC and the
ODE of the deterministic kinetic model.d H(t)/dt = k−1(N − H(t)) − k1H(t).
The probability transition matrix .Π∗(T ) is calculated by the Chapman–
Kolmogorov equation in (4.2.6). In the biology literature this system of equations is
termed Chemical Master Equation [8, 19].4.4 Chemical Reaction Markov Models 77
.Π∗(T1) =
⎡
⎢
⎣
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
0.0394 0.1688 0.3015 0.2872 0.1539 0.0440 0.0052
⎤
⎥
⎦ ,
Π∗(T2) =
⎡
⎢
⎣
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
0.1015 0.2878 0.3273 0.1985 0.0677 0.0123 0.0009
⎤
⎥
⎦ ,
Π∗(T3) =
⎡
⎢
⎣
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
0.1101 0.2936 0.3262 0.1933 0.0644 0.0115 0.0008
⎤
⎥
⎦ .
To illustrate the optimization problem, suppose that a negative reward (cost) is
obtained according to the velocity of the individual reaction in the direction.N H2 +
H2 −→ H + N H3, this idea is reflected in the following cost vector:
.rn = −k1(T )n ∀n ∈ S,
then the functional (4.2.9) to minimize, represents the mathematical expectation of
the rate at which consumes.[H], once the steady state is reach.
The rewards.r(T ) for every temperature are:
.r(T1) =
⎡
⎢
⎣
0 −5.01 −10.02 −15.03 −20.04 −25.05 −30.06
⎤
⎥
⎦ ,r(T2) =
⎡
⎢
⎣
0 −13.06 −26.12 −39.18 −52.24 −65.30 −78.36
⎤
⎥
⎦ ,r(T3) =
⎡
⎢
⎣
0 −20.77 −41.54 −62.31 −83.08 −103.85 −124.62
⎤
⎥
⎦ .
The results of the c-variable method with the constraints described in expressions
(4.3.1), (4.3.2), (4.3.3), are presented below:
.c =
⎡
⎢
⎢
⎢
⎣
2.5×10−11 4.4×10−11 1.1×10−1
9.4×10−11 1.0×10−10 2.9×10−1
1.3×10−10 1.4×10−10 3.2×10−1
1.0×10−10 1.2×10−10 1.9×10−1
3.6×10−11 8.7×10−11 6.4×10−2
1.1×10−12 4.1×10−11 1.1×10−2
2.3×10−12 3.1×1013 8.4×10−4
⎤
⎥
⎥
⎥
⎦ , d =
⎡
⎢
⎢
⎢
⎣,
2.2×10−10 4.0×10−10 9.9×10−1
3.2×10−10 3.6×10−10 9.9×10−1
4.0×10−10 4.5×10−10 9.9×10−1
5.3×10−10 6.5×10−10 9.9×10−1
5.6×10−10 1.3×10−9 9.9×10−1
9.8×10−11 3.6×10−9 9.9×10−1
2.7×10−9 3.7×10−10 9.9×10−1
⎤
⎥
⎥
⎥
⎦ .
From the results obtained in .d observe that the optimal temperature that minimize
the functional bellow is.T3 = 1620◦C (Fig. 4.3).78 4 Continuous-Time Markov Chains
Fig. 4.3 Comparison between CTMC for the embedded chain for the state,.[H(t)], under a pure
strategy.T = 1260 and.Q(1260) (line black), and the CTMC for the embedded chain under a mixed
strategy.d, generated by.Q(d)
Fig. 4.4 Proton transfer,
hydration and tautomeric
reaction of anthocyanin
pigments
4.4.2 Example 2. Proton Transfer, Hydration and Tautomeric
Reaction of Anthocyanin Pigments
In this example we study a CRN of three reversible reactions: proton transfer, hydra￾tion and tautomeric reaction of anthocyanin pigments.
The CRN of Fig. 4.4 is represented symbolically by:4.4 Chemical Reaction Markov Models 79
Table 4.2 Rate constants of the structural transformations of Malvidin 3-Glucoside in 0.2 M Ionic
Aqueous Acid Medium at.25◦C
.k'
12 = 4.7(±0.4)X104s−1 .k'
21 = 6.7(±0.5)X108M−1s−1
.k'
13 = 8.5(±1)X10−2s−1 .k'
31 = 34(±3)M−1s−1
.k'
43 = 3.8(±0.2)X10−4s−1 .k'
34 = 4.5(±0.3)X10−5s−1
.
AH+ A + H+,
AH+ B + H+,
B + catalyst C + catalyst.
(4.4.3)
We model the dynamics of the CRN as a CTMC where every state represent a
possible path for the reactions in (4.4.3). The construction of the chain states is as
follows:
.
AH+k
→12 A+H+: (AH+,A,H+,B,C)
k
→12(AH+−1,A+1,H++1,B,C),
A+H+k
→21 AH+: (AH+,A,H+,B,C)
k
→21(AH++1,A−1,H+−1,B,C),
AH+k
→13B+H+: (AH+,A,H+,B,C)
k
→13(AH+−1,A,H++1,B+1,C),
B+H+k
→13 AH+: (AH+,A,H+,B,C)
k
→31(AH++1,A,H+−1,B−1,C),
B+H+k
→13 AH+: (AH+,A,H+,B,C)
k
→31(AH++1,A,H+−1,B−1,C),
B+catalyst k
→34C+catalyst: (AH+,A,H+,B,C)
k
→43(AH+,A,H+,B−1,C+1).
In particular for the initial condition.(AH+
0 , A0, H+
0 , B0,C0),.(AH+
0 , 0, H+
0 , 0, 0) =
(2, 0, 5, 0, 0), the transitions between the states are schematized in Fig. 4.5.
The experimental rate constants for the CRN are reported in Table 4.2.
When the order of the chemical reaction is greater than one, the relationship of
the stochastic reaction rate with the deterministic reaction rate depends on both, the
volume of the system and the numbers of each reactant required for the reaction [12],
therefore the kinetic constants for CTMDP used are.k12 = 4.7 × 104s−1,.k21 = 1.1 ×
102s−1, .k13 = 8.5 × 10−2s−1, .k31 = 5.5 × 10−5s−1, .k34 = 4.5 × 10−5s−1, .k43 =
43.8 × 10−4s−1.
The control action are the pH of the medium, i.e., the number of molecules of.H+
such that,.A = {
H+
1 , H+
2 , H+
3
}
= {5, 120, 530}.
The set of actions.A, results in the following generator matrices:
.Q(H+
1 ) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−7.7 × 102 3.8 × 10−4 0 0 0 0 0 0 07.7 × 102
4.5 × 10−5 −7.7 × 102 0 3.08 × 10−3 0 7.7 × 102 0 0 00
0 0 −1.54 × 103 1.54 × 103 00 0 0 00
0 8.5 × 10−2 4.7 × 104 −4.76 × 104 6.6 × 102 0 0 0 00
0 0 09.4 × 104 −9.4 × 104 1.7 × 10−1 0 0 00
0 4.7X104 0 02.31X10−3 −4.7X104 8.5X10−2 0 04.5X10−5
0 0 0 0 06.93X10−3 −6.975X10−3 4.5X10−5 0 0
0 0 0 0 0 03.8X10−4 −3.505X10−3 4.5X10−5 3.08X10−3
0 0 0 0 0 0 03.8X10−4 −3.8X10−4 0
4.7X104 0 0 0 0 0 08.5X10−2 0 −4.7X104
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,80 4 Continuous-Time Markov Chains
S3:AH-2A+2H+2BC 
k12 k21
k21 
AH+ H+
H+ 
H+ 
H+ 
A H+ 
H+ 
k12 
AH+ H+
H+ 
H+ H+ 
AH+ H+ 
S4:AH-1A+1H+1BC S5:AH+1AH+1BC
k31 k13
k21 
k31 k13
k13 
H+ H+ 
H+ 
H+ B
H+ 
A H+ 
H+ H+ 
H+ 
k12 
H+ 
H+ B
AH+ H+
H+ 
k31 
H+ 
B 
H+ B H+ 
H+ 
H+ H+ 
S2:AH-2A+1H+1B+1C S6:AH-1AH+1B+1C S7:AH-2AH+1B+2C 
k34 k43
k21 
k34 k43
H+ H+ 
H+ 
H+ C
H+ 
A H+ 
H+ H+ 
H+ 
k12 
H+ 
H+ C H+ 
AH+ H+
S1:AH-2A+1H+2BC+1 S10:AH-1AH+1BC+1 S8:AH-2AH+2B+1C+1
S8:AH-2AH+2BC+2 
k34 k43
H+
C
H+ C H+ 
H+
H+ H+
A H+ 
H+ 
H+ H+ H+ 
H+ 
A H+ 
k34 k43
H+
C
H+ B H+ 
H+
H+ H+
Fig. 4.5 Schematic representation of reaction as a continuous Markov Chain under action.H+
1 = 54.4 Chemical Reaction Markov Models 81
.Q(H+
2 ) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−1.342 × 104 3.8 × 10−4 0 0 0 0 0 0 01.342 × 104
4.5 × 10−5 −1.3421 × 104 0 8.2533 × 10−2 0 1.342 × 104 0 0 00
0 0 −2.684 × 104 2.684 × 104 0 00 0 0 0
0 8.5 × 10−2 4.7 × 104 −6.031 × 104 1.331 × 104 00 0 0 0
0 0 09.4 × 104 −9.4 × 104 1.7 × 10−1 0 0 00
0 4.7 × 104 0 08.1191 × 10−1 −4.7 × 104 8.5 × 10−2 0 04.5 × 10−5
0 0 0 0 01.6671 −1.6641 4.5 × 10−5 0 0
0 0 0 0 0 03.8 × 10−4 −8.2576 × 10−1 4.5 × 10−5 8.151 × 10−1
0 0 0 0 0 0 03.8 × 10−4 −3.8 × 10−4 0
4.7 × 104 0 0 0 0 0 08.5 × 10−2 0 −4.7 × 104
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.Q(H+
3 ) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−1.419 × 105 3.8 × 10−4 0 0 0 0 0 0 01.419 × 105
4.5 × 10−5 −1.419 × 105 0 9.2235 × 10−1 0 1.419 × 105 0 0 00
0 0 −2.838 × 105 2.838 × 105 0 00 0 00
0 8.5 × 10−2 4.7 × 104 −1.878 × 105 1.408 × 105 00 0 00
0 0 09.4 × 104 −9.4 × 104 1.7 × 10−1 0 0 00
0 4.7 × 104 0 09.081 × 10−1 −4.7001 × 104 8.5 × 10−2 0 04.5 × 10−5
0 0 0 0 01.8589 −1.8589 4.5 × 10−5 0 0
0 0 0 0 0 03.8 × 10−4 −8.1935 × 10−2 4.5 × 10−5 8.151 × 10−2
0 0 0 0 0 0 03.8 × 10−4 −3.8 × 10−4 0
4.7 × 104 0 0 0 0 0 08.5 × 10−2 0 −4.7 × 104
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
The probability transition matrix .Π∗ is calculated by the Chapman–Kolmogorov
equation in Eq. (4.2.6).
.Π∗(H+
1 ) =
⎡
⎢
⎢
⎢
⎢
⎣
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
0.0402 0.3984 0.4399 0.0014 0.0001 0.0065 0.0805 0.0172 0.0020 0.0007
⎤
⎥
⎥
⎥
⎥
⎦
,
.Π∗(H+
2 ) =
⎡
⎢
⎢
⎢
⎢
⎣
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
0.0051 0.0339 0.5747 0.3282 0.0465 0.0097 0.0005 0.0001 0.00001 0.0014
⎤
⎥
⎥
⎥
⎥
⎦
,
.Π∗(H+
3 ) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
0.0007 0.0027 0.1970 0.4906 0.3048 0.0033 9.0×10−6 4.8×10−6 5.6×10−7 0.0009
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
For every action.a ∈ A and state.s ∈ S the reward.rik is given by the total number
of molecules, i.e.,82 4 Continuous-Time Markov Chains
.r(a) =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
(A+1)(H++2)+(C+1)
(B+1)+((B+1)(H++2))+((A+1)(H++2))
(A+2)(H++2)
(AH+−1)+(AH+−1)+(A+1)(H++1)
(AH+)+(AH+)
(B+1)(H++1)+(AH+−1)+(AH+−1)+(B+1)
(B+2)(H++2)+(B+1)
(C+1)+(B+1)+((H++2)(B+1))
C+1
(AH+−1)+(AH+−1)
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,∀a ∈ A
.r(H+
1 ) =
⎡
⎢
⎢
⎢
⎢
⎣
8
15
14
8
4
9
15
9
1
2
,
⎤
⎥
⎥
⎥
⎥
⎦
,r(H+
2 ) =
⎡
⎢
⎢
⎢
⎢
⎣
123
15219
244
123
4
14765
30257
15008
1
2
⎤
⎥
⎥
⎥
⎥
⎦
,r(H+
3 ) =
⎡
⎢
⎢
⎢
⎢
⎣
533
1065
1064
533
4
534
1065
534
1
2
⎤
⎥
⎥
⎥
⎥
⎦
.
The functional to minimize is the expected average total number of molecules,
given a set of admissible values for the pH. Solving the linear programing problem
given the constraints described in Eqs. (4.3.1), (4.3.2) and (4.3.3), we obtain the
following results:
.c =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
3.21×10−3 4.65×10−3 1.12×10−14
3.20×10−2 3.08×10−2 7.42×10−5
3.46×10−2 5.29×10−1 1.12×10−14
4.09×10−14 3.03×10−1 6.14×10−13
4.02×10−14 4.29×10−2 7.79×10−12
1.54×10−3 7.87×10−3 2.23×10−13
6.85×10−3 4.06×10−14 2.40×10−5
1.50×10−3 3.12×10−12 7.22×10−6
4.05×10−14 3.49×10−12 1.79×10−4
4.01×10−14 1.00×10−3 3.73×10−4
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,d =
⎡
⎢
⎢
⎢
⎢
⎣
0.4081 0.5919 0
0.5095 0.4893 0.0012
0.0615 0.9385 0
010
010
0.1637 0.8363 0
0.9965 0 0.0035
0.9952 0 0.0048
001
0 0.7302 0.2698
⎤
⎥
⎥
⎥
⎥
⎦
.
The Fig. 4.6, shows the CTMC generated by.Q(d) using a mixed strategy.d, and
using a pure strategy (.Q(H+
1 )). Where the states.9 and .10 have a small number of
total molecules when the CTMC is generated by.Q(d).
Fig. 4.6 Comparison between CTMC for the embedded chain for the state, under a pure determin￾istic action.H+
1 and.Q(H+
1 ) (line black), and the CTMC for the embedded chain under the strategy
.d, mixed action,.Q(d)References 83
References
1. Aragon-Gómez, R., Clempner, J.B.: Traffic-signal control reinforcement learning approach for
continuous-time markov games. Eng. Appl. Artif. Intell. 89, 103415 (2020)
2. Borkar, V.S.: Topics in Controlled Markov Chains. Longman Sc & Tech (1991)
3. Buchholz, P., Schulz, I.: Numerical analysis of continuous time markov decision processes
over finite horizons. Comput. Oper. Res. 38(3), 651–659 (2011)
4. Carrillo, L., Escobar, J.A., Clempner, J.B., Poznyak, A.S.: Optimization problems in chemical
reactions using continuous-time markov chains. J. Math. Chem. 54, 1233–1254 (2016)
5. Cavazos-Cadena, R.: Recent results on conditions for the existence of average optimal station￾ary policies. Ann. Oper. Res. 28(1–4), 3 (1991)
6. Clempner, J.B.: A continuous-time markov stackelberg security game approach for reasoning
about real patrol strategies. Int. J. Control 91(11), 2494–2510 (2018)
7. Clempner, J.B.: Learning attack-defense response in continuous-time discrete-states stack￾elberg security markov games. J. Exp. Theor. Artif. Intell. (2022). https://doi.org/10.1080/
0952813X.2022.2135615
8. Didier, F., Henzinger, T.A., Mateescu, M., Wolf, V.: Fast adaptive uniformization of the chem￾ical master equation. In: 2009 International Workshop on High Performance Computational
Systems Biology, pp. 118–127. IEEE, Trento, Italy (2009)
9. van Dijk, N.M.: Approximate uniformization for continuous-time markov chains with an appli￾cation to performability analysis. Stoch. Process. Appl. 40(2), 339–357 (1992)
10. Feinberg, E.A., Mandava, M., Shiryaev, A.N.: Sufficiency of markov policies for continuous￾time markov decision processes and solutions to kolmogorov’s forward equation for jump
markov processes. In: 2013 IEEE 52nd Annual Conference on Decision and Control (CDC),
pp. 5728–5732. IEEE (2013)
11. González, R.C., Clempner, J.B., Poznyak, A.S.: Solving traffic queues at controlled-signalized
intersections in continuous-time markov games. Math. Comput. Simul. 166, 283–297 (2019)
12. Goss, P.J.E., Peccoud, J.: Quantitative modeling of stochastic systems in molecular biology by
using stochastic petri nets. Proc. Natl. Acad. Sci. 95(12), 6750–6755 (1998)
13. Grassmann, W.K.: Finding transient solutions in markovian event systems through randomiza￾tion. California State University, San Bernardino, Technical report (1991)
14. Guo, X., Hernández-Lerma, O.: Continuous-time controlled markov chains. Ann. Appl. Probab.
13(1), 363–388 (2003)
15. Guo, X., Hernández-Lerma, O.: Continuous-time Markov Decision Processes. Springer (2009)
16. Hill, C.G., Root, T.W.: Introduction to Chemical Engineering Kinetics and Reactor Design.
Wiley (2014)
17. Janssen, J.: Book review: discrete-time markov control processes: basic optimality crite￾ria. o. hernandexz-lerma and j.-b. lasserre. Springer, Berlin (1996). xiv+216pp. dm84 (hard￾cover/softcover) ISBN 0-387-94579-2. Appl. Stoch. 12(4), 281–282 (1996)
18. Jensen, A.: Markoff chains as an aid in the study of markoff processes. Scand. Actuar. J. 87–91
(1953)
19. Koeppl, H., Densmore, D., Setti, G., di Bernardo, M.: Design and Analysis of Biomolecu￾lar Circuits: Engineering Approaches to Systems and Synthetic Biology. Springer Science &
Business Media (2011)
20. Miguel, M.d.G.M., Formosinho, S.J.: Markov chains for plotting the course of complex reac￾tions. J. Chem. Educ. 56(9), 582 (1979)
21. Miller, B.L.: Finite state continuous time markov decision processes with an infinite planning
horizon. J. Math. Anal. 22(3), 552–569 (1968)
22. Nazarathy, Y., Weiss, G.: The asymptotic variance rate of the output process of finite capacity
birth-death queues. Queueing Syst. 59(2), 135–156 (2008)
23. Octave, L.: Chemical Reaction Engineering. Wiley (1999)
24. Paulsson, J.: Summing up the noise in gene networks. Nature 427(6973), 415–418 (2004)
25. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley (2014)84 4 Continuous-Time Markov Chains
26. Rao, C.V., Wolf, D.M., Arkin, A.P.: Control, exploitation and tolerance of intracellular noise.
Nature 420(6912), 231–237 (2002)
27. Ross, S.M.: Introduction to Stochastic Dynamic Programming. Academic (2014)
28. Sutherland, J.W., Michael, J.V.: The kinetics and thermodynamics of the reaction h+ nh3 nh2+
h2 by the flash photolysis shock tube technique: Determination of the equilibrium constant,
the rate constant for the back reaction, and the enthalpy of formation of the amidogen radical.
J. Chem. Phys. 88(2), 830–834 (1988)
29. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the bargaining approach for equalizing
the ratios of maximal gains in continuous-time markov chains games. Comput. Econ. 54, 933–
955 (2019)
30. Turner, T.E., Schnell, S., Burrage, K.: Stochastic approaches for modelling in vivo reactions.
Comput. Biol. Chem. 28(3), 165–178 (2004)
31. Van Moorsel, A.P.A., Sanders, W.H.: Transient solution of markov models by combining adap￾tive and standard uniformization. IEEE Trans. Reliab. 46(3), 430–440 (1997)
32. Wilkinson, D.J.: Stochastic Modelling for Systems Biology. CRC Press (2011)Chapter 5
Nash and Stackelberg Equilibrium
Abstract We provide an approach to locating the Nash equilibrium in this chapter.
The technique depends on identifying a scalar.λ∗ and the associated strategies.d∗(λ∗)
fixing particular boundaries (min and max) that belong to the Pareto front. Bounds
refer to limits placed by the player over the Pareto front that form a specific decision
region where the strategies can be selected. We first use a nonlinear programming
issue to illustrate the Pareto front of the game, introducing a set of linear constraints
for the Markov chain game based on the.c-variable technique. We suggest using the
Euler method and a penalty function with regularization to solve the strong Nash
equilibrium issue. The convergence to a single (strong) equilibrium point is ensured
using Tikhonov’s regularization method. The subsequent single-objective restricted
problems that result from using the regularized functional of the game were then
solved using a nonlinear programming technique. We use the gradient approach
to resolve the first-order optimality requirements in order to accomplish the aim.
The approach solves an optimization issue by adding linear constraints necessary to
identify the best strong strategy, .d (lambda .d), starting from a utopia point (Pareto
optimum point) given an initial .lambda of the individual objectives. We demon￾strate that the game’s functional in the regularized issue decreases and ultimately
converges, demonstrating the presence and exclusivity of strong Nash equilibrium
(Pareto-optimal Nash equilibrium). We also provide a method for calculating the
Markov chain games’ strong Stackelberg/Nash equilibrium. The minimization of
the .L p−norm, which shortens the distance to the utopian point in Euclidian space,
is taken into consideration while solving the cooperative.n-leaders and.m-followers
Markov game. Next, we formulate a Pareto-optimal solution to the optimization
issue. For finding the strong .L p-Stackelberg/Nash equilibrium, we use a bi-level
programming technique that is carried out through extraproximal optimization.
5.1 Optimization and Equilibrium
TheNash equilibrium[16, 18], named after the mathematician John Nash, is the most
typical technique to characterize the solution of a non-cooperative game involving
two or more players in game theory [24]. Each player in a Nash equilibrium is
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable
Markov Chains, Studies in Systems, Decision and Control 504,
https://doi.org/10.1007/978-3-031-43575-1_5
8586 5 Nash and Stackelberg Equilibrium
considered to be aware of the equilibrium tactics of the other players, and altering
one’s own strategy will not benefit anybody.1
The current set of strategy options represents a Nash equilibrium if each player has
selected a strategy—an action plan based on what has occurred so far in the game—
and no one can improve their personal expected payoff by altering their strategy
while the other players remain theirs the same.
Remark 5.1 In optimization problems (with one player), the scenario is the same:
if the cost function is strongly convex and we are in the minimal point, then any
movement away from this point will result in the greatest number of payout values.
From this viewpoint, the Nash equilibrium generalizes the conventional optimum
approach to one participant situations.
Remark 5.2 The Nash equilibrium formulation assume that in the realization of the
the game participants play the strategies simultaneously.
Several algorithms have been reported in the literature elaborated for the compu￾tation of one solution of the Nash equilibrium problems. For instance, Nabetani et al.
[17] provided two different types of parametrized variational inequalities related to
the generalized Nash equilibrium problem, one price-directed and the other resource￾directed, showing that under mild constraints their solutions yield all the generalized
Nash equilibria. Facchinei and Lampariello [14] suggested a new approach for the
computation of non-variational solutions of jointly convex problems. Dreves et al.
[12] presented a constrained optimization reformulation of the player convex gener￾alized Nash equilibrium problem using a regularized Nikaido-Isoda function. Clemp￾ner and Poznyak [4] presented a natural existence of the Nash equilibrium point is
ensured by definition using Lyapunov games. Gabriel et al. [15] provided a method￾ology to solve Nash-Cournot energy production games allowing some variables to be
discrete. Facchinei et al. [13] proposed to solve a general quasi-variational inequality
by using its Karush-Kuhn-Tucker conditions employing a globally convergent algo￾rithm. Clempner [2] suggested that the stability conditions and the equilibrium point
properties of Cournot and Lyapunov meet in potential games. Clempner and Poznyak
[6] proposed a method for computing the strong Nash equilibrium for Markov chains
games. Trejo et al. [21, 22] extended the method of Antipin [1] for computing the
strong equilibrium for Stackelberg-Markov chains games. Dreves [11] considering
linear generalized Nash equilibrium problems designed an algorithm that is able
to compute the entire solution set of Nash equilibrium problems. Clempner and
Poznyak [5, 7] showed that for Markov games the best-reply strategies lead neces￾sarily to a Lyapunov/Nash equilibrium point. Trejo et al. [23] proposed a method
for computing the Lp-strong Nash equilibrium for Markov chains games. The same
authors [10] presented a procedure to construct the Pareto frontier and efficiently
compute the strong Nash equilibrium for a class of discrete-time ergodic control￾lable Markov-chain games. Clempner [3] proposed an algorithm for computing the
1 The concept of Nash equilibrium dates back to Cournot, who used it to explain how rival enterprises
choose their outputs in 1838 [19].5.2 ε-Nash Equilibrium and Tanaka’s Function 87
Nash equilibrium based on an iterative approach of both the proximal and the gradient
method.
The main results of this chapter are as follows:
– Presents the extraproximal method for computing the Nash and Stackelberg equi￾libria.
– Transforms the game theory problem into a system of equations, in which each
equation itself is an independent optimization problem.
– Each equation in this system is an optimization problem for which the necessary
condition of a minimum is solved using the projectional gradient method.
– Conceptualizes the problem as a poly-linear programming problem. This general
framework captures most of the basic Markov game models.
– Restricts the solution to a class of homogeneous, finite, ergodic and controllable
Markov chains.
– Regularizes the poly-linear functional employing a Lagrange regularization method
for ensuring the method to converge to some of the Nash and Stackelberg equilib￾ria.
– Provides the convergence analysis of the method and compute the rate of conver￾gence of the step-size parameter.
– Shows that the iterated approach provides a quick rate of convergence in the
numerical simulation.
5.2 .ε-Nash Equilibrium and Tanaka’s Function
5.2.1 Individual Cost Function
The dynamic of the game for Markov chains is described as follows. The game
consists of a set of .N = {1, ..., n} players (indexed by .l = 1, n). Below we will
consider only stationary strategies.dl
kl|il
(t) = dl
kl|il
. In the ergodic case we have (see
Chap. 1)
.Pl (
sjl
)
=
∑
Nl
il=1
⎛
⎝∑
Ml
kl=1
πl
jl|il kl
dl
kl|il
⎞
⎠ Pl (
s(il)
)
. (5.2.1)
The cost function of each player, depending on the states and actions of all partici￾pants, are given by the values.vl
(i1,k1,j1;..;in ,kn ,jn ), which describes the cost for the player
.l when the each participant .q = 1, ..., n of the this game realizes the transfer from
the state.iq to the state. jq after the application of the action.kq . So, the “average cost
function”.Jl for each player.l in the stationary regime can be expressed as88 5 Nash and Stackelberg Equilibrium
.
Jl (
c1, .., cn
)
:= ∑
i1,k1,j1
··· ∑
in ,kn ,jn
vl
(i1,k1,j1;..;in ,kn ,jn )
|n
q=1
πq
jq |iq ,kq
dq
kq |iq Pq (
siq
)
 
c
q
iq kq
=
∑
i1,k1,j1
··· ∑
in ,kn ,jn
vl
i1,k1,j1;..;in ,kn ,jn
|n
q=1
πq
jq |iq ,kl
 
wl
i1,k1,j
1;..;i n ,kn ,jn
c
q
iq kq =
∑
i1,k1,j1
··· ∑
in ,kn ,jn
wl
i1,k1,j1;..;in ,kn ,jn
|n
q=1
c
q
iq kq =
∑
i1,k1
··· ∑
in ,kn
⎛
⎝ ∑
j1,...,jn
wl
i1,k1,j1;..;in ,kn ,jn
⎞
⎠
 
w˜l
(i1,k1;..;in ,kn )
|n
q=1
c
q
iq kq
,
or equivalently,
. Jl (
c1
, .., cn)
:=∑
i1,k1
···∑
in ,kn
w˜l
i1,k1;..;in ,kn
|n
q=1
c
q
iq kq
, (5.2.2)
where.cq := 
 c
q
il kl
 
 
il=1,Nl;kl=1,Ml
is a matrix with elements
. c
q
il kl = dq
kl|il
Pq (
sil
)
, , (5.2.3)
satisfying (see Chap. 1)
. cl ∈ Cl
adm=
⎧
⎪⎪⎨
⎪⎪⎩
cl :
∑
il,kl
cl
il kl
=1, cl
il kl ≥ 0,
∑
kl
cl
jl kl
= ∑
il,kl
πl
jl|il kl
cl
il kl
,
(5.2.4)
and
.
w˜l
(i1,k1;..;in ,kn ) = ∑
j1,...,jn
wl
(i1,k1,j1;..;in ,kn ,jn ),
wl
i1,k1,j1;..;in ,kn ,jn = vl
i1,k1,j1;..;in ,kn ,jn
|n
q=1
πq
jq |iq kq
.
(5.2.5)
Remark 5.3 Notice that for a single participant case.(n = 1) the expression (5.2.4)
becomes5.2 ε-Nash Equilibrium and Tanaka’s Function 89
.
Jl=1 (
c1
)
:= ∑
i1,k1
w˜ 1
i1,k1
c1
i1k1
,
w˜ 1
i1,k1 =∑
j1
w1
i1,k1,j1 =∑
j1
v1
(i1,k1,j1)π1
j1|i1k1
,
which coincides with the definitions in Chap. 1.
The individual aim of each participant is
. Jl
(c1
, .., cn) → min
cl∈Cl
adm
. (5.2.6)
Obviously, here we have the conflict situation which can be resolved by the Nash
equilibrium concept discussed above.
Remark 5.4 We have the following Nash equilibrium definitions:
A Nash equilibrium is a strategy.c∗ = (
c0∗, .., cn∗
)
such that
.J
(
c1∗, .., cn∗)
≤ J
(
c1∗, .., cl
, ..., cn∗) for cl ∈ Cadm.
A strong Nash equilibrium (global unique) is a strategy.c∗∗ = (
c1∗∗, .., cn∗∗)
such
that there does not exist any.cl ∈ Cadm
.J
(
c1∗∗, .., cl
, ..., cn∗∗)
< J
(
c1∗∗, .., cn∗∗)
.
5.2.2 Regularized Lagrange Function
The local optimal condition (5.2.6) can be equivalently represented, using the regu￾larized Lagrange function method [8, 9], as
.
Ll
δ(c1, .., cn|λ0,l
, λl,jl,l = 1, n, jl = 1, Nl) =
Jl
δ(c1, .., cn) + ∑n
l=1
λ0,l
(
∑
il,kl
cl
il kl − 1
)
+
∑n
l=1
∑
jl
λl,jl
(
∑
kl
cl
jl kl − ∑
il,kl
πl
jl|il kl
cl
il kl
)
− δ
2
∑n
l=1
∑
jl
(
λl,jl
)2 → min ci
l kl ≥0,λ0,l,λl,j
l
,
(5.2.7)
where
.Jl
δ(c1
, .., cn) = Jl
(c1
, .., cn) +
δ
2
∑n
l=1
⎛
⎝∑
Nl
il=1
∑
Ml
kl=1
(
cl
il kl
)2
⎞
⎠ , δ > 0 (5.2.8)90 5 Nash and Stackelberg Equilibrium
and the regularized Lagrange function.Ll
δ includes only the equality constraints from
.Cl
adm.
5.2.3 Tanaka’s Function
To simplify the description let us introduce below the new variables
.
xl := col c(l) =
(
c
(l)
11 , ..., c
(l)
1Ml
;...; c
(l)
Nl 1, ..., c
(l)
Nl Ml
) 
,
Xl := Xl
+ = {xl
1 ≥ 0, ..., xl
Nl Ml ≥ 0}.
⎫
⎪⎬
⎪⎭
(5.2.9)
Consider a multi-players game with the joint strategy
.x = (x 1
, ..., x n) ∈ X := n
l=1
Xl
.
The players are trying to reach one of .ε-Nash equilibria, that is, find a joint strat￾egy .x ∈ X satisfying for any admissible .xl ∈ Xl and any .l = 1, n the system of
inequalities (the.ε-Nash equilibrium condition)
.
gl
δ
(
yl
, x ˆl
|λ0,l
, λl,jl
)
≤ ε for any yl ∈ Xl and all l = 1, n, jl = 1, Nl, ε ≥ 0,
gl
δ
(
yl
, x ˆl
|λ0,l
, λl,jl
)
:= Ll
δ(x|λ0,l
, λl,jl) − Ll
δ(yl
, x ˆl
|λ0,l
, λl,jl),
(5.2.10)
where.x ˆl is a strategy of the rest of the followers adjoint to.xl
, namely,
.x ˆl := (
x 1
, ..., xl−1
, xl+1
, ..., x n)
∈ Xˆl := n
m=1,m/=l
X m,
Here .Ll
δ(yl
, x ˆl
|λ0,l
, λl,jl) is the Lagrange function of the player .l which plays the
strategy.yl ∈ Xl and the rest of the players the strategy.x ˆl ∈ Xˆl
.
Lemma 5.1 ([20]) The joint strategy .x ∈ X (5.2.10) is an .ε-Nash equilibrium if
and only if
.
Gδ(x, y|λ) := ∑n
l=1
gl
δ
(
yl
, x ˆl
|λ0,l
, λl,jl
)
=
∑n
l=1
[
Ll
δ(x|λ0,l
, λl,jl) − Ll
δ(yl
, x ˆl
|λ0,l
, λl,jl)
]
≤ ε
⎫
⎪⎪⎬
⎪⎪⎭
(5.2.11)5.2 ε-Nash Equilibrium and Tanaka’s Function 91
for all.y ∈ X, where.λ = [λ0,l
, λl,jl]l=1,n,jl=1,Nl
. The function.Gδ(x, y|λ) is referred
to as a regularized Tanaka’s function , which for.δ = 0 and .λ = 0 is the original
Tanaka’s function [20].
Proof Summing (5.2.10) implies (5.2.11). And inverse, taking.ym = xm for all.m /= l
in (5.2.11), which is valid for any admissible.yl
, we obtain (5.2.10). This means that
.x belongs to the.ε-Nash equilibrium set.. 
Notice that the condition.Gδ(x, y) ≤ ε (5.2.11), is equivalent to
. max
λ0,l,λl,j
l
max
y∈X
Gδ(x, y|λ0,l
, λl,jl
) ≤ ε. (5.2.12)
So, any strategy.x ∈ X is a Nash equilibrium if it satisfies (5.2.12).
Remark 5.5 The considered multiparticipant game has at least one.ε-Nash equilib￾rium if
. min
x∈X
(
max
λ0,l,λl,j
l
max
y∈X
Gδ(x, y|λ0,l
, λl,jl
)
)
≤ ε. (5.2.13)
5.2.4 ASG Continuous-Time Algorithm
Let us consider the following continuous-time procedure
.
x˚ (t) = (t + θ)
−1 [ζx (t) − x (t) − ηxC] ,
xˆ (t) := x (t0) + t
τ=t0
x˚ (τ ) dτ ,
x (t) = [ˆx (t)]+, x (t0) ∈ X, d
dt xˆ (t) = x˚ (t),
˙
ζx (t) = −∂xGδ(x, y|λ0,l
, λl,jl),
∂xGδ(x, y|λ0,l
, λl,jl) ∈ Dx
(
x, y|λ0,l
, λl,jl
)
, ζx (t0) = 0,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.2.14)
and92 5 Nash and Stackelberg Equilibrium
.
y˚ (t) = (t + θ)
−1 [
ζy (t) + y (t) + ηy
]
,
yˆ (t) := y (t0) + t
τ=t0
y˚ (τ ) dτ , X = Y,
y (t) = [ˆy (t)]+, y (t0) ∈ Y, d
dt yˆ (t) = y˚ (t)
˙
ζy (t) = ∂yGδ(x, y|λ0,l
, λl,jl),
∂yGδ(x, y|λ0,l
, λl,jl) ∈ Dy
(
x, y|λ0,l
, λl,jl
)
, ζy (t0) = 0,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.2.15)
where the operator.[·]+ makes each component equal to zero if the corresponding
argument is negative,.Dx
(
x, y|λ0,l
, λl,jl
)
and.Dy
(
x, y|λ0,l
, λl,jl
)
are the sets of the
subgradients in the corresponding points, and the Lagrange multipliers.λ0,l and.λl,jl
evolve according to the following differential rule:
.
d
dt
λ0,l = −γ0,l
∂λ0,l Gδ(x, y|λ0,l
, λl,jl), γ0,l > 0,
d
dt
λl,jl = −γl,jl ∂λl,j
l Gδ(x, y|λ0,l
, λl,jl), γl,jl > 0
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(5.2.16)
with any fixed initial conditions.
Theorem 5.1 (On the functional convergence of the AGS algorithm) Under the
conditions of the Tanaka’s function convexity, which results from the polylinearity
property (5.2.2) of.Jl (
c1, .., cn
)
on.Cl
adm, and in view of the Lemma 8.2, for any.ε > 0
exists .θ ≥ c
ε
and any constant vectors .ηx , ηy ∈ RN , the ASG strategies (5.2.14),
(5.2.15) guarantee the following property for any.t ≥ t0
. max
y∈X
Gδ(xˆ (t), y) ≤ ε (5.2.17)
where
. c :=
1
2 |(x∗ + ηx )|2 +
1
2
 
 y∗ + ηy
 
 
2 + (t0 + θ) Gδ(x, y)|t0 (5.2.18)
and.x∗, y∗ defined by
.
x∗ = arg minx∈X maxy∈X Gδ(x, y),
y∗ = arg maxy∈X minx∈X Gδ(x, y).
⎫
⎬
⎭
(5.2.19)5.3 Extraproximal Method 93
Remark 5.6 Notice that the points.x∗ and.y∗ satisfy the saddle point condition
.Gδ(x∗, y) ≤ Gδ(x∗, y∗) ≤ Gδ(x, y∗) (5.2.20)
for all.x, y ∈ X.
As it follows from the inequality (5.2.17), the trajectories.xˆ (t),.t ≥ t0, belong to
the set of.ε-Nash equilibrium points. But we are interesting in the trajectories.x (t),
.t ≥ t0. What can we say about this?
The next Lemma responds this questions.
5.3 Extraproximal Method
The Extraproximal Method for the conditional optimization problems was suggested
in [1]. We describe below this method for the Stackelberg-Nash game in a general
format.
5.3.1 Proximal Format
The problem
. min
x∈X
(
max
λ≥0
max
y∈X
Gδ(x, y|λ)
)
(5.3.1)
can be expressed in proximal format as (see, [1, 21])
.
λ∗
δ = arg max λ≥0
{
−1
2 |λ − λ∗
δ|2 + γGδ(x∗
δ , y∗
δ |λ)
}
,
x∗
δ = arg min x∈X
{ 1
2 |x − x∗
δ |2 + γGδ(x, y∗
δ |λ∗
δ )
}
,
y∗
δ = arg max
y∈X
{
−1
2 |y − y∗
δ |2 + γGδ(x∗
δ , y|λ∗
δ )
}
,
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(5.3.2)
where the solutions.x∗
δ ,.y∗
δ and.λ∗
δ depend on the small parameters.δ, γ > 0.
The general format iterative version .(n = 0, 1, ...) of the extraproximal method
for computing the Nash equilibrium with some fixed admissible initial values
.(x0, y0 ∈ X and.λ0 ≥ 0) is as follows:94 5 Nash and Stackelberg Equilibrium
1. The first half-step (prediction):
.
λ¯ n = arg max λ≥0
{ 1
2 |λ − λn|2 + γGδ(xn, yn|λ)
}
,
x¯n = arg min x∈X
{ 1
2 |x − xn|2 + γGδ(x, yn|λn)
}
,
y¯n = arg max
y∈X
{
−1
2 |y − yn|2 + γGδ(xn, y|λn)
}
.
(5.3.3)
2. The second (basic) half-step
.
λn+1 = arg max λ≥0
{ 1
2 |λ − λn|2 + γGδ(x¯n, y¯n|λ)
}
,
xn+1 = arg min x∈X
{ 1
2 |x − xn|2 + γGδ(x, y¯n|λ¯ n)
}
,
yn+1 = arg max
y∈X
{
−1
2 |y − yn|2 + γGδ(x¯n, y|λ¯ n)
}
.
(5.3.4)
Remark 5.7 The direct one-step procedure, when .λn+1 = λn, does not work (see
the counter-example in [1]).
5.4 Numerical Example: Strong Nash Equilibrium in
Pareto Front
5.4.1 Euler Approach
Let us define the regularized penalty function as follows
.
Φδ (λ) := Φ (λ) +
δ
2 |λ|2 , 0 < δ < 1,
Φ (λ) := ∑n
l
Φl (λ),
Φl (λ) := 1
2
[
Jl (v∗ (λ)) − Jl+]2
+ + 1
2
[
Jl− − Jl (v∗ (λ))]2
+ ,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.4.1)
where
. [ζ]+ :=
⎧
⎨
⎩
ζ if ζ ≥ 0,
0 if ζ < 0.
(5.4.2)5.4 Numerical Example: Strong Nash Equilibrium in Pareto Front 95
The minimizing solution.λ∗ can be expressed mathematically as follows:
.λ∗ ∈ Arg min
λ∈[0,1]
Φ (λ) = Arg min
λ∈[0,1]
Φδ=0 (λ). (5.4.3)
As it has been mentioned above.λ∗, satisfying (5.4.3), may be not unique that provokes
several problems for the numerical procedure implementation. Taking.δ > 0 we can
guarantee the uniqueness of the solution so that we will try to find
.λ∗∗
δ = arg min
λ∈SN
Φδ (λ), δ > 0. (5.4.4)
This solution corresponds to one of the solutions.λ∗ (5.4.3) which has the minimal
norm of the vector.|˘|2
.
To find.λ∗∗
δ let us apply the following numerical procedure
.
λn+1 = PrSN
[
λn − γn
Φ'
δ (λn)
|
|Φ'
δ (λn)
|
| + ε
]
,
λ0 = (1/n, ...1/n), n = 0, 1, ...
γn > 0,
∑∞
n=0
γn = ∞,
where .Pr is the projection operator into the simplex .S N . Notice that the derivative
.Φ'
δ (λn) is
.
Φ'
δ (λn) = d
dλΦδ (λn) = δ (2λn − 1) +
∑n
l=1
d
dλ Jl (v∗ (λn)) ([
Jl (v∗ (λn)) − Jl+]
+ + [
Jl− − Jl (v∗ (λn))]
+
)
,
where the terms.
d
dλ Jl (v∗ (λn)) may be approximated by the Euler method as
.
d
dλ Jl (
v∗ (λn)
)
 ε−1 [
Jl (
v∗ (λn + ε)
)
− Jl (
v∗ (λn)
)],0 < ε < 1.
Finally, the suggested numerical procedure with .γn = γ for finding .λ∗∗
δ looks as
follows96 5 Nash and Stackelberg Equilibrium
.
˘n+1 = PrSN
{
˘n − γn∇Φ˜δ,ε (˘n)
}
,
˘0 = (1/n, ...1/n), n = 0, 1, ...,
∇Φ˜δ,ε (˘n) := ( ∂
∂λ1 Φ˜δ,ε (˘n), ∂
∂λ2 Φ˜δ,ε (˘n), ..., ∂
∂λN Φ˜δ,ε (˘n)
) 
,
where
∂
∂λi
Φ˜δ,ε (˘n) :=
ε−1 ∑n
l=1
[
Δi Jl (˘n, ε)
] ([
Jl (v∗ (˘n)) − Jl+]
+ − [
Jl− − Jl (v∗ (˘n))]
+
)
+ δ
2
∂
∂λi |˘|2 ,
with Δi Jl (˘n, ε) := [
Jl (v∗ (˘n + εei)) − Jl (v∗ (˘n))]
,
and ei :=
⎛
⎝0, ..., 0, 1 
i
, ..., 0
⎞
⎠
 
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.4.5)
Theorem 5.2 (Uniqueness of the SNE) Let .Φδ (λ) be a continuous and strictly
concave function over the Pareto front .P. Then, the Markov chains game has one
strong Nash equilibrium.
Proof The strategy .v∗
δ is a strong Pareto policy because every point on the Pareto
front.J(P) is regularized and satisfies that.J(vδ) < J(v), specifically.J(v∗
δ ) < J(v).
The continuity of function.Φ (λ) is given by its definition and the construction of the
Pareto front.P established by
.
{
v∗
δ , λ∗
δ , μ∗
δ , θ∗
δ
}
= arg min
λ∈SN ,v≥0,μ
max
θ≥0
{
Lδ, (v, λ, μ, θ)
}
.
The functional .Φ (λn) is monotonically non-increasing and bounded from below
then, by the Weierstrass theorem it converges, i.e. . lim
n→∞Φ (λn) → Φ (λ). The con￾straints (5.2.4) are linear and therefore they are convex. Since the intersection of
convex sets is convex,.V = Cadm is convex. The strict convexity of the Pareto front
.J(P) is ensured by the introduction of the Tikhonov’s regularizator .
δ
2 |λ|2 over
.Φ (λ). Then, by convexity.Φδ (λ) converges to a unique equilibrium point.. 
5.4.2 Numerical Data
Let us consider a game with two players (.l = 1, 2) trying to conform a coalition.
Let.N1 = N2 = 6 and.M1 = M2 = 2. The transition matrices of the example are as
follows:5.4 Numerical Example: Strong Nash Equilibrium in Pareto Front 97
.π1
j|i1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1927 0.0659 0.2264 0.1874 0.1606 0.1670
0.0332 0.1716 0.1523 0.3011 0.2377 0.1041
0.0357 0.2689 0.2248 0.1842 0.2087 0.0778
0.3662 0.3868 0.0569 0.0143 0.1572 0.0185
0.2248 0.0560 0.1499 0.3018 0.2330 0.0345
0.7468 0.0732 0.0712 0.0719 0.0064 0.0306
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.π1
j|i2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1937 0.2134 0.1978 0.0332 0.2094 0.1525
0.1469 0.3683 0.0717 0.2308 0.1182 0.0642
0.3138 0.0617 0.0911 0.3169 0.1671 0.0493
0.0138 0.1958 0.2718 0.1361 0.2795 0.1030
0.1138 0.1156 0.1699 0.1518 0.2310 0.2180
0.4672 0.0377 0.0037 0.0051 0.0220 0.4643
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.π2
j|i1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.3819 0.4364 0.0046 0.0205 0.1536 0.0030
0.0824 0.2956 0.0967 0.0183 0.3281 0.1789
0.3265 0.1587 0.2660 0.1865 0.0042 0.0582
0.0958 0.2274 0.2063 0.2133 0.0923 0.1648
0.0780 0.2321 0.1509 0.3704 0.0643 0.1043
0.0468 0.0022 0.0030 0.0058 0.0354 0.9068
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.π2
j|i2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0478 0.0817 0.0572 0.4207 0.0975 0.2949
0.1998 0.2205 0.2569 0.0801 0.2040 0.0387
0.1916 0.2289 0.0445 0.1105 0.0619 0.3627
0.0248 0.2845 0.2294 0.2369 0.0403 0.1842
0.0825 0.0282 0.2944 0.1554 0.3131 0.1264
0.8611 0.0381 0.0059 0.0009 0.0499 0.0441
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
The individual cost matrices are as follows
.J 1
i j,1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
2 2 10 2 8 6
2 6 10 0 6 8
4 16 10 14 0 10
4 2 8 8 12 6
14 80 12 18 14 18
4 16 6 12 6 40
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, J 1
i j,2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
4 0 6 6 0 10
4 2 10 4 6 2
6 2 12 6 6 6
10 0 6 8 2 10
8 10 14 10 20 60
80 4 16 12 16 12
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.J 2
i j,1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
91 68 59 31 19 20
352437
279312
2 2 3 5 0 16
1 0 8 29 6 4
4 7 4 83 58 83
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, J 2
i j,2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
142 188 60 20 180 10
2 6 16 14 8 2
8 10 8 4 10 0
2 10 20 6 14 8
6 2 0 8 10 4
116 140 12 16 32 6
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.98 5 Nash and Stackelberg Equilibrium
Fig. 5.1 Pareto.min and
.max bounds J J
F
- J2 1 1
+
+ 2 
+ 
2 
- 
2 
- 
2 
J1 - + 1 1 
Fi 
J F i
F
J
J
F
x1*
P
x2*
Fig. 5.2 Pareto front
Fixing.δ = 0.0001,.γ = 0.6,. = 0.03,.n0 = 100 and the bounds.J1− = 183,.J1+ =
260,.J2 = 3300,.J2 = 3870 (see Fig. 5.1). Then, Fig. 5.2 shows the 2000 points of
the Pareto front, Fig. 5.3 shows the Strong Nash equilibrium, Fig. 5.4 shows the
convergence of the parameter.λ f inal = 0.4994 and finally, Fig. 5.5 shows the con￾vergence of the gradient.
The corresponding strong Pareto policies are as follows5.4 Numerical Example: Strong Nash Equilibrium in Pareto Front 99
Fig. 5.3 Strong Nash
equilibrium
Fig. 5.4 Convergence of
Lambda
. c1∗
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1092 0.1088
0.0889 0.0869
0.0815 0.0744
0.0814 0.0895
0.0915 0.0904
0.0946 0.0029
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, d1∗
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.5008 0.4992
0.5057 0.4943
0.5226 0.4774
0.4764 0.5236
0.5029 0.4971
0.9705 0.0295
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,100 5 Nash and Stackelberg Equilibrium
Fig. 5.5 Convergence of the
Gradient
. c2∗
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0006 0.2328
0.0998 0.0549
0.0007 0.1137
0.0911 0.0968
0.0435 0.0796
0.0011 0.1854
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, d2∗
ik =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0028 0.9972
0.6453 0.3547
0.0061 0.9939
0.4847 0.5153
0.3533 0.6467
0.0059 0.9941
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
and the utility.J (d∗) = (214.9335, 3.4054e + 03).
5.5 The Stackelberg-Nash Equilibrium Concept
5.5.1 Specific Features
The Stackelberg model is a game in which the leader moves first and then the follower
moves sequentially (opposite to Remark 5.2 where all players play simultaneously).
Remark 5.8 In our approach we consider one leader and several followers that play
themselves a Nash game, and it is because we called this type of games Stackelberg￾Nash (sequential) games.
To simplify the descriptions below let us introduce the new variables
.
x := col c(0)
, X := C(0)
adm,
vl := col c(l)
, Vl := C(l)
adm (
l = 1, n
)
.
⎫
⎬
⎭
(5.5.1)5.5 The Stackelberg-Nash Equilibrium Concept 101
Consider a non-zero sum game with a leader whose strategies are denoted by.x ∈ X
and .n followers with strategies .vl ∈ Vl .
(
l = 1, n
)
. Denote by .v = (v1, ..., vn) ∈
V := n
l=1
Vl the joint strategy of the followers.
Let us introduce.vˆl is a strategy of the rest of the followers adjoint to.vl
, namely,
.vˆl := (
v1
, ..., vl−1
, vl+1
, ..., vn)
∈ V ˆl := n
m=1,m/=l
V m,
so that.v = (vl
, vˆl
) for any.l.
The leader is assumed to anticipate the reactions of the followers. They are trying
to reach one of Nash equilibria for any fixed strategy.x of the leader.
5.5.2 Individual Aims and Tanaka’s Representation
Denote by.J 0(x|v) and.Jl
(v|x(v)),.l = 1, ..., n the cost functions for all the partici￾pants.
- Individual aim for the leader:
.J 0
(x|v) → min
x∈X
(5.5.2)
with.x(v) ∈ arg min x'
∈X
J 0(x'
|v) as a solution.
- Individual aim for the followers is to find.vl∗ ∈ Vl such that
.gl
(vl∗, v|x(vl∗, vˆl
)) := Jl
(vl∗, vˆl
|x(vl∗, vˆl
)) − Jl
(vl
, vˆl
|x(vl
, vˆl
)) ≤ ε, ε ≥ 0,
(5.5.3)
valid for all.vl ∈ Vl
,.vˆl ∈ V ˆl and all.l = 1, n. Below.vl∗ will be associated with the
.ε-Nash equilibrium in Stackelberg-Nash game.
The definition of individual aim for the followers can be represented in joint
format, using the regularized Tanaka’s function description, as follows:
. Gδ(v∗, v|λ, x(v∗, v)) := ∑n
l=1
gl
δ
(
vl∗, v|λ0,l
, λl,jl, x(vl∗, vˆl
)
)
≤ ε
 
(5.5.4)
for all.v ∈ V,.λ = [λ0,l
, λl,jl]l=1,n,jl=1,Nl and
.gl
δ
(
vl∗, v|λ0,l
, λl,jl, x(vl∗, vˆl
)
)
= Ll
δ (vl∗|λ0,l
, λl,jl, x(vl∗, vˆl
)) − Ll
δ (v|λ0,l
, λl,jl, x(vl
, vˆl
))
(5.5.5)
where.Ll
δ(v|λ0,l
, λl,jl, x(vl
, vˆl
)) corresponds with the Lagrange function in (5.2.7).102 5 Nash and Stackelberg Equilibrium
The function.Gδ(v∗, v|λ, x(v∗, v))is referred to as a regularized Tanaka’s func￾tion for Stackelberg-Nash game, which for.δ = 0 and.λ = 0 is the original Tanaka’s
function [20].
Definition 5.1 A strategy.x∗ ∈ X of the leader together with the collection.v∗ ∈ V
of the followers is said to be a Stackelberg-Nash equilibrium (.ε = 0) if
.
(
x∗, v∗)
∈Arg max
λ≥0
min
v,vˆ∈V
min
x'
∈X
{
J 0
(x'
|v)|Gδ(v, vˆ|λ, x(v, v)) ˆ ≤0
}
. (5.5.6)
Let us use the Lagrange multipliers approach we can represent (5.5.6) as follows:
.Lδ(x'
, v, v,ˆ μ, λ) → max
μ,λ≥0
min
v,vˆ∈V
min
x'
∈X
, (5.5.7)
where
.Lδ(x'
, v, v,ˆ μ, λ) = J 0
(x'
|v) + μGδ(v, vˆ|λ, x(v, v)). ˆ (5.5.8)
With .δ > 0 the considered functions becomes to be strictly convex providing the
uniqueness of the considered conditional optimization problem (5.5.7). Notice also
that the Lagrange function in (5.5.8) satisfies the saddle-point condition, namely, for
all.x' ∈ X and.λ ≥ 0 we have
.Lδ(x'∗, v∗, vˆ∗, μ, λ)≤Lδ(x'∗, v∗, vˆ∗, μ∗, λ∗)≤Lδ(x'
, v, v,ˆ μ∗, λ∗). (5.5.9)
5.5.3 Extraproximal Procedure
The problem (5.5.7) can be represented in the following proximal format as (see, [1,
21])
.
λ∗ = arg max λ≥0
{
−1
2 |λ − λ∗|2 + γLδ(x'∗, v∗, vˆ∗, μ∗, λ)
}
,
μ∗ = arg max
μ≥0
{
−1
2 |μ − μ∗|2 + γLδ(x'∗, v∗, vˆ∗, μ, λ∗)
}
,
x'∗ = arg min x'
∈X
{ 1
2 |x' − x'∗|2 + γLδ(x'
, v∗, vˆ∗, μ∗, λ∗)
}
,
v∗ = arg min v∈V
{ 1
2 |v − v∗|2 + γLδ(x'∗, v, vˆ∗, μ∗, λ∗)
}
,
vˆ∗ = arg min vˆ∗∈V
{ 1
2 | ˆv − ˆv∗|2 + γLδ(x'∗, v∗, v,ˆ μ∗, λ∗)
}
,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.5.10)
where the solutions.x'∗,.v∗,.vˆ∗,.μ∗, and.λ∗ depend on the small parameter.γ > 0.5.5 The Stackelberg-Nash Equilibrium Concept 103
The general format iterative version .(n = 0, 1, ...) of the extraproximal method
for computing the Stackelberg-Nash equilibrium with some fixed admissible initial
values.(x'
0 ∈ X, v0, ¯
vˆ0 ∈ V and.μ0, λ0 ≥ 0) is as follows:
1. The first half-step (prediction):
.
λ¯ n = arg max λ≥0
{
−1
2 |λ − λn|2 + γLδ(x'
n, vn, vˆn, μn, λ)
}
,
μ¯ n = arg max
μ≥0
{
−1
2 |μ − μn|2 + γLδ(x'
n, vn, vˆn, μ, λn)
}
,
x¯'
n = arg min x'
∈X
{ 1
2 |x' − x'
n|2 + γLδ(x'
, vn, vˆn, μn, λn)
}
,
v¯n = arg min v∈V
{ 1
2 |v − vn|2 + γLδ(x'
n, v, vˆn, μn, λn)
}
,
¯
vˆn = arg min vˆ∗∈V
{ 1
2 | ˆv − ˆvn|2 + γLδ(x'
n, vn, v,ˆ μn, λn)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.5.11)
2. The second (basic) half-step
.
λn+1 = arg max λ≥0
{
−1
2 |λ − λn|2 + γLδ(x¯'
n, v¯n, ¯
vˆn, μ¯ n, λ)
}
,
μn+1 = arg max
μ≥0
{
−1
2 |μ − μn|2 + γL(x¯'
n, v¯n, ¯
vˆn, μ, λ¯ n)
}
x'
n+1 = arg min x'
∈X
{
1
2 |x' − x'
n|2 + γLδ(x'
, v¯n, ¯
vˆn, μ¯ n, λ¯ n)
}
,
vn+1 = arg min v∈V
{
1
2 |v − vn|2 + γLδ(x¯'
n, v, ¯
vˆn, μ¯ n, λ¯ n)
}
,
vˆn+1 = arg min vˆ∗∈V
{ 1
2 | ˆv − ˆvn|2 + γLδ(x¯'
n, v¯n, v,ˆ μ¯ n, λ¯ n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(5.5.12)
Define the extended variables
.
x˜:=
⎛
⎝
x'
v
vˆ
⎞
⎠ ∈ X˜ :=X × V × V, y˜:= (μ
λ
)
∈ Y˜:=R+ × R+,
w˜ =
( w˜ 1
w˜ 2
)
∈ X˜ × Y˜, v˜ =
(v˜1
v˜2
)
∈ X˜ × Y˜,104 5 Nash and Stackelberg Equilibrium
and the functions
.
L˜
δ(x˜, y˜) := Lδ(x'
, v, v,ˆ μ, λ),
Ψδ(w, ˜ v)˜ := L˜
δ(w˜ 1, v˜2) − L˜
δ(v˜1, w˜ 2).
(5.5.13)
For.w˜ 1 .=.x˜,.w˜ 2 .=.y˜,.v˜1 .=.v˜∗
1 .=.x˜∗ and.v˜2 .=.v˜∗
2 .=.y˜∗ we have
.Ψδ(w, ˜ v)˜ := L˜
δ(x˜, y˜
∗) − L˜
δ(x˜∗, y˜). (5.5.14)
In these variables the relation (5.5.10) can be represented in a “short format”as
.v˜
∗= arg min w˜ ∈ X×Y
 
{ 1
2 | ˜w − ˜v∗
|2
+γΨδ(w, ˜ v˜
∗
)
} (5.5.15)
with any positive.γ.
5.6 Convergence Analysis
5.6.1 Auxiliary Results
Let us prove the following auxiliary results.
Lemma 5.2 Let . f (z) be a convex function defined on a convex set .Z. If .z∗is a
minimizer of function
.ϕ(z) =1
2 |z − x|2
+α f (z) (5.6.1)
on.Z with fixed.x, then. f (z) satisfies the inequality
.
1
2 |z∗−x|2
+α f (z∗
) ≤1
2 |z − x|2
+α f (z)−1
2 |z − z∗|2
. (5.6.2)
Proof By the necessary condition for a minimum at.z∗
.<z∗−x + α∇ f (z∗
),z − z∗
> ≥ 0,
and the convexity property of. f (z), expressed as
. f (z) ≥ f (z∗
) + <∇ f (z∗
),z − z∗
>,
we derive
. 0 ≤ <z∗−x + α∇ f (z∗
),z − z∗
> =<z∗−x,z − z∗> + α [ f (z) − f (z∗)] .
Using this inequality in the identity5.6 Convergence Analysis 105
.
1
2
|z − x|2
=1
2
|z − z∗|2
+<z∗−x,z − z∗>+
1
2
|z∗−x|2
,
we get (5.6.2). The Lemma is proven.. 
Lemma 5.3 If all partial derivative of.L˜
δ(x˜, y˜) (5.5.13 and 5.5.14) satisfy the Lips￾chitz condition with positive constant.C0, then the following Lipschitz-type condition
holds:
.
 
 
[
Ψδ(w˜ + h, v˜ + g) − Ψδ(w, ˜ v˜ + g)
]
− [
Ψδ(w˜ + h, v)˜ − Ψδ(w, ˜ v)˜
] 
 ≤ C0|h||g|,
(5.6.3)
valid for any.w, ˜ h, v,˜ g ∈ X˜ × Y˜.
Proof By the Lagrange’s formula
. f (x + h) − f (x) =
 1
0
<∇ f (x + th), h>dt,
we have
.
 
 
[
Ψδ(w˜ + h, v˜ + g) − Ψδ(w, ˜ v˜ + g)
]
− [
Ψδ(w˜ + h, v)˜ − Ψδ(w, ˜ v)˜
] 
 =
 
 
 
 1
0 <∇Ψδ(w˜ + th, v˜ + g) − ∇Ψδ(w˜ + th, v), ˜ h>dt
 
 
 ≤
 1
0
|
|<∇Ψδ(w˜ + th, v˜ + g) − ∇Ψδ(w˜ + th, v), ˜ h>
|
| dt ≤
≤ 1
0 C0|h||g|dt ≤ C0|h||k|,
which proves the Lemma.. 
5.6.2 Main Convergence Theorem
The following theorem presents the convergence conditions of (9.7.4)–(5.5.12) and
gives the estimate of its rate of convergence.
Theorem 5.3 Assume that problem (5.5.15) has a solution. Let.L˜
δ(x˜, y˜) be differ￾entiable in.x˜ and.y˜, whose partial derivative with respect to.y˜ satisfies the Lipschitz
condition with positive constant .C. Then, for any .δ ∈ (0, 1), there exists a small￾enough
.γ0= γ0(δ) < C := min 
√
1
2C0
, 1+
√1+2C2
0
2C2
0
 
such that, for any .0 < γ ≤ γ0, sequence .{v˜n}, which generated by the equivalent
extraproximal procedure (9.7.4)–(5.5.12), monotonically converges in norm with
geometric progression rate.q .∈.(0, 1) to one of the equilibrium points. v∗, i.e.,106 5 Nash and Stackelberg Equilibrium
. | vn− v∗|2
≤ qn| v0− v∗|2
, (5.6.4)
where
.q = 1 +
4(δγ)2
1 + 2δγ − 2γ2C2 −2δγ. (5.6.5)
Proof (1) Taking in (5.6.2).α = γ and
.
z = ˜w,x = ˜vn,z∗= ˆvn,
f (z) = Ψδ(w, ˜ v˜n), f (z∗
) = Ψδ(vˆn, v˜n),
we obtain
. 1
2 | ˆvn− vn|2+γΨδ(vˆn, v˜n) ≤1
2 | ˜w − ˜vn|2+γΨδ(w, ˜ v˜n)−1
2
| ˜w − ˆvn|2. (5.6.6)
Again putting in (5.6.2).α = γ and
.
z = ˜w,x = ˜vn,z∗= ˜vn+1,
f (z) = Ψδ(w, ˜ vˆn), f(z∗) = Ψδ(v˜n+1, vˆn),
we get
. 1
2 | ˜vn+1− ˜vn|2+γΨδ(v˜n+1, vˆn) ≤1
2 | ˜w − ˜vn|2+γΨδ(w, ˜ vˆn)−1
2 | ˜w − ˜vn+1|2.
(5.6.7)
Selecting.w˜ = ˜vn+1 in (5.6.6) and.w˜ = ˆvn in (5.6.7) we obtain
. 1
2 | ˆvn− ˜vn|2+γΨδ(vˆn, v˜n) ≤1
2 | ˜vn+1− ˜vn|2+γΨδ(v˜n+1, v˜n)−1
2 | ˜vn+1− ˆvn|2,
(5.6.8)
. 1
2 | ˜vn+1− ˜vn|2+γΨδ(v˜n+1, vˆn) ≤1
2 | ˆvn− ˜vn|2+γΨδ(vˆn, vˆn)−1
2 | ˆvn− ˜vn+1|2.
(5.6.9)
Adding (5.6.8) with (5.6.9) and using (5.6.3) for
.
w˜ + h = ˜vn+1,w˜ = ˆvn,v˜ + g = ˜vn,v˜ = ˆvn,
h = ˜vn+1− ˆvn,g = ˜vn− ˆvn,
we finally conclude
.
| ˜vn+1− ˆvn|2
≤ γ[Ψδ(v˜n+1, v˜n) − Ψδ(vˆn, v˜n)]−γ[Ψδ(v˜n+1, vˆn) − Ψδ(vˆn, vˆn)] ≤
γC| ˜vn+1− vn|| ˜vn− ˆvn|,
which implies5.6 Convergence Analysis 107
.| ˜vn+1 − ˆvn| ≤ γC| ˜vn − ˆvn|. (5.6.10)
(2) Now, taking.w˜ = ˜vn+1 in (5.6.6) and.w˜ = ˜v∗
δ in (5.6.7) we get
. 1
2 | ˆvn− ˜vn|2+γΨδ (vˆn, v˜n) ≤1
2 | ˜vn+1− ˜vn|2+γΨδ(v˜n+1, v˜n)−1
2 | ˜vn+1− ˆvn|2,
. 1
2 | ˜vn+1− ˜vn|2+γΨδ(v˜n+1, vˆn) ≤1
2 | ˜v∗
δ− ˜vn|2+γΨδ (v˜
∗
δ , vˆn)−1
2 | ˜v∗
δ− ˜vn+1|2.
Adding these two inequalities and multiplying by two yields
.
| ˜v∗
δ− ˜vn+1|2+| ˜vn+1− ˆvn|2+| ˆvn− ˜vn|2−2γΨδ(v˜
∗
δ , vˆn)+2γ[Ψδ(v˜n+1, vˆn)
+Ψδ(vˆn, v˜n) − Ψδ(v˜n+1, v˜n)]≤| ˜v∗
δ− ˜vn|2.
Adding and subtracting the term.Ψδ(vˆn, vˆn) we have
.
| ˜v∗
δ− ˜vn+1|2+| ˜vn+1− ˆvn|2+| ˆvn− ˜vn|2+2γ
[
Ψδ (vˆn, vˆn) − Ψδ (v˜∗
δ , vˆn)
]
+2γ
[
Ψδ (v˜n+1, vˆn)−
Ψδ (vˆn, vˆn) + Ψδ (vˆn, v˜n) − Ψδ (v˜n+1, v˜n)
]
≤| ˜v∗
δ − ˜vn|2.
Using (5.6.3) with .w˜ + h = ˜vn+1, .w˜ = ˆvn, .v˜ + k = ˜vn and .v˜ = ˆvn we have .h .=
.v˜n+1 − ˆvn and.k .=.v˜n − ˆvn, and the inequality above becomes
.
| ˜v∗
δ− ˜vn+1|2+| ˜vn+1− ˆvn|2+| ˆvn− ˜vn|2+2γ
[
Ψδ(vˆn, vˆn) − Ψδ(v˜∗
δ , vˆn)
]
−
2γC| ˜vn+1− ˆvn|| ˜vn− ˆvn| ≤ |˜v∗
δ
− ˜vn|2
.
Applying (5.6.10) to the last term in the left-hand side and in view of the strict
convexity property of.Ψδ given by
.Ψδ(vˆn, vˆn) − Ψδ(v˜
∗
δ , vˆn) ≥ δ| ˆvn− ˜v∗
δ|2
,
we get
. | ˜v∗
δ− ˜vn+1|2+| ˜vn+1− ˆvn|2+2γδ| ˆvn − ˜v∗
δ |2+
(
1 − 2γ2C2
)
| ˜vn− ˆvn|2≤ |˜v∗
δ − ˜vn|2.
Applying the identity .2<a − c, c − b> .= .|a − b|2 .− .|a − c|2 .− .|c − b|2 with
.a = ˆvn,.b = ˜v∗
δ and.c = ˜vn, to the left-hand side of the last inequality we have
.
| ˜v∗
δ− ˜vn+1|2+| ˜vn+1− ˆvn|2+
(
1 − 2γ2C2
)
| ˜vn− ˆvn|2 + 2γδ[2< ˆvn − ˜vn, v˜n − ˜v∗
δ >+|˜vn− ˆvn|2+
| ˜vn − ˜v∗
δ |2 = |˜v∗
δ− ˜vn+1|2+| ˜vn+1− ˆvn|2+(1 + 2γδ − 2γ2C2)| ˜vn − ˆvn|2+4γδ< ˆvn − ˜vn, v˜n− ˜v∗
δ >
+2γδ| ˜vn− ˜v∗
δ |2 ≤ |˜v∗
δ − ˜vn|2.108 5 Nash and Stackelberg Equilibrium
31
21 
22 13 
23 
1 2 3 33 
11 12 32 
41 42 43 
24 
34 
14 
4 
44 
Fig. 5.6 Supermarket Markov Chain
Defining .d = 1 + 2γδ − 2γ2
C2 and completing the square form of the third and
fourth terms yields
.
| ˜v∗
δ− ˜vn+1|2 +|˜vn+1− ˆvn|2 + d| ˜vn − ˆvn|2 + 4γδ< ˆvn − ˜vn, v˜n − ˜v∗
δ > + (2γδ)2
d | ˜vn− ˜v∗
δ|2−
(2γδ)2
d | ˜vn− ˜v∗
δ|2 + 2γδ| ˜vn− ˜v∗
δ|2 ≤ |˜v∗
δ − ˜vn|2,
and
.
| ˜v∗
δ− ˜vn+1|2 +|˜vn+1− ˆvn|2+
 
 
 
√d(v˜n − ˆvn) + 2γδ
√d (v˜n− ˜v∗
δ )
 
 
 
2
≤
(
1 − 2γδ + (2γδ)2
d
)
| ˜v∗
δ
− ˜vn|2
,
finally implying
.| ˜v∗
δ− ˜vn+1|2
≤q| ˜v∗
δ − ˜vn|2 ≤ qn+1
| ˜v∗
δ − ˜v0|2 →n→∞ 0,
with.q = 1 − 2γδ + (2γδ)2
d ∈ (0, 1). The Theorem is proven.. 
5.7 Application Example: Four Supermarkets Chain
This example analyzes the effectiveness of relationship marketing strategies within
the department store sector of the retail industry considering two supermarket leaders
with.l = 1, 2 and two supermarkets followers with.m = 3, 4. The three supermarkets5.7 Application Example: Four Supermarkets Chain 109
are branching out into non-food items and they are also department stores in their
own right, selling items as clothes, entertainment products for example toys, books,
cosmetics, non-prescription drugs and many other household goods. All the super￾markets offer loyalty cards having their own system with the purpose to attract cus￾tomers, encourage customer loyalty and build strong customer relationships. As well,
loyalty cards create an advantage for supermarkets developing profiles of individuals’
personal shopping habits. When linked with the personal details that customers dis￾closed when signing up for the scheme, the store is in a position to target promotions
that are tailored around specific customers shopping habits. Based on the available
data, supermarkets discretize the client space in four sub-segments according to the
regularly of purchasing, using frequency of the loyalty card and the revenue. Figure
5.6 describes the segments and promotions corresponding to the Markov chain of
the marketing problem. Here a customer is said to be in state .s1 if he/she become a
Potential customer. A Low-Frequent customer corresponds with the state .s2 and a
Regular customer is a frequent customer of the loyalty card that is said to be in state
.s3. A Loyal Customer corresponds with the state .s4 and he/she is a high-frequency
user of the loyal card. The promotions (actions) offered by the supermarkets include
two different benefits: 1) points and 2) discounts. We are interested in contrasting
the strategies applied by the supermarkets defined over all possible combinations of
states (.i, j) and actions (.k) given a fixed utility.Ji j,k .
Our goal is to analyze a four-player Stackelberg game for the norm .p = 1 in a
class of ergodic controllable finite Markov chains. Let .N1 = N2 = N3 = N4 = 4,
.M1 = M2 = M3 = M4 = 2. The individual utility for each player are defined by
. J (1)
i j,1=
⎡
⎢
⎢
⎣
567 822 733 830
261 896 85 568
30 996 634 261
288 90 806 785
⎤
⎥
⎥
⎦ , J (1)
i j,2=
⎡
⎢
⎢
⎣
170 27 57 699
275 855 224 919
50 205 46 909
398 861 751 806
⎤
⎥
⎥
⎦ ,
. J (2)
i j,1 =
⎡
⎢
⎢
⎣
810 36 27 9
63 90 567 72
81 0 9 45
855 594 441 9
⎤
⎥
⎥
⎦
J (2)
i j,2 =
⎡
⎢
⎢
⎣
5 370 30 0
40 40 195 10
165 20 75 45
250 35 25 125
⎤
⎥
⎥
⎦ ,
. J (3)
i j,1 =
⎡
⎢
⎢
⎣
22 7 11 6
10 0 9 8
23 28 23 9
90 5 12 1
⎤
⎥
⎥
⎦ , J (3)
i j,2 =
⎡
⎢
⎢
⎣
11 0 21 7
3 13 40 1
6 3 10 26
11 17 30 8
⎤
⎥
⎥
⎦ ,
. J (4)
i j,1 =
⎡
⎢
⎢
⎣
0626
10 26 36 48
14 56 28 24
8 12 16 38
⎤
⎥
⎥
⎦ , J (4)
i j,2 =
⎡
⎢
⎢
⎣
6 4 54 12
0 4 2 16
6 8 50 2
12 30 48 14
⎤
⎥
⎥
⎦ .110 5 Nash and Stackelberg Equilibrium
The transition matrices for each player are defined as follows
. π(1)
j|i1=
⎡
⎢
⎢
⎣
0.2759 0.4886 0.0366 0.1989
0.1752 0.0953 0.3825 0.3470
0.1695 0.2629 0.4103 0.1574
0.2612 0.1665 0.4124 0.1600
⎤
⎥
⎥
⎦ , π(1)
j|i2=
⎡
⎢
⎢
⎣
0.0863 0.3672 0.3201 0.2264
0.4339 0.1684 0.1919 0.2058
0.3856 0.2349 0.1324 0.2471
0.1475 0.3500 0.1903 0.3122
⎤
⎥
⎥
⎦ ,
. π(2)
j|i1=
⎡
⎢
⎢
⎣
0.1761 0.1204 0.3883 0.3151
0.2207 0.1632 0.2354 0.3807
0.0708 0.3708 0.1364 0.4219
0.0132 0.5169 0.4127 0.0572
⎤
⎥
⎥
⎦ , π(2)
j|i2=
⎡
⎢
⎢
⎣
0.2033 0.2456 0.2667 0.2844
0.2732 0.1032 0.3046 0.3190
0.1207 0.0930 0.3997 0.3866
0.1032 0.6976 0.1609 0.0383
⎤
⎥
⎥
⎦ ,
. π(3)
j|i1=
⎡
⎢
⎢
⎣
0.4109 0.1654 0.0918 0.3319
0.3015 0.2201 0.1029 0.3756
0.1709 0.5673 0.0292 0.2326
0.1885 0.1491 0.3317 0.3307
⎤
⎥
⎥
⎦ , π(3)
j|i2=
⎡
⎢
⎢
⎣
0.3046 0.2883 0.2573 0.1498
0.2470 0.0978 0.3060 0.3492
0.3006 0.0439 0.4387 0.2169
0.1141 0.3397 0.1855 0.3607
⎤
⎥
⎥
⎦ ,
. π(4)
j|i1=
⎡
⎢
⎢
⎣
0.2610 0.3145 0.2088 0.2158
0.3777 0.1968 0.1574 0.2681
0.2593 0.0308 0.5113 0.1986
0.3401 0.4638 0.1200 0.0761
⎤
⎥
⎥
⎦ , π(4)
j|i2=
⎡
⎢
⎢
⎣
0.0316 0.4652 0.2221 0.2811
0.1624 0.3245 0.3691 0.1440
0.1448 0.5777 0.2087 0.0688
0.2536 0.1996 0.3231 0.2237
⎤
⎥
⎥
⎦ .
Given.δ.= 0.05 and.γ = 0.0001 and applying the extraproximal method we obtain
the convergence of the strategies in terms of the variable .cl
i|k for the leaders (see
Fig. 5.7) and the convergence of the strategies in terms of the variable.cm
i|k for the fol￾lowers (see Fig. 5.8). In addition, the Fig. 5.9 show the convergence of the parameters
Ji and Omega.
With final values.λ(1)∗ = 0.6096 and.λ(2)∗ = 0.3904 for the leaders, and.θ(1)∗ =
0.4952 and .θ(2)∗ = 0.5048 for the followers (see Fig. 5.10), the mixed strategies
c1(1,1)
c1(2,1)
c1(3,1)
c1(4,1)
c1(1,2)
c1(2,2)
c1(3,2)
c1(4,2)
0.25 Strategies for Leader 1 0.3 Strategies for Leader 2
0.2 0.25
0.15 0.2
0.1 0.15
0.05 0.1
0
0 5 10 15 20 25 30
0.05
0 10 20 30 40 50
c2(1,1)
c2(2,1)
c2(3,1)
c2(4,1)
c2(1,2)
c2(2,2)
c2(3,2)
c2(4,2)
Fig. 5.7 Convergence of the strategies of the leader.1 (left) leader.2 (right)5.7 Application Example: Four Supermarkets Chain 111
c3(2,2)
c3(3,2)
c3(4,2)
c4(2,2)
c4(3,2)
c4(4,2)
0.25 Strategies for Follower 1 0.22 Strategies for Follower 2
c3(1,1) 0.2 c4(1,1)
c3(2,1) c4(2,1)
0.2 c3(3,1) 
c3(4,1) 
0.18 c4(3,1)
c4(4,1)
c3(1,2) 0.16 c4(1,2)
0.15 0.14
0.12
0.1 0.1
0.08
0.05
0 50 100 150
0.06
0 10 20 30 40 50
Fig. 5.8 Convergence of the strategies of the follower.1 (left) and follower.2 (right)
0.25
0.2
0.15
0.1
0.05
0
Ji
0 10 20 30 40 50 60 70
0.15
0.1
0.05
0
Omega
0 1 2 3 4 5 6 7 8
Fig. 5.9 Convergence of the parameters Ji and Omega
Lambda1
Lambda2
0.65 Lambda 0.505 Theta
0.6
0.55
0.5 0.5
0.45
0.4
0.35
0 50 100 150
0.495
0 50 100 150
Theta1
Theta2
Fig. 5.10 Convergence of Lambda and Theta
obtained for determining the strong Stackelberg/Nash equilibrium for all the players
are as follows
. d(1)∗ =
⎡
⎢
⎢
⎣
0.8110 0.1890
0.1701 0.8299
0.7720 0.2280
0.2249 0.7751
⎤
⎥
⎥
⎦ , d(2)∗ =
⎡
⎢
⎢
⎣
0.6023 0.3977
0.8408 0.1592
0.8187 0.1813
0.8242 0.1758
⎤
⎥
⎥
⎦ , (5.7.1)112 5 Nash and Stackelberg Equilibrium
. d(3)∗ =
⎡
⎢
⎢
⎣
0.7326 0.2674
0.6958 0.3042
0.6500 0.3500
0.7728 0.2272
⎤
⎥
⎥
⎦ , d(4)∗ =
⎡
⎢
⎢
⎣
0.7337 0.2663
0.7454 0.2546
0.7376 0.2624
0.6418 0.3582
⎤
⎥
⎥
⎦ .
The resulting utilities by segment are as follows:
. J (1)
(si) =
⎡
⎢
⎢
⎣
129, 130
92, 800
84, 590
121, 520
⎤
⎥
⎥
⎦ , J (2)
(si) =
⎡
⎢
⎢
⎣
10, 463
21, 684
618
64, 189
⎤
⎥
⎥
⎦ , (5.7.2)
. J (3)
(si) =
⎡
⎢
⎢
⎣
55.1402
50.1599
92.6922
396.9852
⎤
⎥
⎥
⎦ , J (4)
(si) =
⎡
⎢
⎢
⎣
321.4290
241.9381
171.7218
98.8860
⎤
⎥
⎥
⎦ . (5.7.3)
The resulting utilities by promotion are as follows:
.
J (1)
(ki) = [
226, 830 201, 200 ]
,
J (2)
(ki) = [
93, 934 3, 019 ]
.
⎫
⎬
⎭
(5.7.4)
.
J (3)
(ki) = [
508.5371 86.4404 ]
,
J (4)
(ki) = [
608.5561 225.4188 ]
.
⎫
⎬
⎭
(5.7.5)
Relationship marketing recognizes that the focus of marketing is to build a rela￾tionship with existing customers. The main purpose of the game is to discover the
extent to which customers use and are influenced by relationship marketing strate￾gies. In addition, it is to analyze the impact that these strategies have on customer
loyalty and the development of customer-department store relationship. The super￾market leaders (players 1 and 2) fix their strategies (5.7.1) to ensure high degrees
of customer loyalty and retention as well utility by segment (5.7.2) and promotion.
For segment 1, the leader 1 made a strong emphasis on offering points (0.8110)
for attracting Potential customers. Instead, the leader 2 made emphasis on offering
points (0.6023) and discounts (0.3977) for the same segment. Looking at the utili￾ties of the leaders (5.7.2), the follower1 resolved for competing highlighting points
(0.7326). Instead, the follower 2 decided for offering points (0.7337) and discounts
(0.2663). For segment 2 corresponding to Low-Frequent customers the leader 1 pro￾moted points (0.1701) and discounts (0.8299) and, the leader 2 chose offering points
(0.8408) and discounts (0.1592). However, for competing with the leaders, follower
1 and follower 2 made emphasis on points (0.6958 and 0.7454 respectively). For
Regular customers the leader 1 focused on points (0.7720) and discounts (0.2280)
and, the leader 2 made emphasis on points (0.8187). The follower 1 preferred offeringReferences 113
points (0.6500) and discounts (0.3500). Instead, follower 2 made emphasis on points
(0.7376) and discounts (0.2624). For Loyal customers the leader 1 made emphasis
on points (0.2249) and discounts (0.7751), leader 2 focus on points (0.8242) and dis￾counts (0.1758) as well, follower 1 chose the same strategies—points (0.7728) and
discounts (0.2272)–. The follower 2 made emphasis on points (0.6418) and discounts
(0.3582). For the leaders the most profitable segments are the Potential customers
and the Loyal customers (see 5.7.2 vs. 5.7.3). An insight into the mind of the con￾sumer is obvious from the findings the importance that is placed on a given policy:
the utilities obtained by action for the leaders and followers are shown in Eqs. (5.7.4)
and (5.7.5) respectively.
References
1. Antipin, A.S.: An extraproximal method for solving equilibrium programming problems and
games. Comput. Math. Math. Phys. 45(11), 1893–1914 (2005)
2. Clempner, J.B.: Setting cournot versus lyapunov games stability conditions and equilibrium
point properties. Int. Game Theory Rev. 17, 1–10 (2015)
3. Clempner, J.B.: A proximal/gradient approach for computing the Nash equilibrium in control￾lable Markov games. J. Optim. Theory Appl. 188(3), 847–862 (2021)
4. Clempner, J.B., Poznyak, A.S.: Convergence method, properties and computational complexity
for Lyapunov games. Int. J. Appl. Math. Comput. Sci. 21(2), 349–361 (2011)
5. Clempner, J.B., Poznyak, A.S.: Analysis of best-reply strategies in repeated finite Markov
chains games. In: IEEE Conference on Decision and Control (2013)
6. Clempner, J.B., Poznyak, A.S.: Computing the strong Nash equilibrium for Markov chains
games. Appl. Math. Comput 265, 911–927 (2015)
7. Clempner, J.B., Poznyak, A.S.: Convergence analysis for pure and stationary strategies in
repeated potential games: Nash, Lyapunov and correlated equilibria. Expert Syst. Appl. 46,
474–484 (2016)
8. Clempner, J.B., Poznyak, A.S.: A Tikhonov regularization parameter approach for solving
Lagrange constrained optimization problems. Eng. Optim. 50(11), 1996–2012 (2018)
9. Clempner, J.B., Poznyak, A.S.: A Tikhonov regularized penalty function approach for solving
polylinear programming problems. J. Comput. Appl. Math. 328, 267–286 (2018)
10. Clempner, J.B., Poznyak, A.S.: Finding the strong Nash equilibrium: computation, existence
and characterization for Markov games. J. Optim. Theory Appl. 186, 1029–1052 (2020)
11. Dreves, A.: Computing all solutions of linear generalized Nash equilibrium problems. Math.
Methods Oper. Res. (2016). https://doi.org/10.1007/s00186-016-0562-0
12. Dreves, A., Kanzow, C., Stein, O.: Nonsmooth optimization reformulations of player convex
generalized Nash equilibrium problems. J. Glob. Optim. 53(4), 587–614 (2012)
13. Facchinei, F., Kanzow, C., Sagratella, S.: Solving quasi-variational inequalities via their KKT
conditions. Math. Program. 144(1–2), 369–412 (2014)
14. Facchinei, F., Sagratella, S.: On the computation of all solutions of jointly convex generalized
Nash equilibrium problems. Optim. Lett. 5(3), 531–547 (2011)
15. Gabriel, S.A., Siddiqui, S., Conejo, A.J., Ruiz, C.: Solving discretely-constrained Nash-Cournot
games with an application to power markets. Netw. Spat. Econ. 13(3), 307–326 (2013)
16. Kreps, D.M.: Nash equilibrium. In: Game Theory, pp. 167–177. Springer (1989)
17. Nabetani, K., Tseng, P., Fukushima, M.: Parametrized variational inequality approaches to
generalized Nash equilibrium problems with shared constraints. Comput. Optim. Appl. 8(3),
423–452 (2011)
18. Nash, J.F.: Non-cooperative games. Ann. Math. 54, 286–295 (1951)114 5 Nash and Stackelberg Equilibrium
19. Osborne, M.J., Rubinstein, A.: A Course in Game Theory. MIT Press (1994)
20. Tanaka, K., Yokoyama, K.: On. -equilibrium point in a noncooperative n-person game. J. Math.
Anal. 160, 413–423 (1991)
21. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the Stackelberg/Nash equilibria using
the extraproximal method: convergence analysis and implementation details for Markov chains
games. Int. J. Appl. Math. Comput. Sci. 25(2), 337–351 (2015)
22. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: An optimal strong equilibrium solution for coop￾erative multi-leader-follower Stackelberg Markov chains games. Kibernetika 52(2), 258–279
(2016)
23. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the Lp-strong Nash equilibrium for
Markov chains games. Appl. Math. Modell. 41, 399–418 (2017)
24. von Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior, 2nd rev. Prince￾ton University Press (1947)Chapter 6 
Best-Reply Strategies in Repeated Games 
Abstract The actions that players naturally and frequently choose to perform 
throughout a repeated game are the “best-reply strategy”. Making the determination 
that such strategies lead to an equilibrium point is a difficult and sensitive operation 
since, often, the behavior of a single cost-function when such best-reply techniques 
are used turns out to be non-monotonic. The convergence to a stable equilibrium is 
not always guaranteed, even in repeated games. In this chapter, we demonstrate that 
the best-reply actions surely lead to an equilibrium point for a type of finite controlled 
Markov Chains dynamic games. The Lyapunov Games concept, which is based on the 
design of a unique Lyapunov function (associated with a unique cost function) that 
monotonically reduces (non-increases) throughout the game, achieves this result. We 
provide a technique for creating a Lyapunov-like function that describes how players 
behave in a recurrent Markov chain game. The Lyapunov-like function replaces the 
components of the ergodic system, which simulate players’ anticipated behavior in 
one-shot games, for the recursive process. We first provide a non-converging state￾value function that varies (increases and decreases) between states of the repeating 
Markov game in order to demonstrate our claim. Then, we demonstrate that using 
a one-step-ahead fixed-local-optimal approach, it is possible to describe that func￾tion in a recursive format. Thus, we show that the previous recursive expression for 
the repeated game can be used to construct a Lyapunov-like function; the resulting 
Lyapunov-like function is a monotonic function that can only decrease (or remain 
the same) over time, regardless of the initial distribution of probabilities. The best￾reply methods examined in this study are connected to what are known as pure and 
stationary fixed-local-optimal actions, or, to put it another way, with one step ahead 
optimization algorithms that are extensively employed in the Artificial Intelligence 
theory. The Bank Marketing Campaigns Game and the Duel Game with Best-Reply 
Strategies Application serve as examples of the proposed strategy. 
6.1 Introduction 
The behaviors that players naturally and frequently employ when playing a game 
repeatedly are known as the “best-reply ”strategy. A local (one-step) projected opti￾mization is actually realized by this operation, supposing that the prior history (states 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_6 
115116 6 Best-Reply Strategies in Repeated Games
and actions) cannot be modified going forward. It turns out that the behavior of a 
particular cost-function when such strategies are used is non-monotonic; as a result, 
it is difficult and requires extra study to conclude that such tactics lead to an equilib￾rium point, often the Nash equilibrium (see [ 13]). The convergence to a stationary 
equilibrium is not always guaranteed, even in repeated games (see [ 4, 12]). 
Iterative solution approaches for strategic games postulate that players utilize an 
internal process of reasoning to weed out irrational tactics as they search for an 
equilibrium point [ 14, 17, 20, 22]. The definitions of unreasonable techniques used 
by various iterative solution approaches in the literature vary. On the one hand, there 
are so-called rationalizability notions [ 2, 18– 20], for which a strategy is considered 
irrational if it is not the optimum response to a certain belief. However, there are 
notions known as dominance solutions for which a strategy is considered irrational 
if it is outperformed by another approach [ 3, 19]. 
In this chapter, we demonstrate that the best-reply actions inevitably lead to one of 
the Nash equilibrium points for a class of ergodic finite controllable Markov Chains 
dynamic games. The Lyapunov Games concept, which is based on the creation of a 
unique Lyapunov function (associated with a unique cost function) that monotoni￾cally reduces (non-increases) throughout the game, achieves this result. According 
to [ 7], the equilibrium point is guaranteed to occur naturally in Lyapunov games by 
definition. It is also certain that convergence to an equilibrium point will occur. A 
Lyapunov-like function monotonically decrements and reaches a Lyapunov equilib￾rium point while moving ahead through the state space. The behavior of a Lyapunov￾like function is implemented naturally by the best-reply dynamics. A Lyapunov game 
also has the advantage that all players are aware that just the best reply is chosen. 
Additionally, a Lyapunov equilibrium point has stability characteristics that aren’t 
always present in a Nash equilibrium point [ 5, 6, 9– 11]. 
Without taking into account the starting strategies the players start with, a game 
is said to be stable with regard to a set of strategies if the iterated process of strategy 
selection (in our case, the best-reply dynamics) converges to an equilibrium point. 
Every player chooses their tactics in order to reach an equilibrium point by maximiz￾ing his own cost function while taking into account the available strategies from other 
players, according to [ 16]. Any departure from this equilibrium point would bring 
the system back to it. This is because the iterated process of selecting strategies in 
natural evolution attempts to follow the best ones and corrects the trajectory to arrive 
at a stable equilibrium point (this is the case when the equilibrium point is unique). 
In this sense, we may say that a Lyapunov equilibrium point is stable because once a 
player’s choices have reached a stable state, it is not in their best advantage to modify 
their strategy on their own. The fact that every ergodic system may be represented by 
a Lyapunov-like function is a significant benefit of the Lyapunov games . A recursive 
method is used for a repeating (ergodic) game to support an equilibrium play [ 7, 8]. 
We have found an equilibrium position, and furthermore, a very reasonable one, if 
the stochastic game’s ergodic process converges [ 21]. 
We outline a technique for creating a Lyapunov-like function that is one-to-one 
with a specified cost function and has monotonic behavior. A declining Lyapunov￾like function that is constrained from below enables the convergence of the cost6.2 Preliminaries 117
function to a minimal value while also ensuring the presence of an equilibrium 
point for the used pure and stationary local-optimal techniques, according quote 
[ 7]. The vector Lyapunov-like function that results is monotonic, meaning that its 
components can only get smaller with time. As a consequence, a one-shot game might 
be used to symbolize a repeating game. In our case, repeated games are converted into 
one-shot games, substituting the recursive process with a Lyapunov-like function. 
This complicates the problem’s justification. The Duel game with best-reply actions 
application and repeated asynchronous Bank marketing campaigns serve as examples 
of the offered methodology. 
The main contribution of this chapter is as follows: 
– Demonstrates that the cost sequence associated with the local-optimal (best-reply) 
strategy exhibits non-monotonic behavior, making it impossible to prove with 
exactness the existence of a limit point; 
– Proposes a one-to-one mapping between the current cost function and a new energy 
function (Lyapunov-like function) that is monotonically non-increasing on the 
trajectories of the system. 
– Shows that a Lyapunov equilibrium point is a Nash equilibrium point (the converse 
is false), but it also offers a number of benefits: A Lyapunov equilibrium point is 
guaranteed to exist by definition, a Lyapunov-like function can be built to adhere to 
the Markov game’s constraints, a Lyapunov-like function invariably converges to a 
Lyapunov equilibrium point, and a Lyapunov equilibrium point exhibits properties 
of stability. 
– Provides for a class of ergodic controllable finite Markov chains the convergence 
of the pure and stationary local-optimal (best-reply) strategy. 
– Presents an analytical formula for the numerical implementation of the local￾optimal (best-reply) strategy . 
6.2 Preliminaries 
6.2.1 Controllable Markov Decision Process 
As usual let the set of real numbers be denoted by. R and let the set of non-negative 
integers be denoted by. N. The inner product for two vectors.u, v in.Rn is denoted by 
.<u, v> = vT u. Let .S be a finite set, called the state space, consisting of all positive 
integers.N ∈ N of states.{s1, ...,sN }.A Stationary Markov chain [ 15] is a sequence 
of.S-valued random variables.s(t),.t ∈ N, satisfying the Markov condition: 
.
P(s(t + 1) = sj|s(t) = si,s(t − 1) = sit−1 ,...,s(0) = si0 ) =
P 
s(t + 1) = sj|s(t) = si
 
:= πj|i(t).
(6.2.1)118 6 Best-Reply Strategies in Repeated Games
The Markov chain can be represented by a complete graph whose nodes are the 
states, where each edge.(si,sj) ∈ S2 is labeled by the transition probability (6.2.1). 
The matrix.Π = (πj|i)(si,sj)∈S ∈ [0, 1]S×S determines the evolution of the chain: for 
each .k ∈ N , the power .Πk has in each entry .(si,sj) the probability of going from 
state. si to state.sj in exactly. k steps. 
A Controllable Markov Decision Process is a 5-tuple 
.MDP = {S, A, Υ,Π, V }, (6.2.2) 
where: 
– . S is a finite set of states,.S ⊂ N, endowed with discrete topology; 
– .A is the set of actions, which is a metric space. For each .s ∈ S,.A(s) ⊂ A is the 
non-empty set of admissible actions at state .s ∈ S. Without loss of generality we 
may take.A= ∪s∈S A(s); 
– .Υ = {(s, a)|s ∈ S, a ∈ A(s)} is the set of admissible state-action pairs, which is 
a measurable subset of.S × A; 
– .Π (k) = [
πj|ik ]
is a stationary transition controlled matrix, where 
. πj|ik (t) := P(s(t + 1) = sj|s(t) = si, a(t) = ak )
represents the probability associated with the transition from state .si to state . sj
under an action.ak ∈ A(si),.k = 1, ..., M; 
– .V : S →. R is a cost function, associating to each state a real value. 
The Markov property of the decision process in (6.2.2) is said to be fulfilled if 
. 
P(s(t + 1)|(s(1),s(2), ...,s(t − 1)),s(t) = si, a(t) = ak )
= P(s(t + 1) = sj|s(t) = si, a(t) = ak ).
The strategy (policy) 
. dk|i(t) ≡ P(a(t) = ak |s(t) = si)
represents the probability measure associated with the occurrence of an action. a(t) =
ak from state .s(t) = si . The elements of the transition matrix for the controllable 
Markov chain can be expressed as 
.
P {
s(t + 1) = sj|s(t) = si
}
=
∑
M
k=1
P {
s(t + 1) = sj|s(t) = si, a(t) = ak
}
dk|i(t). (6.2.3)6.2 Preliminaries 119
6.2.2 Game Description 
Let 
| .N = {1, ..., n} be a set of player indexed by .l = 1, n. We use notations . Δ =
l∈N .Δl (the mixed strategies profile), and.Δ−l = |
j∈N\{l} Δj (the mixed strategies 
profile of all the players except for player . l). Let us denote the collection . 
dl
k|i(t)
 
by.Δt as follows 
. Δt = {
dl
k|i(t)
}
k=1,M,i=1,N,l=1,n .
In this chapter we will deal with the class of the, so-called, local-optimal policies 
(strategies) defined below. 
Definition 6.1 A policy.
 
Δl,loc
t
 
t≥0
is said to be local-optimal (or best reply) if for 
each .t ≥ 0 it minimizes the conditional mathematical expectation of the individual 
cost-function .Vl
(sl
(t + 1)), .l = 1, n, under the condition that the prehistory of the 
process 
. Hl
t := {Δl
0, P
 
sl
(0) = sl
j
 
j = 1, N;...; Δl
t−1, P
 
sl
(t) = sl
j
 
j = 1, N}
is fixed and can not be changed hereafter, i.e., it realizes the “one-step ahead”condi￾tional optimization rule 
.Δl,loc
t := arg min
Dl
t
E{
Vl
(sl
(t + 1)) | Hl
t
}
, (6.2.4) 
where .Vl
(sl
(t + 1)) is the cost function of the player. l at the state .sl
(t + 1) and. Dl
t
is the set of admissible strategies.{dl
k|i(t)}. 
Remark 6.1 Locally optimal policy is known as a myopic policy in the games 
literature. 
A non-cooperative stochastic game is a tuple 
. G = 
N , S,
 
Δl
 
l∈N ,
 
Υ l
 
l∈N ,Π, 
Vl
 
l∈N
 
.
For a strategy .d = (d1, ..., dn) ∈ Δ we denote the complement strategy . d−l =
(d1, ..., dl−1, dl+1, ..., dn) and, with an abuse of notation, .d = (dl
, d−l
). The state 
.d = (d1, ..., dn) represents the distribution vector of strategy frequencies and can 
only move on. Δ. 
Let us denote by.Vd (t + 1)the vector average cost function at the state. sl
(t + 1)
and time.(t + 1) under the fixed strategy.dl
(t) = dl
, that is, 
. Vd (t + 1) := 
V1,d1
(t + 1), ..., Vn,dn
(t + 1)
 
,
where.Vl,dl
(t + 1) is the average cost function at the state.sl
(t + 1) and time. (t + 1)
for the player. l, namely,120 6 Best-Reply Strategies in Repeated Games
. Vl,dl
(t + 1) := E(Vl
(sl
(t + 1))|dl
(t)),
and.E(·|dl
(t)) is the operator of the conditional mathematical expectation subject to 
the constraint that at time. t the mixed strategy.dl
(t) has been applied. 
6.3 Problem Formulation 
To tackle this problem we proposed representing the state-value function .V using 
a linear (with respect to the control .d ∈ Π) model. After that we obtain the policy 
. d that results in the minimum trajectory value. Finally, we present .V in a recursive 
matrix format. 
6.3.1 The State-Value Function 
The probability of the player .l ∈ N in the game .G to find itself in the next state is 
as follows: 
. 
Pl
(sl
(t + 1) = sl
j|sl
(t) = sl
i) =
∑
M
k=1
Pl
(sl
(t + 1) = sl
j|sl
(t) = sl
i, al
(t) = al
k )dl
k|i(t) = ∑
M
k=1
πl
j|ikdl
k|i(t).
The cost function.Vl of any fixed policy.dl
(t)is defined over all possible combinations 
of states and actions, and indicates the expected value when taking action.al in state 
. s and following policy .dl
(t) thereafter. The .V-values for all the states of (6.2.2) in 
open format can be expressed by 
. 
Vl,dl
(t + 1) := E(Vl
(sl
(t + 1))|dl
(t)) =
∑
N
j=1
∑
N
i=1
∑
M
k=1
Vl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i, al
(t) = al
k )πl
j|ikdl
k|i(t)Pl
(sl
(t) = sl
i),
 
⎪⎪⎬
⎪⎪⎭
(6.3.1) 
where.Vl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i, al
(t) = al
k ) is a loss value at state. sl
i when the 
action.al
k is applied (without loss of generality it can be assumed to be positive) and 
.Pl
(sl
(t)) for any given.Pl
(sl
(0)) is defined as follows6.3 Problem Formulation 121
. 
Pl
(sl
(t + 1)) = sl
j) = ∑
N
i=1
Pl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i),
Pl
(sl
(t) = sl
i) = ∑
N
i=1
 ∑
M
k=1
πl
j|ikdl
k|i(t)
 
Pl
(sl
(t) = sl
i),
or, in matrix format, 
. 
pl
n+1 = 
˙
l
n
 
pl
n,
 
˙
l
n
 
i j := ∑
M
k=1
∑
M
k=1
πl
j|ikdl
k|i(t).
Remark 6.2 We will assume hereafter that 
. Vl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i, al
(t) = al
k ) > 0
for all. l. Indeed, by the identity 
. 
E(Vl
(sl
(t + 1))|dl
(t)) =
∑N
j=1
∑N
i=1
∑M
k=1
Vl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i, al
(t) = al
k )πl
j|ikdl
k|i(t)Pl
(sl
(t) = sl
i) =
∑N
j=1
∑N
i=1
∑M
k=1
[Vl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i, al
(t) = al
k ) + c] · πl
j|ikdl
k|i(t)Pl
(sl
(t) = sl
i) − c
the minimization of the state-value function .E(Vl
(sl
(t + 1))|dl
(t)) is equivalent to 
the minimization of the function.E(V˜l
(sl
(t + 1))|dl
(t)) where 
. V˜l
(sl
(t + 1))|dl
(t)) = Vl
(sl
(t + 1))|dl
(t)) + c,
which is strictly positive if we take 
.c > max
1≤i≤N,1≤k≤M
Vl
(sn = s(i) | sn = s(i), al
n = al (k)). (6.3.2) 
In a vector format, the formula (6.3.1) can be expressed as 
. 
Vl,dl
t+1 := E(Vl
(sl
(t + 1))|dl
(t)) =
∑N
i=1
[
∑N
j=1
∑M
k=1
Vl
(sl
(t + 1)) = sl
j|sl
(t) = sl
i, al
(t) = al
k )πl
j|ikdl
k|i(t)
]
Pl
(sl
(t) = sl
i) = 
wl
t, pl
t
 
,
where122 6 Best-Reply Strategies in Repeated Games
. 
 
wl
t
 
i := ∑
N
j=1
[
∑
M
k=1
Vl
i j|kπl
j|ikdl
k|i(t)
]
,
Vl
i j|k := Vl
(sl
(t + 1) = sl
j|sl
(t) = sl
i, al
(t) = al
k ),
 
pl
t
 
i := Pl
(sl
(t) = sl
i).
6.3.2 The Recursive Matrix Form 
Let us first introduce the following statement about the unit simplex. 
Let.Δ be the unit simplex in.RM , that is, 
. Δ = {u ∈ RM |
∑
M
k=1
u(k) = 1, u(k) ≥ 0}.
Then, 
. min
u∈Δ
∑
M
k=1
v(k)u(k) = min
k=1,...,M
v(k) = v(α),
and the minimum is achieved at least for.u =
⎛
⎝0, 0, ..., 0, 1 α
, 0, ..., 0
⎞
⎠. 
Indeed, it is evident that 
. ∑
M
k=1
v(k)u(k) ≥ ∑
M
k=1
(min v(k))u(k) = min v(k)
∑
M
k=1
u(k) = min v(k) = v(α),
and the equality is achieved at least for.u =
⎛
⎝0, 0, ..., 0, 1 α
, 0, ..., 0
⎞
⎠. 
As a result we have that 
. 
Vl
t+1 = 
wl
t, pl
t
 
= ∑
N
i=1
 
wl
t
 
i
 
pl
t
 
i ≥
∑
N
i=1
mindl
k|i(t)∈Δ
 
wl
t
 
i
 
pl
t
 
i = ∑
N
i=1
 
pl
t
 
i mink=1,...,M
[
∑
N
j=1
Vl
i j|kπl
j|ik]
.
At this point, let us introduce the following general definition of Lyapunov-like 
function6.3 Problem Formulation 123
Definition 6.2 Let .V : S → R+ be a continuous map. Then, .V is said to be a 
Lyapunov-like function 1 iff it satisfies the following properties : 
(1) .∃s∗, called below a Lyapunov equilibrium point, such that.Vl
(s∗) = 0, 
(2) .Vl
(s) > 0 for all.s /= s∗and all.l ∈ N , 
(3) .Vl
(si
) → ∞ if there exists a sequence .{si}
∞
i=1 with .si → ∞ as .i → ∞ for all 
.l ∈ N , 
(4) .ΔVl
(s'
,s) = Vl
(s'
) − Vl
(s) < 0 for all.s /= s' /= s∗ and.l ∈ N . 
Given fixed history of the process. 
pl
0, dl
(0), dl
(1), ..., dl
(t − 1)
 
. min
dl(t)
Vl
t+1=
∑
N
i=1
 
pl
t
 
i
⎡
⎣
∑
N
j=1
Vl
i j|k∗πl
j|ik∗
⎤
⎦ . (6.3.4) 
(and considering point (4) of Definition 6.2), the identity in (6.3.4) is achieved for 
the pure and stationary local-optimal policy. 
. dl∗
k∗|i(t) = δk∗(i),i n = 0, 1, .. (6.3.5) 
where.δk∗(i),i is the Kronecker symbol and.k∗(i) is an index for which 
.
∑
N
j=1
Vl
i j|k∗πl
j|ik∗ ≤ ∑
N
j=1
Vl
i j|kπl
j|ik := Wl
ik , ∀k = 1, ..., M. (6.3.6) 
As a result we can state the following lemma. 
Definition 6.3 A Lyapunov game is a tuple 
. G = 
N , S,
 
Δl
 
l∈N ,
 
Υ l
 
l∈N ,Π, 
Vl
 
l∈N
 
,
where.Vl is a Lyapunov-like function (monotonically decreasing in time) and satisfies 
Definition 6.2. 
1 By the original definition of A. M. Lyapunov the following conditions must be satisfied for an 
energy function: locally for any small neighborhood .Ωδ of the origin the following inequalities 
must be satisfied for any. x ∈ Ωδ
.
α |x|2 ≤ V(x) ≤ β |x|2 , α > 0,
V(xn+1) < V(xn). (6.3.3) 
If some additional requirements are necessary, or the above conditions hold globally (Lyapunov￾Krasovskii) and the second inequality is fulfilled non-strictly, that is, .V (xn+1) ≤ V(xn), then in 
these cases the considered energy function is commonly referred to as “Lypunov-like function”(see, 
for example [ 1].124 6 Best-Reply Strategies in Repeated Games
Lemma 6.1 Let 
. G = 
N , S,
 
Δl
 
l∈N ,
 
Υ l
 
l∈N ,Π, 
Vl
 
l∈N
 
be a non-cooperative stochastic game. Given a fixed-local-optimal policy, the .V￾values for all state-action pairs from (6.3.1) in the recursive matrix format become 
. Vl
t+1 = 
wl∗, pl
t
 (6.3.7) 
where.wl∗ := wl∗
 
1 , ..., 
wl∗
 
N
 
and 
. 
wl∗ 
i := ∑
N
j=1
Vl
i j|kπl
j|ik∗(i) = min
k=1,M
Wl
ik ,
. Wl
ik = ∑
N
j=1
Vl
i j|kπl
j|ik .
Remark 6.3 Under the local-optimal strategy (6.3.5) the probability state-vector. pl
t
satisfies the following relation 
. pl
t+1 =
 
˙
l∗
 
pl
t =
 
˙
l∗
 t+1
pl
0, (6.3.8) 
where 
. 
˙
l∗ 
i j := ∑
M
k=1
πl
j|ik δk∗(i),i = πl
j|ik∗(i).
6.4 Construction of a Lyapunov-Like Function 
The aim of this section is to associate to any cost function .Vl
t , governed by (6.3.7), 
a Lyapunov-like function which monotonically decreases (non-increases) on the 
trajectories of the given system.6.4 Construction of a Lyapunov-Like Function 125
6.4.1 Recurrent Form for the Cost Function 
In view of (6.3.2) let us represent.Vl
n+1 as 
. 
Vl
t+1 = 
wl∗, pl
t
 
= 
wl∗, pl
t−1
 
+ 
wl∗, pl
n − pl
t−1
 
= Vl
t +
/
wl∗,
[ 
˙
l∗
 t
−
 
˙
l∗
 t−1
]
pl
0
\
= Vl
t +
/
wl∗,
[ 
˙
l∗
 − I
] 
˙
l∗
 t−1
pl
0
\
=
[ 1 +
\[˙
l∗
−I
]
wl∗,pl
t−1
\
Vl
t
 
Vl
t
]
,
and denoting 
. αl
t =
\[˙
l∗
−I
]
wl∗,pt−1
\
Vl
t
=
\[˙
l∗
−I
]
wl∗,pl
t−1
\
<wl∗,pl
t−1> ,
we get 
.Vl
t+1 = (1 + αl
t)Vl
t . (6.4.1) 
6.4.2 The Lyapunov Function Design 
Defining.α˜l
t as 
.α˜l
t =
⎧
⎨
⎩
αl
t if αl
t ≥ 0,
0 if αl
t < 0,
(6.4.2) 
we get 
.Vl
t+1 = (1 + αl
t)Vl
t ≤ (1 + ˜αl
t)Vl
t, (6.4.3) 
which leads to the following statement. 
Theorem 6.1 Let 
. G = 
N , S,
 
Δl
 
l∈N ,
 
Υ l
 
l∈N ,Π, 
Vl
 
l∈N
 
be a non-cooperative stochastic game and let the recursive matrix format be repre￾sented by (6.4.1). Then, a possible Lyapunov-like function .Vl,mon
n (which is mono￾tonically non-increasing) for. G has the form 
.
Vl,mon
t = Vl
n
|t−1
τ=1(1 + ˜αl
τ )−1 = 1+αl
t−1
1+ ˜αl
t−1
Vl,mon
t−1 ,
Vl,mon
0 = Vl
0.
(6.4.4)126 6 Best-Reply Strategies in Repeated Games
Proof Let us consider the recursion 
. xt+1 ≤ (1 + γn)xt + ηt
with.γt, xt, ηt ≥ 0. Defining 
. x˜t := xt
 t−1
τ=1
(1 + γτ )
−1
, η˜t := ηt
 t
τ=1
(1 + γτ )
−1
and 
. yt = ˜xt −∑t−1
τ=1
η˜τ ,
we obtain.yt+1 ≤ yt . Indeed, 
. 
x˜t+1 = xt
|t
τ=1(1 + γτ )−1 ≤
xt
[|t
τ=1(1 + γτ )−1
]
(1 + γt) + ηt
|t
τ=1(1 + γτ )−1 = ˜xt + ˜ηt
which implies 
. yt+1 = ˜xt+1 − ∑t
τ=1 η˜τ ≤ ˜xt + ˜ηt − ∑t
τ=1 η˜τ = ˜xt + ∑t−1
τ=1 η˜τ = yt,
and therefore.yt+1 ≤ yt . In view of this we have 
. 
Vl,mon
t+1 = Vl
t+1
|t
τ=1(1 + ˜αl
τ )−1 ≤ (1 + ˜αl
t)Vl
t
|t
τ=1(1 + ˜αl
τ )−1 =
Vl
t
|t−1
τ=1(1 + ˜αl
τ )−1 = Vl,mon
t ,
that proves the result.. 
Corollary 6.1 Since the sequence .
 
Vl,mon
t
 
is bounded from below and monoton￾ically non-increasing, then by the Weierstrass theorem it converges, that is, there 
exists a limit 
. Vl,mon
∞ := lim
t→∞ Vl,mon
t .
Corollary 6.2 If the series.
∑t
τ=1 α˜l
τ converges, i.e., 
. ∑∞
τ=1
α˜l
τ < ∞,
then the product.
|t
τ=1(1 + ˜αl
τ ) also converges (by the inequality.1 + x ≤ ex appli￾cation that is valid for any.x ∈ R), namely, 
.
 ∞
τ=1
(1 + ˜αl
τ ) < ∞, (6.4.5)6.5 Examples 127
which implies the existence of a limit (a convergence) of the sequence .
{
Vl
t
}
of the 
given loss-function too, i.e., 
.Vl
∞ := lim
n→∞ Vl
t = Vl,mon
∞
 ∞
τ=1
(1 + ˜αl
τ ). (6.4.6) 
Remark 6.4 Notice that by the ergodicity property (see Chap. 1) the infinit product 
in (6.4.6) always exists for ergodic Markov chains, that is,.
|∞
t=1(1 + ˜αl
t) < ∞, since 
by Corollary above 
. 
|∞
τ=1(1 + ˜αl
τ ) ≤ exp {∑∞
τ=1 α˜l
τ
}
≤ exp 
∑∞
τ=1
\[˙
l∗
−I
]
wl∗,pl
t−1
\
Vl
t
 
≤
exp ∑∞
τ=1
<pl
t−pl
τ−1,wl∗
>
Vl
t
 
≤ exp ∑∞
τ=1
<pl
τ−pl
τ−1,wl∗
>
Vl
τ
 
≤
exp 
|wl∗
|
c
∑∞
τ=1
 
 pl
τ − pl
t−1
 
 
 
≤ exp 
|wl∗
|
c
∑∞
τ=1
 
 
 
pl
τ − pl∗
 
− 
pl
τ−1 − pl∗
 
 
 
≤
exp 
2|wl∗
|
c
∑∞
τ=1
 
 
 
pl
τ − pl∗
 
 
 
≤ exp 
2|wl∗
|
c
∑∞
τ=1
Cl exp {
−Dl · τ
}
 
< ∞.
This means that the behavior of the sequence.
 
Vl,mon
t
 
may serve as an indicator of 
the convergence of the game: the approach of the vector-cost function .
 
Vl,mon
t
 
to 
its limit point .Vl,∗
t means that we are close to one of the equilibrium points of the 
game. Note that this convergence is exponential. 
6.5 Examples 
6.5.1 Example 1 (Banks Marketing Planning as Prisoner’s 
Dilemma) 
Consider the case of two Banks that are planning the marketing campaigns expen￾ditures for the next years. The repeated “Prisoner’s Dilemma”game can be used to 
represent the problem. If both Banks have an arrangement to leave marketing bud￾gets unchanged, then their profits stay at high levels. However, if one of the Banks 
defects and increases its marketing expenditures, it may earn a greater income at the 
expense of the other Bank. But, if both Banks increase their marketing budgets, the 
increased marketing efforts may balance each other and prove ineffective, resulting 
in lower profits. Let .N = 2 be the number of states and let .M = 2 be the number 
of actions. Assigning numerical values to the levels of profits of the marketing cam-128 6 Best-Reply Strategies in Repeated Games
paigns, where 10 means profits stay at high levels and 1 implies lower profits, the 
payoff matrices for.l = 1, 2 are as shown below: 
. V1
i j =
[ 5 1
10 3 ]
for Player1, V2
i j =
[
5 10
1 3 ]
for Player 2,
and let the transition matrix for.k = 1, 2 be defined as follows 
. 
π1
j|i1 =
[
0.0247 0.9753
0.9756 0.0244 ]
, π1
j|i2 =
[
0.5668 0.4332
0.9960 0.0040 ]
for Player 1,
. π2
j|i1 =
[
0.1904 0.8096
0.8612 0.1388 ]
, π2
j|i2 =
[
0.0027 0.9973
0.7693 0.2307 ]
for Player2.
The beginning profile is supposed to be uniform, that is, .Pl
(s0 = j) = 0.5 for any 
player.l = 1, 2 and its state. j = 1, 2. But as it follows from the statements above in 
the ergodic case this profile can be arbitrarily selected without any influence to the 
final equilibrium point. 
For .d1∗ and .d2∗ (6.3.5) the fixed local-optimal strategies, and .k∗ the best-reply 
strategy the following results have been obtained: 
. 
π1∗ =
[
0.0247 0.9753
0.9756 0.0244 ]
for n0 = 1, χ1
erg = 0.0247,
k1∗ = [1, 1] ,
w1∗ = [1.0986, 9.8290]
 
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
for Player 1,
. 
π2∗ =
[
0.1904 0.8096
0.8612 0.1388 ]
for n0 = 1, χ2
erg = 0.1904,
k2∗ = [1, 1] ,
w2∗ = [9.0482, 1.2775]
 
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
for Player 2.
• in Figs. 6.1 and 6.3 the state-value function behavior is shown (where during game 
repetition the states of the players fluctuate according to the given probabilistic 
dynamics) showing completely non-monotonic behavior;6.5 Examples 129
Fig. 6.1 Non-monotonic 
behavior of the cost-function 
for Player 1 
Fig. 6.2 Monotonic 
behavior of the 
Lyapunov-like function for 
Player 1 
• in Figs. 6.2 and 6.4 the corresponding Lyapunov-like functions (6.4.4) are plotted 
definitely demonstrating a monotonic decreasing behavior; 
• the results of the two methods clearly show that under the same fixed local￾optimal strategy the original cost functions converge non-monotonically to the 
values 5.4643 (for the first player) and 5.0854 (for the second player) and the cor￾responding Lyapunov-like functions converge monotonically to the values 5.4543 
and 4.9298, respectively which, obviously, are very close. 
6.5.2 Example 2 (Duel Game) 
In this example we consider the “Duel game”where Player I and Player II each have a 
gun loaded with exactly one bullet and stand 10 steps apart. The same mathematical 
interpretation has a military application in other repeated games such as “Fighter-130 6 Best-Reply Strategies in Repeated Games
Fig. 6.3 Non-monotonic 
behavior of the cost-function 
for Player 2 
Fig. 6.4 Monotonic 
behavior of the 
Lyapunov-like function for 
Player 2 
Bomber Duel Game”and “The Pursuit Game.”Starting with Player I, they take turns 
deciding whether to fire or not. Each time a player chooses not to fire, the other player 
takes one step forward before choosing whether to fire in turn. In other words, they 
start 10 steps apart facing each other and Player I decides whether to take a shot at 
Player II. If Player I does not, Player II takes a step forward and decides whether 
to take a shot at Player I. If Player II does not, Player I takes a step forward and 
decides whether to take a shot at Player II, and so on (players repeat the actions). 
The situation is grave because if a player fires and misses, the other can then simply 
not fire until they get next to each other and then shoot the opponent point blank. 
(Assume that if the players are next to each other, the one whose turn it is to shoot 
will certainly do so and will certainly hit the opponent.) The probability of hitting the 
opponent depends on the distance between them and on the skill of the shooter. Let 
.σ = 0, 1, 2, ..., 10 be the total number of steps taken by the players. Let .φσ denote6.5 Examples 131
the decision distance, i.e., the distance between the players, after. σ steps. Thus, the 
initial decision distance for Player I is. φ0, the next one is.φ1 for Player II, and so on. 
If players use pure strategies .(φI , φI I) where .φI is the decision distance at which 
Player I opens fire, and .φI I is the decision distance at which Player II opens fire, 
then the outcome of the game depends on who fires first and successfully shoots the 
other one. If .φI > φI I then Player I fires first with probability .pI(φI). If .φI < φI I
then Player II fires with probability .1 − pI I(φI I). Then their payoff functions are 
given by: 
.V I (φI , φI I) =
 pI(φI) if φI > φI I ,
1 − pI I(φI I) if φI < φI I , (6.5.1) 
and 
.V I I (φI , φI I) = 1 − V I (φI , φI I). (6.5.2) 
To obtain a payoff matrix we assign values to the parameters of the game. Let the 
total distance .Σ = 1 and let .φσ .= (0.1σ). Such that .pI(φσ).= 1 − φσ is the prob￾ability of Player I hitting Player II firing at distance .φσ, and . pI I(φσ) = 1 − (φσ)2
is the probability of Player II hitting Player I firing at distance .φσ. Let .N = 5 for 
the number of states and let .M = 2 be the number of actions (for simplicity). The 
payoff functions reflect the players’ desire to maximize the probability of survival. 
Then the payoff matrices for.l = 1, 2 are as follows: 
For Player1 
. V1
i j =
⎡
⎢
⎢
⎢
⎢
⎣
0.19 0.80 0.80 0.80 0.80
0.19 0.51 0.60 0.60 0.60
0.19 0.51 0.75 0.40 0.40
0.19 0.51 0.75 0.91 0.20
0.19 0.51 0.75 0.91 0.99
⎤
⎥
⎥
⎥
⎥
⎦
.
For Player2 
. V2
i j =
⎡
⎢
⎢
⎢
⎢
⎣
0.81 0.20 0.20 0.20 0.20
0.81 0.49 0.40 0.40 0.40
0.81 0.49 0.25 0.60 0.60
0.81 0.49 0.25 0.09 0.80
0.81 0.49 0.25 0.09 0.01
⎤
⎥
⎥
⎥
⎥
⎦
,
and let the transition matrices for.k = 1, 2 be defined as follows 
For Player 1 
. π1
j|i1 =
⎡
⎢
⎢
⎢
⎢
⎣
0.0107 0.0011 0.7950 0.0795 0.1136
0.0129 0.0012 0.9858 0.0000 0.0001
0.9869 0.0123 0.0000 0.0006 0.0001
0.6171 0.0818 0.0755 0.0887 0.1369
0.4784 0.0066 0.0368 0.1104 0.3679
⎤
⎥
⎥
⎥
⎥
⎦
, π1
j|i2 =
⎡
⎢
⎢
⎢
⎢
⎣
0.0112 0.0012 0.8514 0.0178 0.1185
0.0149 0.0000 0.9850 0.0001 0.0000
0.8117 0.0901 0.0089 0.0893 0.0000
0.3706 0.0740 0.4810 0.0078 0.0666
0.7246 0.0000 0.2415 0.0098 0.0241
⎤
⎥
⎥
⎥
⎥
⎦
.132 6 Best-Reply Strategies in Repeated Games
For Player 2 
. π2
j|i1 =
⎡
⎢
⎢
⎢
⎢
⎣
0.0836 0.0009 0.7397 0.0832 0.0925
0.1029 0.0013 0.8957 0.0000 0.0001
1.0000 0.0000 0.0000 0.0000 0.0000
0.3733 0.0121 0.6137 0.0001 0.0008
0.9984 0.0000 0.0001 0.0000 0.0015
⎤
⎥
⎥
⎥
⎥
⎦
, π2
j|i2 =
⎡
⎢
⎢
⎢
⎢
⎣
0.0637 0.0233 0.8187 0.0001 0.0943
0.1418 0.0000 0.8430 0.0001 0.0152
0.8346 0.0000 0.0002 0.1653 0.0000
0.7924 0.0900 0.0361 0.0005 0.0810
0.9988 0.0000 0.0001 0.0000 0.0011
⎤
⎥
⎥
⎥
⎥
⎦
.
The beginning profile is supposed to be uniform, that is,.Pl
(s0 = j) = 0.5 for any 
player.l = 1, 2 and its states. j = 1, ..., 5. But as it follows from the statements above 
in the ergodic case this profile can be arbitrarily selected without any influence to 
the final equilibrium point. 
For .d1∗ and .d2∗ (6.3.5) the fixed local-optimal strategies, and .k∗ the best-reply 
strategy the following results have been obtained: 
For Player 1 
. 
π1∗ =
⎡
⎢
⎢
⎢
⎢
⎣
0.0112 0.0012 0.8514 0.0178 0.1185
0.0149 0.0000 0.9850 0.0001 0.0000
0.9869 0.0123 0.0000 0.0006 0.0001
0.6171 0.0818 0.0755 0.0887 0.1369
0.7246 0.0000 0.2415 0.0098 0.0241
⎤
⎥
⎥
⎥
⎥
⎦
for n0 = 1, χ1
erg = 0.0112,
k1∗ = [2, 2, 1, 1, 2] ,
w1∗ = [0.7932, 0.5939, 0.1941, 0.3237, 0.3516] .
For Player 2 
. π2∗ =
⎡
⎢
⎢
⎢
⎢
⎣
0.0637 0.0233 0.8187 0.0001 0.0943
0.1029 0.0013 0.8957 0.0000 0.0001
0.8346 0.0000 0.0002 0.1653 0.0000
0.3733 0.0121 0.6137 0.0001 0.0008
0.9984 0.0000 0.0001 0.0000 0.0015
⎤
⎥
⎥
⎥
⎥
⎦
. 
for n0 = 1, χ2
erg = 0.0637,
k2∗ = [2, 1, 2, 1, 1] ,
w2∗ = [0.2389, 0.4423, 0.7752, 0.4624, 0.8087] .
• in Figs. 6.5 and 6.7 the state-value function behavior is shown (where during game 
repetition the states of the players fluctuate according to the given probabilistic 
dynamics) showing completely non-monotonic behavior; 
• in Figs. 6.6 and 6.8 the corresponding Lyapunov-like functions (6.4.4) are plotted 
definitely demonstrating a monotonic decreasing behavior;6.5 Examples 133
Fig. 6.5 Non-monotonic 
behavior of the cost-function 
for Player 1 
Fig. 6.6 Monotonic 
behavior of the 
Lyapunov-like function for 
Player 1 
Fig. 6.7 Non-monotonic 
behavior of the cost-function 
for Player 2134 6 Best-Reply Strategies in Repeated Games
Fig. 6.8 Monotonic 
behavior of the 
Lyapunov-like function for 
Player 2 
• the results of the two methods clearly show that under the same fixed local￾optimal strategy the original cost functions converge non-monotonically to the 
values 0.4543 (for the first player) and 0.5453 (for the second player) and the cor￾responding Lyapunov-like functions converge monotonically to the values 0.4456 
and 0.5316, respectively which, obviously, are very close. 
As it follows from both examples, the existence of a monotonic decreasing behav￾ior in the constructed Lyapunov-like functions for all players allows to conclude that 
the considered game has a tendency to evaluate (converge) to an equilibrium point. 
Conversely, the non-monotonic cost-functions do not permit to get this conclusion: 
one can not say during the repeating game whether the selected best-reply strategy 
leads to an equilibrium or not. There also is no guarantee that the non-monotonicity 
of the cost-functions will converge on an equilibrium point. 
From these examples, we also conclude that the proposed method solves a game 
via the elimination of sequentially unreasonable strategies. A strategy for player. l is 
eventually optimal if,. l’s strategy is the optimal strategy among all strategies. l could 
play using a Lyapunov-like function. A strategy for player. l is eventually dominant if 
it is eventually the best-reply against every strategy of the other player. We conclude 
that the Lyapunov-like function process eliminates strategies that are not the best 
reply to some strategy profile. Then, there is a best-reply strategy that follows the 
monotonic decreasing behavior of the Lyapunov-like function which is eventually 
dominant for the class of rational strategies against regular strategies. 
References 
1. Bellman, R.: Vector Lyapunov functions. SIAM J. Control Optim. 1, 32–34 (1962) 
2. Bernheim, B.D.: Rationalizable strategic behavior. Econometrica 52, 1007–1028 (1984) 
3. Börgers, T.: Pure strategy dominance. Econometrica 61, 423–430 (1993)References 135
4. Chen, X., Deng, X.: Setting the complexity of 2-player Nash equilibrium. In: Proceedings of 
IEEE FOCS (2006) 
5. Clempner, J.B.: On Lyapunov game theory equilibrium: static and dynamic approaches. Int. 
Game Theory Rev. 20(2), 1750033 (2018) 
6. Clempner, J.B.: A Lyapunov approach for stable reinforcement learning. Comput. Appl. Math. 
41, 279 (2022) 
7. Clempner, J.B., Poznyak, A.S.: Convergence method, properties and computational complexity 
for Lyapunov games. Int. J. Appl. Math. Comput. Sci. 21(2), 349–361 (2011) 
8. Clempner, J.B., Poznyak, A.S.: Analysis of best-reply strategies in repeated finite Markov 
chains games. In: IEEE Conference on Decision and Control (2013) 
9. Clempner, J.B., Poznyak, A.S.: Analyzing an optimistic attitude for the leader firm in duopoly 
models: a strong Stackelberg equilibrium based on a Lyapunov game theory approach. Econ. 
Comput. Econ. Cybern. Stud. Res. 50(4), 41–60 (2016) 
10. Clempner, J.B., Poznyak, A.S.: Convergence analysis for pure stationary strategies in repeated 
potential games: Nash, Lyapunov and correlated equilibria. Expert Syst. Appl. 46, 474–484 
(2016) 
11. Clempner, J.B., Poznyak, A.S.: Using the extraproximal method for computing the shortest￾path mixed Lyapunov equilibrium in Stackelberg security games. Math. Comput. Simul. 138, 
14–30 (2017) 
12. Daskalakis, C., Goldberg, P., Papadimitriou, C.: The complexity of computing a Nash equilib￾rium. In: Proceedings of ACM STOC, pp. 71–78 (2006) 
13. Goemans, M., Mirrokni, V., Vetta, A.: Sink equilibria and convergence. In: Proceedings of the 
46th IEEE Symposium on Foundations of Computer Science (2005) 
14. Guesnerie, R.: Anchoring economic predictions in common knowledge. Econometrica 70, 
439–480 (1996) 
15. Hernández-Lerma, O., Lasserre, J.B.: Discrete-Time Markov Control Process: Basic Optimality 
Criteria. Springer, Berlin (1996) 
16. Hilas, J., Jansen, M., Potters, J., Vermeulen, D.: Independence of inadmissible strategies and 
best reply stability: a direct proof. Int. J. Game Theory 32, 371–377 (2003) 
17. Hofbauer, J., Sandholm, W.: Stable games and their dynamics. J. Econ. Theory 144(4), 1665– 
1693 (2009) 
18. Moulin, H.: Dominance solvability and Cournot stability. Math. Social Sci. 7, 83–102 (1984) 
19. Osborne, M., Rubinstein, A.: A Course in Game Theory. M.I.T. Press, Cambridge, MA (1994) 
20. Pearce, D.G.: Raionalizable strategic behavior and the problem of perfection. Econometrica 
52, 1029–1050 (1984) 
21. Poznyak, A.S., Najim, K., Gomez-Ramirez, E.: Self-learning Control of Finite Markov Chains. 
Marcel Dekker, New York (2000) 
22. Tan, T., Costa Da Werlang, S.R.: On layman’s notion of common knowledge, an alternative 
approach. J. Econ. Theory 45, 370–391 (1988)Chapter 7 
Mechanism Design 
Abstract This chapter presents an analytical method for computing Bayesian 
incentive-compatible mechanisms where the private information is revealed follow￾ing a class of controllable Markov games. We take into account a dynamic setting 
where decisions are made after a number of limited time periods. Our approach 
includes a new variable that denotes the outcome of the distribution vector, the 
strategies, and the mechanism design. We develop the relationships needed to calcu￾late the relevant variables analytically. The issue becomes computationally tractable 
with the addition of this variable. The technique uses a Reinforcement Learning 
(RL) methodology to calculate a mechanism that is nearly optimum and in equilib￾rium with the game’s winning strategy. We employ the Bayesian-Nash equilibrium 
concept as the default equilibrium idea in the game. There are several equilibria, 
which presents an intriguing problem because there isn’t a single mechanism that is 
ideal for the goal of profit maximization. To address this issue, we apply Tikhonov’s 
approach and offer a regularization parameter. We show the game’s equilibrium and 
convergence to a single mechanism that is compatible with incentives. This results in 
numerous game theory issue areas having unique and significantly improved results, 
as well as incentive-compatible processes that are consistent with the equilibrium of 
the game. To illustrate the recommended method, we provide a numerical example 
in the context of a dynamic public finance model with incomplete knowledge. 
7.1 Introduction 
7.1.1 Brief Review 
Informally studied notions for producing specific results in a class of self-interested 
private information games are known as mechanism design. It is an engineering￾based approach to resolving problems in game theory. The mechanisms aim to reach a 
Bayesian-Nash equilibrium and make the assumptions that incentives are provided by 
monetary transfers and that valuations are private. The players attempt to maximize 
their own self-interest since they are logical beings. Players who are operating in 
their own self-interest are not driven to provide accurate information. The presence 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_7 
137138 7 Mechanism Design
of an incentive-compatible mechanism was established by the mechanism design. 
If a player consistently optimizes his rewards by disclosing his real type, regardless 
of what the other players do (declare), the mechanism is considered to be honest 
(subject to an incentive-compatibility limitations). It can be difficult to calculate an 
efficient process that maximizes rewards. The basic objective is to provide a system 
for selfish actors that maximizes rewards at equilibrium [ 5]. 
There has been a lot of interest in learning how to construct mechanisms and RL 
approaches during the past ten years. A worldwide difficulty is broken down into 
smaller, more manageable problems that are handled each time the agents exchange 
data, according to Goldman and Zilberstein [ 16]. Jain and Walrandb suggested a 
method for auctioning collections of various divisible products on a network. Berge￾mann and Välimäk [ 2, 6] described a dynamic Vickrey-Clarke-Groves strategy that 
takes into account quasilinear payoffs in which agents view private information. 
Pavan et. al. [ 23] considers dynamical mechanism design in dynamic quasilinear 
environments where private information arrives over time. Sinha and Anastasopou￾los [ 25] proposed a mechanism design for a network in which fixed groups are 
formed from strategic agents that are competing for resource allocation. Using extra 
rewards and punishments based on the actions of the learners, Baumann et al. [ 4] 
investigated how an external agent may motivate artificial learners to collaborate. 
Mguni [ 20] suggested a method that incorporates stochastic optimization and RL 
into mechanism design in order to efficiently compute the best incentive compatible 
mechanisms. Clempner and Poznyak [ 11] suggested a Bayesian technique for games 
that expanded the design theory to incorporate mechanism design and joint observer 
design while accounting for the imperfect information in the Bayesian model and the 
incomplete knowledge about the states of the Markov system. A Bayes-adaptive RL 
method based on model-based online planning was proposed by Grover et al. [ 17]. 
A BRL model for robots based on an approximative parametric technique, incorpo￾rating live Bayesian estimation and planning for an estimated model, was suggested 
by Senda et al. [ 24]. Kassab and Simeone [ 19] suggested a gradient descent method 
for federated learning in a Bayesian setting. In order to efficiently perform Bayesian 
estimate, Nolan et al. [ 22] presented a parameter estimation approach based on a clas￾sification task. van Geen and Gerraty [27] devised a technique for obtaining empirical 
priors using hierarchical Bayesian modeling. A multiscale strategy for model order 
reduction and machine learning technology were the two separate preconditioning 
strategies that Vasilyeva et al. [ 26] provided. Clempner and Poznyak [ 12] suggested 
an analytical approach for calculating the mechanism design in the context of a 
paradigm in which participants in a non-cooperative Markov game with imperfect 
state knowledge pursue an average utility. For a class of controllable homogeneous 
Markov games, Clempner [ 7] developed a dynamic Bayesian-Stackelberg incentive￾compatible mechanism in which many agents see private information and learn their 
behavior through a series of encounters in a repeating game, where it is presumed that 
leaders may commit to their disclosure approach and method in advance and influ￾ence followers’ behavior. Clempner [ 8] presented the Price of Anarchy and the Price 
of Stability for incomplete information in Bayesian-Markov games. Clempner [ 13] 
suggested an analytical method for computing Bayesian incentive-compatible mech-7.1 Introduction 139
anisms where the private information is revealed following a class of controllable 
Markov games. Clempner and Poznyak [15] developed the Price of Anarchy in mech￾anism design for Pareto-Bayesian-Markov games in which the players privately know 
their information. The same authors [ 14] presented a dynamic Bayesian-Stackelberg 
incentive-compatible mechanism, in which multiple agents observe private informa￾tion and learn their behavior through a sequence of interactions in a repeated game 
for a class of controllable homogeneous Markov games. 
The main results of this chapter are as follows: 
– Suggests the strategy for building an incentive-compatible mechanism in a dynamic 
environment where participants learn about their preferences via repeated interac￾tions. 
– Exposes the private information after a class of controlled Markov games using 
the analytical approach we suggest for constructing Bayesian incentive-compatible 
mechanisms . 
– Presents a new variable that denotes the outcome of the distribution vector, as well 
as the strategies, and the mechanism design. We develop the relationships needed 
to calculate the relevant variables analytically. The problem becomes computa￾tionally tractable with the addition of this variable. 
– Uses an iterative method divided into two halves: the gradient method and the 
proximal approach, in order to calculate the equilibrium in Markov games. For 
our game, the idea of Bayesian-Nash equilibrium serves as the equilibrium concept. 
With the help of a nonlinear programming solver, this method transforms the game 
theory issue into a system of equations, which is an independent optimization 
problem. 
– Demonstrates how the strategy will eventually reach the equilibrium point. We 
also take into account a dynamic setting where participants learn about their pref￾erences via repeated encounters and choose their course of action after a number 
of limited intervals. The technique uses an RL methodology to compute the nearly 
optimum mechanism in equilibrium with the ensuing high profit maximization 
game strategy. In order to compute near-optimal policies, we design a controller 
exploitation-exploration architecture. 
– Employs the controller Kullback-Leibler divergence among the distribution of the 
policies to trade-off between the exploration and the exploitation processes [ 1]. 
Using our method, players adjust their behavior in response to a mechanism that 
is computed in equilibrium by choosing the best-reply strategies. We demonstrate 
convergence of the RL technique. There are several equilibria, which presents an 
intriguing problem because there isn’t a single mechanism that is ideal for the goal 
of profit maximization. 
– Applies the regularization Tikhonov’s strategy to tackle this issue, which is a well￾known method for resolving ill-posed presented minimization problems [ 9, 10]. 
– We demonstrate the game’s equilibrium as well as the convergence to a singular 
incentive-compatible mechanism. This generates novel and considerably better 
answers for many game theory problem areas, as well as incentive-compatible 
mechanisms that correspond to the game’s equilibrium.140 7 Mechanism Design
– The approach seeks approximate to. y by substituting the ill-posed problem given 
by . min
y∈Yadm
ζ(y) introducing a penalized problem given by . ζδ(y) = ζ(y) + δ
2 |y|2
where .|·| denotes the Euclidean vector norm and the scalar .δ > 0 is known as 
the regularization parameter. The expression.
δ
2 |y|2 penalizes the large values of 
. y. In game theory, regularization plays a fundamental role in order to ensure the 
convergence to one of the Nash equilibria. 
7.2 Description of the Model 
We consider a discrete-time repeated game played by a set .N = {1, ..., n} players 
indexed by.l ∈ N . At each time.t ≥ 1, the player. l privately informed about his type 
(state).θl
t ∈ Θl
t . Players simultaneously take an action (make decisions).al
t ∈ Al
t . Let 
.At = ʘn
l=1
Al
t . We write .Θt for .
ʘ
l∈N
Θl
t , .Θ−l
t for .
ʘ
h∈N,h/=l
Θh
t and .Δ(A) for the set of 
all probability distributions over.A. We assume that.Al
t and.Θl
t are finite sets for all 
.l ∈ N . 
A social choice function is a mapping from profile of types to lotteries over 
alternatives if all players report their type .θl
t ∈ Θt , i.e., . f : Θt → Δ(At). A social 
choice function represents the goals of the game, e.g., to maximize revenues, etc. 
Finally, each player has a known valuation function.vl
(al
t, θl
t) ≥ 0, which determines 
actual value for the player. l. Each.vl : A × Θ → R+ is a concave mapping from the 
alternatives and the set of type profiles to a set of non-negative real numbers. We 
assume that the type. θl
t of player. l follows a controllable Markov process on the state 
space.Θl
t as follows. A controllable Markov chain is a sequence of.θ-valued random 
variables.θt,.t ∈ T, satisfying the Markov condition: 
.pl
(θl
t+1|θl
t, al
t, θl
t−1, al
t−1 ..., θl
0, al
0)=pl (
θl
t+1|θl
tal
t
)
, (7.2.1) 
which represents the probability associated with the transition function (or stochas￾tic kernel) from state .θl
t to state .θl
t+1, under an alternative . al
t . The common prior 
(initial distribution) of the state-alternative process for each player . l denoted by 
.{(Θt, At)|t ∈ T }. In this case, the process described by a Markov chain is completely 
described by the transition function.pl (
θl
t+1|θl
t, al
t
)
and the initial distribution vector 
.Pl
0(ΔΘl
0) such that .Pl
t
(
θl
t
)
∈ ΔΘl
t , where .ΔΘl
t denotes the set of all probability 
distributions over.Θl
t . The Markov chains are mutually independent. We assume that 
each chain.(Pl
, p(θl
t+1|θl
t, al
t)) is irreducible and aperiodic, and that.Pl is its unique 
invariant distribution. 
Definition 7.1 A mechanism . μ in the above environment assigns a set of possible 
messages.Ml
t to the player. l. At each time. t, the player. l sends a message.ml
t from this 
set and the mechanism . μ responds with a (possibly randomized) decision that may 
depend on the entire history of messages sent up to time. t, and on past decisions.7.2 Description of the Model 141
Hence, the mechanism.μ(at|mt) has inputs.at that is the current allocation for the 
players and.mt = (m1
t , ..., mn
t ), which is the joint set of messages made by the player 
. l. Let us consider an allocation rule . g, which represents the probability measure 
associated with the occurrence of an alternative .at from the profile of messages . mt
at time .t ∈ T . It is is a mapping from the message .mt to lotteries over alternatives 
.Δ(At), i.e.,.g : Mt → Δ(At). Formally, a mechanism. μ is a pair.(Mt, g) where . Mt
is the set of messages for player. l and.g : Mt → Δ(At) is the allocation rule. Let. U
be a set of admissible mechanisms . We have that 
. Uadm=
⎧
⎨
⎩
μ(at|mt) ≥ 0|
∑
at∈At
μ(at|mt)=1, mt ∈ Mt
⎫
⎬
⎭ . (7.2.2) 
The mechanism . μ is interpreted as the probability that .at will be the outcome if 
the profile of types.θt and messages.mt are the players’ types. We write 
.μ(g, M1
t × .... × Mn
t ) ∈ Uadm, g(mt) = Δ(At), (7.2.3) 
where.mt ∈ Mt . 
A dynamic mechanism induces a dynamic game with incomplete information. 
The following sequence of events takes place in each period. t: 
– At the beginning of each period. t, the designer announces publicly the mechanism 
to the players and each agent privately learns his current type.θl
t ∈ Θl
t drawn from 
.pl (
θl
t+1|θl
t, al
t
)
. 
– Next, players sent messages .ml
t simultaneously, and the profile of messages is 
publicly observed.mt . 
– The mechanism selects a decision .at ∈ At according to .μ(at|mt) and the imple￾mented alternative.at is publicly announced. 
With this assumption, the public history in period. t is a sequence of messages. ml
t
and alternatives.at(θt) until period.t − 1. The state of an observer is the vector 
.hl
t = (ml
0, al
0, ..., ml
t−1, al
t−1, ml
t), (7.2.4) 
which is a trajectory of length. t called the history (public history). The public history 
.hl
t stands for a generic element of.Ht , which is the set of possible public histories in 
period. t: 
(i) .Ht is the set of possible states in period. t, captures all information relevant to 
the decision by the observer in that period, and 
(ii) each.mt = (ml
0, ..., ml
t) is a report profile of the players. l. 
The sequence of reports by the players is part of the public history and we assume 
that the past reports of each player are observable to all the players. The private 
history of player . l in period . t consists of the sequence of private observations and 
the public history until period. t,142 7 Mechanism Design
.h˜l
t = (θl
0, ml
0, al
0, ...θl
t−1, ml
t−1, al
t−1, θl
t). (7.2.5) 
The set of possible private histories for each player . l in period . t is denoted by 
.Hl
t = Θt × Ht . 
Definition 7.2 A (behavioral) strategy .σl
(ml
t|θl
t) for player . l is a mapping . σl :
Hl × Θl → Δ(Ml
). The set of all admissible policies is denoted by.Sl
adm: 
. Sl
adm=
(
σl
(ml
t|θl
t) ≥ 0 | ∑
ml
t∈Ml
t
σl
(ml
t|θl
t)=1, θl
t ∈ Θl
t
)
. (7.2.6) 
The valuation functions.vl
(al
t, θl
t) and the transition functions.pl (
θl
t+1|θl
tal
t
)
are all 
common knowledge at time. t. The common prior initial distribution vector. Pl
0(ΔΘl
0)
and the transition function.pl (
θl
t+1|θl
tal
t
)
are assumed to be independent across play￾ers. The interaction between players induces a Markov game given by the 5-tuple 
. Γ = (N , Θl
, Al
, Pl
, Ul
)l∈N ,
where 
. 
Ul
T (μ, σ) = ∑
t∈T
∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t ∈Al
t
υl
(al
t(θt), θl
t)pl
(θl
t+1|θl
tal
t) ∏
ι∈N
μ(at|mt)σι
(mι
t|θι
t)Pι
(θι
t) =
∑
t∈T
∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t ∈Al
t
Wl
(al
t, θl
t) ∏
ι∈N
μ(at|mt)σι
(mι
t|θι
t)Pι
(θι
t).
Here 
. Wl
(θl
tal
t) = ∑
θl
t+1∈Θl
t
υl
(al
t(ml
t), θl
t)pl
(θl
t+1|θl
t, al
t).
7.3 Mechanism and Equilibrium 
We assume that players know their payoffs. A mechanism.μ(at|mt) and the strategy 
.σl
(ml
t|θl
t) maximize the payoff function.Ul
(μ, σ) realizing the rule given by 
.(μ∗, σ∗) := arg max
μ∈Uadm
max
σ∈Sadm
∑
l∈N
Ul
(μ, σ). (7.3.1) 
The mechanism.μ∗ and the strategy.σ∗ satisfies the Bayesian-Nash equilibrium ful￾filling for all. σ the condition 
.Ul
(μ∗, σ∗) ≥ Ul
(μ, σl
, σ−l
). (7.3.2)7.3 Mechanism and Equilibrium 143
where the mechanism . μ is unique for all participants and the strategy . σ∗ = (
σ1∗, ..., σn∗
)
is referred to as a Bayesian-Nash equilibrium where . σ is such that 
.σ−l∗ =.
(
σ1∗, ..., σl−1∗, σl+1,∗, ..., σn∗
)
. 
The dynamic revelation principle in [ 18, 21] shows that there is no loss of gen￾erality in restricting attention to direct mechanisms where the players report their 
information truthfully on the equilibrium path: “for any equilibrium of any other coor￾dination game, which the individuals might play, there exists an equivalent incentive￾compatible mechanism.” This idea, called the revelation principle showed that direct 
mechanisms are the same as indirect mechanisms (.g(at|mt)) = f (at|θt))). 
Let us introduce the.ξ-variable as follows 
. ξl
(θl
tml
tal
t) := μ(at|mt)σl
(ml
t|θl
t)Pl
(θl
t). (7.3.3) 
Formulation of the problem. We will try to find an auxiliary variable . ξι
(θι
tmι
taι
t)
which solve the following individual nonlinear programming problem 
.
Ul
(μ, σ) = ∑
l∈N
U¯ l
(ξ) → max
ξ∈Ξadm
,
U¯ l
(ξ) = ∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t∈Al
t
Wl
(al
t, θl
t) ∏
ι∈N
ξι
(θι
tmι
taι
t)
,
⎫
⎪⎪⎬
⎪⎪⎭
(7.3.4) 
where.ξl
(θl
tml
tal
t) is given in Eq. (7.3.3) and.Ξadm = ʘ
l Ξl
adm with 
.
Ξl
adm:=(
ξl
(θl
tml
tal
t)
|
|
|
|
|
∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t∈Al
t
ξl
(θl
tml
tal
t)=1,
∑
ml
t∈Θl
t
∑
al
t∈Al
t
ξl
(θl
tml
tal
t) = Pl
(θl
t) > 0,
∑
ml
t∈Ml
t
∑
al
t∈Al
t
∑
θl
t∈Θl
t
[δθl
t θl
t+1 − pl
(θl
t+1|θl
tal
t)]ξl
(θl
tml
tal
t)=0, θl
t+1 ∈ Θl
t
)
.
(7.3.5) 
Note that the following relations holds. 
. 
∑
al
t∈Al
t
μ(at|mt) = 1, ∑
mt∈Mt
σ(mt|θt) = 1, ∑
θl
t∈Θl
t
Pl
(θl
t) = 1.
It is easy to check that.ξl ∈ Δl
, where 
. Δl
:=
⎧
⎨
⎩
ξl
(θl
tml
tal
t)
|
|
|
|
|
|
∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t ∈Al
t
ξl
(θl
tml
tal
t)=1, ∑
ml
t∈Ml
t
∑
al
t ∈Al
t
ξl
(θl
tml
tal
t) = Pl
(θl
t) > 0,
⎫
⎬
⎭ .
(7.3.6)144 7 Mechanism Design
Define the solution of the problem (7.3.4) as.ξl∗. The next theorem and lemma clarify 
how we may recover.μ∗(at|mt),.σl∗(ml
t|θl
t) and.Pl∗(θl
t). 
Theorem 7.1 The strategy .σl∗(ml
t|θl
t) and the mechanism .μ∗(at|mt) are in a 
Bayesian-Nash equilibrium where every agent maximizes its expected payoff, for 
every.l ∈ N , 
. Ul
(μ∗(at|mt), σ∗(ml
t|θl
t)) ≥ Ul
(μ(at|mt), σ(ml
t|θl
t)),
if the quantities of.ξl
(θl
tml
tal
t) satisfies 
.
∑
ml
t∈Ml
t
∑
al
t∈Al
t
∑
θl
t∈Θl
t
[δθl
t θl
t+1 − pl
(θl
t+1|θl
tal
t)]ξl
(θl
tml
tal
t) = 0, θl
t+1 ∈ Θl
t . (7.3.7) 
Proof Let.ξl := [
ξl
(θl
tml
tal
t)
]
as in (7.3.3) and.U˜ l
(ξ) as in (7.3.4). Suppose that the 
set of strategies.Ξl
adm satisfies that: 
(a) each.ξl
(θl
tml
tal
t) represents a mixed joint strategy that belongs to the simplex. Δ
defined by 
. Δ=
⎧
⎨
⎩
ξl
(θl
tml
t al
t)
|
|
|
|
|
|
∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t ∈Al
t
ξl
(θl
tml
t al
t)=1, ξl
(θl
tml
t al
t)≥0, ∑
ml
t∈Ml
t
∑
al
t ∈Al
t
ξl
(θl
tml
t al
t) > 0
⎫
⎬
⎭ ,
(b) the joint strategy.ξl
(θl
tml
tal
t) fulfill the ergodicity constraint and, then it belongs 
to the convex, closed and bounded set given by 
. 
El
=
(
ξl
(θl
tml
tal
t)
|
|
|
|
|
∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t∈Al
t
pl
(θl
t+1|θl
tal
t)ξl
(θl
tml
tal
t)
− ∑
ml
t∈Ml
t
∑
al
t∈Al
t
ξl
(θl
t+1ml
tal
t)=0, ∑
ml
t∈Ml
t
∑
al
t∈Al
t
ξl
(θl
tml
tal
t) > 0
)
.
Then, .ξl
(θl
tml
tal
t) ∈ Ξl
adm := Δl × El
, and we have that the ergodicity constraints 
defined in (7.3.7) satisfies 
. 
∑
ml
t∈Ml
t
( ∑
al
t∈Al
t
∑
θl
t∈Θl
t
[
pl
(θl
t+1|θl
tal
t)ξl
(θl
tml
tal
t) − ξl
(θl
t+1ml
tal
t)
]
)
=
∑
ml
t∈Ml
t
( ∑
al
t∈Al
t
∑
θl
t∈Θl
t
[
pl
(θl
t+1|θl
tal
t) − δθl
t θl
t+1
]
)
ξl
(θl
tml
tal
t) = 0.
. ∎
Lemma 7.1 Let us suppose that the problem (7.3.4) is solved, then the variable 
.μ∗(at|mt) can be recovered from.ξl
(θl
tml
tal
t) as follows:7.3 Mechanism and Equilibrium 145
. μ∗(at|mt) =
∑
l∈N
∑
θl
t∈Θl
t
ξl
(θl
tml
tal
t)
∑
l∈N
∑
θl
t∈Θl
t
∑
al
t∈Al
t
ξl(θl
tml
tal
t)
. (7.3.8) 
Proof The mechanism design .μ∗(at|mt) may be obtained from Eqs. (7.3.5) and 
(7.3.6) as follows: 
. 
∑
l∈N
∑
θl
t∈Θl
t
ξl
(θl
tml
tal
t):=μ∗(at|mt)
∑
l∈N
∑
θl
t∈Θl
t
σl∗(ml
t|θl
t)Pl∗(θl
t).
Hence, 
. μ∗(at|mt) =
∑
l∈N
∑
θl
t ∈Θl
t
ξl∗(θl
tml
t al
t)
∑
l∈N
∑
θl
t ∈Θl
t
σl∗(ml
t|θl
t)Pl∗(θl
t) =
∑
l∈N
∑
θl
t ∈Θl
t
ξl∗(θl
tml
t al
t)
∑
l∈N
∑
θl
t ∈Θl
t
∑
ml
t ∈Ml
t
∑
al
t ∈Al
t
ξl∗(θl
tml
t al
t)
.
To verify that the definition of.μ∗(at|mt) is correct we need to check the fulfilling of 
Eq. (7.2.2), i.e..μ∗(at|mt) ∈ Uadm. This property holds directly 
.μ∗(at|mt) =
∑
l∈N
∑
θl
t ∈Θl
t
ξl∗(θl
tml
t al
t)
∑
l∈N
∑
θl
t ∈Θl
t
σl∗(ml
t|θl
t)Pl∗(θl
t) =
∑
l∈N
∑
θl
t ∈Θl
t
ξl∗(θl
tml
t al
t)
∑
l∈N
∑
θl
t ∈Θl
t
∑
al
t ∈Al
t
ξl∗(θl
tml
t al
t) ≥ 0. (7.3.9) 
since .ξl∗(θl
tml
tal
t) ≥ 0. Summing (7.3.9) by .al
t directly leads to the property 
.
∑
at∈At
μ∗(at|mt).= 1.. ∎
In order to recover.σl∗(ml
t|θl
t), we have that for each player.l = 1, n the quantity 
of interest is given by 
. σl∗(ml
t|θl
t) =
∑
al
t ∈Al
t
ξl
(θl
tml
t al
t)
∑
ml
t ∈Ml
t
∑
al
t ∈Al
t
ξl(θl
tml
t al
t)
. (7.3.10) 
As well as, for distribution.Pl∗(θl
t) we have 
. Pl∗(θl
t) = ∑
ml
t∈Ml
t
∑
al
t∈Al
t
ξl
(θl
tml
tal
t) > 0. (7.3.11) 
We have derived the formulas, which solve the problem (7.3.4) based on the vari￾ables.ξl
(θl
tml
tal
t) and the formulas to recover the strategy.σl∗(ml
t|θl
t), the mechanism 
.μ∗(at|mt) and the distribution .Pl∗(θl
t). If the players report their type using some 
reporting strategy.σl
(ml
t|θl
t), they are maximizing the expected payoff given in prob￾lem (7.3.4). The resulting strategy profile.σl∗(ml
t|θl
t)is a Bayesian-Nash equilibrium.146 7 Mechanism Design
A mechanism .μ∗(at|mt) is said to be incentive compatible if the relation (7.3.1) is 
satisfied. 
Lemma 7.2 The obtained mechanism.μ∗(at|mt) and the strategies.σl∗(ml
t|θl
t)satisfy 
the Bayesian-Nash equilibrium defines by Eq. (7.3.2). 
Proof It results straightforward from: 
.
max
ξ∈Ξadm
U¯ (ξ) = U¯ (ξ∗) = ∑
l∈N
U¯ l
(ξ∗) = ∑
l∈N
Ul
(μ∗, σ∗) =
∑
l∈N
( ∑
θl
t∈Θl
t
∑
ml
t∈Ml
t
∑
al
t∈Al
t
Wl
(al
t, θl
t, ml
t)(μ∗(at|mt))nσl∗(ml∗
t |θl∗
t )Pl∗(θl∗
t )·
∏
ι/=l∈N
σι∗(mι∗
t |θι∗
t )Pι∗(θι∗
t )
)
= ∑
l∈N
(
max
σl∈Sl
adm
∑
ml
t∈Ml
t
∑
al
t∈Al
t
(μ∗(at|mt))n·
∑
θl
t∈Θl
t
Wl
(al
t, θl
t, ml
t)σl
(ml
t|θl
t)Pl
(θl
t) ∏
ι/=l∈N
σι∗(mι∗
t |θι∗
t )Pι∗(θι∗
t )
)
≥
∑
l∈N
( ∑
ml
t∈Ml
t
∑
al
t∈Al
t
(μ∗(at|mt))n ∑
θl
t∈Θl
t
Wl
(al
t, θl
t, ml
t)σl
(ml
t|θl
t)Pl
(θl
t)·
∏
ι/=l∈N
σι
(mι
t|θι
t)Pι
(θι
t)
)
= ∑
l∈N
Ul
(μ, σl
, σ−l
)
(7.3.12) 
From this inequality it follows that 
.
∑
l∈N
(
Ul
(μ∗, σ∗) − Ul
(μ∗, σl
, σ−l
)
)
≥ 0. (7.3.13) 
Given that the inequality in Eq. (7.3.13) is valid for all admissible strategies. σ, it is 
valid when.σ j = σ j∗ for. j /= l, having 
.Ul
(μ∗, σ∗) − Ul
(μ∗, σl
, σ−l
) ≥ 0, (7.3.14) 
which coincides with Eq. (7.3.2) when.μ = μ∗. Lemma is proven.. ∎
7.4 Reinforcement Learning Approach 
In this section, we are interested in the problem of finding a strategy and mechanism 
that maximizes the expected reward of the players that chooses actions sequentially. 
We suggest an asymptotic algorithm of the controlled Markov game based on a 
stochastic approximation method [ 1]. This method consists of constructing a recur￾rent procedure which produces the randomized Markov laws such that the expected 
reward takes its extreme value in the limit (.t → ∞).7.4 Reinforcement Learning Approach 147
Formally, we consider a discrete-time.t ∈ T repeated game played by a set. N =
{1, ..., n} players indexed by .l ∈ N . Let us denote .H∞ the set of complete infinite 
histories, i.e., the set of sequences of realized types, mechanisms, messages and 
alternatives. For an history .h ∈ H∞ and .(θl
t, al
t) ∈ Θl
t × Al
t its corresponding finite 
sequence of realized types and actions and let us denote for each . (θl
t+1, θl
t, al
t) ∈
Θl
t × Θl
t × Al
t
. ηl
(ˆ
θl
t+1 ˆ
θl
tal
t) = ∑
t∈T
χ(ˆ
θl
t+1, ˆ
θl
t, al
t)
the average discounted number of times along the history. h where the realized type 
profile. θˆ implement the action. a, as well as, let us denote 
. ηl
(ˆ
θl
tal
t) = ∑
t∈T
χ(ˆ
θl
t = ˆ
θl
, al
t = al
)
the average discounted number of times along the history. h where the realized type 
profile is .ˆ
θl
t implement the action .at such that the indicator function .χ(·) is defined 
as follows: 
(i) .χ(et) = 1 if the event. e occurs at period. t, and 
(ii) .χ(·) = 0 otherwise. 
We have that 
.p˜
l
(
θˆl
t+1|θˆl
t, al
t
)
= ηl
(ˆ
θl
t+1 ˆ
θl
tal
t)
ηl(ˆ
θl
tal
t) . (7.4.1) 
The Eq. (7.4.1) denotes the number of periods. t where. x is implemented considering 
the periods where the type profile is. θ. We can write the discounted payoff of player 
. l as a function of the discounted measure.ηl
(θˆl
t+1, θˆl
t, al
t) of states and outcomes, 
.
υ˜l
(
ˆ
θl
t+1, ˆ
θl
t, al
t
)
=
∑
T
t=0
ζl
(ˆ
θl
t+1, ˆ
θl
t, al
t)χ(ˆ
θl
t+1 ˆ
θl
tal
t)
ηl(ˆ
θl
t+1, ˆ
θl
t, al
t) , (7.4.2) 
where .ζl
(θˆl
t+1, θˆl
t, al
t) = υl
(θˆl
t+1, θˆl
t, al
t) + ζl
r such that .ζl ≤ Ul and . r takes ran￾domly the values.−1 or. 1. We estimate.υ˜l
(
ˆ
θl
t+1, ˆ
θl
t, al
t
)
in a finite number of steps. 
Let us consider a game whose strategies are denoted by .ψl ∈ Ψl where .Ψ is a 
convex and compact set where 
.ψl := col (
ξl
(θl
tml
tal
t)
)
, Ψl := Ξl
adm , Ψ := ʘn
l=1
Ψl
. (7.4.3) 
Denote by .ψ = (ψ1, ..., ψn)T ∈ Ψ the joint strategy and .ψˆl is a strategy of the rest 
of the players adjoint to. ψl
,148 7 Mechanism Design
. ψˆl := (
ψ1
, ..., ψl−1
, ψl+1
, ..., ψn)T
∈ Ψ ˆl := ʘn
m=1,m/=l
Ψ m
such that.ψ = (ψl
, ψˆl
).
(
l = 1, n
)
. 
The method of Lagrange multipliers is an approach for finding the local maximum 
of a function subject to equality constraints given in Eq. (7.4.3). For regularization 
see [ 9, 10]. Let us consider the Lagrange function given by 
. L
(
ψ, ψˆ(ψ), λ
)
:= αG(ψ, ψˆ(ψ)) − λTΦeqψ − δ
2
(
|ψ|2 +
|
|
|ψˆ
|
|
|
2
− |λT|2
)
,
(7.4.4) 
where 
.G
(
ψ, ψˆ(ψ)
)
:= ∑n
l=1
[
ul
(
ψl
, ψˆl − ul
(
ψ˚l
, ψˆl
))] (7.4.5) 
and 
.ψ˚l := arg min
ψl∈Ψl
ul
(
ψl
, ψˆl
)
, (7.4.6) 
.Φeq is the restriction matrix of the Markov game, and the parameters. α and. δ are pos￾itive and the Lagrange vector-multipliers.λ ∈ Λ may have any sign. Here. ul
(
ψl
, ψˆl
)
is the payoff-function of the player . l which plays the strategy .ψl ∈ Ψl and the rest 
of the players the strategy.ψˆl ∈ Ψ ˆl
. The optimization problem 
.Lα,δ
(
ψ, ψˆ(ψ), λ
)
→ max
ψ∈Ψadm ,ψˆ(ψ)∈Ψˆadm
min
λ∈Λ
(7.4.7) 
has a unique saddle-point on. ψ since the optimized function (7.4.4) is strongly con￾cave on . ψ if 
.
∂2
∂ψ∂ψT Lα,δ
(
ψ, ψˆ(ψ), λ
)
< 0, ∀ ψ ∈ Ψadm, ψˆ(ψ) ∈ Ψˆadm ⊂ Rn, (7.4.8) 
and is strongly convex on the Lagrange multipliers. λ for any.δ > 0. 
We shall refer to .
(
ψ∗ (δ), ψˆ∗ (δ), λ∗ (α, δ)
)
of this set as the saddle point of 
the convex–concave Lagrangian function .Lα,δ
(
ψ (δ), ψˆ (δ), λ (α, δ)
)
on the set 
.Ψ∗ × Ψˆ ∗ × Λ∗, which for all .
(
ψ (δ), ψˆ (δ), λ (α, δ)
)
∈ Ψ∗ × Ψˆ ∗ × Λ∗ satisfies 
the system of inequalities 
. Lα,δ
(
ψ (δ), ψˆ (δ), λ∗ (α, δ)
)
≤ Lα,δ
(
ψ∗ (δ), ψˆ ∗ (δ), λ∗ (α, δ)
)
≤ Lα,δ
(
ψ∗ (δ), ψˆ ∗ (δ), λ
)7.4 Reinforcement Learning Approach 149
for any .λ ∈ Λ with non-negative components and, any .ψ ∈ Ψadm and .ψˆ ∈ Ψˆadm. 
Using the estimated values, we propose to update the strategy and the mechanism 
iteratively following the extragradient method given by: 
1. Proximal prediction step: 
.
λ¯ n= arg max λ≥0
{
−1
2 |λ − λn|2+γLα,δ
(
ψn, ψˆ
n, λ
)} ,
ψ¯
n= arg max ψ∈Ψ
{
1
2 |ψ-ψn|2 +γLα,δ
(
ψ, ψˆ
n, λ¯ n
)} ,
ψˆ
n= arg max
ψˆ ∈Ψˆ
{
1
2 |ψˆ − ψˆ
n|2 +γLα,δ
(
ψn, ψˆ, λ¯ n
)} .
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(7.4.9) 
2. Gradient approximation step: 
.
λn+1= λn−γ∇λLα,δ
(
ψ¯
n, ψˆ
n, μ¯ n, λ
)
,
ψn+1= Prψ∈Ψ
{
ψn+γ∇ψLα,δ
(
ψ, ψˆ
n, λ¯ n
)} ,
ψˆ
n+1(ψ) = Prψˆ ∈Ψˆ
{
ψˆ
n+γ∇ψˆ Lα,δ
(
ψ¯
n, ψˆ, λ¯ n
)} ,
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(7.4.10) 
where.Pr is the projection operator. Under proper conditions the extragradient method 
can be shown to converge to the local maximum (minimum). 
Algorithm 1: Learning Processes 
Let.N = {1, ..., n} be the set of players (.l ∈ N ). 
Let.θl
0 = θl be the initial state for player. l. 
Let.pˆl = pl be the initial transition matrix for player. l. 
Let.ε > 0 be the error of the estimated parameters. 
do 
Compute the strategy.σl∗(ml
t|θl
t) and the mechanism.μ∗(at|mt) by applying 
Eqs. (7.4.9) and (7.4.10). 
Select randomly a message.ml
t from the.σl∗(ml
t|θl
t). 
Select randomly an action.at from the.μ∗(at|mt). 
From the transition matrix.pˆl
(θl
t+1|θl
tal
t) get next state.θl
t+1for each player. l. 
Increase the values of.ηl
(θl
t+1θl
tal
t) and.ηl
(θl
tal
t). 
Compute the error mean square error. el
t . 
Estimate.pˆl
(θl
t+1|θl
tal
t) given in Eq. (7.4.1). 
Estimate.υˆl
(θl
t+1θl
tal
t) given in Eq. (7.4.2). 
Update.θl
t = θl
t+1 and increase. t by 1 (.t = t + 1). 
Until for each player.ε > el
t (by Kullback-Leibler) 
In the Algorithm 1, the initial time is.t = 0 and for each player. l we let.θl
0 = θl be 
the initial state and.pˆl = pl the initial transition matrix. We fix the error of the esti￾mated parameters by .ε > 0. For each iteration, we compute the strategy . σl∗(ml
t|θl
t)
and the mechanism .μ∗(al
t|ml
t) using Eqs. (7.4.9) and (7.4.10). Next, we select ran￾domly a message .ml
t from the strategy .σ∗(ml
t|θl
t) and action .al
t from the mecha-150 7 Mechanism Design
nism .μ∗(al
t|ml
t). Then, the system advance by obtaining the state .θl
t+1 randomly 
from .pˆl
(θl
t+1|θl
tal
t) for fixed state .θl
t and action . al
t . Next, we update the values of 
.ηl
(θl
t+1θl
tal
t) and .ηl
(θl
tal
t). So, we get the estimate of .pˆl
(θl
t+1|θl
tal
t) and . υˆl
(θl
t+1θl
tal
t)
using the estimation rules in Eqs. (7.4.1) and (7.4.2), respectively. Finally, we com￾pute the mean square error.e(t) by using 
.el
t = ∑
al
t∈Al
t
(
(pˆ
l
t−1
− ˆpl
t)
T(pˆ
l
t−1
− ˆpl
t)
)
. (7.4.11) 
The process continue until .ε > el
t . At the end, we obtain the estimated transition 
matrices .pˆl
(θl
t+1|θl
tal
t), the estimated values of .υˆl
(θl
t+1θl
tal
t). The resulting strat￾egy profile .σ∗(ml
t|θl
t) is a Bayesian-Nash equilibrium. The resulting mechanism 
.μ∗(at|mt)is incentive compatible because it is in equilibrium and satisfies Eq. (7.3.1). 
7.5 Risk-Averse Agents Strategies in Contracting Problem 
We now show how our method can be used to tackle the contracting problem [ 3]. 
In dynamics mechanism design, the main problem for the designer is how to design 
the most advantageous way of providing the agent with a fixed utility. u. With risk￾averse agents and a risk-neutral principal, optimal contracts provide some amount of 
insurance, but incentive compatibility does not provide full insurance. Particularly, 
we focus on the case of incomplete information where an expected-truthful mech￾anism is considered as a mechanism that is dominant-strategy incentive compatible 
for risk-averse agents. In this case, at each time. t a risk-averse agent facing a payoff. u
can contract with a risk-neutral principal. The way of providing dynamic incentives 
to the agent takes the form .(E{u} − u). This form preserves the player’s expected 
payoff and eliminates all associated risk. 
The setting consists of the set.N = {1, 2, 3} of players (.l ∈ N ), and the expected 
payoff takes the form 
. 
Ul
(μ, σ) = ∑
t∈T
∑
θl
t∈Θl
t
∑
mt∈Mt
∑
al
t∈Al
t
Wl
(al
t, θl
t) ∏
ι∈N
μ(at|ρt)σι
(mι
t|θι
t)Pι
(θι
t).
We fix the values of interest.γ0 = 0.1025,.δ0 = 5.0 × 10−4 .| Θl |= 4 and.| Al |= 2. 
The convergence of the strategies is shown in Figs. 7.1, 7.2 and 7.3. Applying our 
method, we obtain the resulting values of interest:7.5 Risk-Averse Agents Strategies in Contracting Problem 151
Fig. 7.1 Convergence of 
strategies.ξ1(θ1m1a1) for 
Player 1 
70 
0.3 
0.25 
0.2 
0.15 
0.1 
0.05 
0 
1
(1,1,1) 
Dynamic of the strategies 1 for the Player 1 1
(2,1,1) 
1
(3,1,1) 
1
(4,1,1) 
1
(1,2,1) 
1
(2,2,1) 
1
(3,2,1) 
1
(4,2,1) 
1
(1,3,1) 
1
(2,3,1) 
1
(3,3,1) 
1(4,3,1) 
0 10 20 30 40 50 60 
Iterations 
1
(1,4,1) 
1
(2,4,1) 
1
(3,4,1) 
1
(4,4,1) 
1
(1,1,2) 
1
(2,1,2) 
1
(3,1,2) 
1
(4,1,2) 
1
(1,2,2) 
1
(2,2,2) 
1
(3,2,2) 
1
(4,2,2) 
1
(1,3,2) 
1
(2,3,2) 
1
(3,3,2) 
1
(4,3,2) 
1
(1,4,2) 
1
(2,4,2) 
1
(3,4,2) 
1
(4,4,2) 
Probability 
Fig. 7.2 Convergence of 
strategies.ξ2(θ2m2a2) for 
Player 2 
70 
0.3 
0.25 
0.2 
0.15 
0.1 
0.05 
0 
Dynamic of the strategies 2 for the Player 2 
0 10 20 30 40 50 60 
Iterations 
2
(1,1,1) 
2
(2,1,1) 
2
(3,1,1) 
2
(4,1,1) 
2
(1,2,1) 
2
(2,2,1) 
2
(3,2,1) 
2
(4,2,1) 
2
(1,3,1) 
2
(2,3,1) 
2
(3,3,1) 
2
(4,3,1) 
2
(1,4,1) 
2
(2,4,1) 
2
(3,4,1) 
2
(4,4,1) 
2
(1,1,2) 
2
(2,1,2) 
2
(3,1,2) 
2
(4,1,2) 
2
(1,2,2) 
2
(2,2,2) 
2
(3,2,2) 
2
(4,2,2) 
2
(1,3,2) 
2
(2,3,2) 
2
(3,3,2) 
2
(4,3,2) 
2
(1,4,2) 
2
(2,4,2) 
2
(3,4,2) 
2
(4,4,2) 
Probability152 7 Mechanism Design
Fig. 7.3 Convergence of 
strategies.ξ3(θ3m3a3) for 
Player 3 
70 
0.4 
0.35 
0.3 
0.25 
0.2 
0.15 
0.1 
0.05 
0 
Dynamic of the strategies 3 for the Player 3 
0 
3
(1,1,1) 
3
(2,1,1) 
3
(3,1,1) 
3
(4,1,1) 
3
(1,2,1) 
3
(2,2,1) 
3
(3,2,1) 
3
(4,2,1) 
3
(1,3,1) 
3
(2,3,1) 
3
(3,3,1) 
3
(4,3,1) 
3
(1,4,1) 
3
(2,4,1) 
3
(3,4,1) 
3
(4,4,1) 
3
(1,1,2) 
3
(2,1,2) 
3
(3,1,2) 
3
(4,1,2) 
3
(1,2,2) 
3
(2,2,2) 
3
(3,2,2) 
3
(4,2,2) 
3
(1,3,2) 
3
(2,3,2) 
3
(3,3,2) 
3
(4,3,2) 
3
(1,4,2) 
10 20 30 40 50 60 3(2,4,2) 
Iterations 3(3,4,2) 
3
(4,4,2) 
Probability 
. 
μ∗(a|m) =
⎡
⎢
⎢
⎣
0.6843 0.3157
0.6709 0.3291
0.6838 0.3162
0.3313 0.6687
⎤
⎥
⎥
⎦ ,
σ1∗(m|θ)=
⎡
⎢
⎢
⎣
0.0584 0.0584 0.0584 0.8247
0.0340 0.0340 0.0340 0.8981
0.2504 0.2504 0.2504 0.2487
0.1671 0.1538 0.1764 0.5027
⎤
⎥
⎥
⎦ , P1∗(θ)=
⎡
⎢
⎢
⎣
0.1712
0.2955
0.3490
0.1843
⎤
⎥
⎥
⎦ ,
σ2∗(m|θ)=
⎡
⎢
⎢
⎣
0.2921 0.2921 0.2921 0.1237
0.0886 0.0886 0.0886 0.7342
0.0321 0.0321 0.0321 0.9037
0.2416 0.2640 0.2394 0.2549
⎤
⎥
⎥
⎦ , P2∗(θ)=
⎡
⎢
⎢
⎣
0.2790
0.1138
0.3222
0.2850
⎤
⎥
⎥
⎦ ,
σ3∗(m|θ)=
⎡
⎢
⎢
⎣
0.1928 0.1928 0.1928 0.4216
0.3034 0.3566 0.3048 0.0352
0.0399 0.0399 0.0399 0.8802
0.2757 0.2966 0.2714 0.1563
⎤
⎥
⎥
⎦ , P3∗(θ)=
⎡
⎢
⎢
⎣
0.1859
0.2851
0.2526
0.2763
⎤
⎥
⎥
⎦ .
This approach presented a dynamical incentive-compatible mechanism for players 
with risk-averse reward, which results in no loss in the risk-neutral principal’s utility, 
and only increases agents’ utilities.References 153
References 
1. Asiain, E., Clempner, J.B., Poznyak, A.S.: Controller exploitation-exploration: a reinforcement 
learning architecture. Soft Comput. 23(11), 3591–3604 (2019) 
2. Athey, S., Segal, I.: An efficient dynamic mechanism. Econometrica 81(6), 2463–2485 (2013) 
3. Battaglini, M.: Long-term contracting with Markovian consumers. Am. Econ. Rev. 95(3), 637– 
658 (2005) 
4. Baumann, T., Graepel, T., Shawe-Taylor, J.: Adaptive mechanism design: learning to promote 
cooperation (2019). ArXiv:1806.04067, v2 
5. Bergemann, D., Said, M.: Wiley encyclopedia of operations research and management science, 
chap. Dynamic Auctions, pp. 1511–1522. Wiley, Hoboken, NJ (2011) 
6. Bergemann, D., Välimäki, J.: The dynamic pivot mechanism. Econometrica 78(2), 771–789 
(2010) 
7. Clempner, J.B.: A Markovian Stackelberg game approach for computing an optimal dynamic 
mechanism. Comput. Appl. Math. 40(186), 1–25 (2021) 
8. Clempner, J.B.: Algorithmic-gradient approach for the price of anarchy and stability for incom￾plete information. J. Comput. Sci. 60, 101589 (2022) 
9. Clempner, J.B., Poznyak, A.S.: A Tikhonov regularization parameter approach for solving 
Lagrange constrained optimization problems. Eng. Optim. 50(11), 1996–2012 (2018) 
10. Clempner, J.B., Poznyak, A.S.: A Tikhonov regularized penalty function approach for solving 
polylinear programming problems. J. Comput. Appl. Math. 328, 267–286 (2018) 
11. Clempner, J.B., Poznyak, A.S.: A nucleus for Bayesian partially observable Markov games: 
joint observer and mechanism design. Eng. Appl. Artif. Intell. 95, 103876 (2020) 
12. Clempner, J.B., Poznyak, A.S.: Analytical method for mechanism design in partially observable 
Markov games. Mathematics 9(4), 1–15 (2021) 
13. Clempner, J.B., Poznyak, A.S.: A dynamic mechanism design for controllable and ergodic 
Markov games. Comput. Econ. 61, 1151–1171 (2023) 
14. Clempner, J.B., Poznyak, A.S.: Mechanism design in Bayesian partially observable Markov 
games. Int. J. Appl. Math. Comput. Sci. 33(3), 463–478 (2023) 
15. Clempner, J.B., Poznyak, A.S.: The price of anarchy as a classifier for mechanism design in a 
pareto-Bayesian-Nash context. J. Ind. Manag. Optim. 19(9), 6736–6749 (2023) 
16. Goldman, C., Zilberstein, S.: Mechanism design for communication in cooperative systems. 
In: Game Theoretic and Decision Theoretic Agents Workshop at AAMAS’03, Melbourne, 
Australia, pp. 1–9 (2003) 
17. Grover, D., Basu, D., Dimitrakakis, C.: Bayesian reinforcement learning via deep, sparse sam￾pling. In: Chiappa, S., Calandra, R. (eds.) Proceedings of the Twenty Third International Con￾ference on Artificial Intelligence and Statistics, vol. 108, pp. 3036–3045. PMLR (2020) 
18. Groves, T.: Incentives in teams. Econometrica 41, 617–631 (1973) 
19. Kassab, R., Simeone, O.: Federated generalized Bayesian learning via distributed stein varia￾tional gradient descent (2020). ArXiv:2009.06419 
20. Mguni, D.: Efficient reinforcement dynamic mechanism design. In: GAIW: Games, Agents 
and Incentives Workshops, at AAMAS. Montréal, Canada (2019) 
21. Myerson, R.B.: Allocation, Information and Markets, Chap. Mechanism Design, pp. 191–206. 
The New Palgrave. Palgrave Macmillan, London (1989) 
22. Nolan, S., Smerzi, A., Pezzè, L.: A machine learning approach to Bayesian parameter estimation 
(2020). arXiv:2006.02369v2 
23. Pavan, A., Segal, I., Toikka, J.: Dynamic mechanism design: a Myersonian approach. Econo￾metrica 82(2), 601–653 (2014) 
24. Senda, K., Hishinuma, T., Tani, Y.: Approximate Bayesian reinforcement learning based on 
estimation of plant. Auton. Rob. 44, 845–857 (2020) 
25. Sinha, A., Anastasopoulos, A.: Mechanism design for resource allocation in networks with 
intergroup competition and intragroup sharing. IEEE Trans. Control Netw. Syst. 5(3), 1098– 
1109 (2017)154 7 Mechanism Design
26. Vasilyeva, M., Tyrylgin, A., Brown, D., Mondal, A.: Preconditioning Markov chain monte 
Carlo method for geomechanical subsidence using multiscale method and machine learning 
technique. J. Comput. Appl. Math. 392, 113420 (2021) 
27. van Geen, C., Gerraty, R.T.: Hierarchical Bayesian models of reinforcement learning: intro￾duction and comparison to alternative methods (2020). BioRxiv 2020.10.19.345512, https:// 
doi.org/10.1101/2020.10.19.345512Chapter 8
Joint Observer and Mechanism Design
Abstract An intelligent agent suggests an autonomous entity, which manages and
learns actions to be taken towards achieving goals. The issue is that it is difficult
to build a method that can calculate effective judgments that maximize the overall
reward of interacting agents upon an environment with unknown, partial, and uncer￾tain information, according to reports in the literature on Artificial Intelligence (AI).
This chapter offers a solution to these problems: a foundation for Bayesian Partially
Observable Markov Games (BPOMGs) supported by an AI strategy. The nucleus’s
structure is governed by three essential concepts: game theory, learning, and infer￾ence. The first thing we do is provide a brand-new general Bayesian strategy that
is designed for games that take into account both the partial information provided
by the Bayesian model and the incomplete information on the states of the Markov
system. This approach uses a Partly Observable Markov Game (POMG) to deal with
execution uncertainty. Second, we expand design theory to include joint observer
design and mechanism design (both unknown). Because agents behave in their own
self-interest, the mechanism is created to persuade them not to divulge their personal
information and to get a certain result. The purpose of the joint observer design is to
depict the possibility that agents may not be motivated to deliver accurate information
about their states. The transition matrices, which are also unknown, are estimated
by the agents using a model that uses a Reinforcement Learning (RL) technique at
each time step. The result is an expanded POMG model that adds a new variable
and suggests an analytical method for computing both the observer design and the
mechanism design (both unknown). The suggested expansion makes the computa￾tionally challenging game theory issue tractable. The variables of relevance for each
agent, such as the observation kernels, joint observers, mechanisms, strategies, and
distribution vectors, are derived relations in order to retrieve them analytically. By
simulating a game-theoretic examination of the patrolling issue that involves devel￾oping the mechanism, calculating the observers, and using an RL technique, the
utility and efficacy of the suggested nucleus are confirmed.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable
Markov Chains, Studies in Systems, Decision and Control 504,
https://doi.org/10.1007/978-3-031-43575-1_8
155156 8 Joint Observer and Mechanism Design
8.1 Introduction
8.1.1 Brief Problem Analysis
The agent’s understanding of its surroundings in real-world settings is unknown,
imperfect, or ambiguous. The AI field searches for analytical techniques capable of
resolving issues of this nature. In our scenario, a Partly Observable Markov Games
(POMGs) [6], which is a controlled stochastic process where it is assumed that the
system dynamics are dictated by a Markov process, is used to manage the execution
of the uncertainty since the agent cannot directly view the underlying set of states.
An agent must revise its belief in the state that the environment may (or may not be
in) after acting and perceiving a condition. Agents use a variety of tools to function
in these gaming contexts.
– First, in order to maximize a reward that depends on the order of system states
and the agent’s activities, agents should execute learning actions using an RL
algorithm in an effort to learn more about their surroundings. The RL method
does not (necessarily) presuppose knowledge of a precise mathematical model;
see [7] for further information.
– Second, the environment is in a certain state at every point in time. Each agent
decides on a course of action that probabilistically moves the environment to the
next state. The agent also gets an observation at this moment that is based on the
altered condition of the environment. He creates a belief about the state the system
is in based on these observations. Agents calculate the environment’s observation
kernel in our unknown information game model.
– Third, the mechanism design is a game theory framework for comprehending how
an agent might get the “best” results when the primary objective of the game is
the agent’s self-interest.
This approach arises from the limitations of individual’s self-interest incentives
and motives, and how these might work in the agent’s favor. Mechanism design con￾siders incentives and private information to increase agent rewards and demonstrates
how the correct incentives may persuade agents to reveal their private information
and produce a certain result. Because agents operate in their own self-interest and are
not incentivized to deliver accurate information, POMG is a suitable model for sim￾ulating this environment. As a result, the observation kernel may also be created and
adopts an engineering methodology to create incentives that lead to desired results
(outcomes). The mechanism and joint observer designs both conform to the design
theory.8.1 Introduction 157
8.1.2 Related Work
Many facets of mechanism design have been covered in earlier research. Mechanism
design, which Hurwicz [44] initially described in 1960, has emerged as a tech￾nique to design (model, analyze, and solve) game theory issues using an engineering
approach, incorporating numerous players who interact rationally [41, 48]. This the￾ory is based on games with incomplete information that attempt to represent how
private information might be reported and address the issue of how the information
problem limits behavior by allowing social decisions to take the player’s preferences
into account. Designing a computational mechanism that is both computationally
tractable and yet maintains the essential properties of game theory is a difficult task.
Based on the idea of game theory, the mechanism design [49, 60] simulates how
people interact with one another. The primary focus is on creating games that take
into account independent private values and quasilinear payoffs [16, 41]. In these
games, participants get messages that contain information about the payoffs [42].
Players in the game report a type that can be a strategic falsehood. Players are paid in
accordance with the payment structure following the report. Players commit to a sys￾tem in the game’s dynamics that offers an outcome based on the sort of information
that may have been falsely provided. It is essential to remember that the mechanism
remains unknown. The social choice function is mapped directly from the (true) type
profile to the alternatives of commodities obtained by the mechanism designer.The
mechanism implements a social choice function that maximizes some value criterion
(profit). In this sense, a mechanism is considered to implement a social-choice func￾tion if it leads to a Nash equilibrium for every possible combination of individual
preferences.
The 1980s, 1990s, and recent years have seen considerable advancements in mech￾anism design. See [11, 50] for a survey. Models of negotiations, auctions, public
utility regulation, the supply of public goods, nonlinear pricing, and labor market
contracts are just a few examples of the many applications for which the mechanism
design theory is used. This idea enables the management of player information control
and limitation [15, 39, 47]. In this regard, Arrow [5] proposed a method of demand
disclosure that maximizes efficiency and prevents resource wastage in incentive pay￾ments. In order to construct a mechanism with partial knowledge, d’Aspremont and
Gerard-Varet [33] presented two distinct methods. The first method does not incor￾porate individual beliefs, whereas the second method does. Rogerson [55] developed
a generic version of the hold-up issue where numerous participants make relation￾specific investments and subsequently agree on some collective action, demonstrating
that first-best solutions exist under a range of different assumptions about the nature
of information asymmetries. Mailath and Postlewaite [46] proposes a solution for
the bargaining problems with asymmetric knowledge involving numerous agents.
The model of price discrimination proposed by Courty and Li [31] assumes that
customers are aware of the distribution of their values at the time of contracting,
and the best mechanisms to use rely on the informational quality of the consumers’
initial valuation knowledge. Battaglini [10] provided an ideal infinite-horizon mech-158 8 Joint Observer and Mechanism Design
anism design that takes into account a Markov process in which an agent reacts to
a linear model with a continuum of states. Using a two-period state representation
to orthogonalize an agent’s future information, Es ˝o and Szentes [35] examined a
monopolist model in which it sells an indivisible item to numerous agents for opti￾mum information disclosure in auctions. Board [13] used a multi-agent framework
with an infinite-time horizon to augment the model described in [35]. Using bandit
actions and demonstrating that the ideal mechanism is a variation of the dynamic
pivot mechanism, Kakade et al. [45] improved the work presented in [13]. Ger￾shkov and Moldovanu [38] developed a mechanism method that takes into account a
dynamic revenue maximization issue in which agents arrive stochastically over time.
Wang et al. [61] proposed an ideal double auction method, utilizing a multi-objective
model to optimize the predicted total revenue of sellers and buyers. Athey and Segal
[9] developed a Bayesian incentive-compatible method for a dynamic framework
with quasilinear payments where agents view private information and choices are
made across countably many periods. In dynamic quasilinear frameworks, where
private knowledge is acquired over time and judgments are made across a number of
periods, Pavan et al. [51] constructed a Myersonian process. The work given in [38]
was improved by Board and Skrzypacz [14], who proposed a method for creating a
mechanism in which agents consider a single piece of private knowledge but arrive
stochastically over time. A method for transforming a possibly suboptimal algo￾rithm approach for optimization into a Bayesian incentive-compatible mechanism
that improves societal welfare and income was presented by Hartline and Lucier in
their paper [43].
In repeated games that take communication into account, useful results on com￾puting the equilibria have been attained. In repeated games that take communication
into account, useful results on computing the equilibria have been attained according
to Rotemberg and Dutta [32, 56], the play pathways are comparable to those in games
with comprehensive knowledge. Citations for non-cooperative repeated models in
the literature include [1, 8, 36, 40, 62]. Rahman, Bernheim and Escobar (2018)
[12, 34, 54] all provide cooperative models. Clempner and Poznyak presented non
cooperative models [17, 27–30].
In this chapter we consider recurrent games involving communication and observ￾able behavior in which the agents’ known payoffs change over time in accordance
with an irreducible partly observed Markov chain whose transitions are independent
of one another. Below we follow [26].
Our chapter’s findings are connected to research on the design of mechanisms and
observers for partially observable Markov repeating games [19]. We add to this field
by proposing a technique for constructing a mechanism with partial state knowledge,
the observers design, and describing an essentially equilibrium behavior in recurrent
game models that take unfriendly actions on the course of play. Agents are not
driven to give truthful information since they behave in their own self-interest. The
mechanism design considers a specific result based on an agent’s self-interest as well
as the actions required to attain it. The observer design, on the other hand, assumes
that agents are not motivated to expose their true states. Last but not least, our game
theory simulates how actors could possibly impact different goals.8.2 Markov Games 159
The main results of the chapter are summarized as follows:
– Provides a game-theory model that uses both the incomplete information of the
Bayesian-Markov model and the incomplete information over the states.
– Proposes the joint observer design, which derives a particular (unknown) obser￾vation kernel for each agent.
– Extends the design theory: joint observer design and mechanism design.
– Derives the relations to recover the variables of interest for each agent, i.e. observa￾tion kernels, observers design, mechanism, (behavior) strategies and distribution
vectors.
– Follows a model that employs an RL approach that responds to the game theory
model proposed.
– Studies the problem of computing the equilibrium for the game.
– Suggests a new random walk approach.
8.2 Markov Games
Let.MC = (S, A,{A(s)}s∈S, K, P) be a Markov chain [20, 53], where .S is a finite
set of states, .S ⊂ N and .A is a finite set of actions. For each .s ∈ S, .A(s) ⊂ A is
the non-empty set of admissible actions at state .s ∈ S. Without loss of generality
we may take .A= ∪s∈S A(s). Whereas, .K = {(s, a)|s ∈ S, a ∈ A(s)} is the set of
admissible state-action pairs. The variable.pj|ik is a stationary controlled transition
matrix, where.πj|ik := P(Xt+1 = sj|Xt = si, At = ak ).∀t ∈ N represents the prob￾ability associated with the transition from state.si to state.sj ,.i = 1, N (.i = 1, ..., N)
and . j = 1, N (. j = 1, ..., N), under an action .ak ∈ A(si),.k = 1, K (.k = 1, ..., K).
The distribution vector is given by .P (Xt = si) = Pi such that .Pi ∈ S N , where
.S N = {s ∈ RN :
∑N
i=1 P(si) = 1, P(si) ≥ 0}.
We consider the case where the process is not directly observable [25]. Let
us associate with .S the observation set .Y which takes values in a finite space
.{1, ..., M}, .M ∈ N. The stochastic process .{Yt, t ∈ N} is called the observation
process. By observing .Yt at time .t information regarding the true value of .Xt is
obtained. If .Xt = si and .At = ak an observation .Yt = ym will have a probability
.qm|ik := P(Yt = ym|Xt = si, At = ak ), that denotes the relationship between the
state and the observation when an action.ak ∈ A(xi) is chosen at time.t. The obser￾vation kernel is a stochastic kernel on.Y given by.Q = [qm|ik ]. We restrict ourselves
to consider.Q = [qm|i].
A controllable partially observable Markov decision process(POMDP) is a tuple
.POMDP = {MC, Y, Q, Q0, P, V },160 8 Joint Observer and Mechanism Design
where:
• .MC is a Markov chain;
• .Y is the observation set, which takes values in a finite space.{1, ..., M},.M ∈ N;
• .Q = [qm|i]m=1,M,i=1,N denotes the observation kernel is a stochastic kernel on.Y ;
such that
.
∑
m
qm|i = 1;
• .Q0 = [qm|i]m=1,M,i=1,N denotes the initial observation kernel;
• .P is the (a priori) initial distribution;
• .uijmk , is the reward function at time .t given the state .si , the observable state .ym,
when the action.ak ∈ A(si, ym) is taken.
A realization of the Bayesian partially observable system at time.t is given by
.(s(0), x(0), y(0), a(0),s(1), x(1), y(1), a(1), ...) ∈ Ω := (SSY A)
∞ ,
where.s0 has a given by the distribution.P (X0 = s0) and.{At}is a control sequence in
.A determined by a control policy. To define a policy we cannot use the (unobservable)
states.s(0),s(1), .... Then, we introduce the observable histories.h0 := (p, Y0) ∈ H0
and .Ht := Ht−1(AY ), if .t ≥ 1. To define a (behavioral) strategy we cannot also
use the (unobservable) states.s0,s1, .... Then, we introduce the observable histories
.h0 ∈ H0 and
.ht := (s(0), x(0), y(0), a(0), ...,s(t − 1), x(t − 1), y(t − 1), a(t − 1), y(t)) ∈ Ht for all t ≥ 1,
as well as.Ht = Ξt × Ht−1, if.t ≥ 1. Now, a (behavioral) strategy.σr|i is a mapping
.σ : S → Δ(S). The set of all feasible policies is denoted by.Sadm as follows
.Sadm =
[
σr|i ≥ 0 |
∑
N
r=1
σr|i = 1,i = 1, N
]
. (8.2.1)
In the Bayesian form, if .Xt = xi and .At = ak an observation .Yt = ym will have
a probability .qm|rk := P(Yt = ym|Xt = xr, At = ak ), that denotes the relationship
between the observed state and the observation when an action.ak ∈ A(xi) is chosen
at time .t. The observation kernel is a stochastic kernel on .Y given by .Q = [qm|rk ].
We restrict ourselves to the case where.[qm|rk ]=[qm|r].
A game consists of a set .N = {1, ..., n} of players (indexed by .l = 1, n). We
employ .l to emphasize the .l th player’s variables and .−l subsumes all the other
players’ variables. The dynamics is described as follows. At time .t = 0, the initial
(unobservable) state.s0 has a given a priori distribution.Pl
i , and the initial observation
.y0 is generated according to the initial observation kernel.Ql
0(y0|x0). If at time.t the
state of the system is.Xt and the control .Al
t ∈ Al is applied, then each strategy is
allowed to randomize, with distribution .σl
r|i(t), over the state choices.Xl
t based on
the mechanism.μk|m over the action choices.Al
t ∈ Al
.8.2 Markov Games 161
Formally, a mechanism is any function.μk|m such that
.Madm =
[
μk|m ≥ 0 |
∑
K
k=1
μk|m = 1, m = 1, M
]
. (8.2.2)
These choices induce immediate utilities.Ul
T (μ, σ) where the system tries to maxi￾mize the corresponding one-step valuation functions.Vl
ijmk . The system tries to max￾imize the corresponding one-step utility. Next, the system moves to the new state
.Xt+1 = sj according to the transition probabilities.Pl
(Xt+1 = sj|Xt = si, At = ak ).
Then, the observation .Yt is generated by the observation kernel .Ql
(Yt|Xt). Based
on the obtained utility, the systems adapt its behavior strategy computing.σl
r|i(t + 1)
based on the mechanism .μk|m for the next selection of the control actions. The
one-step valuation functions.Vl
ijmk and the transition functions.Pl
(Xt+1 = sj|Xt =
si, At = ak ) are all common knowledge at.t = 0. The common prior initial distribu￾tion vector.Pl
(Xt = si) and the transition function.Pl
(Xt+1 = sj|Xt = si, At = ak )
are assumed to be independent across players. The interaction between players
induces a Bayes partially observable Markov game where the average payoff of
player.l is the expected value of the summed payoff.Ul
T (μ, o), obtained under the
mechanism.μ and the observer.o, is defined as
.
Ul
T (μ, o) := ∑
T
t=1
∑
M
m=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
∑
N
j=1
ul
ijmk (t)πl
j|ik (t) |n
ι=1
μk|m(t)σι
r|i
(t)qι
m|r(t)Pι
i (t) =
∑
T
t=1
∑
M
m=1
∑
K
k=1
(μk|m(t))n ∑
N
i=1
∑
N
r=1
Wl
imk (t)ol
irm(t)
|n
ι/=l
oι
irm(t)
(8.2.3)
where .Wl
imk (t) = ∑
N
j=1
ul
ijmk (t)πl
j|ik (t) and the joint observer is defined as.ol
irm(t) =
σl
r|i(t)ql
m|r(t)Pl
i (t) such that.
∑
M
m=1
∑
N
i=1
∑
N
r=1
ol
irm(t) = 1. We have that
. ol
im(t) = ∑
N
r=1
ol
irm(t) = ql
m|i(t)Pl
i (t) (8.2.4)
as defined in [25]. Let us denote by.Oadm the set of “feasible joint observers”:
. Oadm =
[
ol
irm(t) ≥ 0 | ∑
M
m=1
∑
N
i=1
∑
N
r=1
ol
irm(t) = 1,l = 1, n
]
. (8.2.5)162 8 Joint Observer and Mechanism Design
8.3 Problem Formulation
In this section, we provide an analytical method for computing a mechanism and the
joint observer.
8.3.1 Initial Problem
We assume that players know their individual payoffs.Ul as are defined in Chaps. 5
and 7.
Problem 8.1 We will find a mechanism .μk|m and the joint observers.oirm, which
solve the following nonlinear programming problem
.(μ∗, o∗(μ∗)) = arg max
μ∈Uadm ,
∑n
l=1
Ul
(μ, o∗(μ)), (8.3.1)
where for a given mechanism.μk|m and observer.o∗(μ)theBayesian-Nash equilibrium
condition fulfills:
.Ul
(μ, o∗(μ)) ≥ Ul
(μ, ol
, o−l∗). (8.3.2)
Remark 8.1 The mechanism.μ is unique for all participants.
8.3.2 Auxiliary Problem
Now, introduce the.z-variable:
.zl
irmk = μk|mσl
r|iql
m|r Pl
i = μk|mol
irm, (8.3.3)
and consider the following auxiliary problem:
Problem 8.2 We will try to find an auxiliary variable.z, which solves the following
nonlinear programming problem
.
U˜ (z) = ∑n
l=1
U˜ l
(z) → max
z∈Zadm
,
U˜ l
(z) := ∑
M
m=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
Wl
imk
|n
ι=1
zι
irmk ,
⎫
⎪⎪⎬
⎪⎪⎭
(8.3.4)
where.zl
irmk is given by (8.3.3) and.Zadm = ⊗l Zl
adm with8.4 Relation of Solutions for Initial and Auxiliary Problems 163
.
Zl
adm := [
zl
irmk ≥ 0 | ∑M
m=1
∑N
i=1
∑N
r=1
∑K
k=1
zl
irmk = 1; ∑M
m=1
∑N
r=1
∑K
k=1
zl
irmk = Pl
i > 0;
∑M
m=1
∑N
i=1
∑N
r=1
∑K
k=1
[δi j − πl
j|ik ]zl
irmk , j = 1, N;
∑M
ρ=1
∑N
i=1
∑N
r=1
∑K
k=1
[δρm − ql
m|r]zl
irρk , m = 1, M; ∑M
m=1
∑N
i=1
∑N
r=1
∑K
k=1
ql
ρ|r zl
irmk ≥ 0, ρ = 1, M
]
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎭
(8.3.5)
Notice that the following relations hold:
.
∑
K
k=1
μk|m = 1, ∑
N
r=1
σl
r|i = 1,
∑
M
m=1
ql
m|r = 1,
∑
N
i=1
Pl
i = 1.
It is easy to check that.Zl
adm includes the simplex.Δl
, namely,.zl ∈ Δl ⊂ Zl
adm:
.Δl := [
zl
irmk ≥ 0 | ∑
M
m=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
zl
irmk = 1;
∑
M
m=1
∑
N
r=1
∑
K
k=1
zl
irmk = Pl
i > 0
]
(8.3.6)
Define the solution of the problem (8.3.4) as.z∗ = (z1∗, ...,zn∗).
8.4 Relation of Solutions for Initial and Auxiliary Problems
The Lemma below clarifies how we may recover mechanism.μ∗
a|m.
Theorem 8.1 Suppose that the problem (8.3.4) is solved. Then the mechanism.μ∗
a|m
can be recovered from.zl∗
irmk as follows:
. μ∗
k|m =
∑n
l=1
∑
N
i=1
∑
N
r=1
zl∗
irmk
∑n
l=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
zl∗
irmk
. (8.4.1)
Proof The mechanism.μ∗
k|m can be recovered from Eqs. (8.3.5) and (8.3.6) as follows:
.
∑n
l=1
∑
N
i=1
∑
N
r=1
zl∗
irmk := μ∗
k|m.
Now, we have that
.
∑n
l=1
∑N
i=1
∑N
r=1
μ∗
k|mσl∗
r|i
ql∗
m|r Pl∗
i = μk|m
∑n
l=1
∑N
i=1
∑N
r=1
σl∗
r|i
ql∗
m|r Pl∗
i = μ∗
k|m
∑n
l=1
∑N
i=1
∑N
r=1
ol∗
irm = μk|m
∑n
l=1
∑N
i=1
ol∗
im .164 8 Joint Observer and Mechanism Design
Then, one has that
.μ∗
k|m =
∑n
l=1
∑
N
i=1
∑
N
r=1
zl∗
irmk
μk|m
∑n
l=1
∑
N
i=1
ol∗
im
=
∑n
l=1
∑
N
i=1
∑
N
r=1
zl∗
irmk
∑n
l=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
zl∗
irmk
. (8.4.2)
Corollary 8.1 The relation to recover the observer.q∗
m|i is given by
. ql∗
m|r =
∑
N
i=1
∑
K
k=1
zl∗
irmk
∑
N
i=1
∑
M
ρ=1
∑
K
k=1
zl∗
irρk
. (8.4.3)
So, finally to recover the optimal joint observer we need to use the formula
. ol∗
irm = σl
r|iq∗
m|r Pl∗
i . (8.4.4)
We have that if.σr|i = δir (see [25])
. ol
im = ∑
N
r=1
ol
irm = ql
m|i Pl
i . (8.4.5)
Variables.σl
r|i and.Pl∗
m may be recovered as it is presented below.
Corollary 8.2 The equilibrium (behavior) strategies.σl∗
r|i are given by
. σl∗
r|i =
∑
M
m=1
∑
K
k=1
zl∗
irmk
∑
N
 =1
∑
M
m=1
∑
K
k=1
zl∗
i mk
, (8.4.6)
.l ∈ N and the corresponding stationary distributions.P¯l∗
m are as follows:
. P¯l∗
m = ∑
N
i=1
∑
N
r=1
∑
K
k=1
zl∗
irmk ,l = 1, n. (8.4.7)
Lemma 8.1 The obtained mechanism .μ∗
k|m, the observers.ol∗
irm and the strategies
.σl∗
r|i satisfy the Bayesian-Nash equilibrium given in Eq. (8.3.2).8.4 Relation of Solutions for Initial and Auxiliary Problems 165
Proof It results directly from the following consideration:
.
max
z∈Zadm
U˜ (z) = U˜ (z∗) = ∑n
l=1
U˜ l
(z∗) = ∑n
l=1
Ul
(μ∗, o∗(μ∗)) =
∑n
l=1
(
∑
M
m=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
Wl
imk (μ∗
k|m)nol∗
irm
|n
ι/=l
oι∗
irm)
=
∑n
l=1
max
ol∈Ol
adm (
∑
M
m=1
∑
K
k=1
(μ∗
k|m)n ∑
N
i=1
∑
N
r=1
Wl
imkol
irm
|n
ι/=l
oι∗
irm)
≥
∑n
l=1
(
∑
M
m=1
∑
K
k=1
(μ∗
k|m)n ∑
N
i=1
∑
N
r=1
Wl
imkol
irm
|n
ι/=l
oι∗
irm)
= ∑n
l=1
Ul
(μ∗, ol
, o−l∗).
(8.4.8)
From this inequality it follows that
.
∑n
l=1
(
Ul
(μ∗, o∗(μ∗)) − Ul
(μ∗, ol
, o−l∗)
)
≥ 0. (8.4.9)
Since the above inequality is valid for all admissible observer .o, it is valid when
.σ j = σ j∗ and.oj = oj∗ for. j /= l, implying
.Ul
(μ∗, o∗(μ∗)) − Ul
(μ∗, ol
, o−l∗) ≥ 0, (8.4.10)
which coincides with Eq. (8.3.2) when.μ = μ∗. The lemma is proven.. 
8.4.1 Ergodicity Condition
The next lemma presents the necessary egodicity conditions that the solutions of the
problem (8.3.4) must satisfy. Let us define .Ql = [
ql
m|r
]−1 is the inverse matrix of
.
[
ql
m|r
]
where.M = N.
1
Lemma 8.2 If the mechanism .μ∗
k|m, the observers.ol∗
irm and the strategies.σl∗
r|i are
solutions of the problem (8.3.4), which correspond to a Bayesian-Nash equilibrium
in Eq. (8.3.2), then variables .zl∗
irmk for all .l ∈ N satisfy the following ergodicity
constraints:
. i) ∑
M
m=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
[δi j − πl
j|ik ]zl
irmk , j = 1, N, M = N, (8.4.11)
. ii) ∑
M
ρ=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
[δρm − ql
m|r]zl
irρk , m = 1, M, M = N, (8.4.12)
1 Notice that.
∑M
m=1
Ql
m|hql
m|r = δhr - delta Kronecker symbol.166 8 Joint Observer and Mechanism Design
. iii) ∑
M
ρ=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
Ql
ρ|h zl
irρk > 0, h = 1, N. (8.4.13)
Proof (i) To show the relation given in Eq. (8.4.11) one has that:
. Pl∗
j = ∑
N
α=1
[
∑
N
κ=1
∑
M
β=1
∑
K
γ=1
πl
j|αγ zl∗
ακβγ]
.
On the other hand,
.
∑
N
j=1
[
∑
N
κ=1
∑
M
β=1
∑
K
γ=1
πl
j|αγ zl∗
jκβγ]
= ∑
N
α=1
[
∑
N
κ=1
∑
M
β=1
∑
K
γ=1
πl
j|αγ zl∗
ακβγ]
,
developing, one obtains that
.
∑
N
α=1
∑
N
κ=1
∑
M
β=1
∑
K
γ=1
[δαl j − πl
j|αγ]zl∗
ακβγ = 0, , j = 1, N,
satisfying
.zl∗ ∈ E := [
zl∗
ακβγ
|
|
|
|
|
∑
N
α=1
∑
N
κ=1
∑
M
β=1
∑
K
γ=1
[δα j − πl
j|αγ]zl∗
ακβγ, j = 1, N,
]
. (8.4.14)
(ii) To prove the relation in Eq. (8.4.12) note that:
.
∑
M
 =1
∑
N
α=1
∑
N
κ=1
∑
K
γ=1
[δ β − ql
β|κ]zl
ακ γ = ∑
M
 =1
∑
N
κ=1
[δ β − ql
β|κ]
∑
N
α=1
∑
K
γ=1
zl∗
ακ γ.
In addition,
.
∑N
α=1
∑K
γ=1
zl∗
ακ γ = ∑N
α=1
∑K
γt=1
μγ| ol
ακ = ∑N
α=1
∑K
γt=1
μγ| σl
κ|αql
 |κPl
α = ql
 |κ
∑N
α=1
∑K
γt=1
μγ| σl
κ|α Pl
α,
and
.
∑n
l=1
∑
N
α=1
∑
N
κ=1
∑
K
γ=1
zl∗
ακ γ = ∑n
l=1
∑
N
α=1
∑
K
γ=1
μγ| 
(∑
N
κ=1
σl
κ|α
)
ql
 |κPl
α = ql
 |κ
∑n
l=1
∑
N
α=1
∑
K
γ=1
μγ| Pl
α.
Hence,8.5 Reinforcement Learning Approach 167
.
∑n
l=1
∑N
α=1
∑N
κ=1
∑K
γ=1
zl∗
ακ γ
∑N
α=1
∑K
γ=1
zl∗
ακ γ
∑n
l=1
∑N
α=1
∑N
κ=1
∑K
γ=1
zl∗
ακ γ
= ∑n
l=1
∑N
α=1
∑N
κ=1
∑K
γ=1
zl∗
ακ γ
∑N
α=1
∑K
γ=1
μγ| σl
κ|αql
 |κ Pl
α
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| 
( ∑N
κ=1
σl
κ|α
)
ql
 |κ Pl
α
=
∑n
l=1
∑N
α=1
∑N
κ=1
∑K
γ=1
zl∗
ακ γ
ql
 |κ
∑N
α=1
∑K
γ=1
μγ| σl
κ|α Pl
α
ql
 |κ
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| 
( ∑N
κ=1
σl
κ|α
)
Pl
α
= ∑n
l=1
∑N
α=1
∑N
κ=1
∑K
γ=1
zl∗
ακ γ
∑N
α=1
∑K
γ=1
μγ| σl
κ|α Pl
α
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| Pl
α
=
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| 
( ∑N
κ=1
σl
κ|α
)
ql
 |κ Pl
α
∑N
α=1
∑K
γ=1
μγ| σl
κ|α Pl
α
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| Pl
α
= ql
 |κ
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| Pl
α
∑N
α=1
∑K
γ=1
μγ| σl
κ|α Pl
α
∑n
l=1
∑N
α=1
∑K
γ=1
μγ| Pl
α
=
ql
 |κ
∑N
α=1
∑K
γ=1
μγ| σl
κ|α Pl
α = ql
 |κσl
κ|α.
Adding, one has
.
∑
M
 =1
∑
N
κ=1
[δ β − ql
β|κ]ql
 |κσl
κ|α = ∑
M
 =1
∑
N
κ=1
[δ βql
 |κσl
κ|α] − ∑
M
 =1
∑
N
κ=1
[ql
β|κql
 |κσl
κ|α] =
∑
N
κ=1
[ql
β|κσl
κ|α] − ∑
N
κ=1
[ql
β|κσl
κ|α] = 0.
(iii) For showing the relation in Eq. (8.4.13) one has that for any.h = 1, N
.
∑
M
ρ=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
Ql
ρ|h zl
irρk = ∑
N
i=1
∑
N
r=1
∑
M
ρ=1
∑
K
k=1
Ql
ρ|hμk|ρol
irρ =
∑
M
ρ=1
∑
N
i=1
∑
N
r=1
∑
K
k=1
Ql
ρ|hμk|ρσl
r|iql
ρ|r Pl
i = ∑
M
ρ=1
∑
N
i=1
∑
N
r=1
Ql
ρ|hσl
r|iql
ρ|r Pl
i =
∑
N
i=1
∑
N
r=1
σl
r|i Pl
i
∑
M
ρ=1
Ql
ρ|hql
ρ|r = ∑
N
i=1
∑
N
r=1
σl
r|i Pl
i δhr = ∑
N
i=1
σl
h|i Pl
i > 0.
8.5 Reinforcement Learning Approach
8.5.1 Iterative Procedure
We propose a model from experiences [7, 18, 52] that is computed by count￾ing the number .ηT of private experiences defining the variables recursively as
follows: let .ht ∈ Ht (.t ∈ T ) be finite history and .(sl
(t), al
(t)) ∈ Sl
t × Al
t is its
corresponding finite sequence of states and actions, and let us denote for each
.(sl
(t + 1),sl
(t), al
(t)) ∈ Sl
t × Sl
t × Al
t168 8 Joint Observer and Mechanism Design
.ηl
ˆj,ˆi,a(t)|t∈T (T ) = ∑
t∈T
1(sˆ
l
(t + 1),sˆ
l
(t), al
(t))
the discounted number of times along the history .ht (.t ∈ T ) where the state is.sˆ(t)
implement the action.a(t), as well as,
.ηl
ˆit,a(t)|t∈T (T ) = ∑
t∈T
1(sˆ
l
(t) = ˆsl
, al
(t) = al
)
is the discounted number of times along the history.ht .(t ∈ T ), where the profile is
.sˆ
l
t implement the action.a(t) where the indicator function.1(·) is defined as:
.1(et) =
[
1 if the event et occurs during period t
0 otherwise.
The estimated conditional probability.pˆl
ˆj,ˆi,k
, when all states are observable, is given
by
.πˆl
ˆj,ˆi,k =
ηl
ˆj,ˆi,k|t∈T
ηl
ˆi,k|t∈T
.
We can write the discounted payoff of player.l as a function of the discounted measure
.ηl
ˆj,ˆi,mˆ ,k of states and outcomes,
.uˆ
l
ˆj,ˆi,mˆ ,k =
∑
t∈T ςl
ˆj,ˆi,mˆ ,k
1(sˆ
l
(t + 1),sˆ
l
(t), yˆl
(t), al
(t)
ηl
ˆj,ˆi,mˆ ,k
, (8.5.1)
where
.ςl
ˆj,ˆi,mˆ ,k = ul
ˆj,ˆi,mˆ ,k
1 + rt
2 .
such that.ςl ≤ ul and.rt takes randomly the values within the interval.[−1, 1].
For each player .l the mechanism design .μ∗
a|m, the strategy .σl∗
r|i , the distribution
.Pl
m and the observation kernel .[ql
m|r] the construction of the dynamic mechanism
design .μ∗
k|m is supported by the elements of the estimated transition matrix .pˆl
ˆj,ˆi,a
.
We suppose that the det.[ql
m|r] /= 0 such that.Q = [q]
−1 exists where.q = [ql
m|r].
We propose a model from experiences that is computed by counting the num￾ber.η(T ) of unobserved experiences defining the following variables recursively as
follows:
.
ηl
m,k|t∈T (T ) = ∑
t∈T
1(m, a),
ηl
m'
,m,k|t∈T (T ) = ∑
t∈T
1(m'
, m, a),8.5 Reinforcement Learning Approach 169
Table 8.1 BPOMG learning process
Algorithm 1: Learning processes
Let.N = {1, ..., n} be the set of players (.l ∈ N )
Let.sl
0 = sl be the initial state for player.l
Let.Πˆ l = Πl be the initial transition matrix for player.l
Let.ε > 0 be the error of the estimated parameters
do
Compute the strategy.σl∗
r|i , the observers.ol
irm and the mechanism.μ∗
k|m
by applying the game theory solver (see Sect. 8.6)
Select randomly a type.r from the.σl∗
r|i
Select randomly an.m from the.ql
m|r
Select randomly an action.a(t) from the.μ∗
k|m
From the transition matrix.πˆl
ˆj|ˆi,k get next state. j for each player.l
Increase the values of.ηl
m',m,k|t∈T (T ) and.ηl
m,k|t∈T (T )
Compute the mean square error.el
t
Estimate.πˆl
ˆj|ˆi,k given in Eq. (8.5.2)
Estimate.uˆl
ˆj,ˆi,mˆ ,k given in Eq. (8.5.1)
Update.i = j and increase.t by 1 (.t = t + 1)
Until for each player.ε > el
t
where.ηl
m,k|t∈T (T ) is the number of visits in the reported state.m and,.ηl
m'
,m,k|t∈T (T )
denotes the total number of times that the process evolves from the reported state.m to
.m' applying action.a. We have that.ηl
m,k|t∈T (T ) = ∑
M
m'
=1
ηl
m'
,m,k|t∈T (T ). The frequency
is defined by.φm'
,m,k|t∈T = t−1ηl
m'
,m,k|t∈T (T ) (Table 8.1).
Remark 8.2 For each player .l, the estimated transition matrix .πˆl
ˆj|ˆi,k
(T ) is repre￾sented by
.πˆl
ˆj|ˆi,k
(T ) =
∑
M
m=1
∑
M
m'
=1
Ql
m'
| ˆr Ql
m| ˆrt+1
ηl
m'
,m,k|t∈T (T )
∑
N
rˆt+1=1
[ ∑
M
m=1
∑
M
m'
=1
Ql
m'
| ˆrt+1
Ql
m| ˆrl
t+1
ηl
m'
,m,k|t∈T (T )
]. (8.5.2)
8.5.2 Learning Algorithm
In the Algorithm 1, the initial time is.t = 0 and for each player.l we let.sl
0 = sl be
the initial state and .Πˆ l = Πl the initial transition matrix. We fix the error of the
estimated parameters by.ε > 0. For each iteration, we compute the strategy.σl∗
r|i , the170 8 Joint Observer and Mechanism Design
observers.ol
irm and the mechanism.μ∗
k|m. Next, we select randomly.r from the strategy
.σl∗
r|i , randomly an .m from the .ql
m|r, and action .k from the mechanism .μ∗
k|m. Then,
the system advances by obtaining the state . j randomly from.πˆl
ˆj|ˆi,k
for fixed state .i
and action .k. Next, we update the values of.ηl
m'
,m,k|t∈T and .ηl
m,k|t∈T . So, we get the
estimate of.πˆl
ˆj|ˆi,k and .uˆl
ˆj,ˆi,mˆ ,k using the estimation rules in Eqs. (8.5.2) and (8.5.1),
respectively. Finally, we compute the mean square error.et by using
.el
t = ∑
K
k=1
tr (
(Πˆ l
(t − 1) − Πˆ l
(t)) (Πˆ l
(t − 1) − Πˆ l
(t)))
. (8.5.3)
The process continues until.ε > el
t for each.l. At the end, we obtain the estimated tran￾sition matrices.πˆl
ˆj|ˆi,k
, the estimated values of.uˆl
ˆj,ˆi,mˆ ,k
. The resulting strategy profile
.σl∗
r|i is a Bayesian-Nash equilibrium. The resulting mechanism .μ∗
k|m is incentive￾compatible because it is in equilibrium and satisfies Eq. (8.3.1).
8.6 Nash Equilibrium as a Solution of a Max-Min Problem
We consider the Nash equilibrium problem [21, 22]. To this end, we first recall the
definition of the (standard) Nash equilibrium problem.
We consider a game played by a set.N = {1, ..., n} players indexed by.l ∈ N . Each
player.l ∈ N control the variable.zl
irmk = μk|mσl
r|iol
irm. Let us consider a game whose
strategies are denoted by .vl ∈ Vl where .Vl is a convex and compact set, and .vl :=
col (
zl
irmk )
. Let.v = (v1, ..., vn) ∈ V, the vector formed by all these decision vari￾ables, be the joint strategy of the players and .v−l := (
v1, ..., vl−1, vl+1, ..., vn
) 
∈
V −l be a strategy of the rest of the players adjoint to .vl ∈ Vl
. To emphasize the .l
th player’s variables within the vector.v, we sometimes write .v = (vl
, v−l
) ∈ Rn
where where .v−l subsumes all the other players’ variables. We consider a Nash
equilibrium problem with .n players and denote by .v = (vl
, v−l
) ∈ Rn the vector
representing the .v-th player’s strategy where .v ∈ Vadm and .Vadm = Vl
adm × V −l
adm,
such that.Vadm := {
v|v ≥ 0, Aeq v = beq ∈ Rr0 , Aineq v ≤ bineq ∈ Rr1
}
.
Let us consider.vl : Rn → R be the.l-th player’s reward function (cost function).
We assume that these reward functions are at least continuous, and we further assume
that the functions .φl (
vl
, v−l
) are concave in the variable .vl
. The players try to
reach one of the non-cooperative equilibrium, that is, they try to find a strategy
.v∗ = (
v1∗, ..., vn∗
)
satisfying for any.vl and any.l ∈ N that
.U¯ (v) := ∑
l∈N
[(max
vl∈Vl
φl (
vl
, v−l
)
)
− φl (
vl
, v−l
)
]
. (8.6.1)
Let us consider that8.6 Nash Equilibrium as a Solution of a Max-Min Problem 171
.U¯ (v) := ∑
l∈N
[
φl (
v¯
l
, v−l
)
− φl (
vl
, v−l
)], (8.6.2)
where
.v¯
l := arg max
vl∈Vl
φl (
vl
, v−l
)
, (8.6.3)
and the function.φl satisfies that
.φl (
v¯
l
, v−l
)
− φl (
vl
, v−l
)
≥ 0 (8.6.4)
for any.vl ∈ Vl and.l ∈ N . Then a vector.v∗ ∈ V is a non-cooperative equilibrium,or
a solution of the Nash equilibrium problem if
.v∗∈Arg max
v∈V
{
U¯ (v)
}
. (8.6.5)
In the case where.U¯ (v) is a strictly concave function we have that
.v∗= arg max v∈V
{
U¯ (v)
}
.
Lagrange’s method is employed for finding the local maximum (minimum) of a
function, subject to equality constraints [37, 63] such that,
.
L(
v, λeq , λineq )
:=
θU(v) − λ 
eq (
Aeq v − beq )
− λ 
ineq (
Aineq v − bineq )
− δ
2
(
|v|2 − |
|λ 
eq
|
|2 −
|
|
|
λ 
ineq
|
|
|
2
)
,
(8.6.6)
where the parameters.θ and.δ are positive, the Lagrange vector-multipliers.λineq ∈ Rr1
are non-negative and the components of.λeq ∈ Rr0 may have any sign (.λ ∈ Λ). The
problem
.Lθ,δ
(
v, λeq , λineq )
→ max
x∈Xadm
min
λeq ,λineq≥0
(8.6.7)
has a unique saddle-point on.x since the Eq. (8.6.6) is strongly concave if the param￾eters.θ and.δ > 0 provide the condition that
.
∂2
∂v∂v Lθ,δ
(
v, λeq , λineq )
< 0∀v ∈ Vadm (8.6.8)
and it is strongly convex on the Lagrange multipliers.λeq , λineq for any.δ > 0. As a
result, the Eq. (8.6.6) has the unique saddle point .
(
v∗ (δ), λ∗
eq (θ, δ), λ∗
ineq (θ, δ)
)
(see the Kuhn-Tucker Theorem 21.13 in [52]) for which the following inequalities
hold:172 8 Joint Observer and Mechanism Design
.
Lθ,δ
(
v (δ), λ∗
eq (θ, δ), λ∗
ineq (θ, δ)
)
≤ Lθ,δ
(
v∗ (δ), λ∗
eq (θ, δ), λ∗
ineq (θ, δ)
)
≤
Lθ,δ
(
v∗ (δ), λeq , λineq )
(8.6.9)
for any.λeq , λineq with nonnegative components and any.v ∈ Rn.
We suppose that the parameter .θ and the regularizing parameter .δ tend to zero
(.θ, δ ↓ 0). It is also assumed that .d {a; Y } is the Hausdorff distance defined as
.d {c; Y } = min
y∈Y
|c − y|2
. Then, the solutions of.v∗ (θ, δ) and.λ∗
eq (θ, δ),.λ∗
ineq (θ, δ)
of the max-min problem given in Eq. (8.6.7) tend to the set.X∗ ⊗ Λ∗ ⊗ Λ∗ of all the
saddle points of the original game theory problem given in Eq. (8.6.1), that is,
.d
{
v∗ (θ, δ), λ∗
eq (θ, δ), λ∗
ineq (θ, δ); V∗ ⊗ Λ∗ ⊗ Λ∗} →θ,δ↓0
0. (8.6.10)
Then, the following theorem holds.
Theorem 8.2 The solution.x∗ of the problem
.Lθ,δ
(
v, λeq , λineq )
→ max
v∈Vadm
min
λeq ,λineq≥0
is a (unique) Nash equilibrium point.
Proof Straightforward following [23, 24].. 
8.7 Application: Patrolling
This section present an application of our results and methods.
8.7.1 Description of the Patrolling Problem
Patrolling an environment can be considered as finding efficient ways of performing
visits to target locations of a given area [2, 3, 59]. The challenge of protecting different
locations employing limited patrolling resources, makes it impossible to protect all
targets properly. This problem is usually represented by a game theory approach,
which seems to be appropriate to solve many real-world security situations. This
task can be considered inherently multiagent-like since in most cases the process
will be started in a distributed manner, by a group of agents. The players react to
each other’s changes until an equilibrium is achieved. A Nash equilibrium occurs
when defenders and attackers react to each other’s strategic changes until their actions
reach an equilibrium. An approach for solving this problem is to design a mechanism
where its equilibrium state coincides with the state of the environment. It is possible8.7 Application: Patrolling 173
to consider different designs of mechanisms as strategic games that analyses the
existence of Nash equilibrium in dominating strategies (agents may not be motivated
to provide accurate information and they are induced to reveal private information
and create a particular outcome). This example presents a game-theoretic analysis
of mechanisms and observers using an RL approach.
For representing the patrolling game, we will consider three players, an attacker
(.h = 1) that try to make the most of the expected damage and two defenders (.l = 1, 2)
that try to stop the attacker. In the dynamics of the game the players take alternate
turns: defenders commit first to a strategy and then, the attacker strategy is played.
Let the number of states of each player be.θ = 4, and the number of actions of each
player be.a = 3. We will recover analytically the variables of interest for each agent,
i.e. observation kernels (.q), joint observers (.o), mechanism (.μ), behavior strategies
(.σ), and distribution vectors (.P).
8.7.2 Solver
The Lagrange function.Lθ,δ
(
vl
, v−l
, λ
)
is poly-linear in.vl ∈ Vl
, and therefore cannot
be solved analytically. So, we need to apply an iterative method to find a minimizing
solution which, additionally, may be not unique. The general format iterative version
.(n = 0, 1, ...) of the iterative step method for computing the Nash equilibrium for
Markov chains [4, 57, 58] with some fixed admissible initial values.(v0 ∈ V,.v−l
0 (v) ∈
Vˆ) is as follows
1. Proximal prediction step:
.
λ¯ n = arg min λ≥0
{
−1
2 |λ − λn |2+γLθ,δ
(
xl
n, x−l
n , λ
)} ,
v¯l
n= arg min vl∈Vl
{ 1
2 |vl − v−l
n |2+γLθ,δ
(
vl
, v−l
n , λ¯ n
)} ,
v−l n = arg min v−l∈V −l
{ 1
2 |v−l − v−l
n |2 +γLθ,δ
(
vn, v−l
, λ¯ n
)} .
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
(8.7.1)
2. Gradient approximation step:
.
λn+1 = arg min λ≥0
{
−1
2 |λ − λn |2+γLθ,δ
(
v¯l
n, v−l n , λ
)} ,
vl
n+1= arg min vl∈Vl
{
1
2 |vl − v−l
n |2+γLθ,δ
(
vl
, v−l n , λ¯ n
)} ,
v−l
n+1 = arg min v−l∈V −l
{ 1
2 |v−l − v−l
n |2 +γLθ,δ
(
v¯n, v−l
, λ¯ n
)} .
⎫
⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎭
(8.7.2)
The formulas in Eq. (8.7.1) and Eq. (8.7.2) have an interpretation: they involve
two nonlinear systems, corresponding to evaluation of the extraproximal operators.
Evaluating the Eq. (8.7.1) and Eq. (8.7.2) of the objective involves solving two related
optimization problems, one for each possible sequence of outcomes. The first step
computes the direction of the future evolution of the optimization problem at a given174 8 Joint Observer and Mechanism Design
point and, the second step makes the gradient step from the same point along the
predicted direction of the optimization problem.
Conditions to the parameters [23, 24] are given by
.
δn =
⎧
⎨
⎩
δ0 if n ≤ n0
δ0
[1 + ln (n − n0)]
(1 + n − n0)
βδ
if n > n0 , θn =
⎧
⎨
⎩
θ0 if n < n0
θ0
(1 + n − n0)
βθ
if n ≥ n0,
βδ = 1/2, βθ = 3/4.
The optimal orders.δ, θ and.γ are as follows
.γ = γ∗ = 1
2
, δ = δ∗ = 1
2
, θ∗ = 3
4
,κ∗ = 1.
providing the order of the convergence.O
( 1
nκ∗
)
= O
(1
n
)
.
8.7.3 Random Walk Problem Formulation
For the simulation of the security game, we take into account two defenders and
one attackers in which, at each time instant.n, the defenders and the attacker select
randomly a type.r from the.σl∗
r|i and the.σh∗
r|i . Then, choose an observable state.yl
(n)
and .yh(n) from.ql
m|i and .qh
m|i . Next, they select randomly stochastic actions.k from
.μ∗
k|m. From the transition matrix .pˆl
ˆj|ˆi,k get next state . j for each player .l, so as to
respectively minimize and maximize the probability of damage jumping from the
partially observed state .yl
(n) and .yh(n) at the next time instant .n + 1 (given by
.yl
(n + 1) and.yh(n + 1)). The defenders and attacker attempt finishing the game at
the next time instant.n + 1: this until the realization of the Markov game satisfies the
game-over or capture condition given by
.
∑
2
l=1
∑
1
h=1
∑
M
m=1
1(yl
(n) = ym)1(yh(n) = ym). (8.7.3)
The algorithm for the partially observable random walk is given by:8.7 Application: Patrolling 175
Algorithm 8.1: Random walk
1. Select randomly an initial state.iι for each player.ι (defender.l or attacker.h) from.Pι∗
i .
2. Let.μ∗
k|m be the mechanism design, strategies.σι∗
r|i and kernel.qι∗
m|r of the security game .
3. For each player.ι Do
4. For select randomly a type.r from the.σι∗
r|i
5. For the matrix.qι∗
m|r select randomly an observable
state.y(t) = ym with random.m ∈ {1, M} distributed according to the vector.qι∗
m|r.
6. For the matrix.μ∗
k|m choose randomly an action.a(t) = ak .
7. For the matrix.pι
j|ik find the state.sj with random. j ∈ {1, N} distributed according
to the stochastic vector.πι
j|ik for a fixed.i ∈ {1, N} and action.a = ak for.k ∈ {1, K}.
8. Add.sj to the random walk and update the initial value of.i with. j.
9. Repeat steps (3) to (7) until the capture condition.1(ω : sl
(n) = sj)1(ω : sh (n) = sj)
is satisfied, (.l = 1, 2) and (.h = 1).
8.7.4 Resulting Values
Applying the proposed method, the resulting mechanism is given by
.μ∗
k|m=
⎡
⎢
⎢
⎣
0.2915 0.7085
0.5061 0.4939
0.3985 0.6015
0.4775 0.5225
⎤
⎥
⎥
⎦ .
The resulting (behavior) strategies of the attacker are given by
. σ1∗
r|i=
⎡
⎢
⎢
⎣
0.2304 0.2306 0.2304 0.3085
0.2500 0.2500 0.2500 0.2500
0.1962 0.1962 0.1962 0.4115
0.5454 0.1516 0.1515 0.1516
⎤
⎥
⎥
⎦ ,
and the behavior) strategies of the defenders are given by
. σ2∗
r|i=
⎡
⎢
⎢
⎣
0.2400 0.2400 0.2400 0.2800
0.1479 0.1479 0.1479 0.5563
0.2499 0.2499 0.2499 0.2504
0.2489 0.2504 0.2504 0.2504
⎤
⎥
⎥
⎦ , σ3∗
r|i=
⎡
⎢
⎢
⎣
0.2401 0.2401 0.2401 0.2797
0.1180 0.1180 0.1180 0.6459
0.2127 0.2127 0.2133 0.3613
0.4690 0.1770 0.1770 0.1770
⎤
⎥
⎥
⎦ .
The resulting observers of the attacker are given by
. q1∗
m|r=
⎡
⎢
⎢
⎣
0.1224 0.2026 0.5090 0.1660
0.1995 0.3304 0.1995 0.2706
0.1996 0.3302 0.1996 0.2705
0.3153 0.3300 0.1506 0.2042
⎤
⎥
⎥
⎦ ,176 8 Joint Observer and Mechanism Design
and the resulting observers of the defenders are given by
. q2∗
m|r=
⎡
⎢
⎢
⎣
0.1862 0.1860 0.1860 0.4425
0.2982 0.2978 0.2978 0.2285
0.3125 0.3133 0.3133 0.1997
0.2032 0.2029 0.2029 0.1293
⎤
⎥
⎥
⎦ , q3∗
m|r=
⎡
⎢
⎢
⎣
0.1648 0.2330 0.4375 0.1647
0.2265 0.3202 0.2269 0.2264
0.2263 0.3207 0.2268 0.2262
0.6540 0.1434 0.1014 0.1012
⎤
⎥
⎥
⎦ .
The distribution vectors are as follows:
. P1∗
i =
⎡
⎢
⎢
⎣
0.2765
0.1959
0.2048
0.3228
⎤
⎥
⎥
⎦ , P2∗
i =
⎡
⎢
⎢
⎣
0.3304
0.2724
0.2329
0.1643
⎤
⎥
⎥
⎦ , P3∗
i =
⎡
⎢
⎢
⎣
0.2253
0.3437
0.2026
0.2284
⎤
⎥
⎥
⎦ .
The resulting joint observers are given by
. o1∗
im=
⎡
⎢
⎢
⎣
0.0601 0.0831 0.0707 0.0625
0.0410 0.0584 0.0519 0.0446
0.0475 0.0625 0.0492 0.0456
0.0565 0.0841 0.1165 0.0657
⎤
⎥
⎥
⎦ ,
. o2∗
im=
⎡
⎢
⎢
⎣
0.0615 0.0985 0.1033 0.0671
0.0506 0.0811 0.0853 0.0553
0.0433 0.0694 0.0730 0.0473
0.0727 0.0376 0.0328 0.0213
⎤
⎥
⎥
⎦ , o3∗
im=
⎡
⎢
⎢
⎣
0.0371 0.0525 0.0985 0.0371
0.0778 0.1101 0.0780 0.0778
0.0459 0.0650 0.0460 0.0458
0.1494 0.0327 0.0232 0.0231
⎤
⎥
⎥
⎦ .
The convergence of the strategies are given in Figs. 8.1, 8.2 and 8.3. The conver￾gence of the error of the transition matrices are given in Figs. 8.4, 8.5 and 8.6. The
convergence of the error of the utility matrices are given in Figs. 8.7, 8.8 and 8.9.
We consider the random walk process presented in Algorithm 8.1. A simulation
of the game is shown in Fig. 8.10 where the attacker (black) is caught in state 2 after
16 steps by the defender 1 (blue), so the game is over. A different realization of the
game is shown in in Fig. 8.11. Here, we have that the attacker (black) is caught in
state 1 after 11 steps by the defender 1 (blue) and the defender 2 (red) at the same
time, then the game is over.8.7 Application: Patrolling 177
Fig. 8.1 Convergence of the
Strategies.z1 of the Attacker
0 10 20 30 40 50 60 70
Iterations
0
0.05
0.1
0.15
0.2
0.25
0.3
Probability
Dynamic of the strategies 1 for the Player 1
1
(1,1,1)
1
(2,1,1)
1
(3,1,1)
1
(4,1,1)
1
(1,2,1)
1
(2,2,1)
1
(3,2,1)
1
(4,2,1)
1
(1,3,1)
1
(2,3,1)
1
(3,3,1)
1
(4,3,1)
1
(1,4,1)
1
(2,4,1)
1
(3,4,1)
1
(4,4,1)
1(1,1,2)
1
(2,1,2)
1
(3,1,2)
1(4,1,2)
1
(1,2,2)
1
(2,2,2)
1
(3,2,2)
1
(4,2,2)
1
(1,3,2)
1
(2,3,2)
1
(3,3,2)
1
(4,3,2)
1
(1,4,2)
1
(2,4,2)
1
(3,4,2)
1
(4,4,2)
Fig. 8.2 Convergence of the
Strategies.z2 of the Defender
1
0 10 20 30 40 50 60 70
Iterations
0
0.05
0.1
0.15
0.2
0.25
0.3
Probability
Dynamic of the strategies 2
 for the Player 2
2
(1,1,1)
2
(2,1,1)
2
(3,1,1)
2(4,1,1)
2
(1,2,1)
2
(2,2,1)
2
(3,2,1)
2(4,2,1)
2
(1,3,1)
2(2,3,1)
2
(3,3,1)
2(4,3,1)
2
(1,4,1)
2
(2,4,1)
2
(3,4,1)
2(4,4,1)
2
(1,1,2)
2
(2,1,2)
2
(3,1,2)
2
(4,1,2)
2
(1,2,2)
2
(2,2,2)
2
(3,2,2)
2
(4,2,2)
2
(1,3,2)
2
(2,3,2)
2
(3,3,2)
2(4,3,2)
2
(1,4,2)
2(2,4,2)
2
(3,4,2)
2(4,4,2)178 8 Joint Observer and Mechanism Design
Fig. 8.3 Convergence of the
Strategies.z3 of the Defender
2
0 10 20 30 40 50 60 70
Iterations
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Probability
Dynamic of the strategies 3 for the Player 3
3
(1,1,1)
3
(2,1,1)
3
(3,1,1)
3
(4,1,1)
3
(1,2,1)
3
(2,2,1)
3
(3,2,1)
3
(4,2,1)
3
(1,3,1)
3
(2,3,1)
3
(3,3,1)
3
(4,3,1)
3
(1,4,1)
3
(2,4,1)
3
(3,4,1)
3
(4,4,1)
3
(1,1,2)
3
(2,1,2)
3
(3,1,2)
3
(4,1,2)
3
(1,2,2)
3
(2,2,2)
3
(3,2,2)
3
(4,2,2)
3
(1,3,2)
3
(2,3,2)
3
(3,3,2)
3
(4,3,2)
3
(1,4,2)
3
(2,4,2)
3
(3,4,2)
3
(4,4,2)
Fig. 8.4 Convergence of the
error.p1 of the Attacker
0 10 20 30 40 50 60
Iterations
0
1
2
3
4
5
6
Probability
10-38.7 Application: Patrolling 179
Fig. 8.5 Convergence of the
error.p2 of the Defender 1
0 10 20 30 40 50 60
Iterations
0
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0.01
Probability
Fig. 8.6 Convergence of the
error.p3 of the Defender 2
0 10 20 30 40 50 60
Iterations
0
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0.01
Probability
Fig. 8.7 Convergence of the
error.u1 of the Attacker
0 10 20 30 40 50 60
Iterations
0
50
100
150
200
250
300
350
Value180 8 Joint Observer and Mechanism Design
Fig. 8.8 Convergence of the
error.u2 of the Defender 1
0 10 20 30 40 50 60
Iterations
0
50
100
150
200
250
Value
Fig. 8.9 Convergence of the
error.u3 of the Defender 2
0 10 20 30 40 50 60
Iterations
0
50
100
150
200
250
Value
Fig. 8.10 Realization 1 of
the game. The attacker
(black) is caught by the
defender 1 (blue)
0 2 4 6 8 10 12 14 16
Iterations
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
State
Random walk: Realization of the gameReferences 181
Fig. 8.11 Realization 2 of
the game. The attacker
(black) is caught by the
defender 1 (blue) and the
defender 2 (red)
0 1 2 3 4 5 6 7 8 9 10 11
Iterations
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
State
Random walk: Realization of the game
References
1. Abreu, D., Pearce, D., Stacchetti, E.: Optimal cartel equilibria with imperfect monitoring. J.
Econ. Theory 39, 251–269 (1986)
2. Albarran, S., Clempner, J.B.: A stackelberg security markov game based on partial information
for strategic decision making against unexpected attacks. Eng. Appl. Artif. Intell. 81, 408–419
(2019)
3. Alcantara-Jimenez, G., Clempner, J.B.: Repeated stackelberg security games: Learning with
incomplete state information. Reliab. Eng. Syst. Saf. 195, 106695 (2020)
4. Antipin, A.S.: An extraproximal method for solving equilibrium programming problems and
games. Comput. Math. Math. Phys. 45(11), 1893–1914 (2005)
5. Arrow, K.: Economics and human welfare, chapter. In: The property rights doctrine and demand
revelation under incomplete information, pp. 23–39. Academic, New York (1979)
6. Asiain, E., Clempner, J.B., Poznyak, A.S.: A reinforcement learning approach for solving the
mean variance customer portfolio for partially observable models. Int. J. Artif. Intell. Tools
27(8), 1850034–1–1850034–30 (2018)
7. Asiain, E., Clempner, J.B., Poznyak, A.S.: Controller exploitation-exploration reinforcement
learning architecture for computing near-optimal policies. Soft. Comput. 23(11), 3591–3604
(2019)
8. Athey, S., Bagwell, K.: Optimal collusion with private information. RAND J. Econ 32, 428–465
(2001)
9. Athey, S., Segal, I.: An efficient dynamic mechanism. Econometrica 81(6), 2463–2485 (2013)
10. Battaglini, M.: Long-term contracting with markovian consumers. Am. Econ. Rev. 95(3), 637–
658 (2005)
11. Bergemann, D., Välimäki, J.: Dynamic mechanism design: an introduction. J. Econ. Perspect.
(2018). Forthcoming
12. Bernheim, B., Madsen, E.: Price cutting and business stealing in imperfect cartels. Am. Econ.
Rev. 107, 387–424 (2017)
13. Board, S.: Selling options. J. Econ. Theory 136, 324–340 (2007)
14. Board, S., Skrzypacz, A.: Revenue management with forward-looking buyers forward-looking
buyers. J. Polit. Econ. 124(4), 1046–1087 (2016,)
15. Börgers, T.: An Introduction to the Theory of Mechanism Design. Oxford University Press,
Oxford (2015)182 8 Joint Observer and Mechanism Design
16. Clarke, E.: Multi-part pricing of public goods. Pub. Choice 11, 17–23 (1971)
17. Clempner, J.B.: Algorithmic-gradient approach for the price of anarchy and stability for incom￾plete information. J. Comput. Sci. 60, 101589 (2022)
18. Clempner, J.B.: A bayesian reinforcement learning approach in markov games for computing
near-optimal policies. Ann. Math. Artif. Intell. 91, 675–690 (2023)
19. Clempner, J.B.: Revealing perceived individuals’ self-interest. J. Oper. Res. Soc. 1–10 (2023).
To be published. https://doi.org/10.1080/01605682.2023.2195878
20. Clempner, J.B., Poznyak, A.S.: Simple computing of the customer lifetime value: a fixed local￾optimal policy approach. J. Syst. Sci. Syst. Eng. 23(4), 439–459 (2014)
21. Clempner, J.B., Poznyak, A.S.: Computing the strong nash equilibrium for markov chains
games. Appl. Math. Comput. 265, 911–927 (2015)
22. Clempner, J.B., Poznyak, A.S.: Convergence analysis for pure and stationary strategies in
repeated potential games: nash, lyapunov and correlated equilibria. Expert Syst. Appl. 46,
474–484 (2016)
23. Clempner, J.B., Poznyak, A.S.: A tikhonov regularization parameter approach for solving
lagrange constrained optimization problems. Eng. Optim. 50(11), 1996–2012 (2018)
24. Clempner, J.B., Poznyak, A.S.: A tikhonov regularized penalty function approach for solving
polylinear programming problems. J. Comput. Appl. Math. 328, 267–286 (2018)
25. Clempner, J.B., Poznyak, A.S.: Observer and control design in partially observable finite
markov chains. Automatica 110, 108587 (2019)
26. Clempner, J.B., Poznyak, A.S.: A nucleus for bayesian partially observable markov games:
joint observer and mechanism design. Eng. Appl. Artif. Intell. 95, 103876 (2020)
27. Clempner, J.B., Poznyak, A.S.: Analytical method for mechanism design in partially observable
markov games. Mathematics 9(4), 1–15 (2021)
28. Clempner, J.B., Poznyak, A.S.: A dynamic mechanism design for controllable and ergodic
markov games. Comput. Econ. 61, 1151–1171 (2023)
29. Clempner, J.B., Poznyak, A.S.: Mechanism design in bayesian partially observable markov
games. Int. J. Appl. Math. Comput. Sci. 33(3), 463–478 (2023)
30. Clempner, J.B., Poznyak, A.S.: The price of anarchy as a classifier for mechanism design in a
pareto-bayesian-nash context. J. Ind. Manag. Optim 19(9), 6736–6749 (2023)
31. Courty, P., Li, H.: Sequential screening. Rev. Econ. Stud. 67, 697–717 (2000)
32. Dutta, P.: A folk theorem for stochastic games. J. Econ. Theory 66, 1–32 (1995)
33. d’Aspremont, C., Gerard-Varet, L.: Incentives and incomplete information. J. Public Econ.
11(1), 25–45 (1979)
34. Escobar, J.F., Llanes, G.: Cooperation dynamics in repeated games of adverse selection. J.
Econ. Theory 176, 408–443 (2018)
35. Es ˝o, P., Szentes, B.: Optimal information disclosure in auctions and the handicap auction. Rev.
Econ. Stud. 74(3), 705–731 (2007)
36. Fudenberg, D., Maskin, E.: The folk theorem in repeated games with discounting or with
incomplete information. Econometrica 54, 533–554 (1986)
37. Garcia, C.B., Zangwill, W.I.: Pathways to Solutions. Fixed Points and Equilibria. Prentice-Hall,
Englewood Cliffs (1981)
38. Gershkov, A., Moldovanu, B.: Dynamic revenue maximization with heterogenous objects: A
mechanism design approach. Am. Econ. J.: Microecon 1(2), 168–198 (2009)
39. Gershkov, A., Moldovanu, B.: Dynamic Allocation and Pricing: A Mechanism Design
Approach. MIT Press (2014)
40. Green, E., Porter, R.: Noncooperative collusion under imperfect price information. Economet￾rica 52, 87–100 (1984)
41. Groves, T.: Incentives in teams. Econometrica 41, 617–631 (1973)
42. Harsanyi, J.C.: Games with incomplete information played by bayesian players. part i: the
basic model. Manage. Sci. 14, 159–182 (1967)
43. Hartline, J.D., Lucier, B.: Non-optimal mechanism design. Am. Econ. Rev. 105(10), 3102–3124
(2015)References 183
44. Hurwicz, L.: Optimality and informational efficiency in resource allocation processes. In: K.J.
Arrow, S. Karlin, P. Suppes (eds.), Mathematical Methods in the Social Sciences: Proceedings
of the First Stanford Symposium, pp. 27–46. Stanford University Press, California (1960)
45. Kakade, S., Lobel, I., Nazerzadeh, H.: Optimal dynamic mechanism design and the virtual-pivot
mechanism. Oper. Res. 61(4), 837–854 (2013)
46. Mailath, G., Postlewaite, A.: Asymmetric information bargaining problems with many agents.
Rev. Econ. Stud. 57, 351–360 (1990)
47. Menicucci, D.: Efficient mechanisms for a partially public good. Decis. Econ. Finance 25(1),
71–77 (2002)
48. Myerson, R.B.: Allocation, information and markets, chapter. In: Mechanism Design, pp. 191–
206. The New Palgrave. Palgrave Macmillan, London (1989)
49. Nobel: The sveriges riksbank prize in economic sciences in memory of alfred nobel 2007:
Scientific background. Technical report, The Nobel Foundation, Stockholm, Sweden (2007)
50. Pavan, A.: Dynamic mechanism design: Robustness and endogenous types. In: Honore,
M.P.B., Pakes, A., Samuelson, L. (eds.), Advances in Economics and Econometrics: 11th
World Congress (2017)
51. Pavan, A., Segal, I., Toikka, J.: Dynamic mechanism design: a myersonian approach. Econo￾metrica 82(2), 601–653 (2014)
52. Poznyak, A.S.: Advanced mathematical tools for automatic control engineers. In: Deterministic
Technique, vol. 1. Elsevier, Amsterdam (2008)
53. Poznyak, A.S., Najim, K., Gómez-Ramírez, E.: Self-learning Control of Finite Markov Chains.
Marcel Dekker, Inc. (2000)
54. Rahman, D.: The power of communication. Am. Econ. Rev. (2014)
55. Rogerson, W.: Contractual solutions to the hold-up problem. Rev. Econ. Stud. 59, 777–793
(1992)
56. Rotemberg, J., Saloner, G.: A supergame-theoretic model of price wars during booms. Am.
Econ. Rev. 76, 390–407 (1986)
57. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the stackelberg/nash equilibria using
the extraproximal method: convergence analysis and implementation details for markov chains
games. Int. J. Appl. Math. Comput. Sci. 25(2), 337–351 (2015)
58. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the lp-strong nash equilibrium for
markov chains games. Appl. Math. Modell. 41, 399–418 (2017)
59. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Adapting attackers and defenders preferred strate￾gies: a reinforcement learning approach in stackelberg security games. J. Comput. Syst. Sci.
95, 35–54 (2018)
60. Vickrey, W.: Counterspeculation, auctions, and competitive sealed tenders. J. Finance 16(1),
8–37 (1961)
61. Wang, X., Chin, K.S., Yin, H.: Design of optimal double auction mechanism with multi￾objectives. Expert Syst. Appl. 38, 13749–13756 (2011)
62. Yang, X., Yin, S.: Variational bayesian inference for fir models with randomly missing mea￾surements. IEEE Trans. Ind. Electron. 64(5), 4217–4225 (2017)
63. Zangwill, W.I.: Nonlinear Programming: A Unified Approach. Prentice-Halt, Englewood Cliffs
(1969)Chapter 9 
Bargaining Games or How to Negotiate 
Abstract The term “bargaining game” describes a scenario in which participants 
may reach a win-win agreement. There is a conflict of interest here over whether an 
agreement should be reached or whether no agreement should be reached without the 
consent of each player. Amazingly, negotiating protocols, duopoly market games, 
business transactions, arbitration, and other key scenarios have all used bargaining 
and its game-theoretic solutions. The underpinning for all of these research applica￾tions is equilibrium computation. This chapter explores the theory behind bargaining 
games and offers a way for solving the game-theoretic models of bargaining put out 
by Nash and Kalai-Smorodinsky, which suggest a beautiful axiomatic solution to the 
problem based on several fairness standards. We consider a class of continuous-time, 
controllable, and ergodic Markov games. The Nash bargaining solution is first intro￾duced and axiomatized. The Kalai-Smorodinsky approach, which adds the mono￾tonicity postulate to the Nash’s model, is then presented. We recommend using a 
bargaining solver to resolve the issue, which is carried out iteratively using a set of 
nonlinear equations represented by the Lagrange principle and the Tikhonov regu￾larization approach to guarantee convergence to a particular equilibrium point. Each 
equation in this solver is an optimization problem for which the necessary condi￾tion of a minimum is solved using the projection gradient method. This chapter’s 
key finding illustrates how equilibrium calculations work in bargaining games. We 
specifically discuss the convergence analysis and rate of convergence of the suggested 
technique. A numerical example contrasting the Nash and Kalai-Smorodinsky bar￾gaining solution problem is used to illustrate the value of the considered method. 
9.1 Introduction 
9.1.1 Brief Review 
Numerous significant scenarios, such as arbitration, supply chain agreements, 
duopoly market games, negotiating protocols, etc., have made use of the bargaining 
paradigm. It presents an idea for a cooperative game’s solution and is related to 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_9 
185186 9 Bargaining Games or How to Negotiate
negotiation and group decision-making procedures. Collaboration refers to alliances 
of two or more players acting jointly to achieve a specified goal while keeping in mind 
the need to maximize each player’s personal profit. While discussing the mechanics 
of a bargaining game, it’s important to note that it’s possible for players to reach a 
win-win agreement in certain circumstances.There is a conflict of interest here over 
whether an agreement should be reached or whether no agreement should be reached 
without the consent of each player. Nash [ 27] and Kalai-Smorodinsky [ 19] are two 
theoretical viewpoints that offer a solution for cooperative game-theoretic bargaining 
models that use the axiomatic method to analyze bargaining. It is important to recall 
that in Nash’s model, the disagreement point and the two approaches to a bargaining 
solution have the same viable payoff set. 
Using the game theory framework outlined by von Neumann and Morgenstern 
in their famous 1944 article [ 29], John Nash first presented the bargaining model 
as a game in 1950’s important study [ 27]. According to the von Neumann and 
Morgenstern hypothesis, when players establish a coalition, they anticipate that a 
complementary coalition will react by doing the worst possible harm to them. In 
the literature, there are arguments against this claim. By establishing axioms that 
define a singular outcome and a solution to the problem known as the Nash bar￾gaining solution, Nash extended the work of von Neumann and Morgenstern in this 
way. A possible set of utility allocations established through collaboration and the 
disagreement point that arises when players do not collaborate make up the formal 
description’s two primary parts. A function that chooses a workable utility alloca￾tion for each problem is a solution. It’s noteworthy to note that in the game theory 
literature (see [ 18, 30]), bargaining is one of the first cases of a conflict of interest. 
The Nash technique [ 27], as described in [ 19], is different from the Kalai￾Smorodinsky bargaining strategy. The Kalai-Smorodinsky technique fits monotonic￾ity, but the Nash solution complies with independence of irrelevant alternatives. This 
is the key distinction between the two approaches. According to Kalai and Smorodin￾sky, every alternative must have an impact on the decision made. 
9.1.2 Related Work 
The Nash bargaining issue and the Kalai-Smorodinsky technique have drawn the 
interest of scholars from several academic fields, and it is still a topic that is interest￾ing to game theory practitioners and academics alike. Merlo and Wilson developed 
a .n-player sequential bargaining model for Markov processes, [ 23]. They looked at 
the efficiency and uniqueness of the equilibrium results, the reasons why agreement 
takes longer, and the benefit of suggesting. The sets of stationary subgame perfect 
and subgame perfect equilibria that describe the size of the cake and the sequence in 
which players move define the sequential bargaining model. A model for the nego￾tiation process based on an offer model with an exogenous risk of breakdown for 
Markov games was proposed by Bolt and Houba, [ 6]. In a dynamic setting, they 
examined a modified version of the variable threat game without commitment. An9.1 Introduction 187
alternating-offer bargaining game described by Cai [ 7] shown that the suggested 
model has a limited number of Markov Perfect Equilibria, some of which display 
unnecessary delays. The number of delay periods that Markov Perfect Equilibria 
can sustain rises in the order of the square of the number of participants, and these 
findings hold true even when the Markov constraints are relaxed and more flexible 
surplus functions are used. In a market where pairs of agents interested in carry￾ing out a transaction are brought together by a stochastic process and, upon meet￾ing, initiate a bargaining process over the terms of the transaction, Rubinstein and 
Wolinsky [ 36] developed a bargaining problem treated with a strategic approach. 
They calculated the steady-state equilibrium agreements and examined how much 
they depended on the market. Kalandrakis [ 20] studied an infinitely repeated divide￾the-dollar bargaining game with an endogenous reversion point characterizing a 
Markov equilibrium which is such that irrespective of the discount factor or the 
initial division of the dollar, the proposer eventually extracts the whole dollar in 
all periods. They demonstrated that the status quo’s weak continuity of the offered 
strategies and the failure of the correspondence of the voters’ acceptance set to sat￾isfy lower hemicontinuity. A risk-neutral buyer and seller engage in an alternating 
offer negotiation in which the value of the product being exchanged is determined 
by a Markov process, according to Cripps [ 14]. He demonstrated that if the buyer 
lacks patience compared to the seller, there would be delays in the parties coming 
to a consensus, the buyer will be compelled to follow a poor consuming strategy, 
and the equilibrium will be ex-ante inefficient. Moreover, the author demonstrated 
that there exists a particular and effective equilibrium in which agreement occurs 
instantly if the buyer is more patient than the seller. Kenan [ 21] presented repeated 
contract negotiations involving the same buyer and seller where the size of the sur￾plus being divided, is specified as a two-state Markov chain with transitions that are 
synchronized with contract negotiation dates. The contracts are linked because the 
buyer has persistent private information. Since there is persistence in the Markov 
chain generating the surplus, a successful demand induces the seller to make another 
aggressive demand in the next negotiation, since the buyer’s acceptance reveals that 
the current surplus is large. An alternating offers-bargaining model was developed 
by Coles and Muthoo [ 13], in which the set of potential utility pairings changes over 
time in a non-stationary fashion. They demonstrated the existence of a unique sub￾game perfect equilibrium in the limit when the gap between two successive offers 
grows arbitrarily small and developed a description of the unique subgame perfect 
equilibrium payoffs. In a class of games that contains contract games and the Nash 
demand game, Naidu et al. [ 26] explored purposeful idiosyncratic play in a typical 
stochastic evolutionary model of equilibrium selection, demonstrating the existence 
and uniqueness of a stochastically stable bargaining result under such play. Abreu 
and Manea [ 1] investigated the Markov perfect equilibria of an infinite horizon game 
in which players are randomly matched into pairs. They proved that Markov per￾fect equilibria exist and demonstrated that Markov perfect equilibria payoffs are not 
always exclusive. A technique for creating pure strategy Markov perfect equilibria 
with high discount factors was also devised. In a model of social learning, Agastya 
[ 2] examined the twin problems of coalition formation and allocation, demonstrating188 9 Bargaining Games or How to Negotiate
that all self-perpetuating allocations realized from a straightforward bargaining game 
must be core allocations, even though players simultaneously demand surplus and 
only on their own behalf. Additionally, regardless of the society’s early history, they 
offered a suitable circumstance under which it finally learns to allocate the excess 
according to some fundamental principle. 
The Nash equilibrium regularity property was used by Anant et al. [ 4] to gen￾eralize the Kalai-Smorodinsky result and demonstrate that it is true only if the 
feasibility sets also happen to be Nash equilibrium regular. A one parameter class 
of asymmetric Kalai-Smorodinsky solutions is defined by restricted independence, 
scale invariance, Pareto optimality, and the individual monotonicity axiom of Kalai￾Smorodinsky. Dubra [ 16] established a restricted version of Nash’s Independence 
that resolves its major criticisms. For the Kalai-Smorodinsky bargaining solution, 
Köbberling and Peters [ 22] showed that utilitarian risk aversion and probabilistic risk 
aversion can have very different effects on bargaining solutions with a weak mono￾tonicity characteristic. To demonstrate that .n-player negotiating problems have a 
distinct self-supporting result under the Kalai-Smorodinsky solution, Driesen et al. 
[ 15] investigated bargaining problems under the premise that players are loss-averse. 
In addition to proposing a novel axiom termed proportionate concession invariance, 
they created the bargaining solutions that provide these precise results and described 
them using the conventional axioms of scale invariance, individual monotonicity, and 
strong individual rationality. According to Roth [ 35], no solution, regardless of what 
it is, can possess the other properties that characterize in the two-person case. This is 
because the Kalai-Smorodinsky game for two-person bargaining cannot be simply 
transformed into a general n-person bargaining game, demonstrating that the solution 
is not Pareto Optimal. In this sense, Peters and Tijs [ 31] established the existence 
of a single bargaining solution, defined on the entire class of two-person bargaining 
games, with the following characteristics: individual rationality, Pareto optimality 
independence of equivalent utility representation, individual monotonicity and sym￾metry. The four axioms of the Kalai-Smorodinsky solution are provided, and they 
demonstrate that precisely one of these solutions is symmetric by introducing a siz￾able subclass of n-person bargaining games. They demonstrated the risk-sensitivity 
of each of these strategies as well. Using dictatorial fractions as a threat to accept 
the winner’s proposal, all non-winners of the auction participate in Moulin’s [ 24] 
negotiating game. A subgame perfect equilibrium was how Moulin defined the equi￾librium behavior. The Kalai-Smorodinsky bargaining approach is when businesses 
and unions just bargain on pay, and businesses decide the level of employment in 
order to maximize profits given the negotiated wage, according to Alexander [ 3]. The 
author examined the scenario in which the union’s risk aversion and the pay elastic￾ity of employment are both constant, demonstrating that the Kalai-Smorodinsky and 
Nash solutions in this situation have a clear link with one another. Some applications 
are presented in [ 9, 10].9.1 Introduction 189
9.1.3 Nash Versus Kalai-Smorodinsky 
In this chapter, following [ 37, 38, 42], we analyze the bargaining solutions presented 
by Nash [ 27, 28] and Kalai-Smorodinsky [ 19], which depend on different principles 
of fairness. The bargaining solution allows players to solve the bargaining problem 
that result in a “fair” improved position. 
Following Nash [ 27], a solution to the bargaining problems. B is a function. f that 
takes as input any bargaining problem and returns a vector of utilities that belongs to 
the set of possible agreements. Φ. Several solutions can be proposed for solving the 
problem, but some of them can present inconsistencies. For example, one solution can 
go against symmetry by proposing a total improvement of the position of one player 
obtaining a point in the Pareto frontier of the utility and the other player receives no 
improvement [ 12]. A different solution of the problem could be a disagreement point. 
The first solution violates symmetry, so the solution is unfair, and the second solution 
is not Pareto-efficient, and does not take advantage of the cooperation related to an 
agreement situation. For solving the inconsistencies in the solution of the problem, 
Nash [ 27] proposed several axioms: 
(a) Invariant to affine transformations (or Invariant to equivalent utility representa￾tions): an affine transformation of the utility and disagreement point should not 
alter the outcome of the bargaining process; 
(b) Pareto optimality: the solution selects a point of the Pareto frontier such that the 
players can be made “better” off without making other players “worse off”; 
(c) Symmetry: if the players are indistinguishable, the solution should not discrimi￾nate between them; and 
(d) Independence of irrelevant alternatives: if the solution is chosen from a feasible 
set which is an element of a subset of the original set but containing the point 
selected earlier by the solution, then the solution must still assign the same point 
chosen from the subset. 
As a result, Nash [ 27] proposed the Nash bargaining solution: we say that there is 
a unique solution. b to the bargaining problem that satisfies the four axioms (a to d) 
which is given by the point that maximizes the product of utilities of the players. 
While three of Nash’s axioms are quite uncontroversial, the fourth one (Indepen￾dence of irrelevant alternatives) raised some criticism, which lead to a different line 
of research. Kalai and Smorodinsky [ 19] looked for characterizations of an alter￾native solution which do not use the controversial axiom. The solution idea can 
be represented geometrically in the following way. Let .a(Φ) be the utopia point, 
typically not feasible, which gives the maximum payoff. Now, connect the point of 
disagreement. d and that ideal point.a(Φ) by a line segment. The Kalai-Smorodinsky 
solution is the maximal point in.Φ on that line segment. They replaced Nash arguable 
fourth axiom by a monotonicity axiom:190 9 Bargaining Games or How to Negotiate
(e) If the set of possible agreements.Φ is enlarged such that the maximum utilities 
of the players remain unchanged, then neither of the players must not suffer from 
it. 
Then, Kalai and Smorodinsky [ 19] proposed the following solution: we say that there 
is a unique solution. b to the bargaining problem that satisfies the four axioms (a, b, c 
and e) which is given by the intersection point of the Pareto frontier and the straight 
line segment connecting. d and the utopia point.a(Φ). 
Nash [ 27] showed that there exists a unique standard independent solution for 
the bargaining model, while Kalai and Smorodinsky [ 19] showed that a different 
solution is the unique standard monotonic one. 
9.2 Motivation 
The most basic definition of bargaining refers to a socio-economic class of problems 
involving several players who cooperate in terms of obtaining a mutually better 
position of a desirable surplus whose distribution is in conflict. The features of the 
cooperation of the players in terms of reaching an agreement and the initial situations 
of the players in the status-quo before an agreement has effect will determine how 
the surplus will be distributed. Several social, political and economic problems are 
related to the bargaining problem. 
For instance, let us consider the case of selling a used car. When it comes to selling 
the car, the seller naturally wants to obtain the most money possible. It is practical 
to trade the car at a dealer or make a quick sale to a used car dealership, but these 
options usually leave the seller with significantly less than what the car is actually 
worth. Selling a car by himself allows the seller to get its full value. Then, the seller 
values his car at 3,000 which is the minimum price at which he would sell it. On the 
other hand, there exists a buyer that values the car at 5,000 which is the maximum 
price at which he would buy it. If trade occurs, the price lies between 3,000 and 
5,000, then both the seller and the buyer would become better-off and a conflict of 
interests arises. In any trade the seller and the buyer have the possibility of achieving 
a mutually beneficial agreement by having conflicting interests over the terms of the 
trade. 
The formal theory of bargaining originated with John Nash’s work in the early 
1950’s [ 27, 28]. The term bargaining is usually employed to refer to a situations 
in which players have the possibility of achieving a mutually beneficial agreement, 
there is a conflict of interests about which an agreement should conclude, and no 
agreement may be imposed on any individual without their approval. 
Let us consider two players.l = 1, 2.A bargaining problem is a pair. B = (Φ, d)
in the utility space were.Φ is a set of possible agreements in terms of utilities. u that 
player. 1 and player. 2 can yield. The player’s utility function.ul is strictly increasing 
and concave. The set of possible agreements is . Φ, which is a compact and convex 
set of.R2. An element of.Φ is a pair.u = (u1, u2) ∈ Φ and.d = (d1, d2) is called the9.2 Motivation 191
Fig. 9.1 Bargaining model 
disagreement utility point. Compactness arises from the assumptions related to closed 
production sets and bounded factor endowments. Convexity is obtained from the fact 
that expected utility over outcomes. Also, the set .Φ involves points that dominate 
the disagreement point, i.e., there is a positive surplus to be enjoyed if agreement 
is reached. The function . f takes as input any bargaining problem and returns a 
pair of utilities .u = (u1, u2) ∈ Φ. When we need to refer to the components of . f , 
we write.u1 = f 1(B) and.u2 = f 2(B). The interpretation is that given a bargaining 
problem .B = (Φ, d) there exists an agreement .u = f (Φ, d) ∈ Φ such that . u1 ≥
d1 and .u2 ≥ d2 which ensures that there exists a mutually beneficial agreement. 
Figure 9.1 shows the bargaining problem. 
Two fundamental axioms impose the most important restrictions over the solution 
of the bargaining problem (see Fig. 9.2). Pareto optimality: the function . f (Φ, d)
has the property that there does not exist a point .u = (u1, u2) ∈ Φ such that . u1 ≥
f 1(Φ, d) and.u2 ≥ f 2(Φ, d)such that.(u1, u2) /= f (Φ, d). Symmetry: suppose that 
. B is such that.U is symmetric around the.45◦ line and.d1 = d2, then. f 1(B) = f 2(B). 
The rest of the axioms will be presented in the formalization of the model. 
Nash’s bargaining solution of the bargaining situation described above is the 
unique pair of utilities, denoted by .(u1, u2) ∈ Φ, that solves the following maxi￾mization problem 
. max
u1,u2∈Θ
(u1 − d1
)(u2 − d2
)
where.Θ ≡ {
(u1, u2) ∈ Φ
|
| u1 ≥ d1 and u2 ≥ d2
}
. The maximization problem stated 
above has a unique solution, because the result of .(u1 − d1)(u2 − d2), which is 
referred to as the Nash product, is continuous and strictly concave. Figure 9.3 illus￾trates Nash’s bargaining solution.192 9 Bargaining Games or How to Negotiate
Fig. 9.2 Bargaining axioms 
Fig. 9.3 Nash’s bargaining 
solution 
9.3 Preliminaries 
In this section we introduce the (continuous-time, time-homogeneous) game model 
in which we are interested and the formulation of the problem [ 8]. As usual,. R and 
. N stand for the sets of real numbers and nonnegative integers, respectively. 
Throughout the remainder 
.G = (N , Sl
, Al
,{Al
(s)}s∈S, Vl
, Ql
)l=1,N (9.3.1)9.3 Preliminaries 193
stands for a continuous-time Markov game (CTMG), where.N = {1, ..., n} is the set 
of players and each player is indexed by .l = 1, n, the state space .Sl is a finite set 
.
{
sl
(1), ...,sl
(N)
}
,.N ∈ N, endowed with the discrete topology and the action set.Al is 
the action (or control) space, a finite space endowed with the corresponding Borel 
.σ−algebra.B(Al
). 
For each.sl ∈ Sl
,.Al
(sl
) ⊂ Al is the nonempty set of admissible actions at. sl and we 
shall suppose that it is compact. Whereas, the set. Kl := {
(sl
, al
) : sl ∈ Sl
, al ∈ Al
(sl
)
}
is the class of admissible pairs, which is considered as a topological subspace of 
.Sl × Al and, similarly, the set.K := {
k : k ∈ ×n
l=1 Kl
}
..Vl ∈ B(×n
l=1 Sl × Kl
) is the 
(measurable) one-stage cost function. 
The function .Ql in (9.3.1) is the matrix .
[
ql
jl|il kl
]
of the game’s transition rates, 
satisfying.ql
jl|il kl ≥ 0 for all.(sl
, al
) ∈ Kl and.i /= j such that 
. [
ql
jl|il kl
]
=
⎧
⎨
⎩
−∑N
i/= j λl
i,j(al
), if i = j,
λl
i,j(al
), if i /= j,
where .λl
i,j is a transition rate between state . i and . j, .λl
i = ∑N
j/=i λi,j . This matrix is 
assumed to be conservative.
∑Nl
jl=1 ql
jl|il kl = 0 and stable, which means that 
. ql∗
(il) := sup
al∈Al
ql
(il)(al) < ∞ ∀il ∈ Sl
,
where.ql
il
(al) := −ql
il,il
(al) ≥ 0 for all.al ∈ Al
. 
Now, we denote the probability transition matrix by 
. Πl
(t) = [πl
s,il,τ,jl,kl
]il,jl,kl, τ ≥ s
such that, 
. πl
s,il,τ,jl,kl = πl
0,il,t,jl,kl
, t = τ − s ∀il, jl ∈ Sl
and.
∑Nl
jl=1 πl
jl|il kl = 1. 
The Kolmogorov forward equations can be written as the matrix differential equa￾tion as follows: 
. Π'
(t) = Π (t)Q; Π (0) = I,
.Π (t) ∈ RN×N ,.I ∈ RN×N is the identity matrix. This system can be solved by 
.Π (t) = Π (0)eQt = eQt := ∑∞
t=0
t n Qn
n! , (9.3.2)194 9 Bargaining Games or How to Negotiate
and at the stationary state, the probability transition matrix is defined as 
. Π∗ = lim
t→∞Π (t).
Definition 9.1 The vector.Pl ∈ RN is called stationary distribution vector if 
. (
ΠlT)∗ Pl = Pl
,
where.
∑Nl
il=1 Pl
(il) = 1. 
This vector can be seen as the long-run proportion of time that the process is in 
state.il ∈ Sl
. 
Theorem 9.1 The following statements are equivalent: 
• . QlT Pl = 0
• .ΠlT(t)Pl = Pl
;. ∀t ≥ 0
The proof of this fact is easy in the case of a finite state space, recalling the 
Kolmogorov backward equation. 
A strategy for player. l is then defined as a sequence.dl = {
dl
(t), t ≥ 0
}
of stochas￾tic kernels.dl
(t) such that: 
(a) for each time.t ≥ 0,.dl
kl|il
(t)is a probability measure on.Al such that. dl
Al(il)|il
(t) =
1 and, 
(b) for every.El ∈ B(Al
).dl
El|il
(t) is a Borel-measurable function in.t ≥ 0. 
We denoted by.Dl the family of all strategies for player. l. A multistrategy is a vec￾tor .d = (d1, ..., dn) ∈ D := ʘn
l=1
Dl
. From now on, we will consider only stationary 
strategies.dl
kl|il
(t) = dl
kl|il
. For each strategy.dl
kl|il the associated transition rate matrix 
is defined as: 
. Ql
(dl
) := [ql
jl|il
(dl
)] = ∑
Ml
kl=1
ql
jl|il kl
dl
kl|il
such that on a stationary state distribution for all.dl
kl|il and.t ≥ 0 we have that. Πl∗(d) =
lim
t→∞ eQl
(dl
)t
, where.Πl∗ (
dl
)
is a stationary transition controlled matrix. 
The cost function of each player, depending on the states and actions of all the 
other players, is given by the values.Wl
i1,k1;...;in ,kn
, so that the “average cost function” 
.Vl in the stationary regime can be expressed as 
.Vl (d) :=
N
∑1,M1
i1,k1
..
N
∑n ,Mn
in ,kn
Wl
i1,k1,..,in ,kn
∏n
l=1
dl
kl|il
Pl (
sl
=s(il)
)
,9.3 Preliminaries 195
where 
. Wl
i1,k1,..,in ,kn = ∑
N1
j1
..∑
Nn
jn
Vl
i1,j1,k1,..,in ,jn ,kN
∏n
l=1
πl
jl|il kl
.
Given that 
. cl
il kl = dl
kl|il
Pl (
sl
=s(il)
)
,
we have 
.Vl (c) :=
N
∑1,M1
i1,k1
..
N
∑n ,Mn
in ,kn
Wl
i1,k1,..,in ,kn
∏n
l=1
cl
il kl
, (9.3.3) 
.c = (c1, ..., cn). 
The variable.cl
il kl satisfies the following restrictions [ 11, 33]: 
1. Each vector from the matrix.cl := [
cl
il kl
]
il=1,Nl;kl=1,Ml
that represents a stationary 
mixed-strategy that belongs to the simplex 
.S Nl×Ml :=
⎧
⎪⎪⎨
⎪⎪⎩
cl
il kl ∈ RNl×Ml for cl
il kl
≥0,
where
N
∑l,Ml
il kl
cl
il kl
=1.
(9.3.4) 
2. The variable.cl
il kl satisfies the continuous time and the ergodicity constraints, and 
belongs to the convex, closed and bounded set defined as follows: 
.cl ∈ Cl
adm =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
hl
jl
(cl
) = N
∑l,Ml
il kl
πl
jl|il kl
cl
il kl − ∑
Ml
kl
cl
jl,kl = 0,
N
∑l,Ml
il kl
ql
jl|il kl
cl
il kl = 0.
(9.3.5) 
Notice that by (9.3.5) it follows that 
.
Pl (
sl
=s(il)
)
= ∑
Ml
kl
cl
il kl
, dl
kl|il = cl
il kl
∑
Ml
kl
cl
il kl
. (9.3.6) 
In the ergodic case.
∑Ml
kl cl
il kl > 0 for all.l = 1, n.196 9 Bargaining Games or How to Negotiate
9.4 The Nash Bargaining Model 
The Nash bargaining solution is based on a model in which the players are assumed 
to negotiate on which point of the set of feasible payoffs .Φ ⊂ Rn will be agreed 
upon and realized by concerted actions of the members of the coalition.l = 1, ..., n. 
A pivotal element of the model is a fixed disagreement vector.d ∈ Rn which plays the 
role of a deterrent. If negotiations break down and no agreement is reached, then the 
disagreement point will take effect. The players are committed to the disagreement 
point in the case of failing to reach a consensus on which feasible payoff to realize. 
Thus the whole bargaining problem. B will be concisely given by the pair.B = (Φ, d). 
We will call this form the condensed form of the bargaining problem (see [ 17, 27]). 
A bargaining problem can be derived from the normal form of an.n-person game 
.G = C1, ...,Cn; u1, ..., un in a natural way. The set of all feasible payoffs (outcomes) 
is defined as 
. Θ = u : u = (
u1
(c), ..., un(c)
)
, c ∈ C
where.C = C1 × ... × Cn. 
Given a disagreement vector .d ∈ Rn, .B = (Θ, d) it is a bargaining problem in 
condensed form. We can derive another bargaining problem.B = (Φ, d) from. G by 
extending the set of feasible outcomes.Θ to its convex hull. Φ. Notice that any element 
.ϕ ∈ Φ can be represented as 
. ϕ = ∑n
l=1
λl
ul
.
where.u = (u1(c), ..., un(c)), (c ∈ C),.λl ≥ 0 for all. l, and.
∑n
l=1 λl = 1. 
The payoff. ϕ can be realized by playing the strategies. c with probability. λ, and so 
. ϕ is the expected payoff of the players. Thus, when the players face the bargaining 
problem. B the question is, which point of.Φ should be selected, taking into account 
the different position and strength of the players that is reflected in the set .Φ of 
extended payoffs and the disagreement point. d. 
Nash approached this problem by assigning a one-point solution to .B in an 
axiomatic manner. Let. B denote the set of all pairs.(Φ, d) such that 
1. .Φ ⊂ Rn is compact, convex; 
2. there exists at least one.u ∈ Φ such that.u > d. 
A Nash solution to the bargaining problem is a function . f : B → Rn such that 
. f (Φ, d) ∈ Φ. We shall confine ourselves to functions satisfying the following axioms 
and we still call there functions solution (see [ 17, 19, 25, 27]). 
1. Feasibility:. f (Φ, d) ∈ Φ. 
2. Rationality:. f (Φ, d) ≥ d.9.4 The Nash Bargaining Model 197
3. Pareto Optimality: For every .(Φ, d) ∈ B there is .u ∈ Φ such that . u ≥ f (Φ, d)
and imply.u = f (Φ, d). 
4. Symmetry: If for a bargaining problem .(Φ, d) ∈ B, there exist indices .i, j such 
that .ϕ = (ϕ1, ..., ϕn) ∈ Φ if and only if .ϕ¯ = (ϕ¯1, ..., ϕ¯n) ∈ Φ, . (ϕ¯l = ϕl
,l /=
i,l /= j, ϕ¯i = ϕ j
, ϕ¯ j = ϕi
) and .di = d j for .d = (d1, ..., dn), then . f i = f j for 
the solution vector. f (Φ, d) = ( f 1, ..., f n). 
5. Invariance with respect to affine transformations of utility: Let . αl > 0, βl
, (l =
1, ..., n) be arbitrary constants and let 
. d' = (α1
d1 + β1
, ..., αndn + βn) with d = (d1
, ..., dn)
and 
. Φ' = (α1
ϕ1 + β1
, ..., αnϕn + βn) : (ϕ1
, ..., ϕn) ∈ Φ.
Then. f (Φ'
, d'
) = (α1d1 + β1, ..., αn f n + βn), where. f (Φ, d) = ( f 1, ..., f n). 
6. Independence of irrelevant alternatives: If.(Φ, d) and.(T, d) are bargaining pairs 
such that.Φ ⊂ T and. f (T, d) ∈ Φ, then. f (T, d) = f (Φ, d). 
Theorem 9.2 There is a unique function. f satisfying axioms 1-6, furthermore for all 
.(Φ, d) ∈ B, the vector. f (Φ, d) = ( f 1, ..., f n) = (u1, ..., un) is the unique solution 
of the optimization problem 
.
maximize g(u) = ∏n
l=1
(ul − dl
)
subject to u ∈ Φ, u ≥ d.
(9.4.1) 
The objective function of problem (9.4.1) is usually called the Nash product. 
Proof See [ 17]. ∎
Remark 9.1 There are exactly two solutions satisfying axioms 1, 2, 4, 5, and 6. One 
is the Nash’s solution and the other is the disagreement solution. 
For the next conjectures consider a bargaining problem as a pair .(Φ, d) where 
.Φ ⊂ R2 and.d ∈ R2. 
Conjecture 9.4.1 The Pareto frontier .Ωe of the set .Φ is the graph of a concave 
function, denoted by . h, whose domain is a closed interval .B ⊆ R. Furthermore, 
there exists. f 1 ∈ B such that.u1 > d1 and.h(u1) > d2 [ 25]. 
Conjecture 9.4.2 The set.Ωw of weakly Pareto efficient utility pairs is closed [ 25].198 9 Bargaining Games or How to Negotiate
9.5 The Kalai-Smorodinsky Bargaining Model 
With the property of independence of irrelevant alternatives, Nash’s solution is not 
sensitive to the range of outcomes contained in the feasible set, for instance, by the 
utopia point.a(Φ) = (a1(Φ), ..., an(Φ)) defined by 
. al
(Φ) = max{ul
|ul ∈ Φ , u ≥ d}
this point is the highest possible utility payoff player . l can attain in the bargaining 
problem .(Φ, d). Raiffa [ 34] proposed a solution for two-player games which is 
sensitive to changes in.a(Φ), he proposed the solution. u for two-player games such 
that.u = f (Φ, d) is the Pareto-optimal point at which. (u1 − d1)/(a1 − d1) = (u2 −
d2)/(a2 − d2). The solution . u selects the maximal point on the line joining . d to . a, 
yielding each player the largest reward consistent with the constraint that the players’ 
actual gains should be in proportion to their maximum gains, as measured by the 
ideal point.a(Φ). 
The Kalai-Smorodinsky solution of the bargaining problem amounts to normal￾izing the utility function of each agent in such a way that it is worth zero at the 
status-quo and one at this agent’s best outcome, given that all others get al least 
their status quo utility level; and to sharing equally the benefit from cooperation. 
This solution has been axiomatically characterized when society. n contains only two 
agents, i.e.,.l = 1, 2. To every two-person game we associate a pair.(Φ, d), where. d
is a point in the plane .d = (d1, d2) is the status quo and .Φ is a subset of the plane, 
every point.u = (u1, u2) ∈ Φ represents levels of utility for players. 1 and. 2 that can 
be reached by an outcome of the game which is feasible for the two players when 
they do cooperate. 
Let. B denote the set of all pairs.(Φ, d) such that 
1. .Φ ⊂ R2 is compact, convex; 
2. there exists at least one point.u ∈ Φ such that.ul > dl
, for.l = 1, 2. 
A solution to the bargaining problem is a function. f : B → R2 such that. f (Φ, d) ∈
Φ. We shall confine ourselves to functions satisfying the following axioms and we 
still call there functions solution (see [ 19]). 
1. Pareto Optimality: For every.(Φ, d) ∈ B there is no.u ∈ Φ such that. u ≥ f (Φ, d)
and imply.u /= f (Φ, d). 
2. Symmetry: We let . f : R2 → R2 be defined by .T ((u1, u2)) = (u2, u1) and we 
require that for every.(Φ, d) ∈ B,. f (T (Φ), T (d)) = T ( f (Φ, d))
3. Invariance with respect to affine transformations of utility:. A is an affine transfor￾mation of utility if .A = (A1, A2) : R2 → R2, .A((u1, u2)) = (A1(u1), A2(u2)), 
and the maps .Al
(u) are of the form .cl
u + dl for some positive constant .cl and 
some constant. dl
. We require that for such a transformation. A,. f (A(Φ), A(d)) =
A( f (Φ, d)). 
4. Monotonicity: For a pair.(Φ, d) ∈ B ,let.a(Φ) = (a1(Φ), a2(Φ)) and.gΦ(u1) be 
a function defined for.u1 ≤ a1(Φ) in the following way9.5 The Kalai-Smorodinsky Bargaining Model 199
. gΦ(u1
) = u2 if (u1
, u2
) is the Pareto of (Φ, d)
= a2
(Φ) if there is no such u2
.
If.(Φ2, d) and.(Φ1, d) are bargaining pairs such that.a1(Φ1) = a1(φ2) and. gΦ1 ≤
gΦ2 , then. f 2(Φ1, d) ≤ f 2(Φ2, d), where. f (Φ, d) = ( f 1(Φ, d), f 2(Φ, d)). 
The axiom of monotonicity states that if, for every utility level that player. 1 may 
demand, the maximum feasible utility level that player. 2 can simultaneously reach is 
increased, then the utility level assigned to player. 2 according to the solution should 
also be increased. 
Theorem 9.3 Let . f be a bargaining solution. Then . f satisfies Pareto optimality, 
symmetry, invariance with respect to affine transformations of utility and monotonic￾ity if, and only if,. f is Kalai-Smorodinsky solution. (See the proof in Roth [34]). 
9.5.1 The .n-Person Kalai-Smorodinsky Solution 
Kalai and Smorodinsky [ 19] defined their solution only on two-player bargaining 
problems. We consider the set of all.n-player bargaining problems defined by Peters 
and Tijs [ 31], and on this set we define a class of asymmetric .n-person Kalai￾Smorodinsky solutions. The set of players is denoted by .l = (1, ..., n), with .n ≥ 2. 
A set.Φ ⊆ Rn is comprehensive if.x ∈ Φ and.x ≥ y imply.y ∈ Φ, for all.x, y ∈ Rn. 
A bargaining problem for. n is a pair.(Φ, d) where: 
1. .Φ ⊆ Rn is compact, convex, and comprehensive, 
2. there exists a.u ∈ Φ such that.u > d and.d ∈ Φ, 
We talk about comprehensiveness in the sense that any player can choose a lower 
utility payoff without this leading to an infeasible outcome. Players seek agreement 
on an outcome.u ∈ Φ, yielding utility.ul to player. l. In case no agreement is reached 
the disagreement outcome. d results. For all bargaining problem.(Φ, d) ∈ B we define 
the Pareto set of.Φ as 
. P(Φ) = {u ∈ Φ| for all x ∈ Rn, if x ≥ u and x /= u, then x ∈/ Φ}.
A bargaining solution is a map. f : B → Rn that assigns to each bargaining prob￾lem.(Φ, d) ∈ B a single point. f (Φ, d) ∈ Φ. 
Raiffa [ 34] and Kalai and Smorodinsky [ 19] defined and characterized the Kalai￾Smorodinsky solution for two-person bargaining problems. Roth [ 35] observed that 
the.n-player extension of the solution is not Pareto-optimal on all bargaining problems 
in. B, i.e., does not assign an element of.P(Φ) to each.(Φ, d) ∈ B. Therefore, Peters 
and Tijs [ 31] introduced a subclass of bargaining problems in .B for which this 
shortcoming does not occur.200 9 Bargaining Games or How to Negotiate
Theorem 9.4 For bargaining games with three or more players, no solution exists 
which possesses the properties of Pareto optimality, symmetry, and restricted mono￾tonicity. (See the proof in Roth [35]). 
Condition For all.u ∈ Φ,.u ≥ d,.l = (1, ..., n):.u ∈/ P(Φ) and. ul < al
(Φ) ⇒ ∃ ε >
0 with.u + εel ∈ Φ, where the vector. el in.Rn has the.l-th coordinate equal to. 1 and 
all other coordinates equal to. 0. 
If a feasible outcome . u is not Pareto optimal, then for any player . l who receives 
less than his utopia payoff it is possible to increase his utility while all other 
players still receive . u. Let .I ⊆ B consist of all bargaining problems satisfying 
Condition 9.5.1. The class of bargaining problems.(Φ, 0) ∈ I is denoted by. I0. 
Peters and Tijs [ 31] defined the.n-player extension of the solution by making use 
of monotonic curves. A monotonic curve for. n is a map 
. ψ : [1, n] → (
u ∈ Rn
+|ul ≤ 1 for all player l, and 1 ≤ ∑n
l=1
ul
)
such that for all .1 ≤ s ≤ t ≤ n we have .ψ(s) ≤ ψ(t) and .
∑n
l=1 ψl
(s) = s. The set 
of all monotonic curves for. n is denoted by. ψ. 
Lemma 9.1 For each.ψ ∈ ψ and.(Φ, 0) ∈ I0 with. f (Φ, 0) = en, the set 
. P(Φ) ∩ {ψ(t)|t ∈ [1, n]}
contains exactly one point (see [31]). 
Let .ψ be some monotonic curve in . ψ. Following the Lemma 9.1 we can define 
.ρψ : I → Rn, the solution associated with. ψ. Let.(Φ, 0) ∈ I0; if.a(Φ, 0) = en, then 
. {
ρψ (Φ, 0)
}
:= P(Φ) ∩ {ψ(t)|t ∈ [1, n]}
and if.a(Φ, 0) = a, then.ρψ (Φ, 0) := aρψ (a−1Φ). For.(Φ, d) ∈ I, we define 
. ρψ (Φ, d) = d + ρψ (Φaˆ' − d).
The class of all solutions associated with a monotonic curve in .ψ is referred to as 
the class of individually monotonic bargaining solutions, the Kalai–Smorodinsky 
solution is an element of this class. Observe that .ψˆ defines a straight line in .Rn, 
which for bargaining games.(Φ, 0) ∈ I0 with.a(Φ, 0) = en, coincides with the line 
connecting the disagreement point. 0 and the utopia point. en. For general bargaining 
problems.(Φ, d) ∈ I, the solution is the intersection of the Pareto set.P(Φ) and the 
straight line that connects the disagreement point. d and the utopia point.a(Φ, d).9.6 The Bargaining Solver 201
9.6 The Bargaining Solver 
9.6.1 The Nash Bargaining Solver 
Stated in general terms, a.n-person bargaining situation is a situation in which. n play￾ers have a common interest to cooperate, but have conflicting interests over exactly 
how to cooperate. This process involves the players making offers and counteroffers 
to each other. 
Consider a .n-person bargaining problem. Let us denote the disagreement utility 
that depends on the strategies .cl
(il,kl) as .dl
(c1, ..., cn) for each player .(l = 1, ..., n), 
and the solution for the Nash bargaining problem as the point.(u1, ..., un). Following 
(9.3.3) the utilities. ul
, in the same way that the disagreement utilities, are for Markov 
chains as follows 
.ul = ul (
c1
, ..., cn)
:=
N
∑1,M1
i1,k1
...
N
∑n ,Mn
in ,kn
Wl
i1,k1,...,in ,kn
∏n
l=1
cl
il kl
, (9.6.1) 
where the matrices.Wl represent the behavior of each player. This point is better than 
the disagreement point, therefore must satisfy that.ul > dl
. 
The function for finding the solution to the Nash bargaining problem is 
. g(c1
, ..., cn) = ∏n
l=1
(ul − dl
)
αl
χ (ul
>dl
)
, (9.6.2) 
where.αl ≥ 0 and.
∑n
l αl = 1,.(l = 1, .., n), which are weighting parameters for each 
player. We can rewrite (9.6.2) for purposes of implementation as follows 
. g˜(c1
, ..., cn) = ∑n
l=1
αl
χ (ul > dl
)ln(ul − dl
). (9.6.3) 
Thus, the strategy . x∗, which is the vector .x∗ = (c1, ..., cn) ∈ Xadm := ʘn
l=1 Cl
adm, 
is the solution for the Nash bargaining problem 
. x∗∈ Arg max
x∈Xadm
{
g˜(c1
, .., cn)
}
,
the strategies .cl satisfy the restrictions (9.3.4) and (9.3.5). Applying the Lagrange 
principle, (see, for example, [ 32, 33]) let us introduce the Lagrange function202 9 Bargaining Games or How to Negotiate
. 
L(x, μ, ξ, η) = ˜g(c1, ..., cn) − ∑n
l=1
∑
Nl
jl=1
μl
(jl)hl
(jl)(cl
)−
∑n
l=1
∑
Nl
il=1
∑
Nl
jl=1
∑
Ml
kl=1
ξl
(jl)ql
jl|il kl
cl
il kl − ∑n
l=1
∑
Nl
il=1
∑
Ml
kl=1
ηl (
cl
il kl − 1
)
.
The approximative solution obtained by the Tikhonov’s regularization with . δ > 0
(see [ 33]) is given by 
. x∗, μ∗, ξ ∗, η∗ = arg max x∈Xadm
min
μ,ξ,η≥0
Lδ (x, μ, ξ , η),
where 
.
Lδ (x, μ, ξ, η) = ˜g(c1, ..., cn) − ∑n
l=1
∑
Nl
jl=1
μl
(jl)hl
(jl)(cl
)−
∑n
l=1
∑
Nl
il=1
∑
Nl
jl=1
∑
Ml
kl=1
ξl
(jl)ql
jl|il kl
cl
il kl − ∑n
l=1
∑
Nl
il=1
∑
Ml
kl=1
ηl (
cl
il kl − 1
)
−
δ
2
(
|x|2 − |μ|2 − |ξ|2 − |η|2)
.
(9.6.4) 
Notice that the Lagrange function (9.6.4) satisfies the saddle-point [ 32] condition, 
namely, for all.x ∈ Xadm and.μ, ξ, η≥ 0 we have 
. Lδ (xδ , μ∗
δ , ξ ∗
δ , η∗
δ ) ≤ Lδ (x∗
δ , μ∗
δ , ξ ∗
δ , η∗
δ ) ≤ Lδ (x∗
δ , μδ , ξδ , ηδ ).
9.6.2 Kalai-Smorodinsky Solver 
Consider a.n-person bargaining problem. Let us denote the disagreement utility that 
depends on the strategies.cl
il kl as.dl
(c1, ..., cn) for each player.(l = 1, ..., n), and the 
solution for the bargaining problem as the point .(u1, ..., un). Following (9.3.3) the 
utilities.ul are for Markov chains as follows 
.ul = ul (
c1
, ..., cn)
:=
N
∑1,M1
i1,k1
...
N
∑n ,Mn
in ,kn
Wl
(i1,k1,...,in ,kn )
∏n
l=1
cl
il kl
, (9.6.5) 
where the matrices.Wl represent the behavior of each player. 
The Kalai-Smorodinsky solution chooses the maximum individually rational pay￾off profile at which each player’s payoff has the same proportion from disagreement 
point to the utopia point. For solving the bargaining problem we consider that there 
exists an optimal solution that is a strong Pareto optimal point and it is the closest9.6 The Bargaining Solver 203
solution to the utopia point. To find the Pareto optimal solution, we formulate the 
problem as the.L p-norm that reduces the distance to the utopian point in the Euclidian 
space. Following [ 41], the function for finding the solution to the bargaining problem 
is 
. g(c1
, ..., cn) =
[
∑n
l=1
|
|
|
|
|
λl (ul − dl
)αl
χ (ul
>dl
)
(al − dl)αlχ (al>dl)
|
|
|
|
|
p]1/p
, (9.6.6) 
where .al is the utopia point, .αl ≥ 0 are weighting parameters for each player, and 
.λ ∈ Δn such that 
. Δn := (
λ ∈ Rn : λ ∈ [0, 1] , ∑n
l=1
λl = 1
)
.
We can rewrite (9.6.6) for purposes of implementation as follows 
. g˜(c1, ..., cn) =
⎡
⎣
∑n
l=1
|
|
|
λl
(
αl
χ (ul > dl
)ln(ul − dl
) − αl
χ (al > dl
)ln(al − dl
)
)|
|
|
p
⎤
⎦
1/p
.
Thus, the strategy . x∗, which is the vector .x∗ = (c1, ..., cn) ∈ Xadm := ʘn
l=1 Cl
adm, 
is the solution for the bargaining problem 
. x∗∈ Arg max x∈Xadm ,λ∈Δn
{
g˜(c1
, ..., cn)
}
,
the strategies .cl satisfy the restrictions (9.3.4) and (9.3.5). Applying the Lagrange 
principle let us introduce the Lagrage function 
. 
L(x, λ, μ, ξ, η) = ˜g(c1, ..., cn) − ∑n
l=1
∑
Nl
jl=1
μl
(jl)hl
(jl)(cl
)−
∑n
l=1
∑
Nl
il=1
∑
Nl
jl=1
∑
Ml
kl=1
ξl
(jl)ql
jl|il kl
cl
il kl − ∑n
l=1
∑
Nl
il=1
∑
Ml
kl=1
ηl (
cl
il kl − 1
)
,
The approximative solution obtained by the Tikhonov’s regularization with.δ > 0 is 
given by 
.x∗, λ∗, μ∗, ξ ∗, η∗ = arg max x∈Xadm ,λ∈Δn min
μ,ξ,η≥0
Lδ (x, λ, μ, ξ, η)204 9 Bargaining Games or How to Negotiate
where 
.
Lδ (x, λ, μ, ξ, η) = ˜g(c1, ..., cn) − ∑n
l=1
∑
Nl
jl=1
μl
(jl)hl
(jl)(cl
)−
∑n
l=1
∑
Nl
il=1
∑
Nl
jl=1
∑
Ml
kl=1
ξl
(jl)ql
jl|il kl
cl
il kl − ∑n
l=1
∑
Nl
il=1
∑
Ml
kl=1
ηl (
cl
il kl − 1
)
−
δ
2
(
|x|2 + |λ|2 − |μ|2 − |ξ|2 − |η|2)
.
(9.6.7) 
Notice that the Lagrange function (9.6.7) satisfies the saddle-point condition, 
namely, for all.x ∈ Xadm,.λ ∈ Δn and.μ, ξ, η≥ 0 we have 
. Lδ (xδ , λδ , μ∗
δ , ξ ∗
δ , η∗
δ ) ≤ Lδ (x∗
δ , λ∗
δ , μ∗
δ , ξ ∗
δ , η∗
δ ) ≤ Lδ (x∗
δ , λ∗
δ , μδ , ξδ , ηδ ).
9.6.3 The Extraproximal Solver Method 
In the proximal format (see, [ 5]) the relation (9.6.4) can be expressed as 
.
μ∗
δ = arg min μ≥0
{ 1
2 |μ − μ∗
δ|2 + γLδ (x∗
δ , μ, ξ ∗
δ , η∗
δ )
}
,
ξ ∗
δ = arg min ξ≥0
{ 1
2 |ξ − ξ ∗
δ |2 + γLδ (x∗
δ , μ∗
δ ,ξ,η∗
δ )
}
,
η∗
δ = arg min η≥0
{ 1
2 |η − η∗
δ |2 + γLδ (x∗
δ , μ∗
δ , ξ ∗
δ , η)}
,
x∗
δ = arg max x∈X
{
−1
2 |x − x∗
δ |2 + γLδ (x, μ∗
δ , ξ ∗
δ , η∗
δ )
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(9.6.8) 
For the relation (9.6.7) the proximal format will be extended with. Lδ (x, λ, μ, ξ, η)
and the following equation 
. λ∗
δ = arg max λ∈Δn
{
−1
2 |λ − λ∗
δ|2 + γLδ (x∗
δ , λ,μ∗
δ , ξ ∗
δ , η∗
δ )
}
,
where the solutions. x∗
δ ,. λ∗
δ ,.μ∗
δ ,.ξ ∗
δ and.η∗
δ depend on the parameters.δ > 0 and.γ > 0. 
The Extraproximal Method for the conditional optimization problems was sug￾gested in [ 5, 40]. We design the method for the static bargaining game in a general for￾mat with some fixed admissible initial values (.x0 ∈ X,.λ0 ∈ Δn and.μ0, ξ0, η0 ≥ 0), 
considering that we want to maximize the function as follows:9.6 The Bargaining Solver 205
1. The first half-step (prediction): 
.
μ¯ n = arg max μ≥0
{
−1
2 |μ − μn|2 − γLδ (xn, μ, ξn, ηn)
}
,
ξ¯
n = arg max ξ≥0
{
−1
2 |ξ − ξn|2 − γLδ (xn,μ¯ n,ξ,ηn)
}
,
η¯n = arg max η≥0
{
−1
2 |η − ηn|2 − γLδ (xn,μ¯ n, ξ¯
n, η)}
,
x¯n = arg max x∈X
{
−1
2 |x − xn|2 + γLδ (x,μ¯ n, ξ¯
n, η¯n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(9.6.9) 
2. The second half-step (basic) 
.
μn+1 = arg max μ≥0
{
−1
2 |μ − μn|2 − γLδ (x¯n, μ, ξ¯
n, η¯n)
}
,
ξn+1 = arg max ξ≥0
{
−1
2 |ξ − ξn|2 − γLδ (x¯n,μ¯ n,ξ, η¯n)
}
,
ηn+1 = arg max η≥0
{
−1
2 |η − ηn|2 − γLδ (x¯n,μ¯ n, ξ¯
n, η)}
,
xn+1 = arg max x∈X
{
−1
2 |x − xn|2 + γLδ (x,μ¯ n, ξ¯
n, η¯n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎭
(9.6.10) 
For the Kalai-Smorodinsky solution the presented extraproximal method will be 
extended employing the relation (9.6.7) and the following equations: 
1. The first half-step (prediction): 
. λ¯ n = arg max λ∈Δn
{
−1
2 |λ − λn|2 + γLδ (xn, λ,μ¯ n, ξ¯
n, η¯n)
}
.
2. The second half-step (basic) 
. λn+1 = arg max λ∈Δn
{
−1
2 |λ − λn|2 + γLδ (x¯n, λ,μ¯ n, ξ¯
n, η¯n)
}
.
The following theorem presents the convergence conditions of (9.6.9) and (9.6.10) 
and gives the estimate of its rate of convergence for the bargaining equilibrium. As 
well, we prove that the extraproximal method converges to a unique equilibrium 
point. Let us define the following extended vectors 
. x˜ =
( x
λ
)
∈ X˜ := X × R+, μ˜ =
⎛
⎝
μ
ξ
η
⎞
⎠ ∈ R+ × R+ × R+.
Then, the regularized Lagrange function can be expressed as 
.L˜
δ (x˜,μ)˜ := Lδ (x, λ, μ, ξ , η).206 9 Bargaining Games or How to Negotiate
The equilibrium point that satisfies (9.6.8) can be expressed as 
. 
μ˜ ∗
δ = arg min μ˜ ≥0
{
1
2 | ˜μ − ˜μ∗
δ|2 + γL˜
δ (x˜∗
δ ,μ)˜
}
,
x˜∗
δ = arg max x˜∈X˜
{
−1
2 | ˜x − ˜x∗
δ |2 + γL˜
δ (x˜,μ˜ ∗
δ )
}
.
Now, let us introduce the following variables 
. y˜ =
( y˜1
y˜2
)
∈ X˜ × R+, z˜ =
( z˜1
z˜2
)
∈ X˜ × R+.
and let define the Lagrange function in term of. y˜ and. z˜
. Lδ (y˜,z˜) := Lδ (y˜1,z˜2) − Lδ (z˜1, y˜2).
For.y˜1 .=.x˜,.y˜2 .= ˜μ,.z˜1 .=.z˜∗
1 .=.x˜∗
δ and.z˜2 .=.z˜∗
2 .=.μ˜ ∗
δ we have 
. Lδ (y˜,z˜
∗) := L˜
δ (x˜,μ˜ ∗
δ ) − L˜
δ (x˜∗
δ ,μ). ˜
In these variables the relation (9.6.8) can be represented by 
.z˜
∗= arg max
y˜∈X˜ ×R+
{
−1
2 | ˜y − ˜z
∗
|2
+γ Lδ (y˜,z˜
∗
)
}
. (9.6.11) 
Finally, we have that the extraproximal method can be expressed by 
1. First step 
.yˆn= arg max z˜∈X˜ ×R+
{
−1
2 |˜z − ˜yn|2
+γ Lδ (z˜, y˜n)
}
. (9.6.12) 
2. Second step 
.y˜n+1= arg max z˜∈X˜ ×R+
{
−1
2 |˜z − ˜yn|2
+γ Lδ (z˜, yˆn)
}
. (9.6.13) 
Lemma 9.2 Let.L˜
δ (x˜,μ)˜ be differentiable in. x˜ and. μ˜, whose partial derivative with 
respect to. μ satisfies the Lipschitz condition with positive constant.K0. Then, 
. |˜zn+1 − ˆzn| ≤ γ K0|˜zn − ˆzn|.
Proof See [ 39, 41]. ∎9.6 The Bargaining Solver 207
Lemma 9.3 Let us consider the set of regularized solutions of a non-empty game. 
The behavior of the regularized function is described by the following inequality: 
. Lδ (y˜, y˜) − Lδ (z˜
∗
δ , y˜) ≥ δ| ˜y − ˜z∗
δ|
for all.y˜ ∈
{
y˜ | ˜y ∈X × R+}
and.δ > 0. 
Proof See [ 41]. ∎
Theorem 9.5 (Convergence and rate of convergence). Let.L˜
δ (x˜,μ)˜ be differentiable 
in. x˜ and. μ˜, whose partial derivative with respect to. μ˜ satisfies the Lipschitz condition 
with positive constant. K. Then, for any.δ > 0 there exists a small-enough 
. γ0 = γ0(δ) < K:= min (
1
√2K0
,
1 + √
1 + 2 (K0)
2
2 (K0)
2
)
such that, for any .0 < γ ≤ γ0, sequence .{z˜n}, which generated by the equivalent 
extraproximal procedure (9.6.9) and (9.6.10), monotonically converges with expo￾nential rate. r .∈.(0, 1) to a unique equilibrium point. z˜∗, i.e., 
. |˜zn−˜z∗|2
≤ en ln r
|˜z0−˜z∗|2
,
where 
. r = 1+ 4(δγ )2
1+2δγ −2γ 2K2 −2δγ < 1,
and.rmin is given by 
. rmin= 1− 2δγ
1+2δγ = 1
1+2δγ .
Proof Following Theorem.18 in [ 41] we obtain that 
. r = 1 − 2γ δ + (2γ δ)2
1+2γ δ−2γ 2K2 < 1.
Iterating over the previous inequality we have 
.|˜z
∗
δ
−˜zn+1|2
≤r|˜z∗
δ
− ˜zn|2 ≤ ... ≤ en+1 ln r
|˜z∗
δ
− ˜z0|2
. (9.6.14) 
That implies that the series converge and also that the trajectories are bounded. Then, 
by (9.6.14) we have that 
. |˜z
∗
δ
−˜zn+1|2 →n→∞ 0.
Given that. z˜ is a bounded sequence, by the Weierstrass Theorem there exists a point 
.z˜' such that any subsequence .z˜ni satisfies that .z˜ni →ni→∞ z˜'
. In addition, we have 
that .
|
|z˜ni − ˜zni+1
|
|
2 → 0. Fixing, .n = ni in (9.6.11) and computing the limit when 
.ni → ∞ we have208 9 Bargaining Games or How to Negotiate
. z˜
' = arg min
y˜∈X˜ ×R+
{ 1
2 | ˜y − ˜z
'
|2
+γ Lδ (y˜,z˜
'
)
}
.
Then, we have that .z˜' = ˜z∗
δ , i.e., any limit point of the sequence .z˜n is a solution of 
the problem. Given that.
|
|z˜n − ˜z∗
δ
|
|
2 is monotonically decreasing then, there exists a 
unique limit point (equilibrium point). As a consequence, we have that the sequence 
.z˜n satisfies that.z˜n →n→∞ z˜∗
δ with a convergence velocity of.en ln r. 
See the complete proof in [ 41]. ∎
Remark 9.2 The exponential rate. r .∈.(0, 1) satisfies 
. r ≅ r0
(
1 + 1
N2
)
.
9.7 The Model for the Disagreement Point 
A pivotal element of the model is a fixed disagreement vector (sometimes also called 
as status quo or threat point). If negotiations break down and no agreement is reached, 
then inevitably the disagreement point will take effect. The player are committed to 
the disagreement point in the case of failing to reach a consensus on which feasible 
payoff to realize. 
Let us introduce the variables (see [ 40]) 
. x := col cl
, xˆ := col cˆl
, (l = 1, ..., n).
The strategies of the players are denoted by the vector . x, and . xˆ is a strategy of the 
rest of the players adjoint to. x. For reaching the goal of the game, players try to find 
a join strategy.x∗ = (c1, ..., cn) ∈ Xadm := ʘn
l=1 Cl
adm satisfying 
. g(x, xˆ) := ∑n
l=1
[
dl
(
cl
, c
ˆl
)
−
(
max
cl∈Cl
dl
(
cl
, c
ˆl
))] . (9.7.1) 
Here .dl
(
cl
, c
ˆl
)
(see 9.3.3) is the utility-function of the player . l which plays the 
strategy .cl ∈ Cl and the other plays the strategy .cˆl ∈ Cˆl
. If we consider the utopia 
point 
.c¯
l := arg max
cl∈Cl
dl
(
cl
, c
ˆl
)
,9.7 The Model for the Disagreement Point 209
then, we can rewrite (9.7.1) as follows 
.g(x, xˆ) := ∑n
l=1
[
dl
(
cl
, c
ˆl
)
− dl
(
c¯
l
, c
ˆl
)] . (9.7.2) 
The functions .dl
(
cl
, cˆl
)
.(l = 1, ..., n) are assumed to be concave in all their argu￾ments. 
Condition The function.g(x, xˆ) satisfies the Nash condition 
. dl
(
cl
, c
ˆl
)
− dl
(
c¯
l
, c
ˆl
)
≤ 0,
for any.cl ∈ Cl and all. l = 1, ..., n
Definition 9.2 A strategy.x∗ is said to be a Nash equilibrium if 
. x∗∈ arg max
x∈Xadm
{
g(x, xˆ)
}
.
Applying the regularized Lagrange principle we have the solution for the Nash 
equilibrium 
. x∗, xˆ∗, μ∗, ξ ∗, η∗ = arg max x∈X,xˆ∈Xˆ
min
μ,ξ,η≥0
Lθ ,δ (x, xˆ, μ, ξ , η),
where 
.
Lθ ,δ (x, xˆ, μ, ξ, η) := (1 − θ )g(x, xˆ) − ∑n
l=1
∑
Nl
jl=1
μl
(jl)hl
(jl)(cl
)−
∑n
l=1
∑
Nl
il=1
∑
Nl
jl=1
∑
Ml
kl=1
ξl
(jl)ql
jl|il kl
cl
il kl − ∑n
l=1
∑
Nl
il=1
∑
Ml
kl=1
ηl (
cl
il kl − 1
)
−
δ
2
(
|x|2 + |
|xˆ
|
|
2 − |μ|2 − |ξ|2 − |η|2
)
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(9.7.3) 
Notice also that the Lagrange function (9.7.3) satisfies the saddle-point condition, 
namely, for all.x ∈ X,.xˆ ∈ Xˆ , and.μ, ξ, η≥ 0 we have 
.Lθ ,δ (xδ , xˆδ , μ∗
δ , ξ ∗
δ , η∗
δ )≤Lθ ,δ (x∗
δ , xˆ∗
δ , μ∗
δ , ξ ∗
δ , η∗
δ )≤Lθ ,δ (x∗
δ , xˆ∗
δ , μδ , ξδ , ηδ ).210 9 Bargaining Games or How to Negotiate
9.7.1 The Extraproximal Solver Method 
In the proximal format the relation (9.7.3) can be expressed as 
. 
μ∗
δ = arg min μ≥0
{ 1
2 |μ − μ∗
δ|2 + γLθ ,δ (x∗
δ , xˆ∗
δ , μ, ξ ∗
δ , η∗
δ )
}
,
ξ ∗
δ = arg min ξ≥0
{ 1
2 |ξ − ξ ∗
δ |2 + γLθ ,δ (x∗
δ , xˆ∗
δ , μ∗
δ ,ξ,η∗
δ )
}
,
η∗
δ = arg min η≥0
{ 1
2 |η − η∗
δ |2 + γLθ ,δ (x∗
δ , xˆ∗
δ , μ∗
δ , ξ ∗
δ , η)}
,
x∗
δ = arg max x∈X
{
−1
2 |x − x∗
δ |2 + γLθ ,δ (x, xˆ∗
δ , μ∗
δ , ξ ∗
δ , η∗
δ )
}
,
xˆ∗
δ = arg max xˆ∈Xˆ
{
−1
2 | ˆx − ˆx∗
δ |2 + γLθ ,δ (x∗
δ , xˆ, μ∗
δ , ξ ∗
δ , η∗
δ )
}
,
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
where the solutions . x∗
δ , .xˆ∗
δ (u), .μ∗
δ , .ξ ∗
δ and .η∗
δ depend on the parameters .δ > 0 and 
.γ > 0. 
We design the method for the static Nash game in a general format with some 
fixed admissible initial values (.x0 ∈ X,.xˆ0 ∈ Xˆ , and.μ0, ξ0, η0 ≥ 0), considering that 
we want to maximize the function, as follows: 
1. The first half-step: 
.
μ¯ n = arg max μ≥0
{
−1
2 |μ − μn|2 − γLθ ,δ (xn, xˆn, μ, ξn, ηn)
}
,
ξ¯
n = arg max ξ≥0
{
−1
2 |ξ − ξn|2 − γLθ ,δ (xn, xˆn,μ¯ n,ξ,ηn)
}
,
η¯n = arg max η≥0
{
−1
2 |η − ηn|2 − γLθ ,δ (xn, xˆn,μ¯ n, ξ¯
n, η)}
,
x¯n = arg max x∈X
{
−1
2 |x − xn|2 + γLθ ,δ (x, xˆn,μ¯ n, ξ¯
n, η¯n)
}
,
xˆn = arg max xˆ∈Xˆ
{
−1
2 | ˆx − ˆxn|2 + γLθ ,δ (xn, xˆ,μ¯ n, ξ¯
n, η¯n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(9.7.4) 
2. The second half-step 
.
μn+1 = arg max μ≥0
{
−1
2 |μ − μn|2 − γLθ ,δ (x¯n, xˆn, μ, ξ¯
n, η¯n)
}
,
ξn+1 = arg max ξ≥0
{
−1
2 |ξ − ξn|2 − γLθ ,δ (x¯n, xˆn,μ¯ n,ξ, η¯n)
}
,
ηn+1 = arg max η≥0
{
−1
2 |η − ηn|2 − γLθ ,δ (x¯n, xˆn,μ¯ n, ξ¯
n, η)}
,
xn+1 = arg max x∈X
{
−1
2 |x − xn|2 + γLθ ,δ (x, xˆn,μ¯ n, ξ¯
n, η¯n)
}
,
xˆn+1 = arg max xˆ∈Xˆ
{
−1
2 | ˆx − ˆxn|2 + γLθ ,δ (x¯n, xˆ,μ¯ n, ξ¯
n, η¯n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(9.7.5)9.8 Numerical Illustration 211
9.8 Numerical Illustration 
Consider a two-person bargaining problem in a class of continuous-time controllable 
Markov chains. Let us denote the disagreement cost that depends on the strategies 
.cl
(il,kl).(l = 1, 2) for players. 1 and. 2 as.d1(c1, c2) and.d2(c1, c2) respectively, and the 
solution for the bargaining problem as the point.(u1, u2). 
The process to solve the bargaining problem consists of two main steps, firstly to 
find the disagreement point we define it as the Nash equilibrium point of the problem 
[ 28]; while for the solution of the bargaining process we follow the models presented 
by Nash and Kalai-Smorodinsky. 
Let the states.N1 = N2 = 6, and the number of actions.M1 = M2 = 3. The indi￾vidual utility for each player are defined by 
. U1
(i,j|1)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
34 45 1 28 7 23
27 43 25 47 26 24
15 45 14 15 43 48
36 47 12 17 20 5
20 41 22 43 35 14
29 29 18 18 32 23
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, U2
(i,j|1)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
31 1 30 38 2 17
18 41 10 13 42 11
5 8 34 33 12 31
2 44 13 43 3 40
25 5 22 5 28 10
13 18 7 29 48 3
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
. U1
(i,j|2)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
30 44 14 47 25 31
44 24 45 37 11 30
24 25 12 20 32 22
22 25 44 50 12 33
38 12 36 33 27 22
24 5 44 45 37 1
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, U2
(i,j|2)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
15 15 43 9 18 14
13 13 2 36 32 30
25 25 15 42 18 22
39 23 45 2 11 5
18 41 27 38 40 2
29 5 7 18 17 25
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
. U1
(i,j|3)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
28 27 48 8 16 27
43 47 33 24 22 28
21 37 19 28 15 42
24 29 24 3 50 42
42 49 46 33 31 42
50 42 51 45 13 11
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, U2
(i,j|3)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
14 11 31 48 50 11
17 34 14 39 39 20
15 23 28 31 24 2
9 22 48 48 35 24
20 9 36 3 21 17
35 10 34 14 20 49
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
The transition rate matrices for each player are defined as follows212 9 Bargaining Games or How to Negotiate
. 
q1
(i,j|1)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−0.5371 0.0444 0.2305 0.0946 0.0705 0.0970
0.0208 −0.5381 0.0294 0.0665 0.0471 0.3743
0.1179 0.0965 −0.6554 0.0939 0.1042 0.2429
0.1871 0.0965 0.1622 −0.5826 0.0285 0.1083
0.0825 0.1871 0.0671 0.0431 −0.4624 0.0827
0.0831 0.1685 0.1221 0.3425 0.0432 −0.7593
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
q1
(i,j|2)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−1.6112 0.1333 0.6916 0.2839 0.2114 0.2911
0.0624 −1.6142 0.0881 0.1996 0.1412 1.1228
0.3538 0.2894 −1.9662 0.2817 0.3127 0.7287
0.5614 0.2894 0.4867 −1.7477 0.0855 0.3248
0.2474 0.5614 0.2012 0.1292 −1.3873 0.2482
0.2492 0.5055 0.3662 1.0275 0.1295 −2.2780
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
q1
(i,j|3)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−0.5371 0.0444 0.2305 0.0946 0.0705 0.0970
0.0208 −0.5381 0.0294 0.0665 0.0471 0.3743
0.1179 0.0965 −0.6554 0.0939 0.1042 0.2429
0.1871 0.0965 0.1622 −0.5826 0.0285 0.1083
0.0825 0.1871 0.0671 0.0431 −0.4624 0.0827
0.0831 0.1685 0.1221 0.3425 0.0432 −0.7593
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
.
q2
(i,j|1)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−0.8499 0.2201 0.3707 0.1271 0.0374 0.0947
0.3467 −0.6729 0.1271 0.0376 0.0970 0.0644
0.2831 0.0856 −0.6306 0.0706 0.0376 0.1537
0.0703 0.1577 0.1369 −0.8573 0.3673 0.1250
0.3727 0.0964 0.0944 0.1298 −0.8026 0.1092
0.1627 0.1095 0.1237 0.0754 0.4537 −0.9250
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
q2
(i,j|2)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−0.8499 0.2201 0.3707 0.1271 0.0374 0.0947
0.3467 −0.6729 0.1271 0.0376 0.0970 0.0644
0.2831 0.0856 −0.6306 0.0706 0.0376 0.1537
0.0703 0.1577 0.1369 −0.8573 0.3673 0.1250
0.3727 0.0964 0.0944 0.1298 −0.8026 0.1092
0.1627 0.1095 0.1237 0.0754 0.4537 −0.9250
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
,
q2
(i,j|3)=
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
−1.1332 0.2934 0.4942 0.1694 0.0498 0.1263
0.4623 −0.8972 0.1694 0.0502 0.1294 0.0859
0.3774 0.1141 −0.8408 0.0942 0.0501 0.2050
0.0938 0.2102 0.1825 −1.1431 0.4898 0.1667
0.4970 0.1286 0.1258 0.1730 −1.0701 0.1456
0.2169 0.1460 0.1650 0.1005 0.6049 −1.2334
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.9.8 Numerical Illustration 213
9.8.1 Computing the Disagreement Point 
Given. δ and. γ and applying the extraproximal method we obtain the convergence of 
the strategies for the disagreement point in terms of the variable.c1
(i1,k1) for the player 
. 1 (see Fig. 9.4) and the convergence of the strategies.c2
(i2,k2) for the player. 2 (see Fig. 
9.5). 
. c1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0517 0.0540 0.0523
0.0560 0.0605 0.0607
0.0542 0.0548 0.0514
0.0660 0.0672 0.0635
0.0332 0.0372 0.0385
0.0582 0.0679 0.0727
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, c2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0824 0.0766 0.0830
0.0669 0.0449 0.0584
0.0840 0.0736 0.0823
0.0407 0.0215 0.0325
0.0399 0.0564 0.0503
0.0371 0.0329 0.0366
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Following (9.3.6) the mixed strategies obtained for the players are as follows 
. d1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.3273 0.3416 0.3311
0.3160 0.3416 0.3424
0.3378 0.3416 0.3205
0.3354 0.3416 0.3230
0.3051 0.3416 0.3533
0.2926 0.3416 0.3658
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, d2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.3405 0.3166 0.3429
0.3933 0.2637 0.3429
0.3503 0.3068 0.3429
0.4295 0.2275 0.3429
0.2723 0.3847 0.3429
0.3484 0.3087 0.3429
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Fig. 9.4 Convergence of the 
strategies for player. 1 in the 
disagreement point214 9 Bargaining Games or How to Negotiate
Fig. 9.5 Convergence of the 
strategies for player. 2 in the 
disagreement point 
With the strategies calculated, the resulting utilities following (9.3.3), in the disagree￾ment point for each player.dl
(c1, c2), are as follows: 
. d1(c1, c2) = 905.6447, d2(c1, c2) = 704.2493.
9.8.2 Computing the Nash Bargaining Solution 
Given . δ, . γ , .αl and applying the extraproximal method for the Nash bargaining 
solution, we obtain the convergence of the strategies in terms of the variable . c1
(i1,k1)
for the player . 1 (see Fig. 9.6) and the convergence of the strategies .c2
(i2,k2) for the 
player. 2 (see Fig. 9.7). 
. c1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0281 0.0677 0.0623
0.0010 0.0758 0.1003
0.0907 0.0686 0.0010
0.1115 0.0842 0.0010
0.0010 0.0466 0.0613
0.0010 0.0851 0.1127
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, c2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1227 0.0350 0.0842
0.1100 0.0010 0.0592
0.1555 0.0010 0.0835
0.0607 0.0010 0.0329
0.0010 0.0946 0.0510
0.0663 0.0032 0.0371
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.9.8 Numerical Illustration 215
Fig. 9.6 Convergence of the 
strategies for player. 1 in the 
Nash solution 
Fig. 9.7 Convergence of the 
strategies for payer. 2 in the 
Nash solution 
The mixed strategies obtained for the players are as follows 
. d1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.1778 0.4280 0.3942
0.0056 0.4280 0.5663
0.5658 0.4280 0.0062
0.5669 0.4280 0.0051
0.0092 0.4280 0.5628
0.0050 0.4280 0.5670
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, d2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.5073 0.1447 0.3479
0.6462 0.0059 0.3479
0.6479 0.0042 0.3479
0.6415 0.0106 0.3479
0.0068 0.6453 0.3479
0.6221 0.0300 0.3479
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.216 9 Bargaining Games or How to Negotiate
With the strategies calculated, the resulting utilities in the Nash bargaining solution 
for each player, are as follows: 
. u1(c1, c2) = 958.0281, u2(c1, c2) = 813.2879.
9.8.3 Computing the Kalai-Smorodinsky Bargaining Solution 
Given . δ, . γ , .αl and applying the extraproximal method for the Kalai-Smorodinsky 
bargaining solution with the.L1-norm, we obtain the convergence of the strategies in 
terms of the variable.c1
(i1,k1) for the player. 1 (see Fig. 9.8) and the convergence of the 
strategies.c2
(i2,k2) for the player. 2 (see Fig. 9.9). 
. c1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0010 0.0432 0.1139
0.0010 0.0484 0.1278
0.1156 0.0438 0.0010
0.1420 0.0537 0.0010
0.0010 0.0297 0.0782
0.0010 0.0543 0.1435
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
. c2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.2061 0.0010 0.0349
0.1447 0.0010 0.0245
0.2044 0.0010 0.0346
0.0800 0.0010 0.0136
0.0010 0.1245 0.0211
0.0903 0.0010 0.0154
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
Fig. 9.8 Convergence of the 
strategies for player. 1 in the 
KS solution9.8 Numerical Illustration 217
Fig. 9.9 Convergence of the 
strategies for payer. 2 in the 
KS solution 
The mixed strategies obtained for the players are as follows 
. d1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.0063 0.2730 0.7207
0.0056 0.2730 0.7213
0.7207 0.2730 0.0062
0.7219 0.2730 0.0051
0.0092 0.2730 0.7178
0.0050 0.2730 0.7219
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, d2 =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣
0.8518 0.0041 0.1441
0.8500 0.0059 0.1441
0.8517 0.0042 0.1441
0.8454 0.0106 0.1441
0.0068 0.8491 0.1441
0.8465 0.0094 0.1441
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦
.
With the strategies calculated, the resulting utilities in the Kalai-Smorodinsky 
bargaining solution for each player are as follows: 
. u1(c1, c2) = 960.5554, u2(c1, c2) = 841.0831.
Figure 9.10 shows the straight line linking the utilities obtained at the disagreement 
point and those obtained at the utopia point. We can also observe that the Nash 
solution approaches this line while the Kalai-Smorodinsky solution is exactly on this 
line. The utilities on the utopia point for the bargaining problem are for each player 
as follows: 
. a1(c1, c2) = 964.3472, a2(c1, c2) = 849.8365.218 9 Bargaining Games or How to Negotiate
Fig. 9.10 The bargaining 
Solution 
References 
1. Abreu, D., Manea, M.: Markov equilibria in a model of bargaining in networks. Games Econ. 
Behav. 75(1), 1–16 (2012) 
2. Agastya, M.: Adaptive play in multiplayer bargaining situations. Games Econ. Behav. 64(3), 
411–426 (1997) 
3. Alexander, C.: The kalai-smorodinsky bargaining solution in wage negotiations. J. Oper. Res. 
Soc. 43(8), 779–786 (1992) 
4. Anant, T.C.A., Mukherji, B., Basu, K.: Bargaining without convexity: Generalizing the kalai￾smorodinsky solution. Econ. Lett. 33(2), 115–119 (1990) 
5. Antipin, A.S.: An extraproximal method for solving equilibrium programming problems and 
games. Comput. Math. Math. Phys. 45(11), 1893–1914 (2005) 
6. Bolt, W., Houba, H.: Strategic bargaining in the variable threat game. Econ. Theory 11(1), 
57–77 (1998) 
7. Cai, H.: Inefficient markov perfect equilibria in multilateral bargaining. Econ. Theory 22(3), 
583–606 (2003) 
8. Carrillo, L., Escobar, J., Clempner, J.B., Poznyak, A.S.: Solving optimization problems in 
chemical reactions using continuous-time markov chains. J. Math. Chem. 54, 1233–1254 (2016) 
9. Clempner, J.B.: Shaping emotions in negotiation: a nash bargaining solution. Cognit. Comput. 
12, 720–735 (2020) 
10. Clempner, J.B.: Manipulation power in bargaining games using machiavellianism. Econ. Com￾put. Econ. Cybern. Stud. Res. 55(2), 299–313 (2021) 
11. Clempner, J.B., Poznyak, A.S.: Simple computing of the customer lifetime value: a fixed local￾optimal policy approach. J. Syst. Sci. Syst. Eng. 23(4), 439–459 (2014) 
12. Clempner, J.B., Poznyak, A.S.: Multiobjective markov chains optimization problem with strong 
pareto frontier: principles of decision making. Expert Syst. Appl. 68, 123–135 (2017) 
13. Coles, M.G., Muthoo, A.: Bargaining in a non-stationary environment. J. Econ. Theory 109(1), 
70–89 (2003) 
14. Cripps, M.W.: Markov bargaining games. J. Econ. Dyn. Control 22(3), 341–355 (1998) 
15. Driesen, B., Perea, A., Peters, H.: The kalai-smorodinsky bargaining solution with loss aversion. 
Math. Soc. Sci. 61(1), 58–64 (2011) 
16. Dubra, J.: An asymmetric kalai-smorodinsky solution. Econ. Lett. 73(2), 131–136 (2001)References 219
17. Forgó, F., Szép, J., Szidarovszky, F.: Introduction to the Theory of Games: Concepts, Methods, 
Applications. Kluwer Academic Publishers (1999) 
18. Kalai, E.: Social Goals and Social Organization, chap. Solutions to the Bargaining Problem, 
pp. 75–105. Cambridge University Press, Cambridge (1985) 
19. Kalai, E., Smorodinsky, M.: Other solutions to nash’s bargaining problem. Econometrica 43(3), 
513–518 (1975) 
20. Kalandrakis, A.: A three-player dynamic majoritarian bargaining game. J. Econ. Theory 116(2), 
294–322 (2004) 
21. Kennan, J.: Repeated bargaining with persistent private information. Rev. Econ. Stud. 68, 
719–755 (2001) 
22. Köbberling, V., Peters, H.: The effect of decision weights in bargaining problems. J. Econ. 
Theory 110(1), 154–175 (2003) 
23. Merlo, A., Wilson, C.: A stochastic model of sequential bargaining with complete information. 
Econometrica 63(2), 371–399 (1995) 
24. Moulin, H.: Implementing the kalai-smorodinsky bargaining solution. J. Econ. Theory 33(1), 
32–45 (1984) 
25. Muthoo, A.: Bargaining Theory with Applications. Cambridge University Press (2002) 
26. Naidu, S., Hwang, S., Bowles, S.: Evolutiogame bargaining with intentional idiosyncratic play. 
Econ. Lett. 109(1), 31–33 (2010) 
27. Nash, J.F.: The bargaining problem. Econometrica 18(2), 155–162 (1950) 
28. Nash, J.F.: Two person cooperative games. Econometrica 21, 128–140 (1953) 
29. von Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior. Princeton Uni￾versity Press (1944) 
30. Osborne, M., Rubinstein, A.: Bargaining and Markets. Academic Press, Inc. (1990) 
31. Peters, H., Tijs, S.: Individually monotonic bargaining solutions for n-person bargaining games. 
Methods Oper. Res. 51, 377–384 (1984) 
32. Poznyak, A.S.: Advance Mathematical Tools for Automatic Control Engineers. Stochastic 
Techniques, vol. 2. Elsevier, Amsterdam (2009) 
33. Poznyak, A.S., Najim, K., Gomez-Ramirez, E.: Self-learning Control of Finite Markov Chains. 
Marcel Dekker, New York (2000) 
34. Raiffa, H.: Arbitration schemes for generalized two-person games. Ann. Math. Stud. 28, 361– 
387 (1953) 
35. Roth, A.E.: An impossibility result converning n-person bargaining games. Int. J. Game Theory 
8(3), 129–132 (1979) 
36. Rubinstein, A., Wolinsky, A.: Equilibrium in a market with sequential bargaining. Econometrica 
53(5), 1133–1150 (1985) 
37. Trejo, K., Clempner, J., Poznyak, A.: Computing the bargaining approach for equalizing the 
ratios of maximal gains in continuous-time markov chains games. Comput. Econ. 54, 933–955 
(2019) 
38. Trejo, K., Clempner, J., Poznyak, A.: Computing the nash bargaining solution for multiple 
players in discrete-time markov chains games. Cybern. Syst. 51(1), 1–26 (2020) 
39. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the lp-strong nash equilibrium looking 
for cooperative stability in multiple agents markov games. In: 12th International Conference 
on Electrical Engineering, Computing Science and Automatic Control, pp. 309–314. Mexico 
City. Mexico (2015) 
40. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the stackelberg/nash equilibria using 
the extraproximal method: convergence analysis and implementation details for markov chains 
games. Int. J. Appl. Math. Comput. Sci. 25(2), 337–351 (2015) 
41. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the strong .l p-nash equilibrium for 
markov chains games: convergence and uniqueness. Appl. Math. Modell. 41, 399–418 (2017) 
42. Trejo, K.K., Juarez, R., Clempner, J., Poznyak, A.S.: Non-cooperative bargaining with unso￾phisticated agents. Comput. Econ. 61, 937–974 (2023)Chapter 10
Multi-traffic Signal-Control
Synchronization
Abstract In this chapter, we provide a brand-new paradigm for combining game
theory and the extraproximal approach to represent the multi-traffic signal-control
synchronization problem. The intersection’s goal is to reduce queuing time, and
finding the best signal timing strategy, or assigning a green period to each signal
phase, is a challenge for signal controllers. The game’s players are referred to as
signal controllers. Finding a green period at each junction seeks to reduce signal
and queuing delays. The issue poses two inherent limitations: (a) the number of
arriving and departing vehicles varies for each street of the intersection; and (b) for
every intersection, the period of time allowed for vehicles to reach the green light
is equal to the duration of the red light. A leader-follower Stackelberg game model
is determined by the first restriction: streets with higher traffic demand more green
time. The most recent constraint created a simultaneous game-solution as a better
evaluation of the actual circumstance. The study then demonstrates that the Nash
equilibrium provides the solution in order to take use of the game’s structure. To solve
the problem computationally and determine the best signal timing distribution, we
present the.c-variable technique. The extraproximal approach is a two-stage iterative
process. In the first phase, an approximate equilibrium point is calculated, and in
the second step, the previous step is adjusted. The game is described in terms of
linked nonlinear programming issues that use the Lagrange principle. To guarantee
the convergence of the cost-functions to a Nash equilibrium point, the Tikhonov
regularization approach is used. The extraproximal approach is also developed using
Markov chains. The three way intersection example serves as a good demonstration
of the method’s use. Our contributions have significant effects on the applications in
the real world.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable
Markov Chains, Studies in Systems, Decision and Control 504,
https://doi.org/10.1007/978-3-031-43575-1_10
221222 10 Multi-traffic Signal-Control Synchronization
10.1 Introduction
10.1.1 Brief Review
Because of the high expenses and the fact that the space that might be used for roads
already has a set structure and traffic flow, building new roads is no longer a practical
option (especially in older cities). They waste several hours stuck in traffic during
rush hour. Road traffic congestion and travel time are predicted to worsen as a result of
population growth. Because of this, efforts are concentrated on creating intelligent
technologies to reduce traffic congestion. Finding a point of balance between the
supply and demand for roads is a planning conundrum for contemporary cities with
the greatest levels of congestion, as noted by [7, 37].
The most significant aspect affecting the effectiveness of the road network is
the traffic-signal control configuration. In order to lessen traffic congestion and the
amount of time spent trapped in traffic, the challenge consists of creating appropriate
signal patterns by managing the timing of the green/red light cycles at a junction.
Many factors make this a highly difficult problem to solve:
1. The amount of traffic varies constantly depending on when most people commute,
2. An intersection affects other roads’ traffic flow (i.e., the convergence of several
urban blocks),
3. Schools, frequently visited restaurants, and fast food joints, where cars clog the
entrances, are traditional sources of traffic congestion,
4. Unusual congestion may come from an accident, construction, or extended holi￾day weekends, or inclement weather,
5. Other times of congestion may be brought on by different special events, such as
sporting events, festivals, or other religious ceremonies.
10.1.2 Related Work on Traffic Control
Several researchers have actively used various strategies to address the signal con￾trol setting problem in order to reduce delays and increase the intersection capacity.
Allsop [1, 2] looks at the connection between signal setup and traffic assignment
and suggests two programs to solve the problem of traffic equilibrium: 1) SIGSET
for figuring out how much traffic signal-controlled road junctions can handle, and 2)
SIGCAP for evaluating how much traffic certain junctions can handle. A computer
technique called mixed-integer traffic optimization, as described by Gartner et al.
in their citation [26], is intended to concurrently optimize all of the traffic control
variables for network offsets, splits, and cycle time. The signal setting is addressed
as a hybrid optimization issue by Tan et al. [43]. Smith [39, 40] defines equilib￾rium and stability in terms of link-flow and provides requirements that ensure the
existence, singularity, and stability of traffic equilibria. Moreover, Smith [41, 42]
suggests a traffic control technique that distributes traffic across the network’s spare10.1 Introduction 223
capacity while just requiring local data from vehicle detectors, such as traffic flows
and wait durations. As a one-level Cournot game and a Stackelberg game, Chen and
Ben-Akiva [11] provide a combined dynamic traffic assignment and dynamic traffic
control. According to Fisk [19], the traffic agency and network users play a Stackel￾berg game to determine the worldwide optimum signal setting problem. Alvarez et. al.
[3], and Moya and Poznyak [33] treat the signal setup as a noncooperative game issue
and a Stackelberg-Nash game theory method based on the extrapoximal approach,
respectively (but without intersection avoiding). A methodology is presented for the
integration of dynamic traffic assignment with real-time control by Gartner et al. [24,
25, 27]. The framework is extended to the dynamic case, which involves the incor￾poration of cutting-edge intelligent transportation systems, combining the model for
signal control and route choice in urban traffic networks. First, discuss the static
case, which involves the interaction between travelers (demand) and transportation
facilities. Using evolutionary algorithms, Lee and Machemehl [30] analyze the sig￾nal setup. In their study of the combined assignment-intersection control problem,
Cascetta et al. [8] demonstrate that, in the case of locally optimized control systems
(such as adaptive traffic-lights), it deviates from the more general equilibrium net￾work design problem by putting forth a stochastic user equilibrium (SUE) model
with asymmetric delay functions. Castillo-Gonzalez et al. [10] considered a math￾ematically rigorous study of the continuous-time, discrete state, multi-traffic signal
control problem using a non-cooperative game theory approach. Aragon-Gomez and
Clempner [5] introduced a reinforcement learning approach for solving the Traffic￾Signal Control problem for multiple intersections using Continuous-Time Markov
Games.
In addition, Cascetta et al. [9] propose models and algorithms for the optimiza￾tion of signal settings in urban networks, putting forward both a global approach
(optimization of intersection signal settings throughout the whole network) and a
local one (optimization of signal settings intersection by intersection). Models and
applications pertaining to the analysis of transportation networks are presented by
Cascetta [7]. Qu et al. [37] create a traffic control algorithm that takes traffic assign￾ment into account. Dafermos [17, 18], Fisk and Nguyen [20], Florian and Spiess
[21], Gartner et. al. [23], Meneguzzer [31, 32], Cantarella et. al. [6] and D’Acierno
et. al. [16] deal with the local optimization of signal settings problem as a fixed-point
problem, where look for an equilibrium traffic flows congruent with costs and signal
settings obtained in accordance to a local-optimal control policy. The local optimiza￾tion of signal settings problem, which arises when signal control parameters of an
urban road network are locally optimized and have to be consistent with equilibrium
traffic flows, is also studied by Gallo and D’Aciernob [22]. They compare various
solution algorithms put forth in the literature.
Sheffi and Powell [38], Heydecker and Khoo [29], Yang and Yagar [49], Hey￾decker [28], Wong and Yang [47], Wong [48], Chiou [12], Wey [46], Ziyou and Yifan
[50] and Cascetta [9], among others, approach the global optimization of signal set￾tings as a (non-linear) constrained optimization problem where signal settings act
as decision variables. Focusing on multi-objective traffic signal control, Khamis and
Gomaa (Khamis) compute a consistent traffic signal configuration at each junction224 10 Multi-traffic Signal-Control Synchronization
that maximizes many performance indices. In order to forecast the impacts of poten￾tial control measures in a short time frame, Placzek [34] presents a self-organizing
traffic light system for an urban road network.
In this chapter, we discuss a modeling approach based on game theory and the
extraproximal method [4] for the multi-traffic signal-control synchronization prob￾lem [10, 15, 33]. The paper’s main goal is to resolve scenarios with various degrees
of demand (traffic flows) and various numbers of signalized crossings. The issue
presents two natural limitations:
1. the number of entering and exiting vehicles varies for each street of the intersec￾tion; and
2. there is a traffic conflict between the two movements in the intersection area,
preventing them from moving at the same time and resulting in the opposite
traffic signal, where one movement has a green light and the other has a red light.
On the one hand, restriction (a) establishes a leader-follower Stackelberg game model,
where streets with higher traffic volumes need more time for the green light and are
therefore leaders. Contrarily, restriction (b) forces a simultaneous solution to the
game as a better account of actual situations, i.e., the time allotted for cars to reach
the green light is equal to the time allotted to reach the red light at a junction. The
Nash equilibrium then provides the Stackelberg game’s solution, which can be used
to exploit the suggested game’s structure [13, 14].
In order to solve the computationally challenging signal timing distribution prob￾lem, we provide the c-variable technique. The c-variable method has the significant
benefit of being easily deployed in practical circumstances. The capacity of the
c-variable approach to identify abnormal circumstances from data in the simplex is
another crucial component of its introduction. A straightforward test on the c-variable
can be used to identify a no possible solution. We use the extraproximal approach
to determine the game’s Nash equilibrium. We use coupled nonlinear programming
issues that employ the Lagrange principle to illustrate the original game concept.
In order to guarantee the convergence of the cost-functions to a Nash equilibrium
point, Tikhonov’s regularization method is also used. The extraproximal approach
is an iterated, two-step process. A preliminary location to the equilibrium point is
calculated in the first stage, and the prior forecast is adjusted in the second step. The
Projector Gradient Technique is used to find the minimal condition for each equation
in this system, which is an optimization issue. The value of this study comes in how
well a real-world problem for designing signal settings while taking into account
traffic flows and signal-controllers for several junctions was resolved.10.2 Preliminaries 225
10.2 Preliminaries
A Controllable Markov chain is a 4-tuple.MC = {S, A, ϒ,|} where.S is a finite set
of states, .S .⊂ .N, endowed with discrete topology; .A is the set of actions, which
is a metric space. For each .s ∈ S, .A(s) .⊂ .A is the non-empty set of admissi￾ble actions at state .s ∈ S. Without loss of generality we may take .A= .∪s∈S A(s);
.ϒ = {(s, a)|s ∈ S, a ∈ A(s)} is the set of admissible state-action pairs, which is
a measurable subset of.S × A; .| (k) = [
πj|ik ]
is a stationary transition controlled
matrix, where
.πj|ik ≡ P (
s(n + 1) = s(j)|s(n) = s(i), a(n) = a(k)
)
represents the probability associated with the transition from state .s(i) to state .s(j)
under an action.a(k) .∈.A.(k = 1, ..., M) at time.n .=.0, 1, ... .
The dynamic of the game for Markov chains is described as follows. The game
consists of a set .N = {1, ..., n} of players (denoted by .l = 1, n) and begins at the
initial state .sl
(0) which (as well as the states further realized by the process) is
assumed to be completely measurable. Each player.l is allowed to randomize, with
distribution.dl
k|i(n), over the pure action choices.al
(k) .∈.Al
,.i = 1, Nl and.k = 1, Ml .
The players make the strategy selection trying to realize a Nash-equilibrium. Below
we will consider only stationary strategies.dl
k|i(n) = dl
k|i . These choices induce the
state distribution dynamics
. Pl (
sl
(n + 1)=s(jl)
)
= ∑
Nl
il=1
(
∑
Ml
kl=1
πl
jl|il kl
dl
kl|il
)
Pl (
s(l)
(n)=sil
)
.
In the ergodic case (when all Markov chains are ergodic for any stationary strategy
.dl
k|i the distributions.Pl (
sl
(n + 1)=sjl
)
exponentially quickly converge to their limits
.Pl (s = si) satisfying
. Pl (
s(l) = sjl
)
= ∑
Nl
il=1
(
∑
Ml
kl=1
πl
jl|il kl
dl
kl|il
)
Pl (
s(l)
=sil
)
. (10.2.1)
For any player .l, his individual rationality is the player’s cost function .Jl of any
fixed policy .dl is defined over all possible combinations of states and actions, and
indicates the expected value when taking action .al in state .sl and following policy
.dl thereafter. The.J -values can be expressed by
.Jl
(cl
) := ∑
il,kl
Wl
il kl
cl
il kl
,
where226 10 Multi-traffic Signal-Control Synchronization
.Wl
il kl = ∑
jl
Jl
(il,jl,kl)πl
jl|il kl
,
and.cl := |
|cl
il kl
|
|
il=1,Nl;kl=1,Ml
is a matrix with elements
.cl
il kl = dl
kl|il
Pl (
s(l)
=s(il)
)
, (10.2.2)
satisfying
.c(l) ∈ C(l)
adm=
⎧
⎪⎪⎨
⎪⎪⎩
c(l) :
∑
il,kl
cl
il kl
=1, cl
il kl
≥0,
∑
kl
cl
jl kl
= ∑
il,kl
πl
jl|il kl
cl
il kl
.
(10.2.3)
The function.Jl
(il,jl,kl) is a constant cost at state.sil when the action.al
kl is applied and
the transfer to the state.sjl is realized.
Then, the cost function of each player, depending on the states and actions of all
participants, are given by the values.Wl
(i1,k1;...;in ,kn ), so that the “average cost function”
.Jl for each player.l in the stationary regime can be expressed as
.Jl (
c0
, .., cn)
:=∑
i0,k0
..∑
in ,kn
Wl
i1,k1,..,in ,kn
 n
l=0
cl
il kl
. (10.2.4)
Notice that by (10.2.2) it follows that
. Pl (
s(l)
=sil
)
=∑
kl
cl
il kl
, dl
kl|il
= cl
i
∑ l kl
kl
cl
il kl
. (10.2.5)
In the ergodic case.
∑
kl
cl
il kl > 0 for all.l = 0, n. The individual aim of each participant
is
.Jl
(cl
) → min
c(l)∈C(l)
adm
. (10.2.6)
10.3 Nash Equilibrium
Let us introduce the following variables
. ul := col c(l)
, Ul := C(l)
adm (
l = 1, n
)
. (10.3.1)
Consider a non-zero sum game with .n players with strategies .ul ∈ Ul .
(
l = 1, n
)
.
Denote by10.3 Nash Equilibrium 227
.u = (u1
, ..., un) ∈ U := n
l=1
Ul
, (10.3.2)
the joint strategy of the players. They are trying to reach one of the Nash equilibria,
that is, to find a joint strategy.x∗ = (
x 1∗, ..., x n∗
)
.∈.X satisfying for any admissible
.xl ∈ Xl and any.l = 1, n the system of inequalities (the Nash condition)
.
gl
(
ul
, x ˆl∗
)
≤ 0 for any ul ∈ Ul and all l = 1, n,
gl
(
ul
, x ˆl
)
:= ϕl
(
xl
, x ˆl
)
− ϕl
(
ul
, x ˆl
)
,
⎫
⎪⎪⎬
⎪⎪⎭
(10.3.3)
where.x ˆl is a strategy of the rest of the players adjoint to.ul
, namely,
.x ˆl := (
u1
, ..., ul−1
, ul+1
, ..., un)
∈ Xˆl := n
m=1, m/=l
Um,
and
.xl := arg min
ul∈ul
ϕl
(
ul
, x ˆl
)
,
is the best-reply of the .l player. Here .ϕl
(
ul
, x ˆl
)
is the cost-function of the player.l
which plays the strategy.ul ∈ Ul and the rest of the players the strategy.x ˆl ∈ Xˆl
.
Lemma 10.1 The Nash equilibrium.x¯ ∈ X (10.3.3) can be equivalently expressed
in the joint format [44]
.
max
x∈X g (u, x¯) ≤ 0,
g(u, x) := ∑n
l=1
[
ϕl
(
xl
, x ˆl
)
− ϕl
(
ul
, x ˆl
)] ,
ϕl
(
xl
, x ˆl
)
:= min
zl∈Xl
ϕl
(
zl
, x ˆl
)
,
ul ∈ Xl
, x ∈ X := n
l=1
Xl
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(10.3.4)
Proof Summing (10.3.3) implies (10.3.4). And inverse, taking.um = xm for all.m /= l
in (10.3.4), which is valid for any admissible.ul
, we obtain (10.3.3). ◻228 10 Multi-traffic Signal-Control Synchronization
Notice that the condition.g(u, x¯) ≤ 0 (10.3.4) for all admissible.U, is equivalent
to
. max
u∈U
{g(u, x¯)} ≤ 0.
for any fixed.u ∈ U.
The functions.ϕl
(
ul
, x ˆl
)
.
(
l = 1, n
)
are assumed to be convex in all their argu￾ments.
Definition 10.1 A strategy.x∗ ∈ X is said to be a Nash equilibrium if
.x∗∈Arg max
u∈U,x∈X
{g(u, x)|g(u, x)≤0} = {u ∈ U, x ∈ X)|g(u, x)=0}. (10.3.5)
10.3.1 The Regularized Lagrange Principle Application
Applying the Lagrange principle (see, for example, [36]) for Definition 10.1, we may
conclude that (10.3.5) can be rewritten as
.
x∗∈Arg min u∈U,x∈X
max
λ≥0
L(u, x, λ),
L(u, x, λ) := −g(u, x) + λg(u, x) = (λ − 1)g(u, x).
⎫
⎪⎬
⎪⎭
(10.3.6)
The approximative solution obtained by the Tikhonov regularization (see [36]) is
given by
.
x∗
δ = arg min u∈U,x∈X
max
λ≥0
Lδ(u, x, λ),
L◦(u, x, λ) := (λ − 1)gδ(u, x) − δ
2
λ2,
⎫
⎪⎪⎬
⎪⎪⎭
(10.3.7)
where.δ > 0 and
. gδ(u, x)=g(u, x)−δ
2
(|u|2+|x|2). (10.3.8)
Notice that with.δ > 0 the considered functions becomes to be strictly convex provid￾ing the uniqueness of the considered conditional optimization problem (10.3.7). For
.δ = 0 we have (10.3.6). Notice also that the Lagrange function in (10.3.7) satisfies
the saddle-point [35] condition, namely, for all.x ∈ X and.λ ≥ 0 we have
.L◦(u∗
δ , xδ , λδ)≤L◦(u∗
δ , x∗
δ , λ∗
δ )≤L◦(uδ, x∗
δ , λ∗
δ ). (10.3.9)10.3 Nash Equilibrium 229
10.3.2 The Proximal Format
In the proximal format (see, [4]) the relation (10.3.7) can be expressed as
.
λ∗
δ = arg max λ≥0
{
−1
2 |λ − λ∗
δ|2 + γL◦(u∗
δ , x∗
δ , λ)
}
,
u∗
δ = arg min u∈U
{ 1
2 |u − u∗
δ|2 + γL◦(u, x∗
δ , λ∗
δ )
}
,
x∗
δ = arg min x∈X
{ 1
2 |x − x∗
δ |2 + γL◦(u∗
δ , x, λ∗
δ )
}
,
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(10.3.10)
where the solutions.u∗
δ ,x∗
δ , and.λ∗
δ depend on the small parameters.δ, γ > 0.
10.3.3 The Extraproximal Method
The Extraproximal Method for the conditional optimization problems ( 10.3.7) was
suggested in [4]. We design the method for the static Nash game in a general format
following [45].
The general format iterative version .(n = 0, 1, ...) of the extraproximal method
with some fixed admissible initial values.(x0 ∈ X ,.v0 ∈ V and.λ0 ≥ 0) is as follows
1. The first half-step (prediction):
.
λ¯ n= arg min λ≥0
{ 1
2 |λ − λn|2−γLδ(un, xn, λ)
}
,
u¯n= arg min u∈U
{ 1
2 |u − un|2 + γL◦(u, xn, λ¯ n)
}
,
x¯n= arg min x∈X
{ 1
2 |x − x n|2+γLδ(un, x, λ¯ n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(10.3.11)
2. The second (basic) half-step
.
λn+1= arg min λ≥0
{ 1
2 |λ − λn|2−γLδ(u¯n, x¯n, λ)
}
,
un+1= arg min u∈U
{ 1
2 |u − un|2 + γL◦(u, x¯n, λ¯ n)
}
,
xn+1= arg min x∈X
{ 1
2 |x − x n|2+γLδ(u¯n, x, λ¯ n)
}
.
⎫
⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭
(10.3.12)230 10 Multi-traffic Signal-Control Synchronization
10.4 Traffic-Signal-Control Problem Formulation
The simplest game considers a two one-way-street intersection (Fig. 10.1). Let us
suppose that each signal controller phase has been established. There is a traffic
problem in the intersection area between the two movements, therefore they cannot
travel at the same time and the traffic signal will be opposite, i.e. when one movement
has a green light others movements have a red light. The objective of the intersection
is to minimize the queuing delay and the problem for a signal controller is to find
an optimal signal timing strategy, i.e. establishing green time to each signal phase.
The dynamics of the game is as follows. Intersections are considered the players in
the game. Each intersection aims at finding green time that minimizes its signal and
queuing delay. Each player chooses a strategy to achieve the system optimum. So
the conflict appears when each player wants to minimize its queue. The extended
probability vector will be used in the cost function with the constraints on the behavior
of the vehicles.
The vehicle flow is controlled by a signal controller of two color lights (actions)
.a1 and.a2 representing the red and the green lights respectively. Let.x and.y the two
controllers corresponding to the players.l. At time .n the total number of cars in the
street is defined by.x(n) (.y(n)), the number of entering cars is determined by.ξx (n)
(.ξ y (n)), and the number of exiting cars is.υx (n) (.υy (n)). The maximum capacity of
the street (the queue) is determined by.x+ (.y+) meaning (.l
+). The dynamics of the
flow of vehicles in a simple intersection of streets for the players.x and.y is defined
as follows:
.a(n) = a1 : red(x)/green(y)
Fig. 10.1 Traffic signal
control10.4 Traffic-Signal-Control Problem Formulation 231
.x(n + 1) =
⎧
⎨
⎩
x+ x(n) + ξx (n) > x+
x(n) + ξx (n) x(n) + ξx (n) ≤ x+,
.y(n + 1) =
⎧
⎨
⎩
y+ y(n) + ξ y (n) − υy (n) > y+
[y(n) + ξ y (n) − υy (n)]+ y(n) + ξ y (n) − υy (n) ≤ y+,
where
.[Z]+ =
⎧
⎨
⎩
Z Z ≥ 0
0 Z < 0,
.a(n) = a2 : green(x)/red(y)
.x(n + 1) =
⎧
⎨
⎩
x+ x(n) + ξx (n) − υx (n) > x+
[x(n) + ξx (n) − υx (n)]+ x(n) + ξx (n) − υx (n) ≤ x+,
.y(n + 1) =
⎧
⎨
⎩
y+ y(n) + ξ y (n) > y+
y(n) + ξ y (n) y(n) + ξ y (n) ≤ y+.
These expressions describes the principle of a queue processing technique called
FIFO, where the cars leave the queue in the order they arrive, or waiting one’s turn
at a traffic control signal.a. We assume a Poisson distribution given by
. f (l, λt) = P(X = l) = e−(λt) (λt)
l
l! (10.4.1)
for a random variable.X where.l is the actual number of successes of an event and.λ
is the mean number of successes of an event. We will denote the input parameter by
.λ and the output parameter by.μ. Note that the input parameter.λx > 0 (.λy > 0)and
output parameter.μx > 0 (.μy > 0) are different for each player.
10.4.1 Transition Matrix
The following Theorems define a transition matrix .πj|ia for each action .a in the
controlled Markov chain.
Red light (transition matrix.πj|ia1 )
Theorem 10.1 Let. f (l, λ) be a Poisson distribution (10.4.1) with input parameter
.λx > 0 and the dynamics of the flow of the vehicles for the player.x given by equation
(10.4) with.x+ the maximum capacity of the street. Then, the transition matrix.πx
j|ik
to go from state.i to state. j using the action.a(n) = a1 : red(x) is given by232 10 Multi-traffic Signal-Control Synchronization
. πx
j|ia1 = δ j,x+
⎛
⎝1 − e−λx
x
∑+−i
l=0
(λx )
l
l!
⎞
⎠ + e−2λx (λx )
(j−i)
(j − 1)!
x
∑+−i
l=0
(λx )
l
l! χ(j ≥ i),
where.δ j,x+ is the delta Kronecker,
.δ j,x+ =
⎧
⎨
⎩
1 j = x+
0 j /= x+,
.χ(j ≥ i) is a characteristic function given by
.χ(j ≥ i) =
⎧
⎨
⎩
1 j ≥ i
0 j < i,
and.i and. j are the indexes corresponding to the states.xi and.x j .
Proof The transition matrix.πx
j|ia1 is given by
.πx
j|ia1 = P(x(n + 1) = j|x(n) = i, a(n) = a1).
Using Eq. (10.4) and considering that .P(B) = ∑
q
.P(B|Aq )P(Aq ) (see Poznyak
[35]) we have that
.πx
j|ia1 = P(x(n + 1) = j|x(n) = i, a(n) = a1, x(n)
+ ξx (n) > x+)P(x(n) + ξx (n) > x+)|x(n)=i+
P(x(n + 1) = j|x(n) = i, a(n) = a1, x(n)
+ ξx (n) ≤ x+)P(x(n) + ξx (n) ≤ x+)|x(n)=i,
where .x(n + 1) = j and .x(n) = i are the index corresponding to the states.x j and
.xi . Then, we have that
.
P(x(n) + ξx (n) > x+)|x(n)=i = P(ξx (n) > x+ − x(n))|x(n)=i = P(ξx (n) > x+ − i) =
∑∞
l=x+−i+1
e−(λx t)(λx t)
l
l!
|
|
|
|
t=1
= e−λx ∑∞
l=x+−i+1
(λx )
l
l! = 1 − e−λx x+
∑−i
l=0
(λx )
l
l! ,
and considering that the maximum index. j of.x j is.x+ we have that
.P(x(n + 1) = j|x(n) = i, a(n) = a1, x(n) + ξx (n) > x+) = δ j,x+ .
As well, we have that10.4 Traffic-Signal-Control Problem Formulation 233
. P(x(n) + ξx (n) ≤ x+)|x(n)=i = P(ξx (n) ≤ x+ − xi) = e−λx x+
∑−xi
l=0
(λx )
l
l! ,
and
.P(x(n + 1) = j|x(n) = i, a(n) = a1, x(n) + ξx (n) ≤ x+)
P(x(n + 1) = j|x(n) = i, a(n) = a1, ξx (n) ≤ x+ − i)
= P(ξx (n) ≤ j − i) = e−λx (λx )
(j−i)
(j − i)!
.
Then, we have that
.πx
j|ia1 = δ j,x+
⎛
⎝1 − e−λx
x
∑+−i
l=0
(λx )
l
l!
⎞
⎠ + e−2λx (λx )
(j−i)
(j − 1)!
x
∑+−i
l=0
(λx )
l
l! χ(j ≥ i).
◻
Corollary 10.1 Let. f (l, λ) be a Poisson distribution (10.4.1) with input parameter
.λy > 0 and the dynamics of the flow of the vehicles for the player.y given by Eq.
(10.4) with.y+ the maximum capacity of the street. Then, the transition matrix.πy
j|ik
to go from state.i to state. j using the action.a(n) = a2 : red(y) is given by
. πy
j|ia2 = δyj,y+
⎛
⎝1 − e−λy
y
∑+−i
l=0
(λy )
l
l!
⎞
⎠ + e−2λy (λy )
(j−i)
(j − 1)!
y
∑+−i
l=0
(λy )
l
l! χ(j ≥ i),
where .δ j,y+ is the delta Kronecker, .χ(j ≥ i) is a characteristic function, and .i and
. j are the indexes corresponding to the states.yi and.yj .
Green light (transition matrix.πj|ia2 )
Theorem 10.2 Let. f (l, λ) be a Poisson distribution (10.4.1) with input parameter
.λx > 0, output parameter.μx and the dynamics of the flow of vehicles for the player.x
given by Eq. (10.4) with.x+ the maximum capacity of the street. Then, the transition
matrix .πx
j|ia2 to go from state .i to state . j using the action .a(n) = a2 : green(x) is
given by
.
πx
j|ia2 = δ j,x+
(
1 − e−(λx+μx ) ∑∞
m=0
(μx )
m
m!
x+−
∑i+m
l=0
(λx )
l
l!
)
+
( ∑∞
m=0
[
e−2λx
( x+−
∑i+m
l=[m−1]+
(λx )
l
l!
∑∞
q=[m−1]+
(λx )
q
q!
)
+ δ j,0
(
e−λx [m−∑i]++1
l=0
(λx )
l
l!
)] (
e−μx (μx )
m
m!
)
)
×
(
e−(λx+μx ) ∑∞
m=0
(μx )
m
m!
x+−
∑i+m
l=0
(λx )
l
l!
)234 10 Multi-traffic Signal-Control Synchronization
where.δ j,x+ and.δ j,0 are the delta Kronecker and.χ(j ≥ i)is a characteristic function,
and.i and. j are the indexes corresponding to the states.xi and.x j .
Proof The transition matrix.πx
j|ia2 is given by
.πx
j|ia2 = P(x(n + 1) = j|x(n) = i, a(n) = a2).
Using Eq. (10.4) and considering that.P(B) = ∑
l
.P(B|Al)P(Al)(see Poznyak [35])
we have that
.πx
j|ia2 = P(x(n + 1) = j|x(n) = i, a(n) = a2, x(n) + ξx (n)
− υx (n) > x+)P(x(n) + ξx (n) − υx (n) > x+)|x(n)=i+
P(x(n + 1) = j|x(n) = i, a(n) = a2, x(n) + ξx (n) − υx (n)
≤ x+)P(x(n) + ξx (n) − υx (n) ≤ x+)|x(n)=i,
where .x(n + 1) = j and .x(n) = i are the index corresponding to the states.x j and
.xi .
.P(x(n) + ξx (n) − υx (n) > x+)|x(n)=i = P(ξx (n) − υx (n) > x+ − i) =
= ∑∞
m=0
P(ξx (n) − υx (n) > x+ − i)
|
|
υ(n)=m P(υx (n) = m)
= ∑∞
m=0
[(e−λx ∑∞
l=x+−i+m+1
(λx )
l
l!
)(
e−μx (μx )
m
m!
)]
= 1 − ∑∞
m=0
⎡
⎣
⎛
⎝e−λx
x+
∑−i+m
l=0
(λx )
l
l!
⎞
⎠
(
e−μx (μx )
m
m!
)⎤
⎦
= 1 − e−(λx+μx )
∑∞
m=0
⎡
⎣
((μx )
m
m!
) ⎛
⎝
x+
∑−i+m
l=0
(λx )
l
l!
⎞
⎠
⎤
⎦ ,
and considering that the maximum index. j of.x j is.x+ we have that
.P(x(n + 1) = j|x(n) = i, a(n) = a2, x(n) + ξx (n) − υx (n) > x+) = δ j,x+ .
As well, we have that10.4 Traffic-Signal-Control Problem Formulation 235
.P(x(n) + ξx (n) − υx (n) ≤ x+)|x(n)=i = P(ξx (n) − υx (n) ≤ x+ − i)
= ∑∞
m=0
P(ξx (n) − υx (n) ≤ x+ − i)
|
|
υ(n)=m P(υx (n) = m)
= ∑∞
m=0
P(ξx (n) ≤ x+ − i + m)P(υx (n) = m)
= ∑∞
m=0
⎡
⎣
⎛
⎝e−λx
x+
∑−i+m
l=0
(λx )
l
l!
⎞
⎠
(
e−μx (μx )
m
m!
)⎤
⎦
= e−(λx+μx )
∑∞
m=0
(μx )
m
m!
x+
∑−i+m
l=0
(λx )
l
l! ,
and
.P(x(n + 1) = j|x(n) = i, a(n) = a2, x(n) + ξx (n) − υx (n) ≤ x+)|x(n)=i
= P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n) − υx (n) ≤ x+ − i)
= ∑∞
m=0
P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n)
− υx (n) ≤ x+ − i ∧ υx (n) = m)·
P(υx (n) = m) = ∑∞
m=0
P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n)
≤ x+ − i + m)P(υx (n) = m)
∑∞
m=0
[P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n)
≤ x+ − i + m ∧ ξx (n) + i − m ≥ 0)·
P(ξx (n) + i − m ≥ 0) + P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n) ≤ x+ − i + m ∧ ξx (n) + i − m < 0)·
P(ξx (n) + i − m < 0)]P(υx (n) = m).
Now
.P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n) ≤ x+ − i + m ∧ ξx (n) + i − m ≥ 0)
= e−λx
x+
∑−i+m
l=[m−1]+
(λx )
l
l! ,
.P(ξx (n) + i − m ≥ 0) = P(ξx (n) ≥ [m − i]+)
= e−λx ∑∞
q=[m−1]+
(λx )
q
q! ,236 10 Multi-traffic Signal-Control Synchronization
. P(x(n + 1) = j|x(n) = i, a(n) = a2, ξx (n) ≤ x+ − i + m ∧ ξx (n) + i − m < 0) = δ j,0,
.P(ξx (n) + i − m < 0) = P(ξx (n) < m − i) = P(ξx (n) < [m − i]+ + 1)
= e−λx
[m−
∑
i]++1
l=0
(λx )
l
l! .
Then, we have that
.
πx
j|ia2 = δ j,x+
(
1 − e−(λx+μx ) ∑∞
m=0
(μx )
m
m!
x+−
∑i+m
l=0
(λx )
l
l!
)
+
( ∑∞
m=0
[
e−2λx
( x+−
∑i+m
l=[m−1]+
(λx )
l
l!
∑∞
q=[m−1]+
(λx )
q
q!
)
+ δ j,0
(
e−λx [m−∑i]++1
l=0
(λx )
l
l!
)] (
e−μx (μx )
m
m!
)
)
·
(
e−(λx+μx ) ∑∞
m=0
(μx )
m
m!
x+−
∑i+m
l=0
(λx )
l
l!
)
.
◻
Corollary 10.2 Let. f (l, λ) be a Poisson distribution (10.4.1) with input parameter
.λy > 0, output parameter.μy and the dynamics of the flow of vehicles for the player.y
given by Eq. (10.4) with.y+ the maximum capacity of the street. Then, the transition
matrix .πy
j|ia2 to go from state .i to state . j using the action corresponding to .a(n) =
a2 : green(y) is given by
.
πy
j|ia2 = δ j,y+
(
1 − e−(λy+μy ) ∑∞
m=0
(μy )
m
m!
y+−
∑i+m
l=0
(λy )
l
l!
)
+
( ∑∞
m=0
[
e−2λy
( y+−
∑i+m
l=[m−1]+
(λy )
l
l!
∑∞
q=[m−1]+
(λy )
q
q!
)
+ δ j,0
(
e−λy [m−∑i]++1
l=0
(λy )
l
l!
)] (
e−μy (μy )
m
m!
)
)
×
(
e−(λy+μy ) ∑∞
m=0
(μy )
m
m!
y+−
∑i+m
l=0
(λy )
l
l!
)
,
where.δ j,y+ and.δ j,0 is the delta Kronecker and.χ(j ≥ i) is a characteristic function,
and.i and. j are the indexes corresponding to the states.yi and.yj .10.4 Traffic-Signal-Control Problem Formulation 237
10.4.2 Ergodicity
The corresponding Markov chain has the coefficients of ergodicity .kl
erg estimated
(see Chapter 1) from below as
.
kl
erg := min
n(l)
0
max
j(l)=1,...,N
min
i(l)=1,...,N
π˜
(l)∗
j(l)|i(l)
(
n(l)
0 = 1
)
≥
max
j(l)=1,...,N
min
i(l)=1,...,N
min {
π(l)
j(l)|i(l)a(l)
1
; π(l)
j(l)|i(l)a(l)
2
}
:= ρl
.
So that if.ρl is positive for every player, then the Markov chains are ergodic.
10.4.3 Cost Function
The individual aim of player (.x) (under stationary strategies) can be formulated as
follows:
.J(x) =
y
∑+
i(y) j(y)
∑
My
k(y)
x
∑+
i(x) j(x)
∑
Mx
k(x)
k(y)/=k(x)
j
(x)
(
π(x)
j(x)|i(x)k(x)d(x)
k(x)|i(x) P(x)
i(x)
) (π(y)
j(y)|i(y)k(y)d(y)
k(y)|i(y) P(y)
i(y)
)
.
Since.
y
∑+
i(y) j(y)
∑
My
k(y)
π(y)
j(y)|i(y)k(y) = 1 we have that
.J(x) =
x
∑+
i(x) j(x)
∑
Mx
k(x)
j
(x)
(
π(x)
j(x)|i(x)k(x)d(x)
k(x)|i(x) P(x)
i(x)
)
y
∑+
i(y)
∑
My
k(y)
k(y)/=k(x)
d(y)
k(y)|i(y) P(y)
i(y) ,
and we obtain
.J(x) =
x
∑+
i(x)
∑
Mx
k(x)
⎛
⎝
x∑+
j(x)
j
(x)
π(x)
j(x)|i(x)k(x)
⎞
⎠ d(x)
k(x)|i(x) P(x)
i(x)
y
∑+
i(y)
∑
My
k(y)
d(y)
k(y)|i(y) P(y)
i(y) ,
having
.W(x)
i(x)k(x) =
x
∑+
j(x)
j
(x)
π(x)
j(x)|i(x)k(x) ,238 10 Multi-traffic Signal-Control Synchronization
where .(i)+ is the size of the buffer (the maximum admissible number of cars for
each player.l). In general, for each player.l we have
.J(l) =
(i)
∑+
i(l)
∑
Ml
k(l)
W(l)
i(l)k(l)c
(l)
i(l)k(l)
(ˆl)
∑+
i(ˆl)
Mˆ ∑l
k(ˆl)
c
(ˆl)
i(ˆl)k(ˆl)
,
where
. c
(l)
i(l)k(l) = d(l)
k(l)|i(l) P(l)
i(l) , c
(ˆl)
i(ˆl)k(ˆl) = d(ˆl)
k(ˆl)|i(ˆl) P(ˆl)
i(ˆl) .
The signal traffic control is based on the green light duration determined by the
estimated number of vehicles entering the road during a green/red light cycle. The
distance of player.l is calculated in terms of the size of the buffer (.(i)+) multiplied
by the number of entering and exiting cars represented by the the transition matrix
(.π(l)
j(l)|i(l)k(l) ) and, the long-run of the complementary player (.c
(ˆl)
i(ˆl)k(ˆl)
).
10.5 Gradient Solver
The loss-functions (10.2.4) of. Jl
(n) for each player.l in the nonstationary regime can
be expressed as
.
Jl
(c0,..,cn )
(n):=∑
i0,k0
.. ∑
in ,kN
Wl
i1,k1,..,in ,kn
(n)
|n
l=0
cl
il kl
(n) =
∑
i0,k0
.. ∑
in ,kn
(
∑
j0
..∑
jn
Jl
(i1,j1,k1,..,in ,jn ,kn )(n)
|n
l=0
πl
jl|il kl
(n)
)(|n
l=0
cl
il kl
(n)
)
.
The individual aim is
.Jl
(c1,..,cn )
(n) → min cl
i
l kl
(n)∈C(l)
adm
.
Note that the function.Jl
(c0,..,cn )
(n) is polylineal in.cl
il kl
(n) ∈ C(l)
adm , and therefore
can not be solved analytically. So, we need to apply an iterative method to find
a minimizing solution which, additionally, may be not unique. For solving the ill￾posed problem we introduce a Tikhonov’s regularizator with regularization parameter
(.δ > 0) which consists on
.J˜l
(c1,..,cn )
(n) := Jl
(c1,..,cn )
(n) +
δ
2
∑
Nl
il=1
∑
Ml
kl=1
|
|cl
il kl
(n)
|
|
2 → min cl
i
l kl
(n)∈C(l)
adm
,10.5 Gradient Solver 239
(.J˜l
(c1,..,cn )
(n) is as in Eqs. (10.3.11) and Eqs. (10.3.12)). Obviously, when .δ → 0
we obtain the initial problem. In addition, we will ask .J˜l
(c1,..,cn )
(n) to satisfy the
restrictions described in Eq. (10.2.3). Then, defining
.hl
(cl
i
t
l kt
l
(n)) = ∑
kl
cl
jl kl
−
∑
il,kl
πl
jl|il kl
cl
il kl
,
we finally propose the Projection Gradient Method [36] with regularization param￾eter (.ξ > 0) which consists in solution of the following problem:
.V˜ l
(c1,..,cn ,θ1,...,θn )
(n) = J˜l
(c1,..,cn )
(n) +∑
l
θlhl
(cl
i
t
l kt
l
(n)) + ξ
2
∑
l
θ2
l .
Algorithm 10.1: Projector gradient method
Step 1: Initialization
Choose tolerances.ε > 0, starting point .cl
il kl
(0), regularization parameter .ξ0 and
the initial parameter.μc
0.
Step 2: Iterations
for a given.il = 1, Nl, kl = 1, Ml and a fixed.n
2.(a) compute.cl
il kl
(n + 1) using the following equation:
.
c˜
l
il kl
(n + 1) = cl
il kl
(n) − μc
n∇cl
i
l kl
(n)V˜ l
(c0,..,cn ,θ1,...,θn ),
(n)
cl
il kl
(n + 1) =PrSNl Ml
{
c˜
l
il kl
(n + 1)
}
,
⎫
⎪⎬
⎪⎭
(10.5.1)
where.PrSNl Ml {·} is the projection operator of the vector from.RMl into the simplex
.SNl Ml and.J˜l
(c0,..,cn )
(n) is as in Eqs. (10.3.11) and Eqs. (10.3.12).
2.(b) compute.θl(n + 1) using the following equation:
.θl(n + 1) =θl(n)−μθ
n∇θl(n)V˜ l
(c1,..,cn ,θ1,...,θn )
(n)
2.(c) verify descent direction
.
|
|cl
il kl
(n) − PrSNl Ml
{
cl
il kl
(n + 1)
}|
| ≤ |
|cl
il kl
(n) − ˜cl
il kl
(n + 1)
|
|
for any.cl
il kl
(n + 1) ∈ RMl and any.cl
il kl
(n) ∈ SNl Ml .
end
Step 3: Stopping criteria
Check the convergence criteria.
|
|cl
il kl
(n + 1) − cl
il kl
(n)
|
| < ε then stop. Otherwise,
set.n = n + 1 and return to Step 2.240 10 Multi-traffic Signal-Control Synchronization
The corresponding iterative algorithm for the Projector Gradient Method is
described in the Algorithm 10.1. It is shown that we have the convergence of this
method
.cl
il kl
(n) →n→∞ c
l,∗∗
il kl
to one of the solutions.c
l,∗
il kl of the initial problem (10.5) with minimal norm, i.e.,
.c
l,∗∗
il kl (n) := min
c
l,
i
l kl
∈C(l)
adm
∑
Nl
i
t
l =1
∑
Ml
kt
l =1
|
|
|c
l,∗
il kl
(n)
|
|
|
2
,
if the parameters.
{
μc
n
}
and.
{
δc
n
}
of the procedure (10.5.1) fulfill
.
0 < μn →n→∞ 0, 0 < δn →n→∞ 0,
∑∞
n=0
μnδn = ∞, |δn+1 − δn|
μnδn
→n→∞ 0,
μn
δn
→n→∞ ζwhich is small enough.
10.6 Application Example
We consider the three-way intersection game with three one-way-street intersection
(see Fig. 10.2). Let us introduce the following notation for describing the problem
.
Nl = The total number of states of the street,
Ml = The total number of actions = intersections,
λl = The input parameter of player l,
μl The output parameter of player l,
Jl = The cost function,
cl
ik = The long run fraction of the time that the
system is in state i and action k (red-green) is chosen,
l
+ = The size of the buffer (the maximum capacity of the street).
For the players.l = x, y,z the following identities hold10.6 Application Example 241
Fig. 10.2 Traffic three way singnal control
.
J(x)
(c(x)
, c(y)
, c(z)
) =
(x
∑+
i(x)
∑
Mx
k(x)
W(x)
i(x)k(x)c
(x)
i(x)k(x)
) (y
∑+
i(y)
∑
My
k(y)
c
(y)
i(y)k(y)
) (z
∑+
i(z)
∑
Mz
k(z)
c
(z)
i(z)k(z)
)
,
J(y)
(c(x)
, c(y)
, c(z)
) =
(y
∑+
i(y)
∑
My
k(y)
W(y)
i(y)k(y)c
(y)
i(y)k(y)
) (x
∑+
i(x)
∑
Mx
k(x)
c
(x)
i(x)k(x)
) (z
∑+
i(z)
∑
Mz
k(z)
c
(z)
i(z)k(z)
)
,
J(z)
(c(x)
, c(y)
, c(z)
) =
(z
∑+
i(z)
∑
Mz
k(z)
W(z)
i(z)k(z)c
(z)
i(z)k(z)
) (x
∑+
i(x)
∑
Mx
k(x)
c
(x)
i(x)k(x)
) (y
∑+
i(y)
∑
My
k(y)
c
(y)
i(y)k(y)
)
.
The dynamics of the problem in a three one-way intersection for the players.x, y
and.z is defined as follows:
.a(n) = a1 : red(x)/green(y)/red(z)
.x(n + 1) =
⎧
⎨
⎩
x+ x(n) + ξx (n) > x+
x(n) + ξx (n) x(n) + ξx (n) ≤ x+,
.y(n + 1) =
⎧
⎨
⎩
y+ y(n) + ξ y (n) − υy (n) > y+
[y(n) + ξ y (n) − υy (n)]+ y(n) + ξ y (n) − υy (n) ≤ y+,
.z(n + 1) =
⎧
⎨
⎩
z+ z(n) + ξz
(n) > z+
z(n) + ξz
(n) z(n) + ξz
(n) ≤ z+,
.a(n) = a2 : green(x)/red(y)/red(z)
.x(n + 1) =
⎧
⎨
⎩
x+ x(n) + ξx (n) − υx (n) > x+
[x(n) + ξx (n) − υx (n)]+ x(n) + ξx (n) − υx (n) ≤ x+,242 10 Multi-traffic Signal-Control Synchronization
.y(n + 1) =
⎧
⎨
⎩
y+ y(n) + ξ y (n) > y+
y(n) + ξ y (n) y(n) + ξ y (n) ≤ y+,
.z(n + 1) =
⎧
⎨
⎩
z+ z(n) + ξz
(n) > z+
z(n) + ξz
(n) z(n) + ξz
(n) ≤ z+,
.a(n) = a3 : red(x)/red(y)/green(z)
.x(n + 1) =
⎧
⎨
⎩
x+ x(n) + ξx (n) > x+
x(n) + ξx (n) x(n) + ξx (n) ≤ x+,
.y(n + 1) =
⎧
⎨
⎩
y+ y(n) + ξ y (n) > y+
y(n) + ξ y (n) y(n) + ξ y (n) ≤ y+,
.z(n + 1) =
⎧
⎨
⎩
z+ z(n) + ξz
(n) − υz
(n) > z+
[z(n) + ξz
(n) − υz
(n)]+ z(n) + ξz
(n) − υz
(n) ≤ z+.
Associated with the three actions .a1, .a2 and .a3 there are three control strategies
defined as
. cl
il kl = dl
kl|il
Pl (
s(l)
=s(il)
)
(.l = x, y,z) with the following restrictions
.
cl
il1, cl
il2, cl
il3 > 0, cl
il1 + cl
il2 + cl
il3 = 1,
c = |
|cil kl
|
|
kl=1,2,3, il=0,Nl
, (cl
il1, cl
il2, cl
il3) ∈ ∆.
Remark 10.1 (Intersection avoiding condition) The realization of the signal con￾trol problem requires the actions to be synchronized as follows
.
cx
1|1 = c
y
1|1, cx
1|2 = c
y
1|2, cx
(1|3) = c
y
(1|3),
cx
2|1 = c
y
2|1, cx
2|2 = c
y
2|2, cx
(2|3) = c
y
(2|3),
cx
3|1 = c
y
3|1, cx
3|2 = c
y
3|2, cx
3|2 = c
y
3|3.
Fixing.Nx = Ny = Nz = 3,.Mx = My = Mz = 3, and.x+ = y+ = z+ = 3, then
we obtain different configurations of the game:10.6 Application Example 243
(a) No leader (uniform distribution). For.λx = λy = λz = 3,.μx = μy = μz = 3,
.γ = 0.12 and .δ = 0.009 we have that the corresponding transition matrices are as
follows:
. πl
j|i,green =
⎡
⎣
0.3659 0.1446 0.4895
0.2160 0.1417 0.6423
0.1027 0.1027 0.7947
⎤
⎦ , πl
j|i,red =
⎡
⎣
0.0514 0.1543 0.7943
0.0000 0.0319 0.9681
0.0000 0.0000 1.0000
⎤
⎦ .
For player.x we have .{πx
j|i,red , πx
j|i,green, πx
j|i,red }, for.y we have .{πy
j|i,green, πy
j|i,red ,
πy
j|i,red } and for player.z we have .{πz
j|i,red , πz
j|i,red , πz
j|i,green}. The resulting.cl
ik (.l =
x, y,z) will be the same for all the players
. cl
ik =
⎡
⎣
0.0136 0.0136 0.0136
0.0134 0.0134 0.0134
0.3063 0.3063 0.3063
⎤
⎦ .
Then, fixing.k and adding by.i we have that the long run fraction of the time for each
player is as follows:
. cl
k =
[ Red Green Red
0.3333 0.3333 0.3333]
, c
y
k =
[
Green Red Red
0.3333 0.3333 0.3333]
, cz
k =
[ Red Red Green
0.3333 0.3333 0.3333]
.
Then, because the players have the same.λl and.μl the distribution of the time is the
same for Green and Red lights.
The corresponding strategies.dl
k|i (.l = x, y,z) are as follows:
. dl
k|i =
⎡
⎣
0.3333 0.3333 0.3333
0.3333 0.3333 0.3333
0.3333 0.3333 0.3333
⎤
⎦ .
(b) One leader and two followers. We can make player .x a leader by making
.λx = 4, and .λy = λz = 3, .μx = 4 .μy = μz = 3, .γ = 0.14 and .δ = 0.009 we have
that the corresponding transition matrices are as follows:
. πx
j|i,green =
⎡
⎣
0.1620 0.0606 0.7774
0.0717 0.0438 0.8844
0.0228 0.0228 0.9543
⎤
⎦ , πx
j|i,red =
⎡
⎣
0.0119 0.0474 0.9407
0.0000 0.0056 0.9944
0.0000 0.0000 1.0000
⎤
⎦ .
For player.x we have .{πx
j|i,red , πx
j|i,green, πx
j|i,red }.For players.y and .z the transition
matrices are as follows:
. πy,z
j|i,green =
⎡
⎣
0.3659 0.1446 0.4895
0.2160 0.1417 0.6423
0.1027 0.1027 0.7947
⎤
⎦ , πy,z
j|i,red =
⎡
⎣
0.0514 0.1543 0.7943
0.0000 0.0319 0.9681
0.0000 0.0000 1.0000
⎤
⎦ .244 10 Multi-traffic Signal-Control Synchronization
For.y we have.{πy
j|i,green, πy
j|i,red , πy
j|i,red } and for player.z we have.{πz
j|i,red , πz
j|i,red ,
πz
i j|green}. The resulting.cl
ik (.l = x, y,z) will be the same for all the players
. cl
ik =
⎡
⎣
0.0062 0.0050 0.0062
0.0050 0.0072 0.0050
0.1319 0.7013 0.1319
⎤
⎦ .
Then, fixing.k and adding by.i we have that the long run fraction of the time for
each player is as follows:
. cx
k =
[ Red Green Red
0.1431 0.7135 01431]
, c
y
k =
[
Green Red Red
0.1431 0.7135 0.1431]
, cz
k =
[ Red Red Green
0.1431 0.7135 0.1431]
.
Then, because player.x is a leader the time assigned to the Green light (.0.7135) is
larger than to Red light (.0.1431 + 0.1431). The players.y and.z have the same.λ and
.μ, as a result the distribution of the time is the same for Green (.0.1431) and Red
lights (.0.7135 + 0.7135).
The corresponding strategies.dl
k|i (.l = x, y,z) are as follows:
. dl
k|i =
⎡
⎣
0.3571 0.2858 0.3571
0.2909 0.4183 0.2909
0.1367 0.7266 0.1367
⎤
⎦ .
(c) Two leaders and one follower. We can make player .x and .y leaders by
making .λx = λy = 4, and .μx = μy = 3 and player .z follower by making .λz = 3,
.μz = 3,.γ = 0.18 and.δ = 0.009 we have that the corresponding transition matrices
for players.x and.y are as follows:
. πx,y
j|i,green =
⎡
⎣
0.2715 0.1027 0.6258
0.1233 0.0779 0.7988
0.0401 0.0401 0.9198
⎤
⎦ , πx,y
j|i,red =
⎡
⎣
0.0119 0.0474 0.9407
0.0000 0.0056 0.9944
0.0000 0.0000 1.0000
⎤
⎦ .
For player.x we have.{πx
j|i,red , πx
j|i,green, πx
j|i,red } and for player.y we have.{πy
j|i,green,
πy
j|i,red , πy
j|i,red }. For player.z the transition matrices are as follows:
. πz
j|i,green =
⎡
⎣
0.3659 0.1446 0.4895
0.2160 0.1417 0.6423
0.1027 0.1027 0.7947
⎤
⎦ , πz
j|i,red =
⎡
⎣
0.0514 0.1543 0.7943
0.0000 0.0319 0.9681
0.0000 0.0000 1.0000
⎤
⎦ .
For player.z we have .{πz
j|i,red , πz
j|i,red , πz
i j|green}. The resulting .cl
ik (.l = x, y,z) will
be the same for all the playersReferences 245
. cl
ik =
⎡
⎣
0.0054 0.0054 0.0082
0.0066 0.0066 0.0050
0.4116 0.4116 0.1395
⎤
⎦ .
Then, fixing.k and adding by.i we have that the long run fraction of the time for
each player is as follows:
. cx
k =
[ Red Green Red
0.4236 0.4236 0.1527]
, c
y
k =
[
Green Red Red
0.4236 0.4236 0.1527]
, cz
k =
[ Red Red Green
0.4236 0.4236 0.1527]
.
The players.x and .y have the same .λ and .μ, as a result the distribution of the time
is the same for Green (.0.4236) and Red lights (.0.4236 + 0.1527). Player .z, as a
follower, has assigned (.0.1527) to the Green light and (.0.4236 + 0.4236) Red light.
The corresponding strategies.dl
k|i (.l = x, y,z) are as follows:
. dl
k|i =
⎡
⎣
0.2833 0.2833 0.4334
0.3633 0.3633 0.2733
0.4276 0.4276 0.1449
⎤
⎦ .
Remark 10.2 Different results can be obtained by fixing specific values for .λ
and.μ.
References
1. Allsop, R.: Sigset: a computer program for calculating traffic capacity of signal-controlled road
junctions. Traffic Eng. Control 12, 58–60 (1971)
2. Allsop, R.: Sigcap: A computer program for assessing the traffic capacity of signal-controlled
road junctions. Traffic Eng. Control 17, 338–341 (1976)
3. Alvarez, I., Poznyak, A.S., Malo Tamayo, A.: Urban traffic control problem: a game theory
approach. In: Proceedings of the 17th World Congress IFAC, The International Federation of
Automatic Control Seoul, Korea, pp. 7154–7159 (2008)
4. Antipin, A.S.: An extraproximal method for solving equilibrium programming problems and
games. Comput. Math. Math. Phys. 45(11), 1893–1914 (2005)
5. Aragon-Gomez, R., Clempner, J.B.: Traffic-signal control reinforcement learning approach for
continuous-time markov games. Eng. Appl. Artif. Intell. 89, 103415 (2020)
6. Cantarella, G.E., Improta, G., Sforza, A.: Road network signal setting: equilibrium conditions.
In: Concise Encyclopedia of Traffic and Transportation Systems. Pergamon Press, Amsterdam
(1991)
7. Cascetta, E.: Transportation Systems Analysis: Models and Applications. Springer, New York
(2009)
8. Cascetta, E., Gallo, M., Montella, B.: An asymmetric sue model for the combined assignment￾control problem. In: Selected Proceedings of 8th WCTR. Pergamon Press, Amsterdam (1999)
9. Cascetta, E., Gallo, M., Montella, B.: Models and algorithms for the optimization of signal
settings on urban networks with stochastic assignment. Ann. Oper. Res. 144, 301–328 (2006)
10. Castillo-Gonzalez, R., Clempner, J.B., Poznyak, A.S.: Solving traffic queues at controlled￾signalized intersections in continuous-time markov games. Math. Comput. Simul 166, 283–297
(2019)246 10 Multi-traffic Signal-Control Synchronization
11. Chen, O.J., Ben-Akiva, M.E.: Game-theoretic formulations of interaction between dynamic
traffic control and dynamic traffic assignment. Transp. Res. Rec. 1617, 179–188 (1998)
12. Chiou, S.W.: Optimization of area traffic control for equilibrium network flows. Transp. Sci.
33, 279–289 (1999)
13. Clempner, J.B., Poznyak, A.S.: Convergence method, properties and computational complexity
for lyapunov games. Int. J. Appl. Math. Comput. Sci. 21(2), 349–361 (2011)
14. Clempner, J.B., Poznyak, A.S.: Analysis of best-reply strategies in repeated finite markov
chains games. In: 52nd IEEE Conference on Decision and Control, pp. 568–573. Florence,
Italy (2013)
15. Clempner, J.B., Poznyak, A.S.: Modeling the multi-traffic signal-control synchronization: a
markov chains game theory approach. Eng. Appl. Artif. Intell. 43, 147–156 (2015)
16. D’Acierno, L., Gallo, M., Montella, B.: n ant colony optimisation algorithm for solving the
asymmetric traffic assignment problem. Eur. J. Oper. Res. 217, 459–469 (2012)
17. Dafermos, S.: Traffic equilibrium and variational inequalities. Transp. Sci. 14, 42–54 (1980)
18. Dafermos, S.: Relaxation algorithms for the general asymmetric traffic equilibrium problem.
Transp. Sci. 16, 231–240 (1982)
19. Fisk, C.S.: Game theory and transportation systems modelling. Transp. Res. B 18(4–5), 301–
313 (1984)
20. Fisk, C.S., Nguyen, S.: Solution algorithms for network equilibrium models with asymmetric
user costs. Transp. Sci. 16, 361–381 (1982)
21. Florian, M., Spiess, H.: The convergence of diagonalization algorithms for asymmetric network
equilibrium problems. Transp. Res. B 16, 477–483 (1982)
22. Galloa, M., D’Aciernob, L.: Comparing algorithms for solving the local optimisation of the
signal settings (loss) problem under different supply and demand configurations. Proc. Soc.
Behav. Sci. 87, 147–162 (2013)
23. Gartner, N.: Opac: a demand responsive strategy for traffic signal control. Transp. Res. Rec.
906, 75–81 (1983)
24. Gartner, N., Al-Malik, M.: Combined model for signal control and route choice in urban traffic
networks. Transp. Res. Rec. 1554, 27–35 (1996)
25. Gartner, N., Gershwin, S.B., Little, J.D.C., Ross, P.: Pilot study of computer-based urban traffic
management. Transp. Res. B 14(1–2), 203–217 (1980)
26. Gartner, N., Little, J., Gabby, H.: Simultaneous optimization of offsets, splits and cycle time.
Transp. Res. Rec. 596, 6–15 (1976)
27. Gartner, N., Stamatiadis, C.: Framework for the integration of dynamic traffic assignment with
real-time control. In: Proceedings of the 3rd Annual World Congress on Intelligent Transporta￾tion Systems, Orlando (1996)
28. Heydecker, B.: A decomposition approach for signal optimisation in road networks. Transp.
Res. B 30, 99–114 (1996)
29. Heydecker, B., Khoo, T.: The equilibrium network design problem. In: Proceedings of AIRO
’90, Conference on Models and Methods for Decision Support, Sorrento, pp. 587–602 (1990)
30. Lee, C., Machemehl, R.B.: Genetic algorithm, local and iterative searches for combining traffic
assignment and signal control. In: Proceedings of the Conference on Traffic and Transportation
Studies (ICTTS ’98), Beijing, China, pp. 27–29, (1998)
31. Meneguzzer, C.: Implementation and evaluation of an asymmetric equilibrium route choice
model incorporating intersection-related travel times. Master’s thesis, Ph.D. Dissertation,
Department of Civil Engineering, University of Illinois, Urbana (1990)
32. Meneguzzer, C.: An equilibrium route choice model with explicit treatment of the effect of
intersections. Transp. Res. B 29, 329–356 (1995)
33. Moya, S., Poznyak, A.S.: Extraproximal method application for a stackelberg-nash equilibrium
calculation in static hierarchical games. IEEE Trans. Syst. Man. Cybern. B Cybern. 39(6),
1493–1504 (2009)
34. Placzek, B.: A self-organizing system for urban traffic control based on predictive interval
microscopic model. Eng. Appl. Artif. Intell. 34, 75–84 (2014)References 247
35. Poznyak, A.S.: Advance Mathematical Tools for Automatic Control Engineers, vol. 2. Stochas￾tic Techniques. Elsevier, Amsterdam (2009)
36. Poznyak, A.S., Najim, K., Gomez-Ramirez., E.: Self-learning Control of Finite Markov Chains.
Marcel Dekker, Inc., New York, (2000)
37. Qu, Z., Xing, Y., Song, X., Duan, Y., Wei, F.: A study on the coordination of urban traffic
control and traffic assignment. Discrete Dyn. Nat. Soc. (2012). https://doi.org/10.1155/2012/
367468
38. Sheffi, Y., Powell, W.: Optimal signal settings over transportation networks. J. Transp. Eng.
109, 824–839 (1983)
39. Smith, M.J.: The existence, uniqueness and stability of traffic equilibria. Transp. Res. B 13,
295–304 (1979)
40. Smith, M.J.: Traffic control and route-choice; a simple example. Transp. Res. B 13, 289–294
(1979)
41. Smith, M.J.: A local traffic control policy which automatically maximizes the overall travel
capacity of an urban road network. Traffic Eng. Control. 21, 298–302 (1980)
42. Smith, M.J.: Properties of a traffic control policy which ensure the existence of a traffic equi￾librium consistent with the policy. Transp. Res. B 15, 453–462 (1981)
43. Tan, H.N., Gershwin, S.B., Athans, M.: Hybrid Optimization in Urban Traffic Networks. Tech￾nical Report, MIT Press, Cambridge (1979)
44. Tanaka, K., Yokoyama, K.: On.ϵ-equilibrium point in a noncooperative n-person game. J. Math.
Anal. 160, 413–423 (1991)
45. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: A stackelberg security game with random strategies
based on the extraproximal theoretic approach. Eng. Appl. Artif. Intell. 37, 145–153 (2015)
46. Wey, W.M.: Model formulation and solution algorithm of traffic signal control in an urban
network. Comput. Environ. Urban Syst. 24, 355–377 (2000)
47. Wong, S., Yang, H.: Reserve capacity of a signal-controlled road network. Transp. Res. B 31,
397–402 (1997)
48. Wong, S.C.: Group-based optimization of signal timings using parallel computing. Transp.
Res. C 5, 123–139 (1997)
49. Yang, H., Yagar, S.: Traffic assignment and signal control in saturated road networks. Transp.
Res. A 29, 125–139 (1995)
50. Ziyou, G., Yifan, S.: A reserve capacity model of optimal signal control with user-equilibrium
route choice. Transp. Res. B 36, 313–323 (2002)Chapter 11 
Non-cooperative Bargaining 
with Unsophisticated Agents 
Abstract In a conventional non-cooperative negotiating scenario, two or more 
forward-thinking participants make offers and counteroffers alternately until an 
agreement is achieved, with a penalty taking into account the length of time it takes 
players to make a decision. We provide a game that helps myopic participants achieve 
equilibrium as if they were forward-thinking agents. One of the game’s main mechan￾ics is that players are penalized for deviating from their prior best reply plan as well 
as for the amount of time they spend to make decisions at each stage of play. Our 
chapter adds to existing research on typical myopic agent bargaining while also 
broadening the class of processes and functions that may be used to define and apply 
Rubinstein’s non-cooperative bargaining solutions. 
11.1 Introduction 
There is a substantial and expanding body of research on non-cooperative bargaining. 
A bilateral non-cooperative bargaining process was represented by Rubinstein [ 38] 
as an alternating offers game with a cost for each round of negotiations. In several 
articles and scenarios, this model has been examined and expanded for three or 
more players (e.g., [ 3, 10, 11, 20, 23, 31, 35, 41]). The non-cooperative bargaining 
model and its game-theoretic solution have also been used in a variety of significant 
contexts, including market games [ 9, 39], networks [ 1, 2, 18, 25], apex games [ 27], 
union formation [ 22] and water management [ 13]. 
Despite its broad applicability, the traditional bargaining model makes several key 
assumptions, including that players are sophisticated in their behavior (for example, 
when agents are forward-looking or in the presence of externalities) and that players 
have complete information about the characteristics of other agents (for example, 
their discount factor or their utility). It has been demonstrated that the classic equi￾librium idea fails when agents lack sophistication, such as when they are myopic 
(egocentric) [ 21, 24, 32– 34, 40]. Therefore, it is necessary to create a general the￾ory of bargaining that is strong enough to function in the absence of knowledgeable 
individuals. 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_11 
249250 11 Non-cooperative Bargaining with Unsophisticated Agents
Consider the Rubinstein-proposed bargaining scenario, where two players must 
agree on how to divide a pie of size 1 and alternately make proposals to the other player 
at intervals of .0, , 2 , ... where . > 0, to serve as an example. Each suggestion 
outlines how the pie should be divided in.(x 1, x 2) ∈ R2
+ (the case where. x 1 + x 2 =
1). Also take into account the fixed discounting factors for each player. β1 = e(−r 1 )
and.β2 = e(−r 2 ), which are dependent on the individual player’s discount rate. r. 
Keep in mind that for this approach to work, agents must be aware of each other’s 
utility function and discount factor. Since agent 1’s ability to provide .x∗1 depends 
on.β2, this equilibrium does not hold when participants are unaware of the discount 
factor of others. 
Consider a similar naïve (unsophisticated) agent, who would just perform a best￾response equilibrium using their own knowledge, maybe uninformed or dismissing 
the utilities and actions of other participants. Consider, for example, that if a response 
rejects an offer, the game always continues, but the proposer never sees the game 
continue. The conventional Rubinstein model does not converge in such a case. 
In fact, take into account the scenario in which there is a set item that is divided 
between two players, for simplicity, use 1 as the value of the good. In each round 
of the negotiating process, each player’s optimal move is to obtain all for himself 
while making no offers to the other player, i.e., in the first round, player . 1 offers 
.x 2 = 0 to player . 2 and keeps .x 1 = 1 for himself, then player 2 rejects and the best 
response is to offer.x 1 = 0 to player 1 and keep.x 2 = 1 for himself. The conduct of 
both players will be the same in the following round, trapping both players in the 
optimal response situation, and this straightforward game will never have a solution. 
We suggest a different strategy to the conventional bargaining literature, where 
a planner has the ability to set up a game to help the agents reach an equilibrium. 
This strategy is intended to help implement bargaining solutions in the presence of 
inexperienced agents or those lacking knowledge about the characteristics of other 
agents. Consider the traditional bargaining game described above to better understand 
the characteristics of our game. In this game, the planner can penalize the agents based 
on two different criteria: first, they can be punished for deviating from the previous 
proposal, and second, they can be punished for the amount of time it takes to make 
a decision at each stage of the game. Our major finding demonstrates that, even for 
extremely modest penalties, the tatonnement process converges (in exponential time) 
over a very broad class of games and functions (Theorem 11.1). 
Another benefit of our game is that, for two players, it converges to a compromise 
solution that is equivalent to Rubinstein’s answer. It is obvious that when players are 
inexperienced, just imposing a severe penalty for deviating from the prior approach 
may not result in a compromise solution. In fact, the first agent can propose to obtain 
all resources for a sufficiently high penalty, and the second agent would lack the 
motivation to deviate since he would be required to pay a high penalty depending 
on his deviation. Similar to how agents could cycle in their offerings as mentioned 
above, a tiny penalty might not result in the implementation of a compromise solu￾tion. Our key contribution demonstrates that even in the presence of unsophisticated 
(and ignorant) players who just play the best-response, a very limited collection of11.1 Introduction 251
Table 11.1 Characteristics of the game and equilibrium proposed in comparison to the original 
Rubinstein’s model and solution 
Rubinstein’s equilibrium Proposed equilibrium 
Two-player game . . 
3 or more player game . 
Complete information . . 
Incomplete information . 
Alternating offers . . 
Discount factor . . 
Exists for rational players . . 
Exists for unsophisticated agents . 
Markov processes . 
penalties may ensure the convergence of the aforementioned technique. Additionally, 
this convergence holds true for a broad and extremely complete collection of stochas￾tic processes (like Markov processes) and utility functions (Sect. 11.4), in addition 
to the conventional processes and utilities utilized in classic bargaining literature. As 
a result, our study not only adds to existing research on conventional bargaining for 
inexperienced, ignorant actors but also broadens the range of processes and func￾tions that may be used to define and apply the classic Rubinstein’s non-cooperative 
bargaining problem (Table 11.1). 
In order to further illustrate our method and the solution obtained in comparison 
to Rubinstein’s, consider a game where two players have to reach an agreement on 
the partition of a fixed divisible good (for simplicity with value 1). Following the 
alternating offers model, we consider that players makes offers at times. 0, , 2 , ...
where. = 0.5. Both players have a discount rate.r = 0.5, then the fixed discounting 
factor for each player is the same and is given by.β = e(−r ) = e(−0.25) = 0.7788. The 
traditional Rubinstein’s solution computed.x 1∗ = x 2∗ = 0.5622, and implements the 
equilibrium as follows: 
• Player . 1 always offers .(0.5622, 0.4378) and always accepts at least . (0.4378,
0.5622), 
• Player . 2 always offers .(0.4378, 0.5622) and always accepts at least . (0.5622,
0.4378). 
Thus, when player 1 makes the first offer, the agreement is reached in the first offer, 
at time 0, and the final payoffs are.(0.5622, 0.4378). 
In comparison, while the solution employing our method considering a time 
function equal to the discount factor . β proposed by Rubinstein is as Table 11.2. 
Our method also penalizes players for deviating from the previous offer made 
by the players, .δ|xn − xn−1|. In our method, the agreement reached by players is 
.(0.5653, 0.4347) when player . 1 makes the first offer, and the final utilities after252 11 Non-cooperative Bargaining with Unsophisticated Agents
Table 11.2 Behavior of the offers and utilities from our method. The offers column (.x1 and . x2) 
represent the best response made by players in response to their utilities and the penalties imposed by 
the planner. The total penalty at step. n equals.δ|xn − xn−1|. The Accum. penalty column represent 
the accumulated penalty over all steps, and the utilities equal the offers made minus the penalty. 
Even with ten steps, both agents’ best response is equivalent at a small penalty/cost to the players 
Step Offers . δ .×1e4 .|xn −
xn−1|2
Accum. 
penalty 
Utilities 
.x1 .x2 .ψl
(x) . ψ2(x)
0 0.5 0.5 0.5 0.5 
1 .→ 1 0.7472 0.2528 0.7788 0.1222 0.00001 0.74719 0.25279 
2 .→ 2 0.4973 0.5027 0.6065 0.1249 0.00002 0.49728 0.50268 
1 .→ 3 0.6874 0.3126 0.3749 0.0723 0.00002 0.68738 0.31258 
2 .→ 4 0.5145 0.4855 0.2551 0.0598 0.00002 0.51448 0.48548 
1 .→ 5 0.67 0.33 0.1305 0.0484 0.00002 0.66998 0.32998 
2 .→ 6 0.539 0.461 0.0956 0.0343 0.00002 0.53898 0.46098 
1 .→ 7 0.6727 0.3273 0.0707 0.0358 0.00002 0.67268 0.32728 
2 .→ 8 0.5412 0.4588 0.0527 0.0346 0.00002 0.54118 0.45878 
1 .→ 9 0.5655 0.4345 0.0395 0.0012 0.00002 0.56548 0.43448 
2 .→ 10 0.5653 0.4347 0.0297 0.0000 0.00002 0.56528 0.43468 
the penalization equals.ψ1(x) = 0.56528 and.ψ2(x) = 0.43468, which is similar to 
Rubinstein’s solution. 
11.1.1 Related Literature 
The Rubinstein’s Alternate Bargaining Game [ 38] demonstrated the existence of 
equilibrium under complete knowledge through a non-cooperative bargaining pro￾cedure for two players splitting a fixed resource equal to. 1. Fudenberg and Tirole [15], 
who uses the idea of perfect Bayesian equilibrium to analyze and solve a two-player, 
two-period non-cooperative bargaining game with imperfect information. The modi￾fied evolutionary stable strategies (MESS) in Rubinstein’s alternating-offers, infinite￾horizon bargaining game were described by Binmore et al. [ 10]. They demonstrated 
that the presence of a MESS results in agreement being reached right away, with nei￾ther player preferring to postpone the agreement by one period in order to claim the 
other player’s portion of the surplus. In the particular subgame-perfect equilibrium 
of Rubinstein’s game, each player’s share of the surplus is then limited to the shares 
that they each got from the other player. 
By taking into account a.n-player bargaining issue where the utility possibility set 
is compact, convex, and strictly comprehensive, Kultti and Vartiainen [ 23] enhanced 
Rubinstein’s approach. They demonstrated the existence of a stationary subgame 
perfect Nash equilibrium and showed that all such equilibria converge to the Nash11.1 Introduction 253
bargaining solution if the Pareto surface is differentiable as the length of a time inter￾val between offers approaches zero, i.e., they demonstrated that the unique subgame 
perfect equilibrium result of the two-player alternating offers bargaining game pro￾posed by Rubinstein converges to the Nash bargaining solution [ 30] when the length 
of time interval approaches zero. 
It can be interpreted as a weighted majority game in which the major player has 
.n − 2 votes, each minor player has one vote, and .n − 1 votes are necessary for a 
majority. Montero [ 27] studied non-cooperative bargaining with random proposers 
in apex games, a.n-player game with one major player and.n − 1 ≥ 3 minor players. 
The egalitarian protocol picks each player as the proposer with an equal likelihood, 
while the proportional protocol selects each player with a probability proportionate 
to the amount of votes he receives. These are the two procedures that Montero took 
into consideration. 
The negotiating problem has been extensively researched for situations in which 
the participants are shortsighted, such in our scenario. A model of myopic tastes was 
introduced and described by Brown and Lewis [12], both in the context of intertempo￾ral decision-making and choosing under uncertainty, including an uncountable num￾ber of eras or global tastes. By establishing topologies on the space of consumption 
plans that discount unlikely or future occurrences, they formalized myopic (short￾sighted) behavior. With myopic agents and static equilibrium concepts similar to the 
Nash equilibrium, Marden [ 26] proposed a concept he called “state based potential 
games.” This introduces an underlying state space into the framework of potential 
games and shows how state based potential games can be applied to two coopera￾tive control problems, distributed resource allocation and local control design. These 
examples were presented in terms of Markov chains. 
The benefits and drawbacks of non-monotonic offers in alternating-offer bar￾gaining methods were discussed by Winoto et al. [ 52]. He imagined a negotiation 
between a buyer and a seller on a single characteristic item. The justification in 
this case is that certain agents could be risk-averse toward collapse in the future 
and myopic at a different level. Therefore, even if a superior offer later on becomes 
available, people can choose a safe but still viewed as ideal one. Rubinstein’s nego￾tiation between potentially time-inconsistent participants was investigated by Akin 
[ 4]. He looked at how learning and time inconsistency affected the choices made 
by various types of agents in a framework for bargaining. He took into account two 
players-one each of naive, sophisticated, partially naive, or exponential-playing in an 
infinite-horizon alternating-offer negotiation under the presumption that each player 
is aware of the type of the other player and that the naive players can learn about 
their own preferences. He demonstrated that the more ignorant a player is, the bigger 
the share obtained in a game between them and a time-consistent player. He also 
demonstrated that two naive players who never learn from their mistakes will always 
disagree. Players remain ignorant regardless of how the game changes since there is 
no learning; they do not update their beliefs or, as a result, their strategy. The current 
worth of what the naïve and somewhat naive agents believe their opponent expects 
to receive in exchange for rejecting is always what they give. But it turns out that 
from the standpoint of the opponent, rejection based on their ideas is always the best254 11 Non-cooperative Bargaining with Unsophisticated Agents
course of action since their views are different from what their opponent believes. 
Thus, each of them is so optimistic in their upcoming shares and obstinate that they 
insist on giving the other the identical rejected share. 
In contrast to previous work, our game is specified, and there are equilibria for 
a sizable class of games with utility functions and both complete and imperfect 
information. 
The main contributions of this chapter are as follows: 
– Introduces a new non-cooperative bargaining game between two o more myopic 
players that induce them to behave as if they were forward-looking players. 
– Consideres a time penalization related with the time spent for each player for the 
decision-making at each step of the negotiation process as well as their deviation 
from the previous best response strategy. Techniques from linear optimization, 
especially the proximal algorithm, were used to show the existence and feasible 
computability of the equilibrium. 
– Complements the study of bargaining for unsophisticated agents but also enlarges 
the class of processes and functions where non-cooperative bargaining model 
might be defined and applied. 
11.2 The Rubinstein’s Alternating-Offers Model 
Rubinstein [ 38] defined seminal bargaining situation for two players (.n = 2) who 
have to reach an agreement on the partition of a pie of size. 1. Each player takes turns to 
make an offer to the other agents on how the pie should be divided between them. After 
player. 1 has made such an offer, player. 2must decided whether to accept it, in this case 
the bargaining game ends and the players divide the cake according to the accepted 
offer, or to reject it and continue with the bargaining process. If player . 2 rejected, 
then this player has to make a counteroffer which player. 1 would accept or reject it 
and continue with the negotiation process. The bargaining game continues until an 
offer is accepted. Offers are made at discrete points in time, . 0, , 2 , ..., t , ...,
where. > 0, and players experience an exponential discount factor which might be 
different across agents. For a player . ι, and an offer .x∗ accepted at time . t, his final 
utility is.x∗
ι e(−rι
t ), where.βι = e(−rι
 ) is the discount factor associated to player. ι. 
1
Rubinstein’s [ 38] main results shows the existence of a subgame perfect equilib￾rium. Indeed, for 
.x 1∗ = 1 − β2
1 − β1β2 x 2∗ = 1 − β1
1 − β1β2
1 Reference [ 38] also studies the case of a fixed linear cost.cι
t , instead of exponential, associated 
at every step. Our work focuses on exponential discounting rather than linear since it produces 
richer solutions. 11.2 The Rubinstein’s Alternating-Offers Model 255
Fig. 11.1 The Pareto 
solution of the bargaining 
problem at time. 0
a subgame perfect equilibrium can be found when player . 1 always offers .x 1∗ and 
always accepts an offer .x 2 if and only if .x 2 ≤ x 2∗; and player . 2 always offers . x 2∗
and always accepts an offer.x 1 if and only if.x 1 ≤ x 1∗. Such an equilibrium is unique 
and reached at time zero when.rι > 0 for all. ι. 
Such a game can be extended to a general set.X of possible agreements. Indeed, 
consider two players bargaining over.X according to the alternating-offers as above, 
where player . ι has utility function .ψι : X → R and exponential discount factor 
.e(−rι
t ). We denote by . (X) = {(ψ1(x), ψ2(x))|x ∈ X} the set of possible utility 
pairs attainable at time 0, and. e denote the Pareto frontier 2 of the set. . 
When .X is a compact and convex set, and the utility functions are continuous 
and concave, the Pareto frontier . e can be represented by a graph function of a 
strictly decreasing and concave function, denoted by. φ, whose domain is an interval 
.I 1 ⊆ R and range an interval.I 2 ⊆ R. For simplicity, assume that.0 ∈ I 1,.0 ∈ I 2 and 
.φ(0) > 0 (see, Fig. 11.1). Then, 
. e = 
(ψ1
, ψ2
) : ψ1 ∈ I 1
, ψ2 ∈ φ(ψ1
)
 
Consider .φ−1 the inverse of . φ, a strictly decreasing and concave function from 
.I 2 to. I 1, with.φ−1(0) > 0. Then, for any.ψ1 ∈ I 1,.φ(ψ1) is the maximum utility that 
player. 2 receives subject to player. 1 receiving a utility.ψ1; in the same way, for any 
.ψ2 ∈ I 2, .φ−1(ψ2) is the maximum utility that player . 1 receives subject to player . 2
receiving a utility.ψ2. 
Let.Zι
, a non-empty subset of. X, defined as follows 
. Zι =
 
xι := arg max x∈X ψι
(x) : ψm(xι
) = βmψm(xm) , (m = ι)
 
. (11.2.1)
2 A utility pair.(ψ1, ψ2) ∈ e if and only if.(ψ1, ψ2) ∈ and there does not exist another utility 
pair.(ϕ1, ϕ2) ∈ such that.ϕ1 ≥ ψ1,.ϕ2 ≥ ψ2. 256 11 Non-cooperative Bargaining with Unsophisticated Agents
Proposition 11.1 For any .x∗ι ∈ Zι
, .ι = 1, 2, the following pair of strategies is a 
subgame perfect equilibrium of the general Rubinstein model: 
• Player 1 always offers.x 1∗ and always accepts an offer. x 2 if and only if. ψ1(x 2) ≥
β1ψ1∗
• Player 2 always offers.x 2∗ and always accepts an offer. x 1 if and only if. ψ2(x 1) ≥
β2ψ2∗
where.ψ1∗ = φ−1(β2ψ2∗) and. ψ2∗ = φ(β1ψ1∗).
The generality of this Bargaining model and the proof of this result can be found 
in [ 29]. Note that if .Zι contains more than one element, then there exist more than 
one subgame perfect equilibrium in the general Rubinstein model. In any subgame 
perfect equilibrium, if agreement is reached at time . 0 and it is player . 1 who makes 
the offer, then the equilibrium payoff for player. 1 is.ψ1∗ and for player. 2 is.φ(ψ1∗); 
similarly, if it is player. 2 who makes the offer at time. 0, then the equilibrium payoff 
for player . 1 is .φ−1(ψ2∗) and for player . 2 is .ψ2∗. This equilibrium pair is Pareto 
efficient (See Fig. 11.1). 
11.3 Bargaining with Unsophisticated Players 
We present a variation of [ 38]’s model that works when agents use a myopic best￾response behavior rather than the forward-looking behavior used by Rubinstein. The 
game presented below works for any time discount function.T : Rn
+ → Rn
+ that adds 
a set of penalties to agents that deviate from the previous proposed strategy. 
Definition 11.1 (Bargaining game with disagreement penalties) A bargaining game 
with disagreement penalties consists of: 
• A set of players .N = {1, ..., n} making offers alternatively following the order 
. 1, 2,..., n, 1, 2,...
• The common space of offers.X that is a convex and compact set. Once an offer is 
proposed, it needs to be accepted by all players for it to be final. 
• For a given path of offers.(x1, x2,..., xt), where.xl ∈ X and. xt the finally accepted 
offer, the payoff to agent . ι is given by .T ι
(t)(ψι
(xt) − D∗(x1, x2,... xt)), where 
.ψ : X → Rn
+ is a concave twice-differentiable real-valued function that represents 
the utility of agents from accepting. xt , and 
.D∗(x1, x2,... xt) = t
l=1
δl |(xl − xl−1)|2 (11.3.1) 
is the disagreement cost to the agents at the path.(x1, x2,... xt) for some arbitrary 
initial point.x0 ∈ X and a sequence of penalties.δ1, δ2, δ3,... .11.3 Bargaining with Unsophisticated Players 257
Like the general Bargaining model discussed in Sect. 11.2, we only assume that 
offers are made from an arbitrary set .X that is convex and compact and the utility 
function.ψι of every agent is a concave twice-differentiable real-valued function. In 
contrast with such a model, our time discount function.T ι
(t)is an arbitrary decreasing 
function that is not necessarily exponential. 
Our game introduces penalties (costs) imposed to agents when deviating from a 
previously proposed offer. This is captured by the function.D∗ that is increasing on 
both time to reach the offer, as well as the distance from the previously rejected offer. 
In particular, the cost at time . t equals the cost at time .t − 1 plus a penalty .δt of the 
distance from the previous offer,.δt |xt − xt−1|2
. 
3
Note that when the sequence of penalties are equal to zero,.δt = 0 for all. t, and the 
time function equals .T ι
(l) = e(−rι
l ), then this resembles the general Rubinstein’s 
model discussed in Sect. 11.2. 
Definition 11.2 (Tatonnement equilibrium) A path of offers .(x1, x2,..., xt) is a 
Tatonnement (unsophisticated best-response) equilibrium if at every step . l, where 
.1 ≤ l ≤ t the offer.xl proposed by agent. ι satisfies 
. xl ∈ arg max x∈X
T ι
(t)(ψι
(x) − D∗(x1, x2,... xl−1, x)). (11.3.2) 
Under a tatonnement equilibrium, agents choose their best response disregarding 
the behavior of the other agents. This is a standard equilibrium concept formulated 
from the early origins of Nash equilibrium. Benefits of such an equilibrium includes 
that agents only need their own information to make a decision. Furthermore, such 
an equilibrium works whether agents are sophisticated or not. 
Theorem 11.1 Consider an arbitrary time discount function .T : Rn
+ → Rn
+, and 
strategy space . X be a convex and compact polytope. Then, for any initial point. x0,
there exists a sequence of penalties.δ1, δ2,... such that the bargaining game with 
disagreement penalties for utility. ψ, time function. T and penalties.δ1, δ2,... have a 
tatonnement equilibrium that converges. Furthermore, if the derivative of the utility 
function. ψ is Lipschitz continuous with constant. K, the sequence.{xt} generated by 
the procedure, monotonically converges with exponential rate.q(δ, K) to one of the 
equilibria point. x∗, i.e., 
.|xt+1 − x∗|2 ≤ q(δ, K)
t+1
|x0 − x∗|2 as t → ∞. (11.3.3) 
The main implication of this theorem is that even when agents lack sophistica￾tion in their behavior, achieving a compromise equilibrium is possible by imposing 
monetary penalties in their utility. This result is remarkably strong, both on the time
3 While we currently assume that all the agents receive the same penalty .D∗(x1, x2,... xt), our 
work can be extended to asymmetric penalties, for instance, when only the proposing agents is 
penalized. We note that penalizing all the agents symmetrically guarantees a faster convergence 
than an asymmetric penalty. 258 11 Non-cooperative Bargaining with Unsophisticated Agents
Table 11.3 Number of steps 
needed for convergence of the 
strategies of the space.X as a 
function of the minimal 
distance. d between the 
strategy at time. t and an 
equilibrium 
Step . (9/13)t+1
1 0.479289941 
2 0.331816113 
3 0.229718847 
4 0.159036125 
5 0.110101933 
6 0.076224415 
7 0.052770749 
8 0.036533595 
9 0.025292489 
10 0.017510185 
11 0.012122436 
.
.
. .
.
.
18 0.000924026 
19 . 6.3971 × 10−4
20 . 4.42876 × 10−4
discount function used, and regardless of the initial point .x0 that is used. Further￾more, the class of problems that is covers is very general, as minimal concavity 
conditions on the utility functions are assumed. This will be illustrated in the follow￾ing section, where we apply this to very general problems that include, for instance, 
continuous-time Markov chains. 
Important to note is that unlike Rubinstein’s equilibrium, our equilibrium will 
not converge at time zero, as agents are not fully rational and take some time for 
the penalties to incentivize them reach a rational equilibrium outcome (the price of 
unsophistication). As such, there is an implicit loss in the efficiency, which can be 
expressed either as the speed of convergence that it takes for agents to reach a rational 
equilibrium or in terms of the size of penalties.D∗ that need to be imposed to agents 
for them to achieve the equilibrium. Indeed, Remark 11.1 shows that best speed of 
convergence of the process, which can be achieved by implementing the appropriate 
penalties to agents. Example 11.1 illustrates the price of unsophistication for the 
division of one unit of a good. 
Remark 11.1 The non-cooperative bargaining process converges with exponential 
rate .qmin = 9/13, which means that the best rate of convergence at each step of the 
bargaining process is.(9/13)t+1 (see, Table 11.3). 
Example 11.1 (The price of unsophistication for the simplex) In the following exam￾ple, we measure the cost of having players that are unsophisticated in relation to 
Rubinstein’s main result. As shown in the main result, we say that the cost .D at 
the equilibrium is small. Consider two players dividing a good with value 1, the11.4 An Extension to Continuous-Time Markov Chains 259
Table 11.4 Behavior of the offers and utilities 
Step Offers Utilities 
.x1 .x2 .ψl
(x) . ψ2(x)
0 1 0 1 0 
1 .→ 1 0.9997 0.0003 0.9996 0.0002 
2 .→ 2 0.7956 0.2044 0.7955 0.2043 
1 .→ 3 0.9363 0.0637 0.9362 0.0636 
2 .→ 4 0.7322 0.2678 0.7321 0.2677 
solution of Rubinstein when the discount rate.r = 1 and. = 1 is the same for each 
player .(0.7311, 0.2689). Now, for some parameter . δ for both players and an initial 
point.x0 = (1, 0), the solution obtained with our method is.(0.7322, 0.2678) and the 
utilities reached at each step of the process are as follows (Table 11.4). 
Our work generalizes the above example for any time function and a very general 
class of utility functions that are differentiable and satisfy a general Lipschitz con￾dition. Surprisingly, the unsophisticated best-response strategies converge in expo￾nential time. 
11.4 An Extension to Continuous-Time Markov Chains 
In this section, we extend the convergence results of the previous section to the case 
of bargaining in the presence of continuous-time Markov chains. In particular, our 
main results illustrate the computability of the solutions [ 46, 49, 50]. 
Definition 11.3 ([ 19]) A controllable continuous-time Markov chain is a 4-tuple 
.CT MC = (S, A, K, Q), (11.4.1) 
where: 
• . S is the state space, which is a finite set of states.
 
s(1), ...,s(N)
 
,.N ∈ N, endowed 
with a discrete topology; 
• .A is the set of actions, a finite space endowed with the corresponding Borel 
.σ−algebra .B(A). For each .s ∈ S,.A(s) ⊂ A is the non-empty set of admissible 
actions at state.s ∈ S; 
• .K = {(s, a)|s ∈ S, a ∈ A(s)} is the class of admissible state-action pairs, which 
is considered as a topological subspace of.S × A; 
• .Q is the matrix of the transition rates.
 
qj|ik 
, the transition from state.s(i) to state. s(j)
under an action.a(k) ∈ A
 
s(i)
 
,.k = 1, ..., M; satisfying.qj|ik ≥ 0 for all. (s, a) ∈ K
and.i = j such that260 11 Non-cooperative Bargaining with Unsophisticated Agents
. 
qj|ik 
=
 
 ⎨
 ⎩
− 
N
i = j
ρj|i(ak ), if i = j,
ρj|i(ak ), if i = j,
where.ρ(j|i) is a transition rate between state.s(i) and. sj ,.ρi = 
N
i = j
ρj|i . This matrix 
is assumed to be conservative, i.e.,.
 
N
j=1
qj|ik = 0 and stable, which means that 
. q∗
i := sup
a(k)∈A(s(i))
qi,k < ∞ ∀ s(i) ∈ S,
where.qi,k := −qi|i,k ≥ 0. 
Definition 11.4 A continuous-time Markov Decision Process is a pair 
.CTMDP = {CT MC, U}, (11.4.2) 
where: 
• CTMC is a controllable continuous-time Markov chain (11.4.1); 
• .U : S × K →. R is a utility function, associating to each state a real value. 
A strategy (policy) is defined as a sequence.d = {d(t), t ≥ 0} of stochastic kernels 
.d(t) such that: for each time .t ≥ 0, .dk|i(t) is a probability measure on .A such that 
.d(A(si)|i)(t) = 1 and for every .E ∈ B(A).d(E|i)(t) is a Borel measurable function in 
.t ≥ 0. Let us denote the collection.
 
dk|i(t)
 
by. D. 
Definition 11.5 A continuous-time Markov game is a pair 
.G = {N ,CTMDP}, (11.4.3) 
where: 
• CTMDP is a continuous-time Markov decision process (11.4.2); 
• .N = {1, ..., n} is the set of players, each player is indexed by.ι = 1, n. 
From now on, we will consider only stationary strategies.dι
k|i(t) = dι
k|i . For each 
strategy.dι
k|i the associated transition rate matrix is defined as: 
.Qι
(dι
) := [qι
j|i(dι
)] = 
M
k=1
qι
j|ikdι
k|i,11.4 An Extension to Continuous-Time Markov Chains 261
such that on a stationary state distribution for all.dι
k|i and.t ≥ 0 we have that. ι∗(d) =
lim
t→∞ eQι
(dι
)t (see [ 19]), where. ι∗ (dι
) is a stationary transition controlled matrix. 
11.4.1 Solution Method 
Let introduce the joint strategy.cι := 
cι
ik 
i=1,N;k=1,M which is a matrix with elements 
.cι
ik = dι
k|i Pι 
sι
=s(i)
 
, (11.4.4) 
and satisfies the following restrictions: 
1. Each vector from the matrix .cι := 
cι
ik 
represents a stationary mixed-strategy 
that belongs to the simplex 
. S N×M := 
cι
ik ∈ RN×M : cι
ik≥0,
 
N
i=1
 
M
k=1
cι
ik=1
 
.
2. The variable .cι
ik satisfies the continuous time and the ergodicity constraints, and 
belongs to the convex, closed and bounded set defined as follows: 
. cι ∈ Cι
adm =
 
hι
(j)(c) = 
N
i=1
 
M
k=1
πι
j|ik cι
ik − 
M
k=1
cι
(j,k)= 0,
 
N
i=1
 
M
k=1
qι
j|ik cι
ik=0 
.
Notice that by (11.4.4) it follows that 
.Pι 
sι
=s(i)
 
= 
M
k=1
cι
ik , dι
k|i = cι
ik
 
M
k=1
cι
ik
. (11.4.5) 
Considering a utility matrix.Uι
(j,i,k) and the transition matrix.πι
j|ik of each player, 
let us define a utility function of each player that represents the behavior of each 
player given by 
.Wι
ik = 
N
j=1
Uι
(j,i,k)πι
j|ik , (11.4.6) 
so that the “average utility function” in the stationary regime can be expressed as 
.ψι 
c1
, ..., cn 
:= 
N
i=1
 
M
k=1
Wι
ik n
ι=1
cι
ik . (11.4.7)262 11 Non-cooperative Bargaining with Unsophisticated Agents
Let us consider a game with players’ strategies denoted by .xι ∈ Xι . 
ι = 1, n
 
where.X := n
ι=1
Xι is a convex and compact set, 
. xι := col (cι
), Xι := Cι
adm,
where.col is the column operator. 
Denote by .x = (x 1, ..., xn) ∈ X, the joint strategy of the players and .xιˆ is a 
strategy of the rest of the players adjoint to. xι
, namely, 
. xιˆ := 
x 1
, ..., xι−1
, xι+1
, ..., xn 
∈ Xιˆ := n
h=1, h =ι
Xh,
such that.x = (xι
, xιˆ
),.ι = 1, n. 
The process to solve the non-cooperative bargaining game consists of two main 
steps: firstly to find the initial point of the negotiation (an ideal agreement that players 
can reach if they negotiate cooperatively, this point is the Pareto optimal solution of 
the bargaining game), the formulation and solution for this problem is called the 
strong Nash equilibrium (for the complete formulation, solution and convergence 
analysis see [ 47, 48]); finally, for the solution of the non-cooperative bargaining 
process we follow the model presented in Sect. 11.3. 
11.4.2 The Pareto Optimal Solution of the Bargaining 
Problem 
The Pareto set can be defined as [ 16, 17] 
.P := 
x∗ (λ) := arg max x∈X
 
 n
ι=1
λι ψι (x)
 
, λ ∈ Sn
 
, (11.4.8) 
such that 
. Sn := 
λ ∈ Rn : λ ∈ [0, 1] ,
 n
ι=1
λι = 1
 
for 
. ψ(x∗ (λ)) = 
ψ1 
x∗ (λ)
 
, ψ2 
x∗ (λ)
 
, ..., ψn 
x∗ (λ)
 .
The vector.x∗ is called a Pareto optimal solution for. P. Then, a strong Nash equilib￾rium is a strategy.x∗ = 
x 1∗, .., xn∗
 
such that11.4 An Extension to Continuous-Time Markov Chains 263
. ψ 
x 1∗, .., xn∗ 
> ψ 
x 1∗, .., xι
, ..., xn∗ 
for any.xι ∈ X,.xι = xι∗. 
Consider that players try to reach the strong Nash equilibrium, that is, to find a 
joint strategy.x∗ = 
x 1∗, ..., xn∗
 
.∈.X satisfying for any admissible.xι ∈ Xι and any 
. l = ι, n
. GL p
 
x(λ), xˆ(x, λ)
 
:= 
 n
ι=1
 
 
 
λι
 
ψι
 
xι
, xιˆ
 
− ψι
 
x¯ι
, xιˆ
 
 
 
p
 1/p
, (11.4.9) 
where .xˆ(x, λ) = (x 1ˆ , ..., xnˆ ) ∈ Xˆ ⊆ Rn(n−1)
, .p ≥ 1 ([ 42, 43]) and .x¯ι is the 
utopia point defined as follows, 
. x¯ι := arg max xι∈Xι ψι
 
xι
, xιˆ
 
. (11.4.10) 
Here .ψι 
xι
, xιˆ
 
is the cost-function of player . ι which plays the strategy . xι ∈ Xι
and the rest of players the strategy .xιˆ ∈ Xιˆ
. The functions .ψι 
xι
, xιˆ
 
, .ι = 1, n, are 
assumed to be concave in all their arguments. 
Remark 11.2 The function.GL p
 
x(λ), xˆ(x, λ)
 
satisfies the Nash condition 
. ψι 
xι
, xιˆ
 
− ψι 
x¯ι
, xιˆ
 
≤ 0
for any.xι ∈ Xι and all. ι = 1, n
Definition 11.6 A strategy.x∗ ∈ X is said to be a Strong.L p− Nash equilibrium if 
. x∗
L p ∈ Arg max x∈X,λ∈Sn
 
GL p
 
x(λ), xˆ(x, λ)
 .
Remark 11.3 If.GL p
 
x(λ), xˆ(x, λ)
 
is strictly concave then 
. x∗
L p = arg max x∈X,λ∈Sn
 
GL p
 
x(λ), xˆ(x, λ)
 .
Applying the Lagrange principle (see, for example, [ 37]) we may conclude 
. x∗
L p = arg max x∈X,xˆ(x)∈Xˆ ,λ∈Sn
min
μ≥0,ξ≥0,η≥0
Lδ(x, xˆ(x), λ, μ, ξ, η), (11.4.11) 
where264 11 Non-cooperative Bargaining with Unsophisticated Agents
. 
Lδ(x, xˆ(x), λ, μ, ξ, η) := GL p ,δ
 
x(λ), xˆ(x, λ)
 
− n
ι=1
 
N
j=1
μι
(j)hι
(j)(xι
)−
 n
ι=1
 
N
i=1
 
N
j=1
 
M
k=1
ξι
(j)qι
j|ik xι
ik − n
ι=1
 
N
i=1
 
M
k=1
ηι 
xι
ik − 1
 
+ δ
2
 
|μ|2 + |ξ|2 + |η|2 
and 
. 
GL p ,δ
 
x(λ), xˆ(x, λ)
 
= 
 n
ι=1
 
 λι 
ψι 
xι
, xιˆ
 
− ψι 
x¯ι
, xιˆ
 
 p
 1/p
− δ
2 (|x|2 + 
 xˆ(x)
 
 
2 + |λ|2
).
In order to find the Pareto optimal solution, the relation (11.4.11) can be expressed 
in the proximal format (see [ 6]) as 
.
μ∗
δ = arg min
μ≥0
 1
2 |μ − μ∗
δ|2 + γL◦(x∗
δ , xˆ∗
δ (x), λ∗
δ , μ, ξ∗
δ , η∗
δ )
 
,
ξ∗
δ = arg min ξ≥0
 1
2 |ξ − ξ∗
δ |2 + γL◦(x∗
δ , xˆ∗
δ (x), λ∗
δ , μ∗
δ , ξ, η∗
δ )
 
,
η∗
δ = arg min
η≥0
 1
2 |η − η∗
δ |2 + γL◦(x∗
δ , xˆ∗
δ (x), λ∗
δ , μ∗
δ , ξ∗
δ , η)
 
,
x∗
δ = arg max x∈X
 
−1
2 |x − x∗
δ |2 + γL◦(x, xˆ∗
δ (x), λ∗
δ , ξ∗
δ )
 
,
xˆ∗
δ (x) = arg max xˆ∈Xˆ
 
−1
2 | ˆx(x) − ˆx∗
δ (x)|2 + γL◦(x∗
δ , xˆ(x), λ∗
δ , ξ∗
δ )
 
,
λ∗
δ = arg max
λ∈SN
 
−1
2 |λ − λ∗
δ|2 + γL◦(x∗
δ , xˆ∗
δ (x), λ, ξ∗
δ )
 
,
⎫
 ⎬
 ⎭
(11.4.12) 
where the solutions .x∗
δ , .xˆ∗
δ (x), . λ∗
δ , .μ∗
δ , .ξ∗
δ and .η∗
δ depend on the small parameters 
.δ, γ > 0. 
11.4.3 The Non-cooperative Bargaining Solution 
In order to find the non-cooperative bargaining solution, let us define a time function 
that depends of the transition rates between states of each player as follows [ 14, 51] 
. τ ι
j|ik :=
 
 ⎨
 ⎩
1 
 
 
 
 
 N
i = j
qι
j|ik
 
 
 
 
 
if i = j
1
qι
j|ik
, if i = j.
(11.4.13) 
Also, let us redefined the utility function in Eq. (11.4.6) to involves the previous time 
function (11.4.13) 
.Wι
ik = 
N
j=1
 
τ ι
j|ik −1
Uι
(j,i,k)πι
j|ik , (11.4.14)11.4 An Extension to Continuous-Time Markov Chains 265
so that the average utility function in the stationary regime can be expressed as 
.ψι (x) := 
N
i=1
 
M
k=1
Wι
ik n
ι=1
cι
ik . (11.4.15) 
Then, let us define the norm of the strategies . x that depends on the transition time 
cost of each player as follows 
. 
 
 
 
x − x∗ 
 
2
 = n
ι=1
 
M
k=1
 
 
 
 
xι
(k) − xι∗
(k)
 
 
 
2
= n
ι=1
 
M
k=1
 
xι
(k) − xι∗
(k)
 T
 ι
(k)
 
xι
(k) − xι∗
(k)
 
,
(11.4.16) 
where 
. xι
(k) = (cι
(1,k), ..., cι
(N,k))
T ∈ RN , k = 1, M
and 
. ι
(k) :=
1
2
 
 ˜ ι
(k) + ˜ lT
(k)
 
, ˜ ι
(k) := 
τ ι
j|ik 
, ˜ ι
(k) ∈ RN×N .
Considering the utility function that depends on the average utility function . ψι (x)
defined as follows 
. 
Fι
(x, μ, ξ, η) := ψι (x) − ψι
(x∗) − 1
2
 n
ι=1
 
N
j=1
μι
(j)hι
(j)(xι
)−
1
2
 n
ι=1
 
N
i=1
 
N
j=1
 
M
k=1
ξι
(j)qι
j|ik xι
ik − 1
2
 n
ι=1
 
N
i=1
 
M
k=1
ηι 
xι
ik − 1
 
,
we may conclude that 
.x∗ = arg max x∈X
min
μ≥0,ξ≥0,η≥0
Fι
(x, μ, ξ, η). (11.4.17) 
Finally we have that the player in turn has to fix the strategies according to the 
solution of the non-cooperative bargaining problem in proximal format defined as 
follows 
.
μ∗ = arg min
μ≥0
 
δι
|μ − μ∗|2 + αιFι (x∗, μ, ξ∗, η∗)
 
,
ξ∗ = arg min ξ≥0
 
δι
|ξ − ξ∗|2 + αιFι (x∗, μ∗, ξ, η∗)
 
,
η∗ = arg min
η≥0
 
δι
|η − η∗|2 + αιFι (x∗, μ∗, ξ∗, η)
 
,
x∗ = arg max x∈X
 
−δι |(x − x∗)|2
 + αιFι (x, μ∗, ξ∗, η∗)
 
.
⎫
 ⎬
 ⎭
(11.4.18)266 11 Non-cooperative Bargaining with Unsophisticated Agents
11.5 Numeric Simulations 
11.5.1 Division of a Fix Resource 
Consider a bargaining situation where two players have to reach an agreement about 
the partition of certain amount of money. In the process, each player has to make 
in turn an offer, i.e., a proposal as to how it should be divided. Considering the 
bargaining model 1, we have that the bargaining problem is as follows: 
. x∗ = arg max x∈X
{
−δι
t t
ι
(x)
 
 
 
x − x∗ 
 
2 + αι
t t
ι
(x)
 
ψι
(x) − ψι
(x∗)
 }
,
where.x = [x 1, x 2],.x∗ = [x 1∗, x 2∗] and the vector. x belongs to the simplex 
. Sn := 
x ∈ Rn : x ∈ [0, 1] ,
 n
ι=1
xι = 1
 
.
The utility functions for the problem described above are as follows 
. ψ1
(x 1
, x 2
) = x 1
,
. ψ2
(x 1
, x 2
) = x 2
.
Then, we have the bargaining problem for each player as follows 
. xt+1 = arg max x∈X
 
−δι
t t
ι
(x) |(x − xt)|2 + αι
t t
ι
(x)
 
ψι
(x) − ψι
(xt)
 .
Once the player in turn makes a new offer according to equation above, the next 
player must decide either to accept or to reject the offer. If the player rejects the 
offer, then now it is his turn to calculate the strategies that benefit his utility and to 
make a new offer. 
For this example let’s consider a basic time function of the form .t = exp(−n), 
where . n is each step of the negotiation process, and both players with the same 
parameters values .α, δ. Considering Theorem 11.4, we fix initial values .n0 = 5, 
.α0 = 0.1 and .δ0 = 0.05, and to obtain the maximal rate of convergence we take 
.α = 2/3 and .δ = 1/3, i.e., at each step of the process after .n = 5 we compute the 
optimal parameters to ensure the maximal rate of convergence. Then, fixing an initial 
point.x0 = [0.7, 0.3], that means.70% for player. 1 and.30% for player. 2, and solving 
the bargaining problem presented above we obtain the behavior of the proposals 
during the process, see Fig. 11.2. The utilities obtained at each step of the process 
are shown in Table 11.5. 
For this example, the dynamics of the game is as follows. The first player to make 
a proposal is player . 1, who keep the .100% for himself and .0% for the other player. 
In the next step.n = 2, player. 2 rejects the offer and makes a new one, offering.50%11.5 Numeric Simulations 267
Fig. 11.2 Convergence of the utilities 
Table 11.5 Utilities of each 
player .x1 (%) .x2 (%) 
.70 . 30
1 .→ .100 . 0
2 .→ .50 . 50
1 .→ .99.03 . 0.97
2 .→ .49.03 . 50.97
1 .→ .97.50 . 2.5
2 .→ .47.54 . 52.46
1 .→ .84.51 . 15.49
2 .→ .49.96 . 50.04
1 .→ .70.27 . 29.73
2 .→ .56.90 . 43.10
1 .→ .56.90 . 43.10
to player. 1, but in the next step.n = 3 player. 1 rejects and offers.0.97% to player. 2. 
At .n = 4 player . 2 offers .49.03% which player . 1 rejects and offers .2.5% to player 
. 2. In the next step player . 2 decreases his offer to .47.54% for player . 1, this offer is 
rejected and player. 1 makes a new one, offering.15.49% to player. 2. At.n = 8 player 
. 2 offers.49.96% which player. 1 rejects and offers.29.73% to player. 2. Finally at step 
.10 player. 2 offers.56.90% to player. 1, this offer is accepted and the negotiation ends. 
Consider other scenarios of this same example. What happen if we set a dif￾ferent initial point? For example if we fix .x0 = [0.9, 0.1], .x0 = [0.5, 0.5] or . x0 =
[0.3, 0.7] we obtain the same final utilities for both players, but if the initial point 
is .x0 = [0.1, 0.9] then the utilities are different and player . 2 gets a greater utility 
than player . 1. Now, what happen if player 2 starts the process? Basically we have268 11 Non-cooperative Bargaining with Unsophisticated Agents
a symmetrical behavior, for some initial points like.x0 = [0.1, 0.9],.x0 = [0.3, 0.7], 
.x0 = [0.5, 0.5] or.x0 = [0.7, 0.3] both players obtain the same final utilities as in the 
previous scenario .ψ1(x) = 43.10% and .ψ2(x) = 56.90%, and for the initial point 
.x0 = [0.9, 0.1] the final utilities are different, i.e., player. 1 gets a greater utility than 
player. 2. 
11.6 Extensions 
We now present two extensions of the bargaining game provided above that include 
the case when agents have different discount factors, and another where agents might 
coordinate on their demands. The convergence of results follows trivially from our 
general analysis presented in the appendix above. 
11.6.1 Bargaining Under Different Discounting 
In this approach we present a solution where at each step of the negotiation process 
players calculate the Nash equilibrium considering the utility functions of all players 
but with the particularity that internally each player reaches this equilibrium point 
in a different time. Following the description of the model presented previously, we 
redefine the advantage of propose a new offer that depends on the utility function 
. f (xt, xt+1) := n
ι=1
 
ψι
(xt+1) − ψι
(xt)
 
≥ 0
for all players to reject the offer.xt and making a new offer.xt+1 given the time spent 
to benefit of this advantage.T (xt+1) > 0, and.αι
(xt) be the weight that players put on 
their advantages to reject the offer. xt . Thus, the advantages to reject the offer.xt and 
to propose a new offer.xt+1 are given by.A(xt, xt+1) = α(xt)T (xt+1) f (xt, xt+1). 
Remark 11.4 The function. f (xt, xt+1) satisfies the Nash condition 
. ψι
(xt+1) − ψι
(xt) ≥ 0
for any.xt+1 ∈ X and all players. 
Definition 11.7 A strategy.x∗ ∈ X is said to be a Nash equilibrium if 
. x∗ ∈Argmax xt∈X
{ f (xt, xt+1)}
for any.xt+1 ∈ X and all players.11.6 Extensions 269
Then, at each step of the bargaining game we have in proximal format that the 
players must select their strategies according to 
. x∗ = arg max x∈X
{
−δtT (x)
 
 
 
x − x∗ 
 
2 + αtT (x) f (x, x∗)
}
, (11.6.1) 
where 
. f (x, x∗) := n
ι=1
 
ψι
(x) − ψι
(x∗)
 
.
At each step of the bargaining process, players calculate simultaneously the Nash 
equilibrium but considering that each player reach the equilibrium in a different time. 
11.6.1.1 Markov Chains Interpretation 
Let us to define the Nash equilibrium as a strategy.x∗ = 
x 1∗, .., xn 
such that 
. ψ 
x 1∗, .., xn∗ 
≥ ψ 
x 1∗, .., xι
, ..., xn∗ 
for any.xι ∈ X. 
Consider that players try to reach the Nash equilibrium of the bargaining problem, 
that is, to find a joint strategy.x∗ = 
x 1∗, ..., xn∗
 
.∈.X satisfying for any admissible 
.xι ∈ Xι and any. ι = 1, n
. f (x, xˆ(x)) := n
ι=1
 
ψι
 
xι
, x ˆl
 
− ψι
 
x¯ι
, x ˆl
 (11.6.2) 
where.xˆ = (x 1ˆ , ..., xnˆ ) ∈ Xˆ ⊆ Rn(n−1) [ 42, 43],.x¯ι is the utopia point defined as 
Eq. (11.4.10) and.ψι 
xι
, xιˆ
 
is the concave cost-function of player. ι which plays the 
strategy.xι ∈ Xι and the rest of players the strategy.xιˆ ∈ Xιˆ defined as Eq. (11.4.15) 
considering the time function. 
Remark 11.5 The property . f (x, xˆ(x)) less or equal to . 0 is equivalent to the Nash 
condition 
.ψι 
xι
, xιˆ
 
− ψι 
x¯ι
, xιˆ
 
≤ 0 (11.6.3) 
for any.xι ∈ Xι and all. ι = 1, n.
Definition 11.8 A strategy.x∗ ∈ X is said to be a Nash equilibrium if 
. x∗ ∈ Arg max x∈Xadm
 
f (x, xˆ(x)) 
.
Remark 11.6 If. f (x, xˆ(x)) is strictly concave then270 11 Non-cooperative Bargaining with Unsophisticated Agents
. x∗ = arg max x∈Xadm
 
f (x, xˆ(x)) 
.
We redefine the utility function that depends of the average utility function of all 
players as follows 
. 
F(x, xˆ(x)) := f (x, xˆ(x)) − 1
2
 n
ι=1
 
N
j=1
μι
(j)hι
(j)(xι
)−
1
2
 n
ι=1
 
N
i=1
 
N
j=1
 
M
k=1
ξι
(j)qι
j|ik xι
ik − 1
2
 n
ι=1
 
N
i=1
 
M
k=1
ηι 
xι
ik − 1
 
,
then, we may conclude that 
.x∗ = arg max x∈X,xˆ∈Xˆ
min
μ≥0,ξ≥0,η≥0
F(x, xˆ(x), μ, ξ, η). (11.6.4) 
Finally we have that at each step of the bargaining process, players calculate the 
Nash equilibrium (but they reach the equilibrium at different time) according to the 
solution of the non-cooperative bargaining problem in proximal format defined as 
follows 
.
μ∗ = arg min
μ≥0
 
−δ|μ − μ∗|2 + αF 
x∗, xˆ∗(x), μ, ξ∗, η∗
 ,
ξ∗ = arg min ξ≥0
 
−δ|ξ − ξ∗|2 + αF 
x∗, xˆ∗(x), μ∗, ξ, η∗
 ,
η∗ = arg min
η≥0
 
−δ|η − η∗|2 + αF 
x∗, xˆ∗(x), μ∗, ξ∗, η
 ,
x∗ = arg max x∈X
 
−δ |(x − x∗)|2
 + αF 
x, xˆ∗(x), μ∗, ξ∗, η∗
 ,
xˆ∗ = arg max xˆ∈Xˆ
{
−δ
 
 
 
xˆ − ˆx∗
 
 
2
 + αF 
x∗, xˆ(x), μ∗, ξ∗, η∗
 }
.
⎫
 ⎬
 ⎭
(11.6.5) 
11.6.2 Bargaining with Collusive Behavior 
In this approach we analyze a bargaining situation where players make groups and 
alternately each group makes an offer to the others until they reach an equilibrium 
point (agreement). We describe a bargaining model with two teams of players as 
follows. Let us consider a bargaining game with.n + m players. Let . N = {1, ..., n}
denote the set of players called team A and let’s define the behavior of all players 
.ι = 1, n as.xt = (x 1
t , ..., xn
t ) ∈ X, where.X is a convex and compact set. In the same 
way, the rest .M = {1, ..., m} players are the team B and let the set of the strategy 
profiles of all player .m = 1, m be defined by .yt = (y1
t , ..., ym
t ) ∈ Y , where .Y is a 
convex and compact set. Then,.X × Y in the set of full strategy profiles. In this model 
the function .ψ(x, y) represents the utility function of team A which determines the11.6 Extensions 271
decision of accept or reject the offer; similarly, team B makes the decision according 
to its utility function.ϕ(x, y). 
Following the description of the model presented above, we redefine the advantage 
of propose a new offer considering the utility function for team A as follows 
. f (xt, yt, xt+1, yt+1) := n
ι=1
 
ψι
(xt+1, yt) − ψι
(xt, yt)
 
≥ 0,
and, similarly the utility function for team B is as follows 
. g(xt, yt, xt+1, yt+1) := m
m=1
 
ϕι
(xt, yt+1) − ϕι
(xt, yt)
 
≥ 0.
Thus, the advantages for team A to reject the offer.xt and to propose a new offer. xt+1
are given by .A(xt, yt, xt+1, yt+1) = α(xt)T (xt+1) f (xt, yt, xt+1, yt+1); in the same 
way, the advantages for team B to reject the offer.yt and to propose a new offer. yt+1
are given by 
. A(xt, yt, xt+1, yt+1) = α(yt)T (yt+1)g(xt, yt, xt+1, yt+1).
Remark 11.7 The nonpositivity of the function . f (xt, yt, xt+1, yt+1) is equivalent 
to the Nash condition 
. ψι
(xt+1, yt) − ψι
(xt, yt) ≥ 0
for any.x ∈ X,.y ∈ Y and.ι = 1, n players. 
Remark 11.8 The nonpositivity of the function.g(xt, yt, xt+1, yt+1) is also equiva￾lent to the Nash condition 
. ϕι
(xt, yt+1) − ϕι
(xt, yt) ≥ 0
for any.x ∈ X,.y ∈ Y and.m = 1, m players. 
The dynamics of the bargaining game is as follows: at each step of the negoti￾ation process the team A chooses a strategy .x ∈ X considering the utility function 
. f (xt, yt, xt+1, yt+1), then team B must decide between to accept or reject the offer 
calculating a new offer (strategies) .y ∈ Y considering the utility function of the 
group .g(xt, yt, xt+1, yt+1). Following the description of the model 1, now we have 
that teams solve the problem in proximal format as follows:272 11 Non-cooperative Bargaining with Unsophisticated Agents
.
x∗ = arg max x∈X
 
−δtT (x) |(x − x∗)|2 + αtT (x) f (x, y, x∗, y∗)
 
,
y∗ = arg max
y∈Y
 
−δtT (y) |(y − y∗)|2 + αtT (y)g(x, y, x∗, y∗)
 
,
(11.6.6) 
where 
. 
f (x, y, x∗, y∗) := n
ι=1
[ψι
(x, y∗) − ψι
(x∗, y∗)] ,
g(x, y, x∗, y∗) := m
m=1
[ϕm(x∗, y) − ϕm(x∗, y∗)] .
At each step, teams make a new offer according to equation (11.6.6), both teams 
solve the bargaining problem together but they reach the equilibrium at different 
time, the bargaining game continues until the offers (strategies) of all player show 
convergence. 
11.6.2.1 Markov Chains Interpretation 
For this model, in the same way that we define the strategies.x ∈ X, let us consider 
a set of strategies denoted by .ym ∈ Y m .
 
m = 1, m
 
where .Y := m
m=1
Y ι is a convex 
and compact set, 
. ym := col (cm), Y m := Cm
adm,
where.col is the column operator. 
Denote by .y = (y1, ..., ym) ∈ Y , the joint strategy of the players and .ymˆ is a 
strategy of the rest of the players adjoint to.ym, namely, 
. ymˆ := 
y1
, ..., ym−1
, ym+1
, ..., ym 
∈ Y mˆ := m
h=1, h =m
Y h,
such that.y = (ym, ymˆ),.m = 1, m. 
Consider that players of team A try to reach the Nash equilibrium of the bargaining 
problem, that is, to find a joint strategy .x∗ = 
x 1∗, ..., xn∗
 
.∈.X satisfying for any 
admissible.xι ∈ Xι and any. ι = 1, n
. f (x, xˆ(x)|y) := n
ι=1
 
ψι
 
xι
, x ˆl
|y
 
− ψι
 
x¯ι
, xιˆ
|y
 ≤ 0, (11.6.7) 
where.xˆ = (x 1ˆ , ..., xnˆ ) ∈ Xˆ ⊆ Rn(n−1) [ 42, 43],.x¯ι is the utopia point defined as 
Eq. (11.4.10) and.ψι 
xι
, xιˆ
|y
 
is the concave cost-function of player. ι which plays11.6 Extensions 273
the strategy.xι ∈ Xι and the rest of players the strategy.xιˆ ∈ Xιˆ fixing the strategies 
.y ∈ Y of team B, and it is defined as Eq. (11.4.15) considering the time function. 
Similarly, consider that players of team B also try to reach the Nash equilibrium 
of the bargaining problem, that is, to find a joint strategy .y∗ = 
y1∗, ..., ym∗
 
.∈ . Y
satisfying for any admissible.ym ∈ Y m and any. m = 1, m
.g(y, yˆ(y)|x) := m
m=1
 
ψm 
ym, ymˆ
|x
 
− ψm 
y¯
m, ymˆ
|x
 ≤ 0, (11.6.8) 
where .yˆ = (y1ˆ , ..., ymˆ ) ∈ Yˆ ⊆ Rm(m−1)
, .y¯m is the utopia point defined as Eq. 
(11.4.10) and.ψm 
ym, ymˆ |x
 
is the concave cost-function of player. m which plays the 
strategy .ym ∈ Y m and the rest of players the strategy .ymˆ ∈ Y mˆ fixing the strategies 
.x ∈ X of team A, and it is defined as Eq. (11.4.15) considering the time function. 
Then, we have that a strategy .x∗ ∈ X of team A together with the collection 
.y∗ ∈ Y of team B are defined as the equilibrium of a strictly concave bargaining 
problem if 
. (x∗, y∗) = arg max x∈Xadm,y∈Yadm
 
f (x, xˆ(x)|y) ≤ 0, g(y, yˆ(y)|x) ≤ 0
 
We redefine the utility function that depends of the average utility function of all 
players as follows 
. 
F(x, xˆ(x), y, yˆ(y)) := f (x, xˆ(x)|y) + g(y, yˆ(y)|x) − 1
2
 n
ι=1
 
N
j=1
μι
(j)hι
(j)(xι
)−
1
2
 m
m=1
 
N
j=1
μm
(j)hm
(j)(ym) − 1
2
 n
ι=1
 
N
i=1
 
N
j=1
 
M
k=1
ξι
(j)qι
j|ik xι
ik−
1
2
 m
m=1
 
N
i=1
 
N
j=1
 
M
k=1
ξm
(j)qm
j|ik ym
ik − 1
2
 n
ι=1
 
N
i=1
 
M
k=1
ηι 
xι
ik − 1
 
−
1
2
 m
m=1
 
N
i=1
 
M
k=1
ηm 
ym
ik − 1
 
,
then, we may conclude that 
.(x∗, y∗) = arg max x∈X,xˆ∈Xˆ ,y∈Y,yˆ∈Yˆ
min
μ≥0,ξ≥0,η≥0
F(x, xˆ(x), y, yˆ(y), μ, ξ, η). (11.6.9) 
Finally we have that at each step of the bargaining process, players calculate their 
equilibrium according to the solution of the non-cooperative bargaining problem in 
proximal format defined as follows274 11 Non-cooperative Bargaining with Unsophisticated Agents
. 
μ∗ = arg min
μ≥0
 
−δ|μ − μ∗|2 + αF 
x∗, xˆ∗(x), y∗, yˆ∗(y), μ, ξ∗, η∗
 
ξ∗ = arg min ξ≥0
 
−δ|ξ − ξ∗|2 + αF 
x∗, xˆ∗(x), y∗, yˆ∗(y), μ∗, ξ, η∗
 
η∗ = arg min
η≥0
 
−δ|η − η∗|2 + αF 
x∗, xˆ∗(x), y∗, yˆ∗(y), μ∗, ξ∗, η
 
x∗ = arg max x∈X
 
−δ |(x − x∗)|2
 + αF 
x, xˆ∗(x), y∗, yˆ∗(y), μ∗, ξ∗, η∗
 
xˆ∗ = arg max xˆ∈Xˆ
{
−δ
 
 
 
xˆ − ˆx∗
 
 
2
 + αF 
x∗, xˆ(x), y∗, yˆ∗(y), μ∗, ξ∗, η∗
 }
y∗ = arg max
y∈Y
 
−δ |(y − y∗)|2
 + αF 
x∗, xˆ∗(x), y, yˆ∗(y), μ∗, ξ∗, η∗
 
yˆ∗ = arg max
yˆ∈Yˆ
{
−δ
 
 
 
yˆ − ˆy∗
 
 
2
 + αF 
x∗, xˆ∗(x), y∗, yˆ(y), μ∗, ξ∗, η∗
 }
⎫
 ⎬
 ⎭
(11.6.10) 
11.7 Appendix: Proofs 
11.7.1 The Non-cooperative Bargaining Game 
In this section, we present a general version of the model presented in Sect. 11.3. 
Consider the game theory problem of a concave twice-differentiable real-valued 
function. ψ defined on. X, which is a compact and convex subset of. RN
. max
x∈X ψ(x).
Following the proximal point algorithm for solving game theory problems pre￾sented by [ 5], the unique solution is a sequence.(xt).y ∈ N with a initial value.x0 ∈ X, 
.max
x∈X
 
ψ(x) − δt |x − xt|2 
, (11.7.1) 
where .δt > 0, .δt ↓ 0 and the term .|x − xt|2 ensures that the objective function 
(11.7.1) is strictly positive definite and that some iterative method presents conver￾gence [ 44, 45]. The result obtained is not affected by the quadratic term for . δt > 0
and.δt ↓ 0. 
The bargaining game model considered in this paper involves game theory prob￾lems with an additional penalization, a time cost related with the time spent for each 
player to move from one position to another one [ 7, 8, 28], i.e., to decide either to 
accept an offer or to reject it and choose another. 
The Bargaining Model 
In this approach, we consider the model presented by [ 38], and we provide a solu￾tion to a bargaining situation where players are individual-rational and alternately11.7 Appendix: Proofs 275
make offers and counteroffers thinking only of their own interests, i.e., they compute 
independently the strategies that maximize only their own utility. 
In general terms, the dynamic of the multilateral non-cooperative bargaining game 
is as follows. The game consists of a set.N = {1, ..., n} of players bargaining a certain 
transaction according to the alternating-offers procedure. Define the behavior of 
each player .ι = 1, n as a sequence .xι
t ∈ Xι
, .n ∈ N, where .Xι is the decision space 
(strategies) of each player. Then, we can define the strategies set of all players as 
.xt = (x 1
t , ..., xn
t ) ∈ X where .X is a convex and compact set. Players take turns to 
analyze and present their position in the negotiation process, i.e., at each step . n the 
player . l in turn must decide between to stay in the same strategy .xt+1 = xt , that is 
that player . l accepts the offer, or to choose a new strategy .xt+1 = xt , that means 
that player rejects the offer and makes a new one. The function.ψι
(x) represents the 
utility function of each player which determines the decision of to accept or to reject 
the offer. 
At turn.n = 0, the first player to make an offer chooses a strategy set. xt considering 
the utility function.ψι
(x), then, the rest of the players must decided either to accept 
the offer and finish the game or to reject it and continue with the process, in this 
case, at step.n = 1 the next player makes a counteroffer by choosing a strategy set. xt
that benefits him more or in equal measure than the offer proposed by the first player 
according to his utility function, if this counteroffer is accepted then agreement is 
struck, otherwise, the player in turn makes a new offer at step.n = 2, and the process 
continues. 
The time cost between offers is defined for each player as a function . ι :
X × X → R which can be interpreted as a distance function of each player where 
. ι
(xt, xt+1) = κι
(xt, xt+1), we have that .κι
(xt, xt+1) = 0 if .xt+1 = xt (accepts the 
offer) or .κι
(xt, xt+1) > 0 if .xt = xt+1 (rejects and makes a new one). In general, 
the time cost function can be reexpressed as. ι
(xt, xt+1) := T ι
(xt, xt+1)κι
(xt, xt+1)
where .T ι
(xt, xt+1) ≥ 0 is the time spent for each player to reject an offer .xt and to 
make a new one .xt+1 and .κι
(xt, xt+1) is the offer cost function associated to each 
player. 
In the simplest case, each player makes a new offer trying to obtain the highest 
possible payoff according to the utility function, .ψι
(xt+1) − ψι
(xt) ≥ 0 given the 
time spent.T ι
(xt+1) > 0 to analyze the advantage of to reject the offer.xt and make 
a new offer.xt+1, and.αι
(xt) be the weight that players put on their advantages of to 
reject the offer. xt . Thus, the advantages of to reject the offer.xt and to propose a new 
offer.xt+1 are given by.Aι
(xt, xt+1) = αι
(xt)T ι
(xt+1)
 
ψι
(xt+1) − ψι
(xt)
 
. 
The dynamics of the bargaining game with alternating-offers considering the 
time cost is as follows. At each step.n ∈ N, the player in turn considers to reject the 
offer .xt and propose a new offer .xt+1. For each player, to make a new proposal is 
acceptable if the advantages.Aι
(xt, xt+1) are determined by.δι
(xt) ∈ [0, 1] (degree of 
acceptability) of the time cost. ι
(xt, xt+1). Then, the set of strategies that maximizes 
the utility of each player is defined by 
. Fι(xt) = 
xt+1 ∈ X : αι(xt)T ι(xt+1)
 
ψι(xt+1) − ψι(xt)
 
≥ δι(xt)T ι(xt+1)κι(xt, xt+1)
 
.276 11 Non-cooperative Bargaining with Unsophisticated Agents
We define a utility function .ψι : X → R such that the impact of experience on 
cost is constant and limited to the most recent element .xt on the trajectory .(xt). 
In addition, the advantages to change .Aι
(xt, xt+1) are determined by the degree of 
acceptability.δι
t(xt) ∈ [0, 1] of the costs to move. ι
(xt, xt+1). 
Thus, the acceptance criterion to propose a new offer satisfies the condition 
. αι
t(xt)T ι
(xt+1)
 
ψι
(xt+1) − ψι
(xt)
 
≥ δι
t(xt)T ι
(xt+1)κι
(xt, xt+1).
This algorithms are naturally linked with several classical proximal algorithms 
given in Eq. (11.7.1). That is, by taking . δι
t T ι
(x)κι
(x∗, x) = δι
t T ι
(x) |(x − x∗)|2
and .Aι
(x, x∗) := αι
t tι
(x)[ψι
(x) − ψι
(x∗)], the point .x∗ solves the maximization 
problem if remains a fixed point of the proximal mapping, that is, 
.x∗ = arg max x∈X
{
−δι
t T ι
(x)
 
 
 
x − x∗ 
 
2 + αι
tT ι
(x) f (x, x∗)
}
, (11.7.2) 
where 
. f (x, x∗) := ψι
(x) − ψι
(x∗).
Once the player in turn makes a new offer according to equation (11.7.2), the next 
player must decide either to accept or to reject the offer. If the player rejects the offer, 
then now it is his turn to calculate the strategies that benefit his utility and to make 
a new offer. This process continues until an agreement is reached, i.e. the proposals 
(strategies) of the players do not change (convergence). 
11.7.2 Formulation of the Problem 
Consider the following constrained programming problem 
.
max
x∈Xadm
f (x, xt),
Xadm := 
x ∈ Rn : x ≥ 0, A0x = b0 ∈ RM0 , A1x ≤ b1 ∈ RM1
 
,
⎫
 ⎬
 ⎭
(11.7.3) 
where.Xadm is a bounded set. Introducing the vector.u ∈ RM1 with components. ui ≥ 0
for all.i = 1, ..., M1, the original problem (11.7.3) can be rewritten as 
.
max x∈Xadm,u≥0 f (x, xt),
Xadm := {x ∈ Rn : x ≥ 0, A0x = b0, A1x − b1 + u = 0},
⎫
 ⎬
 ⎭
(11.7.4)11.7 Appendix: Proofs 277
Notice that this problem may have non-unique solution and.det 
AT
0 A0
 
= 0. Define 
by.X∗ ⊆ Xadm the set of all solutions of the problem (11.7.4) and consider the objec￾tive function 
.
Pα,δ (x, u|xt) := −
δ
2
T (x) |x − xt|2 + αT (x) f (x, xt)−
1
2 |A0x − b0|2 − 1
2 |A1x − b1 + u|2 − δ
2 |u|2 ,
(11.7.5) 
where the parameters. α,. δ. Then, the game theory problem is as follows 
. max x∈Xadm,u≥0
Pα,δ (x, u|xt). (11.7.6) 
11.7.3 Convergence Analysis 
The game consists of a set.N = {1, ..., n} of players. Let.xι ∈ Xι be the strategy of 
each player.l = 1, n where.Xι is the decision space (strategies) of each player. Then, 
we can define the strategies set of all players as 
. x = (x 1
, ..., xn) ∈ X, X := n
ι=1
Xι
,
where X is a convex and compact set. Then, in order to prove Theorem 11.1, let’s 
present the following lemma and its proof. 
Lemma 11.1 The bounded set. X∗ of all solutions of the original game theory prob￾lem (11.7.4) is not empty and the Slater’s condition holds, that is, there exists a point 
.x˚ ∈ Xadm such that 
.A1x˚ < b1. (11.7.7) 
Moreover, the parameters. α and. δ are time-varying, i.e., 
. α = αt, δ = δt (n = 0, 1, 2, ....),
such that 
.0 < αt ↓ 0, αt
δt
↓ 0 when n → ∞. (11.7.8) 
Then 
.
x∗
t := x∗ (αt, δt) →n→∞ x∗∗,
u∗
t := u∗ (αt, δt) →n→∞ u∗∗,278 11 Non-cooperative Bargaining with Unsophisticated Agents
where .x∗∗ ∈ X∗ and.u∗∗ ∈ RM1 define the solution of the original problem (11.7.4) 
with the minimal weighted norm, 
. 
 
 x∗∗ 
 
2 + 
 u∗∗ 
 
2
≤ 
 x∗
 
 
2 + 
 u∗
 
 
2
,
for all.x∗ ∈ X∗,.u∗ ∈ RM1 and 
. u∗∗ = b1 − A1x∗∗.
Proof 1. First, let us prove that the Hessian matrix . H associated with the objective 
function (11.7.5) is strictly negative definite for any positive. α and . δ, to show that 
the objective function (11.7.5) is strictly concave. If the set of solutions of problem 
(11.7.4) is non-empty then the objective function (11.7.5) is strictly concave. 
It should be proven that for all.x ∈ Rn and. u ∈ RM1
. H =
⎡
⎢
⎣
∂2
∂x 2 Pα,δ (x, u|xt)
∂2
∂u∂x
Pα,δ (x, u|xt)
∂2
∂x∂u
Pα,δ (x, u|xt)
∂2
∂u2 Pα,δ (x, u|xt)
⎤
⎥
⎦
< 0,
Employing Schur’s lemma [ 36] it is necessary and sufficient to prove that 
. 
1. ∂2
∂x2 Pα,δ (x, u|xt) < 0, 2. ∂2
∂u2 Pα,δ (x, u|xt) < 0,
3. ∂2
∂x2 Pα,δ (x, u|xt) < ∂2
∂u∂x
Pα,δ (x, u|xt)
 ∂2
∂u2 Pα,δ (x, u|xt)
 −1 ∂2
∂x∂u
Pα,δ (x, u|xt).
Applying the Schur’s lemma over the objective function (11.7.5) it follows for con￾dition 1 
. 
∂2
∂x 2 Pα,δ (x, u|xt) = −δT (xt)In×n + αT (xt)
∂2
∂x 2 f (x, xt) − AT
0 A0 − AT
1 A1 ≤
αT (xt)
∂2
∂x 2 f (x, xt) − δT (xt)In×n ≤ δT (xt)
 α
δ
λ+ − 1
 
In×n < 0,
for all.δ > 0, where 
. λ+ := max
x∈Xadm 
λmax ( ∂2
∂x 2 f (x, xt)
) < 0.
Then, for condition 2 we have 
. 
∂2
∂u2 Pα,δ (x, u|xt) = − (1 + δ) IM1×M1 < 0.
By condition 3, it is necessary to satisfy that11.7 Appendix: Proofs 279
. 
∂2
∂x2 Pα,δ (x, u|xt) = −δT (xt)In×n + αT (xt)
∂2
∂x2 f (x, xt) − AT
0 A0 − AT
1 A1 <
∂2
∂u∂x
Pα,δ (x, u|xt)
 
∂2
∂u2 Pα,δ (x, u|xt)
 −1
∂2
∂x∂u
Pα,δ (x, u|xt) = − (1 + δ)
−1 AT
1 A1,
or equivalently, 
. αt(xt)
∂2
∂x 2 f (x, xt) − δT (xt)In×n − AT
0 A0 − δ
1 + δ
AT
1 A1 < 0,
which holds for any.δ > 0 having 
. 
T (xt)
 
αλ+ − δ
 
In×n − AT
0 A0 − δ
1 + δ
AT
1 A1 ≤
δT (xt)
 α
δ
λ+ − 1
 
In×n = δT (xt)(o(1) − 1) In×n < 0.
As a result, the Hessian is .H < 0 which means that proximal function (11.7.5) is 
strictly concave and, hence, has a unique maximal point defined below as . x∗ (α, δ)
and.u∗ (α, δ). 
2. If the proximal function (11.7.5) is strictly concave then the sequence.{xt} of 
the proximal function (11.7.5) converges when.n → ∞, i.e. the proximal function 
has a maximal point defined by .x∗ (α, δ) and .u∗ (α, δ). 
Following the strictly concavity property (Theorem 11.1) for any.y := ( x
u
)
and 
any vector .y∗
t .:= .
( x∗
t = x∗ (αt, δt)
u∗
t = u∗ (αt, δt)
)
for the function . Pα,δ (x, u|xt) = Pα,δ (y|xt)
we have 
. 
0 ≤ 
y∗
t − y
 T ∂
∂y
Pαt,δt
 
y∗
t |xt
 
= 
x∗
t − x
 T ∂
∂x
Pαt,δt
 
x∗
t , u∗
t |xt
 
+ 
u∗
t − u
 T ∂
∂u
Pαt,δt
 
x∗
t , u∗
t |xt
 
= 
x∗
t − x
 T
(
−δtT (xt)(x∗
t − xt)+αtT (xt)
∂
∂x f
 
x∗
t , xt
 
− AT
0
 
A0x∗
t − b0
 
−AT
1
 
A1x∗
t − b1 + u∗
t
 + 
u∗
t − u
 T 
−A1x∗
t + b1 − (1 + δt) u∗
t
 
= αtT (xt)
 
x∗
t − x
 T ∂
∂x f
 
x∗
t , xt
 
− 
A0
 
x∗
t − x
 T 
A0x∗
t − b0
 
− 
A1
 
x∗
t − x
 T 
A1x∗
t − b1 + u∗
t
 
− δtT (xt)
 
x∗
t − x
 T (x∗
t − xt)
− 
u∗
t − u
 T 
A1x∗
t − b1 + (1 + δt) u∗
t
 
.
(11.7.9)280 11 Non-cooperative Bargaining with Unsophisticated Agents
Now, selecting.x := x∗ ∈ X∗ (.x∗ is one of the admissible solutions such that. A0x∗ =
b0 and.A1x∗ = b1 − u∗) and.u := (1 + δt)
−1 
b1 − A1x∗
t
 
we obtain 
. 
0 ≤ αtT (xt)
 
x∗
t − x∗
 T ∂
∂x f
 
x∗
t , xt
 
− 
A0
 
x∗
t − x∗
 T 
A0x∗
t − b0
 
− 
A1
 
x∗
t − x∗
 T 
A1x∗
t − b1 + u∗
t
 
− δtT (xt)
 
x∗
t − x∗
 T (x∗
t − xt)−
(1 + δt)
−1 
u∗
t (1 + δt) − b1 + A1x∗
t
 T 
A1x∗
t − b1 + (1 + δt) u∗
t
 
−
δt
 
u∗
t − b1 − A1x∗
t
 T u∗
t .
(11.7.10) 
Simplifying Eq. (11.7.10) we have 
. 
0 ≤ αtT (xt)
 
x∗
t − x∗
 T ∂
∂x f
 
x∗
t , xt
 
− 
 A0
 
x∗
t − x∗
 
 
2 − 
 A1
 
x∗
t − x∗
 
 
2
−
δtT (xt)
 
x∗
t − x∗
 T (x∗
t − xt) − (1 + δt)
−1 
 A1x∗
t − b1 + u∗
t (1 + δt)
 
 
2
−
δt
 
u∗
t − b1 − A1x∗
t
 T u∗
t .
Dividing both sides of this inequality by. δt we obtain 
.
0 ≤
αt
δt
T (xt)
 
x∗
t − x∗
 T ∂
∂x f
 
x∗
t , xt
 
− 1
δt
 
 A0
 
x∗
t − x∗
 
 
2
−
1
δt
 
 A1
 
x∗
t − x∗
 
 
2 − 1
δt
(1 + δt)
−1 
 A1x∗
t − b1 + u∗
t (1 + δt)
 
 
2
−
T (xt)
 
x∗
t − x∗
 T (x∗
t − xt) − 
ux
t − b1 − A1x∗
t
 T u∗
t .
(11.7.11) 
Now, taking.x = x∗
t and.u = 0 from Eq. (11.7.9) one has 
. 
0 ≤ − 
u∗
t
 T 
A1x∗
t − b1 + (1 + δt) u∗
t
 
= − 
u∗
t
 T 
A1x∗
t − b1
 
− (1+δt)
 
 u∗
t
 
 
2
= −
⎛
⎝
 
 
 
√
1 + δtu∗
t
 
 
 
2
+ 2
 √
1 + δtu∗
t
 T
 
A1x∗
t − b1
 
2
√1 + δt
 
+
 
 
 
 
 
 
A1x∗
t − b1
 
2
√1 + δt
 
 
 
 
 
2
−
 
 
 
 
 
 
A1v∗
t − b1
 
2
√1δt
 
 
 
 
 
2
⎞
⎠
= −
 
 
 
 
 
√
1 + δtu∗
t +
 
A1x∗
t − b1
 
2
√1 + δt
 
 
 
 
 
2
+
 
 
 
 
 
 
A1x∗
t − b1
 
2
√1 + δt
 
 
 
 
 
2
,
implying 
.
 
 
 
 
 
 
A1x∗
t − b1
 
2
√1 + δt
 
 
 
 
 
2
≥
 
 
 
 
 
√
1 + δtu∗
t +
 
A1x∗
t − b1
 
2
√1 + δt
 
 
 
 
 
2
,11.7 Appendix: Proofs 281
and 
. 1 ≥
 
 
 e + 2 (1 + δt) u∗
t
 
 
 
A1x∗
t − b1
 
 −1
 
 
 
2
,
where .|e| = 1. Which means that the sequence .
 
u∗
t
 is bounded. In view of this 
and taking into account that by the supposition that.
αt
δt
→n→∞ 0, from Eq. (11.7.11) it 
follows 
. 
Const = lim sup
n→∞
 
 
 
x∗
t − x∗ T 
x∗
t − xt
 
 + 
 
 
u∗
t − b1 − A1x∗
t
 T u∗
t
 
 
 
≥ lim sup
n→∞
1
δt
×
 
 A0
 
x∗
t − x∗ 
 2 + 
 A1
 
x∗
t − x∗ 
 2 + (1 + δt)
−1 
 A1x∗
t − b1 + (1 + δt) u∗
t
 
 2
 
.
(11.7.12) 
From Eq. (11.7.12) we may conclude that 
.
 
 A0
 
x∗
t − x∗
 
 
2 + 
 A1
 
x∗
t − x∗
 
 
2
+
(1 + δt)
−1 
 A1x∗
t − b1 + (1 + δt) u∗
t
 
 
2 = O (δt), (11.7.13) 
and 
. 
A0x∗
∞ − A0x∗ = A0x∗
∞ − b0 = 0,
A1v∗
∞ − A1x∗ = A1x∗
∞ − b1 + u∗
∞ = 0,
where.x∗
∞ ∈ X∗ is a partial limit of the sequence.
 
x∗
t
 
which, obviously, may be not 
unique. The vector.u∗
∞ is also a partial limit of the sequence.
 
u∗
t
 
. 
3. Now, denote by . xˆt the projection of. x∗
t to the set .Xadm, namely, 
. xˆt = Pr
Xadm
 
x∗
t
 
,
where.Pr is the projection operator. And show that 
.
 
 x∗
t − ˆxt
 
 ≤ C
√
δt, C = const > 0. (11.7.14) 
From Eq. (11.7.13) we have that 
. 
 
 A1x∗
t − b1 + u∗
t
 
 ≤ C1
√
δt, C1 = const > 0,
implying 
. A1x∗
t − b1 ≤ C1
√
δt e − u∗
t ≤ C1
√
δt e, |e| = 1,
where the vector inequality is treated in component-wise sense. Hence, 
.
 
 x∗
t − ˆxt
 
 
2
≤ max
y∈Xadm
min
A1 x−b1≤C1
√δt e, x∈Xadm
|x − y|2 := d (δt).282 11 Non-cooperative Bargaining with Unsophisticated Agents
Introduce the new variable 
. x˜ := (1 − vt) x + vt x˚ ∈ Xadm,
where by Slater’s condition given in Eq. (11.7.7) 
. 0 < vt :=
C1
√δt
C1
√δt + max
j=1,...,M1
 
 
 
 
A1x˚ − b1
 
j
 
 
 
< 1.
For the new variable.x = x˜ − vt x˚
1 − vt
we have 
. 
A1x˜ − b1 = (1 − vt) A1x + vt A1x˚ − b1
= (1 − vt) (A1x − b1) + (1 − vt) b1 + vt
 
A1x˚ − b1
 
+ vtb1 − b1
= (1 − vt) (A1x − b1) + vt
 
A1x˚ − b1
 
≤ (1 − vt)C1
√
δt e + vt
 
A1x˚ − b1
 
= C1
√δt
C1
√δt + max
j=1,...,M1
 
 
 
 
A1x˚ − b1
 
j
 
 
 
×
(
max
j=1,...,M1
 
 
 
 
A1x˚ − b1
 
j
 
 
 e + 
A1x˚ − b1
 
)
≤ 0,
and therefore 
. 
d (δt) = max
y∈Xadm
min
A1 x−b1≤C1
√δt e, x∈Xadm
|x − y|2
≤ max
A1 x˜−b1≤0, x˜∈Xadm
 
 
 
 
x˜ − vt x˚
1 − vt
− ˜x
 
 
 
 
2
= v2
t
(1 − vt)
2 min
A1 x˜−b1≤0, x˜∈Xadm
 
 x˜ − x˚
 
 
2
≤ C2δt, C2 > 0.
Given that.
 
 x∗
t − ˆxt
 
 ≤ √d (δt) ≤ √C2
√δt which proves Eq. (11.7.14). 
4. If the proximal function (11.7.5) is strictly concave and the sequence.{xt} of the 
proximal function (11.7.5) converges, then, the necessary and sufficient condition 
for the point .x∗ to be the maximum point of the function .
 
 x∗
∞
 
 
2 on the set .X∗ is 
given by 
.0 ≥ 
x∗
∞ − x∗ T (x∗
∞ − xt) for any x∗
∞ ≤ X∗. (11.7.15) 
In addition, this point is unique and it has a minimal norm among all possible partial 
limits.x∗
∞.11.7 Appendix: Proofs 283
From Eq. (11.7.11) one obtains 
. 
0 ≤ T (xt)
 
x∗
t − x∗ T ∂
∂x f
 
x∗
t , xt
 
− 1
αt
 
 A0
 
x∗
t − x∗ 
 
2 − 1
αt
 
 A1
 
x∗
t − x∗ 
 
2
− 1
αt
(1 + δt)
−1 
 A1x∗
t − b1 + u∗
t (1 + δt)
 
 
2 − δt
αt
T (xt)
 
x∗
t − x∗ T (x∗
t − xt)
≤ T (xt)
 
x∗
t − x∗ T ∂
∂x f
 
x∗
t , xt
 
− δt
αt
T (xt)
 
x∗
t − x∗ T (x∗
t − xt).
(11.7.16) 
By the strong concavity property 
. (y − z)
T
( ∂
∂y
f (y) − ∂
∂y
f (z)
)
≤ 0 for any y,z ∈ RN,
which, in view of the property (11.7.14), implies 
. 
T (xt)
 
x∗
t − ˆxt
 T ∂
∂x f
 
x∗
t , xt
 
= O √δt
 
,
T (xt)
 
xˆt − x∗
 T ∂
∂x f
 
xˆt, xt
 
≤ T (xt)
 
xˆt − x∗
 T ∂
∂x f (x∗, xt) ≤ 0,
then, we have 
. T (xt)
 
x∗
t − x∗ T ∂
∂x f
 
x∗
t , xt
 
. 
= T (xt)
 
x∗
t − ˆxt
 T ∂
∂x f
 
x∗
t , xt
 
+ T (xt)
 
xˆt − x∗ T ∂
∂x f
 
x∗
t , xt
 
= O
 √
δt
 
+ T (xt)
 
xˆt − x∗ T
( ∂
∂x f
 
x∗
t , xt
 
− ∂
∂x f
 
xˆt, xt
 
)
+ T (xt)
 
xˆt − x∗ T ∂
∂x f
 
xˆt, xt
 
≤ O
 √
δt
 
+ T (xt)
 
xˆt − x∗ T
( ∂
∂x f
 
x∗
t , xt
 
− ∂
∂x f
 
xˆt, xt
 
)
+ T (xt)
 
xˆt − x∗ T ∂
∂x f
 
x∗, xt
 
≤ O
 √
δt
 
+ T (xt)
 
 xˆt − x∗
 
 
 
 
 
 
∂
∂x f
 
x∗
t , xt
 
− ∂
∂x f
 
xˆt, xt
 
 
 
 
 .
Since any function is Lipschitz-continuous on any bounded compact set, we can 
conclude that 
.
 
 
 
 
∂
∂x f
 
x∗
t , xt
 
− ∂
∂x f
 
xˆt, xt
 
 
 
 
 
≤ Const 
 x∗
t − ˆxt
 
 = O
 √
δt
 
,284 11 Non-cooperative Bargaining with Unsophisticated Agents
which gives 
. T (xt)
 
x∗
t − ˆxt
 T ∂
∂x f
 
x∗
t , xt
 
= O
 √
δt
 
,
that by Eq. (11.7.16) leads to 
.
0 ≤ T (xt)
 
x∗
t − ˆxt
 T ∂
∂x f
 
x∗
t , xt
 
− δt
αt
T (xt)
 
x∗
t − x∗ T (x∗
t − xt)
= O
 √
δt
 
− δt
αt
T (xt)
 
x∗
t − x∗ T (x∗
t − xt).
(11.7.17) 
Dividing both sides of the inequality (11.7.17) by .
αt
δt
, taking .T (xt) = 1, and given 
that.
 
 x∗
t − ˆxt
 
 ≤ κ
√δt by Eq. (11.7.14) we obtain that 
. 0 ≤ O
( αt √δt
)
− 
x∗
t − x∗ T (x∗
t − xt) = o (1)
√
δt − 
x∗
t − x∗ T (x∗
t − xt),
which, by Eq. (11.7.8), for.n → ∞ leads to Eq. (11.7.15). Finally, for any. x∗ ≤ X∗
it implies 
.
0 ≥ 
x∗
∞ − x∗
 T (x∗
∞ − xt) = 
 x∗
∞ − x∗
 
 
2 + 
x∗
∞ − x∗
 T (x∗ − xt) ≥ 
x∗
∞ − x∗
 T (x∗ − xt).
 
Theorem 11.2 (Antipin [ 5]) If the set of solutions . X is non-empty for .δ > 0 and 
the objective function. fδ(x, x∗) is differentiable in. x, whose partial derivative with 
respect to . x satisfies the Lipschitz condition with positive constant . K. Then, there 
exists a small-enough parameter 
. α <
1
√2 K
such that, the sequence .{xn} generated by the proximal procedure, monotonically 
converges with exponential rate. q to one of the equilibria point. x∗, i.e., 
.|xt+1 − x∗|2 ≤ qt+1
|x0 − x∗|2 (11.7.18) 
as.t → ∞, where 
.q = 1 +
4(αδ)2
1 + 2αδ − 2α2K2 − 2αδ < 1,11.7 Appendix: Proofs 285
and.qmin is given by 
. qmin = 1 − 2αδ
1 + 2αδ = 1
1 + 2αδ .
Proof See [ 5]. 
11.7.4 Convergence Conditions of . δ and . α
Then, we present the convergence conditions and compute the estimate rate of con￾vergence of the variables. α and. δ. 
Theorem 11.3 Within the class of numerical sequences 
. 
αt =
 α0 if t < t0
α0
(t + t0)α if t ≥ t0
α0, t0, α > 0,
δt =
 
⎨
⎩
δ0 if t ≤ t0
δ0
(t + t0)δ if t > t0
δ0, t0, δ > 0,
the parameters. α. t and. δt satisfy the following conditions: 
. 
0 < αt ↓ 0, αt
δt
↓ 0 when t → ∞,
 ∞
t=0
αt δt = ∞ and |δt+1 − δt|
αt δt
→ 0 when t → ∞
for. α.+δ ≤ 1,. α.≥ δ,. α.< 1. 
Proof It follows from the estimates that 
. αt δt = O
( 1
tα+δ
)
we have that 
.
|δt+1 − δt| = O
( 1
t δ − 1
(t + 1)δ
)
= O
(
1
(t + 1)δ
 (
1 +
1
t
)δ
− 1
 )
= O
(
1
(t + 1)δ
 (1
t
)δ
+ o(1)
 ) = O
( 1
t δ + 1
)286 11 Non-cooperative Bargaining with Unsophisticated Agents
and 
.
|δt+1 − δt|
αt δt
= O
( 1
t 1−α
)
.
 
Theorem 11.4 Let . x and . y two variables with non-negative components for the 
players. Then, within the class of numerical sequences we have that 
. 
αt =
 α0 if t < t0
α0
(t + t0)α if t ≥ t0
α0, t0, α > 0,
δt =
 
⎨
⎩
δ0 if t ≤ t0
δ0
(t + t0)δ if t > t0
δ0, t0, δ > 0,
of the procedure (11.7.2) the rate of convergence for the players is given by the 
parameter. αt and. δt
. 
 
 xt − x∗∗ 
 + 
 yt − y∗∗ 
 = O
( 1
tκ
)
,
where. κ is equal to 
.κ = min{α − δ; 1 − α; δ}. (11.7.19) 
Then, the maximal rate. κ∗ of convergence is attained for 
.α = α∗ = 2/3, δ = δ∗ = 1/3. (11.7.20) 
Proof It follows that for. κ0, the rate of convergence is given by 
. rt = 
 xt − x∗(δt)
 
 + 
 yt − y∗(δt)
 
 = O
( 1
tκ0
)
,
where.κ0 = min{α − δ; 1 − α; δ}. It follows that 
. 
 
 xt − x∗∗ 
 + 
 yt − y∗∗ 
 = rt + O(δt) = O
( 1
tκ0
)
+ O
( 1
t δ
)
= O
( 1
tmin{κ0;δ}
)
,
which implies Eq. (11.7.19). The maximal value . κ of .κ∗ is attained when . α − δ =
1 − α = δ, i.e., when condition (11.7.20) holds. 
Remark 11.1 is a result of Theorem 11.2 and the convergence conditions of the 
parameters. δ and. α.References 287
References 
1. Abreu, D., Manea, M.: Bargaining and efficiency in networks. J. Econ. Theory 147(1), 43–70 
(2012) 
2. Abreu, D., Manea, M.: Markov equilibria in a model of bargaining in networks. Games Econ. 
Behav. 75, 1–16 (2012) 
3. Admati, A., Perry, M.: Strategic dalay in bargaining. Rev. Econ. Stud. 54(3) (1987) 
4. Akin, Z.: Time inconsistency and learning in bargaining games. Int. J. Game Theory 36, 275– 
299 (2007) 
5. Antipin, A.S.: The convergence of proximal methods to fixed points of extremal mappings and 
estimates of their rate of convergence. Comput. Math. Math. Phys. 35(5), 539–551 (1995) 
6. Antipin, A.S.: An extraproximal method for solving equilibrium programming problems and 
games. Comput. Math. Math. Phys. 45(11), 1893–1914 (2005) 
7. Attouch, H., Soubeyran, A.: Local search proximal algorithms as decision dynamics with costs 
to move. Set-Valued Anal. 19, 157–177 (2011) 
8. Bao, T.Q., Mordukhovich, B.S., Soubeyran, A.: Variational analysis in psychological modeling. 
J. Optim. Theory Appl. 164(1), 290–315 (2015) 
9. Binmore, K., Osborne, M., Rubinstein, A.: Handbook of Game Theory, chapter. In: Noncoop￾erative Models of Bargaining, pp. 179–225. Elsevier Science Publishers (1992) 
10. Binmore, K., Piccione, M., Samuelson, L.: Evolutionary stability in alternating-offers bargain￾ing games. J. Econ. Theory 80, 257–291 (1998) 
11. Binmore, K., Shaked, A., Sutton, J.: Testing noncooperative bargaining theory: a preliminary 
study. Am. Econ. Assoc. Q. 75(5), 1178–1180 (1985) 
12. Brown, D., Lewis, L.: Mgames economic agents. Econometrica 49(2), 359–368 (1981) 
13. Carraro, C., Marchiori, C., Sgobbi, A.: Negotiating on water: insights from non-cooperative 
bargaining theory. Environ. Dev. Econ. 12, 329–349 (2007) 
14. Clempner, J.B.: Solving the cost to go with time penalization using the lagrange optimization 
approach. Soft. Comput. 25(6), 4191–4199 (2021) 
15. Fudenberg, D., Tirole, J.: Sequential bargaining with incomplete information. Rev. Econ. Stud. 
50(2), 221–247 (1983) 
16. Germeyer, Y.B.: Introduction to the Theory of Operations Research. Nauka, Moscow (1971) 
17. Germeyer, Y.B.: Games with Nonantagonistic Interests. Nauka, Moscow (1976) 
18. Ghosh, P., Roy, N., Das, S., Basu, K.: A pricing strategy for job allocation in mobile grids using 
a non-cooperative bargaining theory framework. J. Parallel Distrib. Comput. 65, 1366–1383 
(2005) 
19. Guo, X., Hernández-Lerma, O.: Continuos–Time Markov Decision Processes: Theory and 
Applications. Springer (2009) 
20. Haller, H.: Non-cooperative bargaining of.n ≥ 3 players. Econ. Lett. 22, 11–13 (1986) 
21. Howard, N.: Paradoxes of Rationality: Theory of Metagames and Political Behaviour. MIT 
Press (1971) 
22. Jun, B.: Non-cooperative bargaining and union formation. Rev. Econ. Stud. 56, 59–76 (1989) 
23. Kultti, K., Vartiainen, H.: Multilateral non-cooperative bargaining in a general utility space. 
Int. J. Game Theory 39, 677–689 (2010) 
24. Madani, K., Hipel, K.: Non-cooperative stability definitions for strategic analysis of generic 
water resources conflicts. Water Resour. 25(8), 1949–1977 (2011) 
25. Manea, M.: Bargaining in stationary networks. Am. Econ. Rev. 101, 2042–2080 (2011) 
26. Marden, J.: State based potential games. Automatica 48, 3075–3088 (2012) 
27. Montero, M.: Non-cooperative bargaining in apex games and the kernel. Games Econ. Behav. 
41, 309–321 (2002) 
28. Moreno, F.G., Oliveira, P.R., Soubeyran, A.: A proximal algorithm with quasidistance. appli￾cation to habit’s formation. Optimization 61, 1383–1403 (2011) 
29. Muthoo, A.: Bargaining Theory with Applications. Cambridge University Press (2002) 
30. Nash, J.F.: The bargaining problem. Econometrica 18(2), 155–162 (1950)288 11 Non-cooperative Bargaining with Unsophisticated Agents
31. Okada, A.: A non-cooperative bargaining theory with incomplete information: Verifiable types. 
J. Econ. Theory 163, 318–341 (2016) 
32. Ostrom, E.: Governing the Commons: The Evolution of Institutions for Collective Action. 
Cambridge University Press (1990) 
33. Ostrom, E.: A behavioral approach to the rational choice theory of collective action. Am. Polit. 
Sci. Rev. 92(1), 1–22 (1998) 
34. Ostrom, E., Gardner, R., Walker, J.: Rules, Games, and Common-Pool Resources. The Uni￾versity of Michigan Press (1994) 
35. Perry, M., Reny, P.: A non-cooperative bargaining model with strastrategic timed offers. J. 
Econ. Theory 59, 50–77 (1993) 
36. Poznyak, A.S.: Advanced mathematical tools for automatic control engineers. In: Deterministic 
Technique, vol. 1. Elsevier, Amsterdam (2008) 
37. Poznyak, A.S., Najim, K., Gomez-Ramirez, E.: Self-learning Control of Finite Markov Chains. 
Marcel Dekker, New York (2000) 
38. Rubinstein, A.: Perfect equilibrium in a bargaining model. Econometrica 50(1), 97–109 (1982) 
39. Rubinstein, A., Wolinsky, A.: Equilibrium in a market with sequential bargaining. Econometrica 
53(5), 1133–1150 (1985) 
40. Selbirak, T.: Some concepts of non-myopic equilibria in games with finite strategy sets and 
their properties. Ann. Oper. Res. 51(2), 73–82 (1994) 
41. Sutton, J.: Non-cooperative bargaining theory: an introduction. Rev. Econ. Stud. 53(5), 709– 
724 (1986) 
42. Tanaka, K.: The closest solution to the shadow minimum of a cooperative dynamic game. 
Comput. Math. with Appl. 18(1–3), 181–188 (1989) 
43. Tanaka, K., Yokoyama, K.: On. -equilibrium point in a noncooperative n-person game. J. Math. 
Anal. 160, 413–423 (1991) 
44. Tikhonov, A.N., Arsenin, V.Y.: Solution of Ill-Posed Problems. Winston & sons, Washington 
(1977) 
45. Tikhonov, A.N.N., Goncharsky, A.V., Stepanov, V.V., G., Y.A.: Numerical Methods for the 
Solution of Ill-Posed Problems. Kluwer Academic Publishers (1995) 
46. Trejo, K.K., Clempner, J.B.: New perspectives and applications of modern control theory, 
chapter. In: Continuous Time Bargaining Model in Controllable Markov Games: Nash versus 
Kalai-Smorodinsky, pp. 335–369. Springer International Publishing (2018) 
47. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: An optimal strong equilibrium solution for coop￾erative multi-leader-follower Stackelberg Markov chains games. Kybernetika 52(2), 258–279 
(2016) 
48. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the strong .L p-Nash equilibrium for 
Markov chains games: convergence and uniqueness. Appl. Math. Modell. 41, 399–418 (2017) 
49. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Nash bargaining equilibria for controllable markov 
chains games. In: The 20th World Congress of The International Federation of Automatic 
Control (IFAC), pp. 12772–12777. Toulouse, France (2017) 
50. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the bargaining approach for equalizing 
the ratios of maximal gains in continuous-time markov chains games. Comput. Econ. 54, 933– 
955 (2019) 
51. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Proximal constrained optimization approach with 
time penalization. Eng. Optim. 51(7), 1207–1228 (2019) 
52. Winoto, P., McCalla, G., Vassileva, J.: Non-monotonic-offers bargaining protocol. Auton. 
Agent Multi Agent Syst. 11(1), 45–67 (2005)Chapter 12 
Transfer Pricing as Bargaining 
Abstract The Nash bargaining game theory technique is used in this chapter to 
examine and provide a solution to the transfer pricing problem. We analyze a com￾pany with sequential transfers among its several divisions, where central management 
decides on the transfer price to maximize operational profitability. Throughout the 
negotiation process, the price shifting between divisions is a tool for bargaining. 
First, we take into account a point of contention (status quo) between the firm’s 
divisions, which serves as a deterrent. We offer a methodology and framework for 
calculating the disagreement point that are based on the Nash equilibrium approach. 
Then, we introduce the bargaining solution, which is a single-valued function that 
chooses an outcome from each bargaining problem’s feasible pay-offs. This solution 
is the result of cooperation between the company divisions involved in the transfer 
pricing problem. The agreement achieved by the divisions in the game is the most 
desirable option within the range of plausible outcomes that results in a distribution 
of the transfer price between divisions that maximizes profit. We provide an opti￾mization approach for computing the negotiating solution. The method’s usefulness 
is demonstrated through a number of examples, including Markov models in both 
continuous and discrete time. 
12.1 Introduction 
12.1.1 Transfer Pricing Process 
Transfer pricing refers to the rules and methods for pricing transactions within and 
between enterprises under common ownership or control. Because of the poten￾tial for cross-border controlled transactions to distort taxable income, tax author￾ities in many countries can adjust intragroup transfer prices that differ from what 
would have been charged by unrelated enterprises dealing at arm’s length (the arm’s￾length principle) [ 44]. The OECD and World Bank recommend intragroup pricing 
rules based on the arm’s-length principle, and 19 of the 20 members of the G20 
have adopted similar measures through bilateral treaties and domestic legislation, 
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1_12 
289290 12 Transfer Pricing as Bargaining
regulations, or administrative practice. Countries with transfer pricing legislation 
generally follow the OECD Transfer Pricing Guidelines for Multinational Enter￾prises and Tax Administrations in most respects, although their rules can differ on 
some important details. 
Where adopted, transfer pricing rules allow tax authorities to adjust prices for most 
cross-border intragroup transactions, including transfers of tangible or intangible 
property, services, and loans. For example, a tax authority may increase a company’s 
taxable income by reducing the price of goods purchased from an affiliated foreign 
manufacturer or raising the royalty the company must charge its foreign subsidiaries 
for rights to use a proprietary technology or brand name. These adjustments are 
generally calculated using one or more of the transfer pricing methods specified in 
the OECD guidelines and are subject to judicial review or other dispute resolution 
mechanisms. 
12.1.2 Brief Review 
The term “transfer pricing” refers to the price at which a business transfers goods 
and services among its cooperating (or not) divisions. It is used as a profit allocation 
approach when a multinational firm crosses international borders to assign its net 
profit before tax. The technique of distributing goods and services among several 
divisions in various places is used by many companies. The product travels through 
these several locations during the product’s production and maintenance stages as 
these divisions belong to more than one company. (see Fig. 12.1)[ 27, 28, 37, 40, 
56]. This dispersed method seeks a variety of advantages, including cheaper costs, 
less restrictions on labor facilities, and lower taxes. However, there are a number of 
issues with international regulations when calculating the maximum firm-wide profit 
surplus among divisions. The arm-length rule mandates that businesses base their 
price decisions on comparable, unrelated-but-at-arm’s-length transactions between 
its divisions [ 44]. The ’fair’ agreement about the negotiation of the transfer price 
among divisions is determined by transfer pricing regulations. According to the 
official definition of the arm’s length standard, a controlled transaction satisfies the 
requirement if the outcomes are comparable to those that would have been obtained 
if uncontrolled taxpayers had participated in the same transaction under the same 
conditions. 
The transfer price is used to calculate expenses in a multidivisional firm when 
divisions are required to conduct business with one another. Due to the fact that 
one of the divisions involved in a transfer transaction loses money, transfer prices 
frequently do not deviate considerably from the market price. The issue is that this 
will impair the performance of the businesses if they start either purchasing for 
more than the current performance market price or selling below the market price. 
Therefore, calculating the optimal transfer price is a very intriguing challenge that 
has drawn the attention of researchers from a variety of fields, but it is still up for 
discussion among both academics and practitioners [ 1, 2, 8, 23, 29, 41, 57].12.1 Introduction 291
Fig. 12.1 Multidivisional firm 
In determining a pricing structure for a mono-product company with a production 
and sales division, Hirshleifer was the first to provide analytical approaches to the 
transfer pricing problem and identify an efficient amount of internal trade [ 32, 33]. 
The Hirshleifer model was expanded by Arrow [ 4] and Baumol and Fabian [ 7] to 
settings with several products and divisions. Kanodia [ 36] made a suggestion on how 
central management may create transfer pricing that allow for risk sharing and benefit 
all managers. Blois [ 10] enhanced the Hirshleifer model [ 32, 33], arguing that even 
if central management permits descentralization, a major customer will be able to 
compel its suppliers to adhere to the transfer price rule of marginal cost. There is prior 
relevant research on transfer pricing that takes into account temporal stability [11] and 
a single period horizon [ 25]. Enzer [ 25] used a linear programming approach to solve 
the transfer pricing puzzle and arrive at an average price. Jennergren [ 34] suggested a 
method that consolidated the decision-making for the divisions, building on Enzer’s 
work. A summary of the literature that took into account psychological and empirical 
evidence supporting transfer pricing was offered by Thomas in [ 51]. He proposed 
that central management enables descentralization while preserving the coordination 
brought on by centralization. The Dearden and Henderson concept and Thomas’ 
notion were shown to be compatible [ 22, 31]. Amershi and Cheng [ 3], Besanko and 
Sibley [ 9] and, Ronen and Balachandran [ 46] found solutions to the transfer pricing 
conundrum by assuming that divisions are either implicitly or overtly subcontractors 
and by having divisions pay a transfer price and incur production expenses. In a 
vertically integrated supply chain, Rosenthal [ 47] created a cooperative game that 
offers transfer prices for the intermediate items (when valuation is known and when292 12 Transfer Pricing as Bargaining
their valuations differ), offering a solution that is just and agreeable to all divisions. 
Leng and Parlarb [ 38] extended Rosenthal’s work [ 47] by creating a cooperative 
game based on calculating the Shapley value-based transfer prices for a vertically 
integrated supply chain firm with an upstream division and numerous downstream 
divisions. These divisions can independently determine their retail prices and choose 
whether or not to buy from the upstream division at negotiated transfer prices. 
The literature has described related work that resolves transfer-price negotiat￾ing difficulties. The examination of negotiated transfer-pricing outcomes between a 
purchasing and selling division where each division had access to private profit infor￾mation was proposed by Chalos and Haka [ 12]. According to Edlin and Reichelstein 
[ 24], transfer price discussions for resolving a bilateral holdup issue in a multina￾tional corporation were explored. They demonstrated that knowledge asymmetry 
will lead to an unjust and ineffective transfer pricing outcome. Vaysman [ 55] created 
a negotiating model for negotiated transfer pricing that takes into account private 
divisional information. The business then constructs a pay scheme that makes use 
of both negotiated transfer pricing and divisional performance evaluation. In their 
study of two transfer-pricing schemes and the accompanying two-division bargaining 
difficulties, Haake and Martini [ 30] investigated two transfer-pricing systems. The 
literature has generally offered both cooperative and non-cooperative game theory 
solutions that center on bargaining [ 6, 13, 35, 58]. 
This chapter analyzes and proposes a solution for computing the transfer pricing 
problem considering a firm consisting of several divisions. The main results are the 
following: 
• Proposes a solution for computing the transfer pricing problem from a point of 
view of the Nash bargaining game theory approach. 
• Suggests a solution for computing the transfer pricing problem from a point of 
view of the continuous-time bargaining game theory approach. 
• In this negotiation process divisions cooperate and all necessarily improve their 
position at the end of the process. 
• Divisions operate over sequential transfers in which central management provides 
the transfer price decision which enables maximization of operating profits. 
• The transfer pricing model involves costs and taxes. 
• The division’s unit production cost is dependent on the production quantity. 
• The negotiation starts at the time that a division considers a disagreement point 
(status quo) which plays a role of a deterrent. 
• Proposes a framework and method based on the Nash equilibrium approach for 
computing the disagreement point. 
• The bargaining solution which is a single-valued function is the result of cooper￾ation by the divisions. 
• The final agreement is the most preferred alternative within the set of feasible 
outcomes which produces a profit-maximizing allocation of the transfer price 
between divisions.12.2 Preliminaries 293
• Proposes an optimization for computing the bargaining solution method. 
• The result of the optimization method is a simultaneous adjustment of quantity 
and transfer price 
12.2 Preliminaries 
12.2.1 Nash’s Bargaining 
Detail description of Nash’s bargaining game problem is given in Chap. 9. 
Nash’s bargaining game (see Fig. 12.2) is based on a model in which players are 
assumed to negotiate on a set of feasible pay-offs [ 52, 54]. A fundamental element 
of the game is the disagreement point (status quo) which plays a role of a deterrent. 
A bargaining solution is a single-valued function that selects an outcome from the 
feasible pay-offs for each bargaining problem, which is the result of cooperation by 
the players involved in the game. The agreement reached in the game is the most 
preferred alternative within the set of feasible outcomes. 
The bargaining problem is described by the pair .(L, d˜), where .L ⊂ Rn is set 
of feasible payoffs, .d˜ ∈ Rn is a fixed disagreement vector and .l = 1, ..., n is the 
number of players. We will call this form the condensed form of the bargaining 
problem (see [ 26, 43]). It can be derived from the normal form of an .n-person 
game.G = 
X1, ..., Xn; f 1, ..., f n
 
in a natural way. The set of all feasible payoffs 
(outcomes) is defined as 
.F = { f : f = ( f1(x), ..., fn(x))}, x ∈ X, (12.2.1) 
where.X = X1 × ... × Xn. Given a disagreement vector.d˜ ∈ Rn, the pair.(F, d˜) is a 
bargaining problem in condensed form. We can derive another bargaining problem 
Fig. 12.2 Nash bargaining294 12 Transfer Pricing as Bargaining
.(L, d˜) from .G by extending the set of feasible outcomes .F to its convex hull . L. 
Notice that any element. f ∈ L can be represented as 
. f (λ) = ∑n
l=1
λl fl(x(λ)) = (λ, f ), (12.2.2) 
where. f = ( f1(x(λ)), ..., fn(x(λ)))),.x ∈ X,.λl ≥ 0 for all. l, and.
∑n
l=1 λl = 1. 
The payoff vector. f can be realized by playing the strategies.xl with probability 
. λl
, and so . f is the expected payoff of the players. Thus, when the players face the 
bargaining problem the question is, which point of. L should be selected taking into 
account the different position and strength of the players that is reflected in the set 
. L of extended payoffs and the disagreement point. d˜. 
Let. B denote the set of all pairs.(L, d˜) such that: 
(a) .L ⊂ Rn is compact, convex; 
(b) there exists at least one. f ∈ L such that. f > d˜ in component-wise sense. 
A Nash solution to the bargaining problem is a vector function.b : B → Rn, charac￾terizing optimal distribution of payoff functions in the considered transfer bargaining 
problem.(L, d˜), such that.b(L, d˜) ∈ L. 
Proposition 12.1 There is a unique function. b such that for all.(L, d˜) ∈ B the vector 
.b(L, d˜) = (b1, ..., bn) is the unique solution of the optimization problem 
.
maximize g(b) = |n
l=1
(bl − d˜
l)
subject to b ∈ L, b ≥ d˜.
(12.2.3) 
The objective function of problem in Eq. (12.2.3) is usually called the Nash product. 
The condition.b ∈ L can be equivalente expressed as 
. min
x∈X fl(x) ≤ bl ≤ max
x∈X fl(x), l = 1, ..., n. (12.2.4) 
12.2.2 Continuous-Time Bargaining 
Detail description of the continuous-time bargaining game problem is given in 
Chap. 11. Rubinstein [ 48] defined seminal bargaining situation for two players 
(.n = 2) who have to reach an agreement on the partition of a pie of size . 1. Each 
player takes turns to make an offer to the other agents on how the pie should be 
divided between them. After player. 1 has made such an offer, player. 2 must decided 
whether to accept it, in this case the bargaining game ends and the players divide the12.2 Preliminaries 295
cake according to the accepted offer, or to reject it and continue with the bargaining 
process. If player. 2 rejected, then this player has to make a counteroffer which player 
. 1 would accept or reject it and continue with the negotiation process. The bargain￾ing game continues until an offer is accepted. Offers are made at discrete points in 
time, and players experience an exponential discount factor which might be different 
across agents. 
Rubinstein’s [ 48] main results shows the existence of a subgame perfect equilib￾rium. 
Such a game can be extended to a general set. X of possible agreements. We denote 
by.Φ(X) = {(ψ1(x), ψ2(x))|x ∈ X}the set of possible utility pairs attainable at time 
0, and.Φe denote the Pareto frontier 1 of the set. Φ. 
When .X is a compact and convex set, and the utility functions are continuous 
and concave, the Pareto frontier .Φe can be represented by a graph function of a 
strictly decreasing and concave function, denoted by. φ, whose domain is an interval 
.I 1 ⊆ R and range an interval.I 2 ⊆ R. For simplicity, assume that.0 ∈ I 1,.0 ∈ I 2 and 
.φ(0) > 0. Then, 
. Φe = 
(ψ1
, ψ2
) : ψ1 ∈ I 1
, ψ2 ∈ φ(ψ1
)
 
.
Consider .φ−1 the inverse of . φ, a strictly decreasing and concave function from 
.I 2 to. I 1, with.φ−1(0) > 0. Then, for any.ψ1 ∈ I 1,.φ(ψ1) is the maximum utility that 
player. 2 receives subject to player. 1 receiving a utility.ψ1; in the same way, for any 
.ψ2 ∈ I 2, .φ−1(ψ2) is the maximum utility that player . 1 receives subject to player . 2
receiving a utility.ψ2. 
Let.Zl
, a non-empty subset of. X, defined as follows 
. Zl =
 
xl := arg max x∈X ψl
(x) : ψm(xl
) = βmψm(xm) , (m /= l)
 
, (12.2.5) 
where.βm = e(−rm ) is the discount factor associated to player. m. 
Proposition 12.2 For any .x∗l ∈ Zl
, .l = 1, 2, the following pair of strategies is a 
subgame perfect equilibrium of the general Rubinstein model: 
• Player 1 always offers.x 1∗ and always accepts an offer.x 2 if and only if. ψ1(x 2) ≥
β1ψ1∗
• Player 2 always offers.x 2∗ and always accepts an offer.x 1 if and only if. ψ2(x 1) ≥
β2ψ2∗
where.ψ1∗ = φ−1(β2ψ2∗) and. ψ2∗ = φ(β1ψ1∗).
The generality of this Bargaining model and the proof of this result can be found 
in [ 42]. Note that if .Zl contains more than one element, then there exist more than 
one subgame perfect equilibrium in the general Rubinstein model. In any subgame
1 A utility pair.(ψ1, ψ2) ∈ Φe if and only if.(ψ1, ψ2) ∈ Φ and there does not exist another utility 
pair.(ϕ1, ϕ2) ∈ Φ such that.ϕ1 ≥ ψ1,.ϕ2 ≥ ψ2. 296 12 Transfer Pricing as Bargaining
perfect equilibrium, if agreement is reached at time . 0 and it is player . 1 who makes 
the offer, then the equilibrium payoff for player. 1 is.ψ1∗ and for player. 2 is.φ(ψ1∗); 
similarly, if it is player. 2 who makes the offer at time. 0, then the equilibrium payoff 
for player . 1 is .φ−1(ψ2∗) and for player . 2 is .ψ2∗. This equilibrium pair is Pareto 
efficient. 
12.3 Transfer Pricing 
We consider a model of vertically integrated divisions [ 47]. The upstream division 
produces an intermediate product considering acquisition and production costs . Θ. 
Divisions sell product either externally at positive market price . p or internally to 
the group’s downstream divisions at transfer price . T . In order to focus the analysis 
on transfer pricing issues, the upstream division profit is given by .p − Θ. As well 
as, divisions’ profit is given by .p − T . The game consists of . n divisions or players 
(.l = 1, ..., n denoted by.l = 1, n) which jointly make their decisions to maximize the 
global profits of the firm. We suppose that divisions are located in different markets 
to reduce the possibility of competition between the divisions. Our model considers 
a centralized structure allowing some divisions to act necessarily cooperatively, but 
divisions jointly make their decisions to maximize the profit. 
The dynamics between divisions is as follows. For.l = 1, ..., n − 1, intermediate 
goods are shipped from level. l to level.l + 1, i.e., along the supply chain. Division. l
makes its market pricing decision .pl and sells .ql
(pl
) units of its final products to a 
given division. l. Following [ 5, 20, 39], division. l sales quantity.ql is determined by 
a linear demand function, i.e. 
.ql
(pl
) = αl − βl
pl
, (12.3.1) 
where.αl
, βl > 0 and.pl ≤.αl
/βl
. Divisions are located in different marketing areas 
then they have independent demands.ql
(pl
) (see Fig. 12.3).Then, divisions profit is 
given by 
.Φ1
(p1
, τ 1
, q1
) = p1
q1
(p1
) = (
p1 − Θ1) (α1 − β1 p1)
, (12.3.2) 
Fig. 12.3 Supply chain divisions12.3 Transfer Pricing 297
where.Θ1 denotes acquisition and production costs,and 
.Φl
(pl
, τl
, ql
) = pl
ql
(pl
) = (
pl − Tl
) (αl − βl
pl
)
, for l ≥ 2. (12.3.3) 
.Tl corresponds to the transfer price that division . l pays to division .l − 1 for .l ≥ 2. 
So Eq. (12.3.3) can be written as 
. Φl
(pl
, pl−1
, ql
) = (
pl − pl−1)
ql
(pl
) = (
pl − pl−1) (αl − βl
pl
)
, for l ≥ 2.
(12.3.4) 
Because of the existence of economies of scale, we consider that the division’s unit 
production cost is dependent on the production quantity. Then, the unit production 
cost which is incurred by division. l when the division sells.ql
(pl
) units of intermediate 
products is represented by.cl
(ql
). The division’s total sales quantity is 
.Q(q) ≡ ∑
l
ql
, (12.3.5) 
where.q = (
q1(p1), ..., qn(pn)
)
Then, the production cost can be written as.c(Q(q)). 
The corresponding effect on divisions’ costs is given by.κl
ccl
(ql
)ql where.κl
c ∈ [0, 1]. 
As well, we represent the taxes that a division. l has to pay as function depending 
on the product and the quantity represented by .τl
(pl
ql
). We do not consider any 
specific function for the costs and the taxes, and we use the general form.cl
(ql
) and 
.τl
(pl
ql
) for our analysis. The corresponding effect on divisions’ costs is given by 
.ωl
τ pl
ql where.ωl
τ ∈ [0, 1]. 
Finally, the model is defined in following manner: 
• Market prices.pl for one intermediate good (sold from. l to.l + 1). 
• Quantities.ql of intermediate good. l shipped from. l to.l + 1 for.l = 1, ..., n − 1; 
• Production costs 
.cl
(Q(q)) := κl
ccl
(ql
)ql
, κl
c ∈ [0, 1] (12.3.6) 
(e.g.: transaction costs, raw materials, components, and their per period inventory 
costs in dollars) at each level.l = 1, n; 
• Taxes 
.τl
(pl
ql
) := ωl
τ pl
ql
, ωl
τ ∈ [0, 1] (12.3.7) 
at each level.l = 1, n. 
Remark 12.1 The taxes .τl
(pl
ql
) can be eliminated from the computation process 
if it will be the case that a multidivisional firm does not use transfer pricing as a tool 
for reducing the firm’s total tax payment. 
We suppose that all the divisions are located in different marketing areas, there￾fore they face independent demands . ql
, costs .cl
(ql
) and taxes .τl
(pl
ql
). Then, the 
division’s utility.Φl for each level.l = 1, n is given by298 12 Transfer Pricing as Bargaining
. 
Φ1(p1, q1) = (
p1 − Θ1
)
q1 − cl
(Q(q)) − τ 1(p1q1),
Φl
(pl
, pl−1, ql
) = (
pl − pl−1
)
ql − cl
(Q(q)) − τl
(pl
ql
), for l = 2, ..., n,
⎫
⎬
⎭
(12.3.8) 
where.l = 1 represents the first division and.l = 2, ..., n the rest of the divisions on 
the vertically integrated supply chain. 
In the proposed model the cost and taxing information asymmetry between divi￾sions validates divisional autonomy: delegate the production and marketing deci￾sions. The proposed linear model for costing, taxing and pricing captures all the 
relevant production and marketing decisions: (1) individual deliberation on the pro￾duction or selling cost, (2) separate consideration on the taxation and, (3) global 
computation of the transfer pricing policy for global profit maximization. 
Usually, transfer pricing is concerned with an intra-firm transaction problem, it 
always involved strategic implications for competitive environment in which the 
divisions operates. The price at which transfers occur depends on the organizational 
structure adopted by the divisions: centralized or decentralized. The behavior of a 
MNE can vary widely within a market. A regulated subsidiary can intentionally have 
its unregulated subsidiaries overprice the parent subsidiary to increase the parent 
subsidiary’s cost and the final price to consumers. In the meantime, the unregulated 
subsidiary can adopt a predatory price to prevent new divisions to entrant into the 
market. We consider the role of internal transfer prices within the context of an 
oligopoly model of competition and how transfer prices can be used as a strategic 
tool for competing firms to achieve tacit collusion. 
Consider a transfer pricing game with. n players with strategies.vl ∈ Vl where. V
is a concave an compact set .
(
l = 1, n
)
. Denote by .v = (v1, ..., vn)T ∈ V the joint 
strategy of the players and .vˆl is a strategy of the rest of the players adjoint to . vl
, 
namely, 
. vˆl := (
v1
, ..., vl−1
, vl+1
, ..., vn)T
∈ V mˆ := n
h=1, h/=l
V h,
such that.v = (vl
, vˆl
). A Nash equilibrium is a strategy.v∗ = (
v1∗, .., vn∗
)
such that 
. Φ (
v1∗, .., vl
, ..., vn∗)
≤ Φ (
v1∗, .., vn∗)
for any .vl ∈ V.A strong Nash equilibrium is a strategy .v∗∗ = (
v1∗∗, .., vn∗∗)
such 
that there does not exist any.vl ∈ V,. vl /= vl∗∗
. Φ (
v1∗∗, .., vn∗∗)
≤ Φ (
v1∗∗, .., vl
, ..., vn∗∗)
for.vl ∈ V. A policy.v∗ is said to be is a Pareto policy (or Pareto-optimal) if there is no 
policy. v such that.Φ(v∗) < Φ(v), and similarly for weak or proper Pareto policies. 
If we let.Φl∗ = sup
v∈V
Φl
(v) and define the utopia maximum as.Φ∗ =. (Φ1∗, ..., Φn∗)
(infeasible in general) the resulting problem is to find the values of12.3 Transfer Pricing 299
. 
λ∗ = arg max
λ∈Sn
∑n
l
λlΦ(v∗(λ)),
Sn := 
λ ∈ Rn : λl ∈ [0, 1] , ∑n
l=1
λl = 1
 
,
in order to find the strong Nash equilibrium.v∗(λ) whose pay-off vector.Φ(v∗(λ)) is 
the “closest” to.Φ∗ in the usual Euclidean norm. 
The bounds for .pl and .ql establish the maximum and minimum transfer price 
legally authorized (see Fig. 12.4)..Padm and.Qadm are established by the arm’s length 
price as well as the quantity of goods traded determining specific decision area where 
the optimal strategies can be selected (see Fig. 12.4). 
. 
pl ∈ 
pl
−, pl
+
 
, ql ∈ 
ql
−, ql
+
 
,
Padm := n
l=1
 
pl
−, pl
+
 
, Qadm := n
l=1
 
ql
−, ql
+
 
.
The admissible strategies are defined as.Vadm = Padm ⊗ Qadm. 
The Pareto set [ 14, 19] can be defined as 
. P := 
v∗ (λ) := arg max v∈Vadm=Padm⊗Qadm
Φ (v|λ), λ ∈ Sn
 
,
where 
. Φ (v|λ) := λ1Φ1 (
p1
, q1)
+∑n
l=2
λlΦ (
pl
, pl−1
, ql
)
Fig. 12.4 Arm’s length price.Padm and quantity.Qadm300 12 Transfer Pricing as Bargaining
given 
. v = {(p, q) : p = (p1, ..., pn)
 and q = (q1, ..., qn)
 }
with the Pareto front given by 
. Φ(v∗ (λ)) = (
Φ1 (v (λ)), Φ2 (
v∗ (λ)
)
, ..., Φn (
v∗ (λ)
)) .
The vector.v∗ is called a Pareto optimal solution for. P. 
12.4 The Transfer Pricing Nash Bargaining Solution 
We start with several notations [ 17, 18, 53]. For a finite set of divisions (players). n
with. n elements, let.Rn denote the .n-dimensional Euclidian space with coordinates 
indexed by.l = 1, ....n. Any point in.X ⊆ Rn, called the joint strategy of the divisions, 
is denoted by.x = (
xl
)
l∈n, and also by.x = (
x 1, ...x n
)
,.l = 1, n. The set. X is a convex 
and compact set. For.l ∈ n and.x = (
xl
)
l∈n ∈ X,. xˆ denotes the.(n − 1)-dimensional 
vector constructed from . x by deleting the .l-th coordinate . xl
. .x ˆl is a strategy of the 
rest of the players adjoint to. xl
, namely, 
.x ˆl := (
x 1
, ..., xl−1
, xl+1
, ..., x n)T
∈ Xˆl := n
m=1, m/=l
X m. (12.4.1) 
The point. x is written as.
 
xl
, x ˆl
 
. 
A .n-division game is defined by a triplet . = (
n,{Al}l∈n ,{ϕl}l∈n
)
where . n is 
the set of divisions and each .Al .(l ∈ n) is finite set of division . l’s actions. The 
Cartesian product .A = n
l=1 Al is the set of action profiles .a = (
a1, ...an
)
for . n
players. Division . l’s utility function .ϕl is a real value function on . A. Each player 
.l ∈ n, for a given a strategy. xl
, obtains the utility 
.ϕl (x) = ∑
x1∈X1
... ∑
xn∈Xn
ϕl
(
a1
, ...an) n
l=1
xl
(al
), (12.4.2) 
where.xl
(al
) is the strategy of the action. al
. 
Divisions try to reach the one of Nash equilibria, that is, each division . l tries to 
find a joint strategy .x∗ = (
x 1∗, ..., x n∗
)
.∈ .X satisfying for any admissible . xl ∈ Xl
and any. l = 1, n
.
ϕl
 
xl
, x ˆl
 
− max
xl∈Xl
ϕl
 
xl
, x ˆl
 
≤ 0,
for any xl ∈ Xl and all l = 1, n
(12.4.3)12.4 The Transfer Pricing Nash Bargaining Solution 301
where 
.xˆ(x) = (x 1ˆT, ..., x nˆT)
T ∈ Xˆ ⊆ Rn(n−1)
, (12.4.4) 
and .p ≥ 1 [ 49, 50]. Here .ϕl
 
xl
, x ˆl
 
is the utility-function of the player . l which 
plays the strategy.xl ∈ Xl and the rest of the players the strategy.x ˆl ∈ Xˆl
. 
Let us consider 
. GL p
(
x, xˆ(x)
)
:= 
∑n
l=1
 
 
 
 
 
max
xl∈Xl
ϕl
 
xl
, x ˆl
 
− ϕl
 
xl
, x ˆl
 
 
 
 
p 1/p
. (12.4.5) 
If we define the utopia point 
.x¯
l := arg max
xl∈Xl
ϕl
 
xl
, x ˆl
 
, (12.4.6) 
then, by replacing Eq. (12.4.6) in Eq. (12.7.2) the original problem given can be 
rewritten as 
. GL p
(
x, xˆ(x)
)
:= 
∑n
l=1
 
 
 ϕl
 
x¯l
, x ˆl
 
− ϕl
 
xl
, x ˆl
 
 
 
p
 1/p
. (12.4.7) 
The functions.ϕl
 
xl
, x ˆl
 
.
(
l = 1, n
)
are assumed to be concave in all their arguments. 
Remark 12.2 The function.GL p
(
x, xˆ(x)
)
satisfies the Nash property 
.
ϕl
 
xl
, x ˆl
 
− ϕl
 
x¯l
, x ˆl
 
≤ 0
for any xl ∈ Xl and all l = 1, n. (12.4.8) 
If the function.GL p
(
x, xˆ(x)
)
is strictly concave and the Hessian matrix is negative 
semi-definite, then.GL p
(
x, xˆ(x)
)
attains a maximum at.
(
x, xˆ(x)
)
and satisfies [ 53] 
.
∇2GL p
(
x, xˆ(x)
)
=
⎡
⎢
⎢
⎢
⎣
∂2
(∂x1)
2 GL p
(
x, xˆ(x)
) .. ∂2
∂x1∂xn GL p
(
x, xˆ(x)
)
∂2
∂x2∂x1
GL p
(
x, xˆ(x)
)
.. ∂2
∂x2∂xn GL p ,δ
(
x, xˆ(x)
)
··· .. ··· ∂2
∂xn∂x1
GL p
(
x, xˆ(x)
)
.. ∂2
(∂xn )
2 GL p
(
x, xˆ(x)
)
⎤
⎥
⎥
⎥
⎦ =
⎡
⎢
⎢
⎣
δI n1×n1 DG1,2(uˆ1,2) .. DG1,n(uˆ1,n)
DG2,1(uˆ2,1) δI n2×n2 .. DG3,2(uˆ3,2)
··· ··· .. ···
DG3,1(uˆ3,1) DG3,2(uˆ3,2) .. δI nn×nn
⎤
⎥
⎥
⎦
< 0302 12 Transfer Pricing as Bargaining
or, equivalently,. δ should provide the inequality 
.δ > max
x∈X
 
 max (
∇2GL p
(
x, xˆ(x)
)) , (12.4.9) 
where.Λmax is the maximum eigenvalue. 
The bargaining game .Γ is based on a model in which players are assumed to 
negotiate on a set of feasible pay-offs. Φ. A fundamental element of the game is the 
disagreement point . f ∗
l (status quo) which plays a role of a deterrent. A bargaining 
solution is a single-valued function that selects an outcome from the feasible pay￾offs for each bargaining problem. The agreement reached in the game is the most 
preferred alternative within the set of feasible outcomes. Φ. Nash [ 43] proposed this 
approach by presenting four axioms and showing that they characterize the Nash 
bargaining solution. 
Definition 12.1 (Nash bargaining transfer pricing). For a finite set of divisions . n
with. n elements of a game. , a strategy.x∗ is called a Nash bargaining solution for 
the transfer price of. if.x∗ is an optimal solution of the maximization problem 
.
|n
l=1(ϕl
 
xl
, x ˆl
 
− ˜fl
 
x˜l
, x˜
ˆl
 
) → max
x∈X
subject to
x ∈ Φ,
ϕl(x) ≥ ˜fl(x˜) for all l = 1, ..., n,
(12.4.10) 
where .ϕ is the payoff and . ˜fl is the disagreement point. The pay-off . Φ(x∗) =
(ϕl(x∗))l=1,n of divisions generated by the Nash bargaining solution .x∗ is the bar￾gaining solution payoff . 
Let.αn = (αl)l=1,n. Then, we rewrite the problem (12.4.10) as follows 
. 
g(x) = ∑n
l=1
αl log 
ϕl
 
xl
, x ˆl
 
− ˜fl
 
x˜l
, x˜
ˆl
 → max
x∈X
subject to
ϕl
(
x, xˆ(x)
)
> f ∗
l ,
where .αl is called the individual weight of each division such that .
∑n
l=1 αl = 1, 
. ˜fl is the disagreement point (.l = 1, n) or the status-quo and .x˜ =
 
x˜l∗, x˜
ˆl
 
is the 
status-quo strategy. 
Specifically, we will consider the disagreement point as division trying to reach 
the one of the.L p−Nash equilibria 
. ˜fl
 
x˜
l∗, x˜
ˆl
 
=
 
∑n
l=1
 
 
 
 
ϕl
 
x˜
l∗
, x˜
ˆl
 
− ϕl
 
xl
, x ˆl
 
 
 
p
 1/p
.12.5 Transfer Price Bargaining Solver with Additional Constraints 303
12.5 Transfer Price Bargaining Solver with Additional 
Constraints 
Let us introduce the “slack” vectors .c ∈ Rn with nonnegative components, that is, 
.c j ≥ 0 for all. j = 1, ..., n, the original problem (12.7.2) can be rewritten as 
.
{g
(
x, xˆ(x)
)
} → max x∈Xadm ,,c≥0
Xadm := {x ∈ X : x ≥ 0, Aeq x = beq , Aineq x − bineq + c = 0
 
.
⎫
⎪⎬
⎪⎭
(12.5.1) 
Notice that this problem may have non-unique solution and.det (
A 
eq Aeq )
= 0. Define 
by.X∗ ⊆ Xadm the set of all solutions of the problem (12.5.1). 
Consider the penalty function given by 
. V˜k (x, c) := μ{g
(
x, xˆ(x)
)
} − k
 
1
2
 
 Aeq x-beq
 
 
2
+1
2
 
 Aineq x-bineq+c
 
 
2
 
,
(12.5.2) 
where the parameters. k and. c are positive. Notice also that 
. arg max x∈Xadm ,c≥0
V˜k,δ (x, c) = arg max x∈Xadm ,c≥0
Vμ,δ (x, c)
where.μ := k−1 > 0 and 
. 
Vμ,δ (x, c) := μ{g
(
x, xˆ(x)
)
} − 1
2
 
 Aeq x − beq
 
 
2 − 1
2
 
 Aineq x − bineq + c
 
 
2
−
δ
2
 
|x|2 + 
 xˆ
 
 
2 + |c|2
 
Obviously, the optimization problem 
.V˜μ,δ (x, c) → max x∈Xadm ,c≥0 (12.5.3) 
has a unique solution since the optimized function (12.5.2) is strongly convex [ 45] if 
.δ > 0. The Penalty Functions Method (PFM) consists on the following idea: If the 
penalty parameter . μ and the regularizing parameter . δ tend to zero by a particular 
manner, then we may expect that .x∗ (μ, δ) and .c∗ (μ, δ), which are the solutions of 
the optimization problem 
. Vμ,δ (x, c) → max x∈Xadm ,c≥0
,
tend to the set.V∗ of all solutions of the original optimization problem (12.5.1), that 
is, 
.ρ
 
x∗ (μ, δ), c∗ (μ, δ); X∗ →μ,δ↓0
0, (12.5.4)304 12 Transfer Pricing as Bargaining
where.ρ {a; X∗} is the Hausdorff distance defined as 
. ρ
 
a; X∗ 
= min
x∈X∗
 
 a − x∗
 
 
2
.
Below we define exactly how the parameters. μ and. δ should tend to zero to provide 
the property (12.5.4). 
Then, if we assume that 
(a) the bounded set.X∗ of all solutions of the original optimization problem (12.5.1) 
is not empty and the Slater’s condition holds, that is, there exists a point. x˚ ∈ Xadm
such that 
.Aineq x˚ < b1, (12.5.5) 
(b) the parameters. μ and. δ are time-varying, i.e., 
. μ = μn, δ = δn (n = 0, 1, 2, ....)
such that 
.0 < μn ↓ 0, μn
δn
↓ 0 when n → ∞. (12.5.6) 
Then 
.
x∗
n := x∗ (μn, δn) →n→∞ x∗∗,
c∗
n := c∗ (μn, δn) →n→∞ c∗∗,
⎫
⎪⎬
⎪⎭
(12.5.7) 
where .x∗∗ ∈ X∗ is the solution of the original problem (12.5.1) with the minimal 
weighted norm, i.e., 
.
 
 x∗∗ 
 ≤ 
 x∗
 
 for all x∗ ∈ X∗ (12.5.8) 
and 
.c∗∗ = b1 − A1x∗∗. (12.5.9) 
The format version.(n = 0, 1, ...) of the proximal method with some fixed admis￾sible initial values.(x0 ∈ X,.xˆ0(x) ∈ Xˆ ,.c0 ≥ 0) is as follows 
.
xn+1= arg max x∈X
 
−1
2 |x − x n|2 + γVμn ,δn (x, xˆn(x), cn)
 
,
xˆn+1(x) = arg max xˆ∈Xˆ
 
−1
2 | ˆx(x) − ˆx n(x)|2+γVμn ,δn (xn, xˆ(x),cn)
 
,
(12.5.10) 
where.Vμn ,δn (x, xˆ(x), c) is given by12.5 Transfer Price Bargaining Solver with Additional Constraints 305
. 
Vμn ,δn (x, xˆn(x), c) := μg (
x, xˆn(x)
)
− 1
2
(
Aineq x − bineq + c
)
− δ
2
 
|x|2 + 
 xˆ
 
 2 + |c|2
 
= μg (
x, xˆn(x)
)
− 1
2
(
Aineq x − bineq + c
)
− δ
2
 
|x|2 + 
 xˆ
 
 2 + |c|2
 
.
Then, developing we have 
. 
xn+1= − 1
2 |x − xn|2 + γVμn ,δn (x, xˆn(x), cn) =
−1
2 |x − xn|2 + γ
 
μg (
x, xˆn(x)
)
− 1
2
(
Aineq x − bineq + c
)
− δ
2
 
|x|2 + 
 xˆ
 
 2 + |c|2
 
. 
xˆn+1(x) = − 1
2 | ˆx(x) − ˆxn(x)|2+γVμn ,δn (xn, xˆ(x),cn)=
−1
2 | ˆx(x) − ˆxn(x)|2 + γ
 
μg (
xn, xˆ(x
)
− 1
2
(
Aineq x − bineq + c
)
− δ
2
 
|x|2 + 
 xˆ
 
 2 + |c|2
 .
12.5.1 Numerical Example for Nash’s Bargaining Transfer 
Pricing 
Our goal is to analyze a three-player non-cooperative bargaining situation in a class 
of discrete-time Markov chains. Let us consider a transfer pricing approach [ 15, 16, 
20, 21], which considers three divisions. We are taking into account that the Markov 
chain game is ergodic. 
Let the number of states.N = 3 and the number of actions.M = 2 for each division. 
The individual utility for each division are defined by 
. U1
i,j,1=
⎡
⎣
11 8 12
7 11 8
11 13 12
⎤
⎦ , U2
i,j,1 =
⎡
⎣
81 9
4 11 13
87 1
⎤
⎦ , U3
i,j,1 =
⎡
⎣
6 8 19
14 11 22
629
⎤
⎦ ,
. U1
i,j,2=
⎡
⎣
3 11 6
2 17 13
17 8 1
⎤
⎦ , U2
i,j,2 =
⎡
⎣
12 7 11
6 9 10
13 9 8
⎤
⎦ , U3
i,j,2 =
⎡
⎣
1 12 5
4 35
21 2 4
⎤
⎦ .
The transition rate matrices, i.e. the matrices with the information about the behavior 
of each division, are defined as follows 
. π1
j|i,1=
⎡
⎣
0.8065 0.0482 0.1454
0.0936 0.7380 0.1684
0.0455 0.0445 0.9100
⎤
⎦ , π1
j|i,2=
⎡
⎣
0.4711 0.1180 0.4109
0.2094 0.3442 0.4464
0.1352 0.1124 0.7524
⎤
⎦ ,306 12 Transfer Pricing as Bargaining
Fig. 12.5 SNE Strategies of 
player. 1
0 5 10 15 20 25 30 35 40 45 50 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 SNE Strategies for Player 1 
c1 (1,1) 
c1 (2,1) 
c1 (3,1) 
c1 (1,2) 
c1 (2,2) 
c1 (3,2) 
Fig. 12.6 SNE Strategies of 
player. 2
0 5 10 15 20 25 30 35 40 45 50 
0.1 
0.12 
0.14 
0.16 
0.18 
0.2 
0.22 SNE Strategies for Player 2 
c2 (1,1) 
c2 (2,1) 
c2 (3,1) 
c2 (1,2) 
c2 (2,2) 
c2 (3,2) 
. π2
j|i,1=
⎡
⎣
0.4741 0.3486 0.1773
0.2143 0.5350 0.2507
0.1094 0.3291 0.5615
⎤
⎦ , π2
j|i,2=
⎡
⎣
0.8009 0.1478 0.0513
0.0941 0.8065 0.0995
0.0253 0.1350 0.8396
⎤
⎦ ,
. π3
j|i,1=
⎡
⎣
0.7298 0.2292 0.0410
0.0503 0.8587 0.0910
0.1956 0.2012 0.6032
⎤
⎦ , π3
j|i,2=
⎡
⎣
0.5079 0.4069 0.0852
0.1116 0.7446 0.1438
0.2817 0.3741 0.3442
⎤
⎦ .
First let’s calculate the starting point of the bargaining process applying the proxi￾mal method (12.5.10) to find the strong Nash equilibrium. We obtain the convergence 
of the strategies in terms of the variable.cl
(i,k) for each player (division).l = 1, n (see 
Figs. 12.5, 12.6 and 12.7) and the convergence of the parameter. λ (see Fig. 12.8).12.5 Transfer Price Bargaining Solver with Additional Constraints 307
Fig. 12.7 SNE Strategies of 
player. 3
0 5 10 15 20 25 30 35 40 45 50 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 SNE Strategies for Player 3 
c3 (1,1) 
c3 (2,1) 
c3 (3,1) 
c3 (1,2) 
c3 (2,2) 
c3 (3,2) 
Fig. 12.8 Convergence of. λ
0 5 10 15 20 25 30 35 40 45 50 
0.326 
0.328 
0.33 
0.332 
0.334 
0.336 
0.338 
0.34 
0.342 Lambda 
1 
2 
3 
The strong Nash equilibrium reached for all players is as follows: 
. c1 =
⎡
⎣
0.1638 0.0843
0.1501 0.0385
0.2445 0.3187
⎤
⎦ , c2 =
⎡
⎣
0.1108 0.1449
0.2168 0.1920
0.1679 0.1676
⎤
⎦ , c3 =
⎡
⎣
0.1482 0.0966
0.2610 0.3168
0.1263 0.0510
⎤
⎦ .
The utilities for each player in the strong Nash equilibrium are . ψ1(c1, c2, c3) =
8.4156,.ψ2(c1, c2, c3) = 8.1339 and.ψ3(c1, c2, c3) = 7.1423. Once the starting point 
is set, the negotiation process between players begins, calculating the strategies until 
they converge.308 12 Transfer Pricing as Bargaining
Fig. 12.9 Strategies of 
player. 1
0 50 100 150 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 
0.5 Strategies for Player 1 
c1 (1,1) 
c1 (2,1) 
c1 (3,1) 
c1 (1,2) 
c1 (2,2) 
c1 (3,2) 
Fig. 12.10 Strategies of 
player. 2
0 50 100 150 
0.04 
0.06 
0.08 
0.1 
0.12 
0.14 
0.16 
0.18 
0.2 
0.22 
0.24 Strategies for Player 2 
c2 (1,1) 
c2 (2,1) 
c2 (3,1) 
c2 (1,2) 
c2 (2,2) 
c2 (3,2) 
In this model each player calculates the strategies independently until they reach an 
agreement. Figures 12.9, 12.10 and 12.11 show the behavior of the offers (strategies) 
during the bargaining process. 
Finally, the agreement reached is as follows: 
. c1 =
⎡
⎣
0.0763 0.1485
0.1093 0.0754
0.1396 0.4508
⎤
⎦ , c2 =
⎡
⎣
0.1604 0.0600
0.2393 0.2027
0.2068 0.1308
⎤
⎦ , c3 =
⎡
⎣
0.1427 0.1160
0.1276 0.4287
0.1250 0.0601
⎤
⎦ .
The mixed strategies obtained for players are as follows12.5 Transfer Price Bargaining Solver with Additional Constraints 309
Fig. 12.11 Strategies of 
player. 3
0 50 100 150 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 
0.4 
0.45 Strategies for Player 3 
c3 (1,1) 
c3 (2,1) 
c3 (3,1) 
c3 (1,2) 
c3 (2,2) 
c3 (3,2) 
Fig. 12.12 Behavior of 
players’ utilities 
0 50 100 150 
6 
6.5 
7 
7.5 
8 
8.5 Utility 
1 (c1 ,c2 ,c3 ) 
2 (c1 ,c2 ,c3 ) 
3 (c1 ,c2 ,c3 ) 
. d1 =
⎡
⎣
0.3395 0.6605
0.5917 0.4083
0.2365 0.7635
⎤
⎦ , d2 =
⎡
⎣
0.7277 0.2723
0.5414 0.4586
0.6126 0.3874
⎤
⎦ , d3 =
⎡
⎣
0.5515 0.4485
0.2294 0.7706
0.6754 0.3246
⎤
⎦ .
With the strategies calculated at each step of the negotiation process, the utilities 
of each player showed a decreasing behavior as shown in the Fig. 12.12, i.e., at 
each step of the bargaining process, the utility of each player decreases until they 
reach an agreement. At the end of the bargaining process, the resulting utilities 
are as follows.ψ1(c1, c2, c3) = 7.0811,.ψ2(c1, c2, c3) = 7.6264 and. ψ2(c1, c2, c3) =
6.0419 for each player.310 12 Transfer Pricing as Bargaining
12.6 Continuous-Time Transfer Pricing 
12.6.1 Revenue of a Passenger Between Members 
of an Airline Alliance 
Our goal is to analyze a three-player non-cooperative bargaining situation in a class 
of continuous-time Markov chains. Let us consider a transfer pricing approach [ 15, 
16, 20, 21] which divide the revenue of a passenger between members of an airline 
alliance. The set of origin-destination time are made up of itineraries. The itineraries 
are either a direct flight or a series of connecting flights within the supply chain 
represented by the airlines network. The game penalizes the revenue taking into 
account the total time that a passenger takes for reaching the final destination. We 
are taking into account only round trips so the Markov chain game is ergodic. 
Let the number of states.N = 3 and the number of actions.M = 2 for each airline. 
The individual utility for each airline are defined by 
. U1
i,j,1=
⎡
⎣
10 8 12
6 11 19
10 14 13
⎤
⎦ , U2
i,j,1 =
⎡
⎣
7 9 11
5 10 14
9 6 10
⎤
⎦ , U3
i,j,1 =
⎡
⎣
17 9 6
19 13 11
328
⎤
⎦ ,
. U1
i,j,2=
⎡
⎣
12 10 5
20 16 14
18 9 11
⎤
⎦ , U2
i,j,2 =
⎡
⎣
15 6 9
15 8 9
12 10 7
⎤
⎦ , U3
i,j,2 =
⎡
⎣
10 12 3
4 10 9
20 17 19
⎤
⎦ .
The transition rate matrices, i.e. the matrices with the information about the behavior 
of each airline, are defined as follows 
. q1
j|i,1=
⎡
⎣
−0.2230 0.0581 0.1649
0.1166 −0.3131 0.1965
0.0504 0.0531 −0.1034
⎤
⎦ q1
j|i,2=
⎡
⎣
−0.8918 0.2323 0.6595
0.4664 −1.2526 0.7862
0.2014 0.2122 −0.4137
⎤
⎦
. q2
j|i,1=
⎡
⎣
−0.9336 0.7250 0.2086
0.4673 −0.9428 0.4755
0.0862 0.6542 −0.7405
⎤
⎦ q2
j|i,2=
⎡
⎣
−0.2334 0.1813 0.0521
0.1168 −0.2357 0.1189
0.0216 0.1636 −0.1851
⎤
⎦
. q3
j|i,1=
⎡
⎣
−0.3297 0.2872 0.0426
0.0473 −0.1738 0.1265
0.2912 0.2401 −0.5313
⎤
⎦ q3
j|i,2=
⎡
⎣
−0.7694 0.6700 0.0993
0.1103 −0.4056 0.2953
0.6794 0.5602 −1.2396
⎤
⎦
First let’s calculate the starting point of the bargaining process applying the prox￾imal method to find the strong Nash equilibrium. We obtain the convergence of the 
strategies in terms of the variable .cl
(i,k) for each player (airline) .l = 1, n (see Figs. 
12.13, 12.14 and 12.15) and the convergence of the parameter. λ (see Fig. 12.16).12.6 Continuous-Time Transfer Pricing 311
Fig. 12.13 SNE Strategies 
of player. 1
Fig. 12.14 SNE Strategies 
of player. 2
Fig. 12.15 SNE Strategies 
of player.3312 12 Transfer Pricing as Bargaining
Fig. 12.16 Convergence of 
. λ
The strong Nash equilibrium reached for all players is as follows: 
. c1 =
⎡
⎣
0.0691 0.1510
0.0464 0.1015
0.1984 0.4336
⎤
⎦ , c2 =
⎡
⎣
0.2163 0.0253
0.3764 0.0440
0.3026 0.0354
⎤
⎦ , c3 =
⎡
⎣
0.0071 0.2237
0.0187 0.5876
0.0050 0.1579
⎤
⎦ .
The utilities for each player in the strong Nash equilibrium are . ψ1(c1, c2, c3) =
3842.4,.ψ2(c1, c2, c3) = 2961.7 and.ψ3(c1, c2, c3) = 3560.3. Once the starting point 
is set, the negotiation process between players begins, calculating the strategies until 
they converge. 
12.6.1.1 The Non-cooperative Bargaining Solution 
In this model each player calculates the strategies independently and alternately 
following the relation (12.5.10) until they reach an agreement. Figures 12.17, 12.18 
and 12.19 show the behavior of the offers (strategies) during the bargaining process. 
Finally, the agreement reached is as follows: 
. c1 =
⎡
⎣
0.2028 0.0173
0.1363 0.0116
0.5824 0.0495
⎤
⎦ , c2 =
⎡
⎣
0.0691 0.1725
0.1202 0.3001
0.0967 0.2413
⎤
⎦ , c3 =
⎡
⎣
0.1320 0.0988
0.3469 0.2594
0.0932 0.0697
⎤
⎦ .
The mixed strategies obtained for players are as follows 
. d1 =
⎡
⎣
0.9216 0.0784
0.9216 0.0784
0.9216 0.0784
⎤
⎦ , d2 =
⎡
⎣
0.2860 0.7140
0.2860 0.7140
0.2860 0.7140
⎤
⎦ , d3 =
⎡
⎣
0.5721 0.4279
0.5721 0.4279
0.5721 0.4279
⎤
⎦ .12.6 Continuous-Time Transfer Pricing 313
Fig. 12.17 Strategies of 
player. 1
Fig. 12.18 Strategies of 
player. 2
Fig. 12.19 Strategies of 
player.3314 12 Transfer Pricing as Bargaining
Fig. 12.20 Behavior of 
players’ utilities 
With the strategies calculated at each step of the negotiation process, the utilities 
of each player showed a decreasing behavior as shown in the Fig. 12.20, i.e., at 
each step of the bargaining process, the utility of each player decreases until they 
reach an agreement. At the end of the bargaining process, the resulting utilities 
are as follows.ψ1(c1, c2, c3) = 678.2,.ψ2(c1, c2, c3) = 1028.0 and. ψ2(c1, c2, c3) =
1394.3 for each player. 
12.7 Extensions 
We now present two extensions of the bargaining game provided above that include 
the case when agents have different discount factors, and another where agents might 
coordinate on their demands. The convergence of results follows trivially from our 
general analysis presented in the appendix above. 
12.7.1 Bargaining Under Different Discounting 
In this approach we present a solution where at each step of the negotiation process 
players calculate the Nash equilibrium considering the utility functions of all players 
but with the particularity that internally each player reaches this equilibrium point 
in a different time. Following the description of the model presented previously, we 
redefine the advantage of propose a new offer that depends on the utility function 
. f (xt, xt+1) := ∑n
l=1
 
ψl
(xt+1) − ψl
(xt)
 
≥ 0,12.7 Extensions 315
for all players to reject the offer.xt and making a new offer.xt+1 given the time spent 
to benefit of this advantage.T (xt+1) > 0, and.αl
(xt) be the weight that players put on 
their advantages to reject the offer. xt . Thus, the advantages to reject the offer.xt and 
to propose a new offer.xt+1 are given by.A(xt, xt+1) = α(xt)T (xt+1) f (xt, xt+1). 
Remark 12.3 The function. f (xt, xt+1) satisfies the Nash condition 
. ψl
(xt+1) − ψl
(xt) ≥ 0,
for any.x ∈ X and all players. 
Definition 12.2 A strategy.x∗ ∈ X is said to be a Nash equilibrium if 
. x∗ ∈Arg max x∈X
{ f (xt, xt+1)}.
Then, at each step of the bargaining game we have in proximal format that the 
players must select their strategies according to 
.x∗ = arg max x∈X
 
−δtT (x)
 
 
(
x − x∗) 
 
2 + αtT (x) f (x, x∗)
 
, (12.7.1) 
where 
. f (x, x∗) := ∑n
l=1
 
ψl
(x) − ψl
(x∗)
 
.
At each step of the bargaining process, players calculate simultaneously the Nash 
equilibrium but considering that each player reach the equilibrium in a different time. 
12.7.1.1 Markov Chains Description 
Let us to define the Nash equilibrium as a strategy.x∗ = (
x 1∗, .., x n
)
such that 
. ψ (
x 1∗, .., x n∗)
≥ ψ (
x 1∗, .., xl
, ..., x n∗)
for any.xl ∈ X. 
Consider that players try to reach the Nash equilibrium of the bargaining problem, 
that is, to find a joint strategy.x∗ = (
x 1∗, ..., x n∗
)
.∈.X satisfying for any admissible 
.xl ∈ Xl and any. l = 1, n
. f (x, xˆ(x)) := ∑n
l=1
 
ψl
 
xl
, x ˆl
 
− ψl
 
x¯
l
, x ˆl
 ≤ 0, (12.7.2) 
where .xˆ = (x 1ˆT, ..., xnˆT)T ∈ Xˆ ⊆ Rn(n−1) [ 49, 50], .x¯l is the utopia point defined 
as Eq. (12.4.6) and.ψl
 
xl
, x ˆl
 
is the concave cost-function of player . l which plays316 12 Transfer Pricing as Bargaining
the strategy.xl ∈ Xl and the rest of players the strategy.x ˆl ∈ Xˆl considering the time 
function. 
Recall that a strategy.x∗ ∈ X is a Nash equilibrium if 
. x∗ ∈ Arg max x∈Xadm
 
f (x, xˆ(x)) 
.
Remark 12.4 If. f (x, xˆ(x)) is strictly concave then 
. x∗ = arg max x∈Xadm
 
f (x, xˆ(x)) 
.
We redefine the utility function that depends of the average utility function of all 
players as follows 
. 
F(x, xˆ(x)) := f (x, xˆ(x)) − 1
2
∑n
l=1
∑
N
j=1
μl
(j)hl
(j)(xl
)−
1
2
∑n
l=1
∑
N
i=1
∑
N
j=1
∑
M
k=1
ξl
(j)ql
(j|i,k)xl
(i,k) − 1
2
∑n
l=1
∑
N
i=1
∑
M
k=1
ηl
 
xl
(i,k) − 1
 
,
then we may conclude that 
.x∗ = arg max x∈X,xˆ∈Xˆ
min
μ≥0,ξ≥0,η≥0
F(x, xˆ(x), μ, ξ, η). (12.7.3) 
Finally we have that at each step of the bargaining process, players calculate the 
Nash equilibrium (but they reach the equilibrium at different time) according to the 
solution of the non-cooperative bargaining problem in proximal format defined as 
follows 
.
μ∗ = arg min
μ≥0
 
−δ|μ − μ∗|2 + αF (
x∗, xˆ∗(x), μ, ξ∗, η∗
) ,
ξ∗ = arg min ξ≥0
 
−δ|ξ − ξ∗|2 + αF (
x∗, xˆ∗(x), μ∗, ξ, η∗
) ,
η∗ = arg min
η≥0
 
−δ|η − η∗|2 + αF (
x∗, xˆ∗(x), μ∗, ξ∗, η
) ,
x∗ = arg max x∈X
 
−δ |(x − x∗)|2
 + αF (
x, xˆ∗(x), μ∗, ξ∗, η∗
) ,
xˆ∗ = arg max xˆ∈Xˆ
 
−δ
 
 
(
xˆ − ˆx∗
) 
 
2
 + αF (
x∗, xˆ(x), μ∗, ξ∗, η∗
) 
.
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
(12.7.4) 
12.7.1.2 Transfer Pricing Simulation 
Following the Section above, in this model each player calculates the strategies 
according the Nash equilibrium formulation where players calculate the Nash equi￾librium simultaneously, but with the characteristic that they reach the equilibrium at 
different time, following the relation (12.7.4) until they reach an agreement (strate-12.7 Extensions 317
Fig. 12.21 Strategies of 
player. 1 in the bargaining 
model 2 
Fig. 12.22 Strategies of 
player. 2 in the bargaining 
model 2 
gies show convergence). Figures 12.21, 12.22 and 12.23 show the behavior of the 
offers (strategies) during the bargaining process. 
Finally, the agreement reached is as follows: 
. c1 =
⎡
⎣
0.2127 0.0074
0.1429 0.0050
0.6106 0.0214
⎤
⎦ , c2 =
⎡
⎣
0.0050 0.2366
0.0087 0.4117
0.0070 0.3310
⎤
⎦ , c3 =
⎡
⎣
0.2237 0.0071
0.5877 0.0186
0.1579 0.0050
⎤
⎦ .
The mixed strategies obtained for players are as follows 
. d1 =
⎡
⎣
0.9662 0.0338
0.9662 0.0338
0.9662 0.0338
⎤
⎦ , d2 =
⎡
⎣
0.0207 0.9793
0.0207 0.9793
0.0207 0.9793
⎤
⎦ , d3 =
⎡
⎣
0.9693 0.0307
0.9693 0.0307
0.9693 0.0307
⎤
⎦ .318 12 Transfer Pricing as Bargaining
Fig. 12.23 Strategies of 
player. 3 in the bargaining 
model 2 
Fig. 12.24 Behavior of 
players’ utilities in the 
bargaining model 2 
With the strategies calculated at each step of the negotiation process, the util￾ities of each player showed a decreasing behavior as shown in the Fig. 12.24, 
i.e., at each step of the bargaining process, the utility of each player decreases 
until they reach an agreement. At the end of the bargaining process, the result￾ing utilities are as follows.ψ1(c1, c2, c3) = 986.8936,.ψ2(c1, c2, c3) = 651.4633 and 
.ψ2(c1, c2, c3) = 949.6980 for each player. 
12.7.2 Bargaining with Collusive Behavior 
In this approach we analyze a bargaining situation where players make groups and 
alternately each group makes an offer to the others until they reach an equilibrium12.7 Extensions 319
point (agreement). We describe a bargaining model with two teams of players as 
follows. Let us consider a bargaining game with .n + m players. Let . n = {1, ..., n}
denote the set of players called team A and let’s define the behavior of all players 
.l = 1, n as.xt = (x 1
t , ..., x n
t ) ∈ X where.X is a convex and compact set. In the same 
way, the rest .M = {1, ..., m} players are the team B and let the set of the strategy 
profiles of all player .m = 1, m be defined by .yt = (y1
t , ..., ym
t ) ∈ Y where .Y is a 
convex and compact set. Then,.X × Y in the set of full strategy profiles. In this model 
the function .ψ(x, y) represents the utility function of team A which determines the 
decision of accept or reject the offer; similarly, team B makes the decision according 
to its utility function.ϕ(x, y). 
Following the description of the model presented above, we redefine the advantage 
of propose a new offer considering the utility function for team A as follows 
. f (xt, yt, xt+1, yt+1) := ∑n
l=1
 
ψl
(xt+1, yt) − ψl
(xt, yt)
 
≥ 0,
and, similarly the utility function for team B is as follows 
. g(xt, yt, xt+1, yt+1) := ∑m
m=1
 
ϕl
(xt, yt+1) − ϕl
(xt, yt)
 
≥ 0.
Thus, the advantages for team A to reject the offer.xt and to propose a new offer. xt+1
are given by .A(xt, yt, xt+1, yt+1) = α(xt)T (xt+1) f (xt, yt, xt+1, yt+1); in the same 
way, the advantages for team B to reject the offer.yt and to propose a new offer. yt+1
are given by 
. A(xt, yt, xt+1, yt+1) = α(yt)T (yt+1)g(xt, yt, xt+1, yt+1).
Remark 12.5 The function. f (xt, yt, xt+1, yt+1) implies the Nash condition 
. ψl
(xt+1, yt) − ψl
(xt, yt) ≥ 0,
for any.x ∈ X,.y ∈ Y and.l = 1, n players. 
Remark 12.6 The function.g(xt, yt, xt+1, yt+1) implies the Nash condition 
. ϕl
(xt, yt+1) − ϕl
(xt, yt) ≥ 0,
for any.x ∈ X,.y ∈ Y and.m = 1, m players. 
The dynamics of the bargaining game is as follows: at each step of the negoti￾ation process the team A chooses a strategy .x ∈ X considering the utility function 
. f (xt, yt, xt+1, yt+1), then team B must decide between to accept or reject the offer320 12 Transfer Pricing as Bargaining
calculating a new offer (strategies) .y ∈ Y considering the utility function of the 
group .g(xt, yt, xt+1, yt+1). Following the description of the model 1, now we have 
that teams solve the problem in proximal format as follows: 
.
x∗ = arg max x∈X
 
−δtT (x) |(x − x∗)|2 + αtT (x) f (x, y, x∗, y∗)
 
,
y∗ = arg max
y∈Y
 
−δtT (y) |(y − y∗)|2 + αtT (y)g(x, y, x∗, y∗)
 
,
⎫
⎪⎬
⎪⎭
(12.7.5) 
where 
. 
f (x, y, x∗, y∗) := ∑n
l=1
 
ψl
(x, y∗) − ψl
(x∗, y∗)
 
,
g(x, y, x∗, y∗) := ∑m
m=1
[ϕm(x∗, y) − ϕm(x∗, y∗)] .
At each step, teams make a new offer according to Eq. (12.7.5), both teams solve 
the bargaining problem together but they reach the equilibrium at different time, the 
bargaining game continues until the offers (strategies) of all player show convergence. 
12.7.2.1 Markov Chains Description 
For this model, in the same way that we define the strategies.x ∈ X, let us consider 
a set of strategies denoted by .ym ∈ Y m .
(
m = 1, m
)
where .Y := m
m=1
Yl is a convex 
and compact set, 
. ym := col (cm), Y m := Cm
adm,
where.col is the column operator. 
Denote by .y = (y1, ..., ym)T ∈ Y , the joint strategy of the players and .ymˆ is a 
strategy of the rest of the players adjoint to.ym, namely, 
. ymˆ := (
y1
, ..., ym−1
, ym+1
, ..., ym)T
∈ Y mˆ := m
h=1, h/=m
Y h
such that.y = (ym, ymˆ),.m = 1, m. 
Consider that players of team A try to reach the Nash equilibrium of the bargaining 
problem, that is, to find a joint strategy .x∗ = (
x 1∗, ..., x n∗
)
.∈ .X satisfying for any 
admissible.xl ∈ Xl and any. l = 1, n
. f (x, xˆ(x)|y) := ∑n
l=1
 
ψl
 
xl
, x ˆl
|y
 
− ψl
 
x¯
l
, x ˆl
|y
 ≤ 0, (12.7.6)12.7 Extensions 321
where.xˆ = (x 1ˆT, ..., xnˆT)T ∈ Xˆ ⊆ Rn(n−1) [ 49, 50],.x¯l is the utopia point defined as 
Eq. (12.4.6) and .ψl
 
xl
, x ˆl
|y
 
is the concave cost-function of player . l which plays 
the strategy.xl ∈ Xl and the rest of players the strategy.x ˆl ∈ Xˆl fixing the strategies 
.y ∈ Y of team B, and it is defined as Eq. (11.4.15) considering the time function. 
Similarly, consider that players of team B also try to reach the Nash equilibrium 
of the bargaining problem, that is, to find a joint strategy .y∗ = (
y1∗, ..., ym∗
)
.∈ . Y
satisfying for any admissible.ym ∈ Y m and any. m = 1, m
.g(y, yˆ(y)|x) := ∑m
m=1
 
ψm 
ym, ymˆ
|x
 
− ψm 
y¯
m, ymˆ
|x
 ≤ 0, (12.7.7) 
where .yˆ = (y1ˆT, ..., ymˆ T)T ∈ Yˆ ⊆ Rm(m−1)
, .y¯m is the utopia point defined as Eq. 
(12.4.6) and.ψm (
ym, ymˆ |x
)
is the concave cost-function of player. m which plays the 
strategy .ym ∈ Y m and the rest of players the strategy .ymˆ ∈ Y mˆ fixing the strategies 
.x ∈ X of team A, and it is defined as Eq. (11.4.15) considering the time function. 
Then, we have that a strategy .x∗ ∈ X of team A together with the collection 
.y∗ ∈ Y of team B are defined as the equilibrium of a strictly concave bargaining 
problem if 
. (x∗, y∗) = arg max x∈Xadm,y∈Yadm
 
f (x, xˆ(x)|y) ≤ 0, g(y, yˆ(y)|x) ≤ 0
 
.
We redefine the utility function that depends of the average utility function of all 
players as follows 
. 
F(x, xˆ(x), y, yˆ(y)) := f (x, xˆ(x)|y) + g(y, yˆ(y)|x) − 1
2
∑n
l=1
∑
N
j=1
μl
(j)hl
(j)(xl
)−
1
2
∑m
m=1
∑
N
j=1
μm
(j)hm
(j)(ym) − 1
2
∑n
l=1
∑
N
i=1
∑
N
j=1
∑
M
k=1
ξl
(j)ql
(j|i,k)xl
(i,k)−
1
2
∑m
m=1
∑
N
i=1
∑
N
j=1
∑
M
k=1
ξm
(j)qm
(j|i,k) ym
(i,k) − 1
2
∑n
l=1
∑
N
i=1
∑
M
k=1
ηl
 
xl
(i,k) − 1
 
−
1
2
∑m
m=1
∑
N
i=1
∑
M
k=1
ηm
 
ym
(i,k) − 1
 
,
then, we may conclude that 
.(x∗, y∗) = arg max x∈X,xˆ∈Xˆ ,y∈Y,yˆ∈Yˆ
min
μ≥0,ξ≥0,η≥0
F(x, xˆ(x), y, yˆ(y), μ, ξ, η). (12.7.8)322 12 Transfer Pricing as Bargaining
Finally, we have that at each step of the bargaining process, players calculate their 
equilibrium according to the solution of the non-cooperative bargaining problem in 
proximal format defined as follows 
. 
μ∗ = arg min
μ≥0
 
−δ|μ − μ∗|2 + αF (
x∗, xˆ∗(x), y∗, yˆ∗(y), μ, ξ∗, η∗
) ,
ξ∗ = arg min ξ≥0
 
−δ|ξ − ξ∗|2 + αF (
x∗, xˆ∗(x), y∗, yˆ∗(y), μ∗, ξ, η∗
) ,
η∗ = arg min
η≥0
 
−δ|η − η∗|2 + αF (
x∗, xˆ∗(x), y∗, yˆ∗(y), μ∗, ξ∗, η
) ,
x∗ = arg max x∈X
 
−δ |(x − x∗)|2
 + αF (
x, xˆ∗(x), y∗, yˆ∗(y), μ∗, ξ∗, η∗
) ,
xˆ∗ = arg max xˆ∈Xˆ
 
−δ
 
 
(
xˆ − ˆx∗
) 
 
2
 + αF (
x∗, xˆ(x), y∗, yˆ∗(y), μ∗, ξ∗, η∗
) 
,
y∗ = arg max
y∈Y
 
−δ |(y − y∗)|2
 + αF (
x∗, xˆ∗(x), y, yˆ∗(y), μ∗, ξ∗, η∗
) ,
yˆ∗ = arg max
yˆ∈Yˆ
 
−δ
 
 
(
yˆ − ˆy∗
) 
 
2
 + αF (
x∗, xˆ∗(x), y∗, yˆ(y), μ∗, ξ∗, η∗
) 
⎫
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭
.
(12.7.9) 
12.7.2.2 Transfer Pricing Simulation 
For this example, the team. 1 is only formed by player. 1 while team. 2 is composed of 
players. 2 and. 3. Although the players calculate the strategies together following the 
relation (12.7.9), we consider that players reach the equilibrium at different times. 
Figures 12.25, 12.26 and 12.27 show the behavior of the offers (strategies) during 
the bargaining process. 
Finally, the agreement reached is as follows: 
Fig. 12.25 Strategies of 
player. 1 in the bargaining 
model 312.7 Extensions 323
Fig. 12.26 Strategies of 
player. 2 in the bargaining 
model 3 
Fig. 12.27 Strategies of 
player. 3 in the bargaining 
model 3 
. c1 =
⎡
⎣
0.2127 0.0074
0.1429 0.0050
0.6106 0.0214
⎤
⎦ , c2 =
⎡
⎣
0.0050 0.2366
0.0087 0.4117
0.0070 0.3310
⎤
⎦ , c3 =
⎡
⎣
0.2237 0.0071
0.5877 0.0186
0.1579 0.0050
⎤
⎦ .
The mixed strategies obtained for players are as follows 
. d1 =
⎡
⎣
0.9662 0.0338
0.9662 0.0338
0.9662 0.0338
⎤
⎦ , d2 =
⎡
⎣
0.0207 0.9793
0.0207 0.9793
0.0207 0.9793
⎤
⎦ , d3 =
⎡
⎣
0.9693 0.0307
0.9693 0.0307
0.9693 0.0307
⎤
⎦ .
With the strategies calculated at each step of the negotiation process, the utilities of 
each player showed a decreasing behavior as shown in the Fig. 12.28, i.e., at each 
step of the bargaining process, the utility of each player decreases until they reach 
an agreement. At the end of the bargaining process, the resulting utilities are as324 12 Transfer Pricing as Bargaining
Fig. 12.28 Behavior of 
players’ utilities in the 
bargaining model 3 
follows.ψ1(c1, c2, c3) = 986.8936, .ψ2(c1, c2, c3) = 651.4631 and. ψ2(c1, c2, c3) =
949.6978 for each player. 
The following figure shows the behavior of the utilities at each of the applied 
models (model 1 is the general bargaining model, model 2 corresponds to bargaining 
under different discounting and model 3 to bargaining with collusive behavior), we 
can see that the utilities begin at the same point, the strong Nash equilibrium, and 
then decrease until the strategies converge (see Fig. 12.29). From the results obtained 
we observed that model. 1 favors the utilities of players. 2 and. 3, while model. 2 and. 3
are better for player. 1. We also observed that even if models. 2 and. 3 reach the same 
agreement (equilibrium point) the strategies and, as a consequence, the utilities have 
a different behavior during the bargaining process. 
Fig. 12.29 Behavior of the utilities at each modelReferences 325
References 
1. Abdel-Khalik, A., Lusk, E.: Transfer pricing - a synthesis. Account Rev. 49(1), 8–23 (1974) 
2. Alm, J.: Measuring, explaining, and controlling tax evasion: lessons from theory, experiments, 
and field studies. Int. Tax Public Finance 19(1), 54–77 (2012) 
3. Amershi, A., Cheng, P.: Intrafirm resource allocation: the economics of transfer pricing and 
cost allocations in accounting. Contemp. Account Res. 7(1), 61–99 (1990) 
4. Arrow, K.J.: Contributions to scientific research in management, chapter. In: Optimization, 
Decentralization, and Internal Pricing in Business Firms, pp. 9–18. Western Data Processing 
Center, Graduate School of Business Administration, UCLA (1959) 
5. Baldenius, T., Reichelstein, S.: External and internal pricing in multidivisional firms. J. Account 
Res. 44(1), 1–28 (2006) 
6. Baldenius, T., Reichelstein, S., Sahay, S.: Negotiated versus cost-based-transfer pricing. Rev. 
Account Stud. 4(2), 67–91 (1999) 
7. Baumol, W.J., Fabian, T.: Decomposition, pricing for decentralization and external economies. 
Manag. Sci. 11(1), 1–32 (1964) 
8. Beer, S., Loeprick, J.: Profit shifting: drivers of transfer (mis)pricing and the potential coun￾termeasures. Int. Tax Public Finance 22(3), 426–451 (2015) 
9. Besanko, D., Sibley, D.S.: Compensation and transfer pricing in a principal-agent model. Int. 
Econ. Rev. 32(1), 55–68 (1991) 
10. Blois, K.J.: Pricing of supplies by large customers. J. Bus. Finance Account 3, 367–379 (1978) 
11. Burton, R.M., Damon, W.W., Loughrid, D.W.: The economics of decomposition: re-source 
allocation versus transfer pricing. Decis. Sci. 5(3), 297–310 (1974) 
12. Chalos, P., Haka, S.: Transfer pricing under bilateral bargaining. Account. Rev. 65(3), 624–641 
(1990) 
13. Chwolka, A., Martini, J.T., Simons, D.: The value of negotiating cost-based transfer prices. 
BuR 3(2), 113–131 (2010) 
14. Clempner, J.B.: Necessary and sufficient karush-kuhn-tucker conditions for multiobjective 
markov chains optimality. Automatica 71, 135–142 (2016) 
15. Clempner, J.B.: Strategic manipulation approach for solving negotiated transfer pricing prob￾lem. J. Optim. Theory Appl. 178(1), 304–316 (2018) 
16. Clempner, J.B.: Penalizing passenger’s transfer time in computing airlines revenue. Omega, 
97, 102099 (2020). https://doi.org/10.1016/j.omega.2019.08.006 
17. Clempner, J.B., Poznyak, A.S.: Convergence method, properties and computational complexity 
for lyapunov games. Int. J. Appl. Math. Comput. Sci. 21(2), 349–361 (2011) 
18. Clempner, J.B., Poznyak, A.S.: Convergence analysis for pure and stationary strategies in 
repeated potential games: nash, lyapunov and correlated equilibria. Expert Syst. Appl. 46, 
474–484 (2016) 
19. Clempner, J.B., Poznyak, A.S.: Multiobjective markov chains optimization problem with strong 
pareto frontier: principles of decision making. Expert Syst. Appl. 68, 123–135 (2017) 
20. Clempner, J.B., Poznyak, A.S.: Negotiating the transfer pricing using the nash bargaining 
solution. Int. J. Appl. Math. Comput. Sci. 27(4), 853–864 (2017) 
21. Clempner, J.B., Poznyak, A.S.: Computing the transfer pricing for a multidivisional firm based 
on cooperative games. Econ. Comput. Econ. Cybern. Stud. Res. 52(1), 107–126 (2018) 
22. Dearden, J.: Cast Accounting and Financial Control Systems. Addison Wesley, Reno (1973) 
23. Devereux, M., Maffini, G.: The Impact of Taxation on the Location of Capital, Firms and Profit: 
A Survey of Empirical Evidence. Oxford University Centre for Business Taxation (2007) 
24. Edlin, A.S., Reichelstein, S.: Specific investment under negotiated transfer pricing: an efficiency 
result. Account Rev. 70(2), 275–291 (1995) 
25. Enzer, H.: The static theory of transfer pricing. Nav. Res. Logist. Q. 22(2), 375–389 (1975) 
26. Forgó, F., Szép, J., Szidarovszky, F.: Introduction to the Theory of Games: Concepts, Methods, 
Applications. Kluwer Academic Publishers (1999) 
27. Fredrickson, J.W.: The strategic decision process and organizational structure. Acad. Manag. 
Rev. 11(2), 280–297 (1986)326 12 Transfer Pricing as Bargaining
28. Ghosh, P., Roy, N., Das, S.K., Basu, K.: A game theory based pricing strategy for job allocation 
in mobile grids. In: Proceedings of the 18th International Parallel and Distributed Processing 
Symposium, pp. 82–92. Santa Fe, New Mexico, USA (2004) 
29. Grabski, S.V.: Readings in accounting for management control, chapter. In: Transfer Pricing 
in Complex Organizations: A Review and Integration of Recent Empirical and Analytical 
Research, pp. 453–495. Springer US (1982) 
30. Haake, C.J., Martini, J.T.: Negotiating transfer prices. Group Decis. Negot. 22(4), 657–680 
(2013) 
31. Henderson, B.D., Dearden, J.: New system for divisional control. Harv. Bus. Rev. 44(5), 144– 
146 (1966) 
32. Hirshleifer, J.: On the economics of transfer pricing. J. Bus. 29, 172–184 (1956) 
33. Hirshleifer, J.: Economics of the divisionalized firm. J. Bus. 30(2), 96–108 (1957) 
34. Jennergren, L.P.: The static theory of transfer pricing. Nav. Res. Logist. Q. 24(2), 373–376 
(1977) 
35. Johnson, N.: Divisional performance measurement and transfer pricing for intangible assets. 
Rev. Account Stud. 11(2/3), 339–365 (2006) 
36. Kanodia, C.: Risk sharing and transfer price systems under uncertainty. J. Account Res. 17(1), 
74–75 (1979) 
37. Karpowicz, M.P.: Nash equilibrium design and price-based coordination in hierarchical sys￾tems. Int. J. Appl. Math. Comput. Sci. 22(4), 951–969 (2012) 
38. Leng, M., Parlarb, M.: Transfer pricing in a multidivisional firm: a cooperative game analysis. 
Oper. Res. Lett. 40(5), 364–369 (2012) 
39. Leng, M., Parlarb, M.: Transfer pricing in a multidivisional firm: a cooperative game analysis. 
Oper. Res. Lett. 40(5), 364–369 (2012) 
40. Markides, C.C., Williamson, P.J.: Corporate diversification and organizational structure: a 
resource-based view. Acad. Manag. J. 39(2), 340–367 (1996) 
41. McAulay, L., Scrace, A., Tomkins, C.: Transferring priorities: a three-act play on transfer 
pricing. Crit. Perspect. Account 12(1), 87–113 (2001) 
42. Muthoo, A.: Bargaining Theory with Applications. Cambridge University Press (2002) 
43. Nash, J.F.: The bargaining problem. Econometrica 18(2), 155–162 (1950) 
44. OECD: OECD Transfer Pricing Guidelines for Multinational Enterprises and Tax Administra￾tions 2010. OECD Publishing (2010) 
45. Poznyak, A.S.: Advanced Mathematical tools for Automatic Control Engineers. Deterministic 
technique, vol. 1. Elsevier, Amsterdam (2008) 
46. Ronen, J., Balachandran, K.R.: An approach to transfer pricing under uncertainty. J. Account 
Res. 26(2), 300–3114 (1988) 
47. Rosenthal, E.C.: A game theoretic approach to transfer pricing in a vertically integrated supply 
chain. Int. J. Prod. Econ. 115(2), 542–552 (2008) 
48. Rubinstein, A.: Perfect equilibrium in a bargaining model. Econometrica 50(1), 97–109 (1982) 
49. Tanaka, K.: The closest solution to the shadow minimum of a cooperative dynamic game. 
Comput. Math. Appl. 18(1–3), 181–188 (1989) 
50. Tanaka, K., Yokoyama, K.: On.∈-equilibrium point in a noncooperative n-person game. J. Math. 
Anal. 160(2), 413–423 (1991) 
51. Thomas, A.: A behavioral analysis of joint cost allocation and transfer pricing. Technical report, 
Artbur Andersen & Co. Lecture Series 1977. Stipes Publishing Company (1980) 
52. Trejo, K.K., Clempner, J.B.: New perspectives and applications of modern control theory: in 
honor of Alexander S. Poznyak, chapter. In: Continuous Time Bargaining Model in Controllable 
Markov Games: Nash versus Kalai-Smorodinsky. Springer International Publishing (2018) 
53. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Computing the stackelberg/nash equilibria using 
the extraproximal method: convergence analysis and implementation details for markov chains 
games. Int. J. Appl. Math. Comput. Sci. 25(2), 337–351 (2015) 
54. Trejo, K.K., Clempner, J.B., Poznyak, A.S.: Nash bargaining equilibria for controllable markov 
chains games. In: The 20th World Congress of The International Federation of Automatic 
Control (IFAC), pp. 12772–12777. Toulouse, France (2017)References 327
55. Vaysman, I.: A model of negotiated transfer pricing. J. Account Econ. 25(3), 349–384 (1998) 
56. Wahab, O.A., Bentahar, J., Otrok, H., Mourad, A.: A stackelberg game for distributed formation 
of business-driven services communities. Expert Syst. Appl. 45(1), 359–372 (2016) 
57. Watson, D.J.H., Baumler, J.V.: Transfer pricing: a behavioral context. Account Rev. 50(3), 
466–574 (1975) 
58. Wielenberg, S.: Negotiated transfer pricing, specific investment, and optimal capacity choice. 
Rev. Account Stud. 5(3), 197–216 (2000)Index 
A 
Adaptive policies, 49 
Agreements, 255 
Allocation rule, 141 
Arm-length rule, 290 
Arm’s length price, 299 
Arm’s-length principle, 289 
Arm’s length standard, 290 
ASG continuous-time algorithm, 91 
Average cost function, 87 
B 
Banks Marketing Planning, 127 
Bargaining game 
collusive behavior, 270 
different discounting, 268 
disagreement penalties, 256 
Bargaining paradigm, 185 
Bargaining problem, 190, 293 
Bargaining solution 
non-cooperative, 264 
Bargaining solution payoff, 302 
Bayesian incentive-compatible mecha￾nisms, 139 
Bayesian-Nash equilibrium, 139, 162 
Bayesian partially observable system, 160 
Best reply strategies, 115, 119 
Birth-death process, 75 
C 
C variables, 9, 52, 88 
Chapman-Kolmogorov equation, 3 
Chemical Master Equation, 76 
Chemical Reaction Network, 74 
Coefficient of ergodicity, 6 
Conflict situation, 89 
Continuous-time bargaining, 294 
Continuous-time bargaining game, 254,294 
Continuous-time Markov decision process, 
65, 67 
Contracting problem, 150 
Controllable Markov chain, 8, 140 
Control policy, 8 
randomized, 8 
stationary, 8 
Cost function 
average, 9 
D 
Disagreement point, 208, 302 
Disagreement point (status quo), 292, 293 
Disagreement vector, 196, 293 
Discounted reward, 65 
Discount factor, 295 
Divisional autonomy, 298 
Division game, 300 
Divisions, 290 
Divisions profit, 296 
Division’s utility, 297 
Division utility function, 300 
Duel game, 129 
Dynamic of the game, 87 
E 
Efficient frontier, 30 
Equilibrium 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer 
Nature Switzerland AG 2024 
J. B. Clempner and A. Poznyak, Optimization and Games for Controllable 
Markov Chains, Studies in Systems, Decision and Control 504, 
https://doi.org/10.1007/978-3-031-43575-1 
329330 Index
Bayesian-Nash, 142 
Equilibrium point 
stable, 116 
Ergodicity constraints, 73, 165 
Euler approach, 94 
Expected returns, 29, 31, 32 
Extraproximal method, 93 
Extraproximal procedure, 102 
F 
Fair agreement, 290 
Feasibility, 196 
Feasible joint observers, 52, 161 
Feasible payoffs, 293 
Function vector format, 121 
G 
Gradient solver, 238 
H 
Hausdorff distance, 304 
Hybrid optimization, 222 
I 
Independence, 197 
Independence of irrelevant alternatives, 189 
Independent demands, 296 
Indicator function, 147 
Individual aim, 89 
leader 
follower, 101 
Intermediate goods, 296 
Intersection avoiding, 242 
Intra-firm transaction problem, 298 
Invariance, 197 
Invariant to affine transformations, 189 
J 
Joint observer, 52, 161 
Joint strategy variable, 71 
K 
Kalai-Smorodinsky, 186 
Kalai-Smorodinsky solver, 202 
Kiefer–Wolfowitz procedure, 21 
Kolmogorov forward equations, 68 
L 
Lagrange function 
regularized, 89 
Lineal programming, 11 
Linear demand function, 296 
Linear programming, 66 
solver, 72 
Linear scalarization, 19 
Local-optimal, 119 
Local-optimal (best-reply) strategy, 117 
Local-optimal policies, 119 
Long-run expected average reward, 65, 70 
Lyapunov equilibrium point, 116 
Lyapunov function, 116 
Lyapunov function design, 125 
Lyapunov games, 116, 123 
Lyapunov games concept, 116 
Lyapunov-like function, 116, 124 
M 
Markov chain, 1 
controllable, 7 
ergodic, 4 
Markov condition, 1, 140 
Markov decision process, 10 
optimal, 11 
Markov property, 2 
Markowitz bullet, 30 
Markowitz function, 31 
Markowitz portfolio 
partially observable, 57 
Mean-variance diagram, 32 
Mean-variance portfolio, 33 
Mechanism, 140, 161 
Mechanism design, 137, 156 
Mechanisms 
admissible, 141 
Message, 140 
Monotonicity, 198 
Multidivisional firm, 290 
Multi-objective optimization problem, 18 
Myopic policy, 119 
N 
Nash axioms, 189 
Nash bargaining, 196 
Nash bargaining issue, 186 
Nash bargaining solver, 201 
Nash equilibrium, 85, 116 
global unique, 89 
regularity property, 188Index 331
strong, 89 
.ε-Nash equilibrium, 90 
.ε-Nash equilibrium condition, 90 
Nash’s bargaining game, 293 
Nash solution, 294 
Non-cooperative bargaining, 249 
O 
Observation kernel, 48 
Observation process, 49, 159 
Optimal transfer price, 290 
P 
Pareto front, 18 
Pareto optimality, 189, 197 
Pareto optimum, 18 
Pareto points 
parametrization, 19 
Pareto policy, 18 
Partially observable Markov chains, 47 
Partially observable Markov decision pro￾cess, 159 
Partly Observable Markov Games, 156 
Patrolling, 172 
Penalty function, 23, 303 
Poisson distribution, 231 
Policies, 72 
admissible, 142 
POMDP, 48, 49 
Portfolio optimization, 29 
Portfolio return variance, 31 
Pricing, 296 
Principles of fairness, 189 
Prisoner’s Dilemma, 127 
Probability space, 1 
Production costs, 297 
Profit allocation approach, 290 
Profit-maximizing allocation, 292 
Projection Gradient Method, 239 
Proximal format, 93 
Proximal method, 304 
R 
Random walk, 174 
Rational investor, 32 
Rationality, 196 
Recover 
behavior strategies, 164 
mechanism, 163 
observer, 164 
stationary distributions, 164 
Recovering relationships, 145 
Reinforcement learning (RL), 146, 167 
Repeated games, 116 
Revelation principle, 143 
Risk, 32 
Risk-averse agents, 150 
Risk-aversion parameter, 58 
Risk-free asset, 33 
Risk-neutral, 187 
Rubinstein bargaining scenario, 250 
S 
Sales quantity, 296 
Signal controller, 230 
Simplex, 20 
Slack vectors, 303 
Social choice function, 140 
Stackelberg model, 100 
Stackelberg-Nash equilibrium, 102 
Standard deviation, 30 
State-value function, 120, 121 
Stationary distribution, 72 
Stationary distribution vector, 69 
Stochastic matrix, 2 
Stockholder, 58 
Strategy 
behavioral, 142 
Subgame perfect equilibrium, 254, 255, 295 
Supermarkets chain, 108 
Supply chain, 296 
Symmetry, 189, 197 
T 
Tanaka’s function, 90, 101 
regularized, 91, 102 
Tatonnement equilibrium, 257 
Taxes, 297 
Time constraints, 73 
Time-homogeneous, 2 
Traffic optimization, 222 
Traffic-signal control, 222 
Traffic-signal-control problem, 230 
Transfer price, 297 
maximum and minimum, 299 
Transfer price negotiation, 290 
Transfer pricing, 289, 290, 296 
airline alliance, 310 
continuous-time, 310 
non-cooperative bargaining, 312 
under different discounting, 314 
with collusive behavior, 318332 Index
Transfer pricing bargaining game, 302 
Transfer pricing game, 298 
Transfer pricing model, 297 
Transfer pricing Nash bargaining, 302 
Transition equation, 7 
Transition matrix, 2 
controlled, 8 
Transition probability, 2 
Transition rates, 68 
U 
Uniqueness of the SNE, 96 
Unit production cost, 297 
Unit simplex, 122 
Unsophisticated agent, 250 
Urban modeling, 27 
Utopia point, 21 
V 
Valuation function, 140 
Vector average cost function, 119 
Vehicle routing planning, 27 
Vertically integrated divisions, 296 
Volatility, 30 
W 
Weighted objective function, 20
