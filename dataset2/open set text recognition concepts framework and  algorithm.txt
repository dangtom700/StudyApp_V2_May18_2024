SpringerBriefs in Computer Science
Xu-Cheng Yin · Chun Yang · Chang Liu
Open-Set Text 
Recognition
Concepts, Framework, and 
AlgorithmsSpringerBriefs in Computer ScienceSpringerBriefs present concise summaries of cutting-edge research and practical 
applications across a wide spectrum of fields. Featuring compact volumes of 50 to 
125 pages, the series covers a range of content from professional to academic. 
Typical topics might include: 
• A timely report of state-of-the art analytical techniques 
• A bridge between new research results, as published in journal articles, and a 
contextual literature review 
• A snapshot of a hot or emerging topic 
• An in-depth case study or clinical example 
• A presentation of core concepts that students must understand in order to make 
independent contributions 
Briefs allow authors to present their ideas and readers to absorb them with 
minimal time investment. Briefs will be published as part of Springer’s eBook 
collection, with millions of users worldwide. In addition, Briefs will be available 
for individual print and electronic purchase. Briefs are characterized by fast, global 
electronic dissemination, standard publishing contracts, easy-to-use manuscript 
preparation and formatting guidelines, and expedited production schedules. We aim 
for publication 8–12 weeks after acceptance. Both solicited and unsolicited 
manuscripts are considered for publication in this series. 
**Indexing: This series is indexed in Scopus, Ei-Compendex, and zbMATH **Xu-Cheng Yin · Chun Yang · Chang Liu 
Open-Set Text Recognition 
Concepts, Framework, and AlgorithmsXu-Cheng Yin 
School of Computer and Communication 
Engineering 
University of Science and Technology 
Beijing 
Beijing, China 
Chang Liu 
School of Computer and Communication 
Engineering 
University of Science and Technology 
Beijing 
Beijing, China 
Chun Yang 
School of Computer and Communication 
Engineering 
University of Science and Technology 
Beijing 
Beijing, China 
ISSN 2191-5768 ISSN 2191-5776 (electronic) 
SpringerBriefs in Computer Science 
ISBN 978-981-97-0360-9 ISBN 978-981-97-0361-6 (eBook) 
https://doi.org/10.1007/978-981-97-0361-6 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations. 
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd. 
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, 
Singapore 
Paper in this product is recyclable.Preface 
In real-world applications, new data, patterns, and categories that were not covered 
by the training data can frequently emerge, necessitating the capability to detect and 
adapt to novel characters incrementally. Researchers refer to these challenges as the 
Open-Set Text Recognition (OSTR) task, which has, in recent years, emerged as one 
of the prominent issues in the field of text recognition. 
This book begins by providing an introduction to the background of the OSTR 
task, covering essential aspects such as open-set identification and recognition, 
conventional OCR methods, and their applications. Subsequently, the concept and 
definition of the OSTR task are presented encompassing its objectives, use cases, 
performance metrics, datasets, and protocols. A general framework for OSTR is 
then detailed, composed of three key components: the Label-to-Representation 
Mapping, the Sample-to-Representation Mapping, and the Open-set Predictor. In 
addition, possible implementations of each module within the framework are 
discussed. Following this, two specific open-set text recognition methods, OSOCR 
and OpenCCD, are introduced. The book concludes by delving into applications and 
future directions of Open-set text recognition tasks. 
This book presents a comprehensive overview of the open-set text recognition task, 
including concepts, framework, and algorithms. It is suitable for graduated students 
and young researchers who are majoring in pattern recognition and computer vision 
especially in document analysis and recognition. 
Beijing, China 
November 2023 
Xu-Cheng Yin 
Chun Yang 
Chang Liu 
Acknowledgements The book is supported by National Science Fund for Distinguished Young 
Scholars (62125601), National Natural Science Foundation of China (62076024, 62006018).
vContents 
1 Introduction ................................................... 1 
1.1 Introduction ............................................... 1 
References ..................................................... 3 
2 Background ................................................... 5 
2.1 Agents in Open World ...................................... 5 
2.1.1 Open-World Learning Tasks ........................... 5 
2.1.2 Open-World Learning Approaches ...................... 10 
2.1.3 Typical Applications ................................. 14 
2.2 Text Recognition ........................................... 15 
2.2.1 Conventional Close-Set Text Recognition ................ 15 
2.2.2 Beyond Close-Set Text Recognition .................... 19 
References ..................................................... 20 
3 Open-Set Text Recognition: Concept, Dataset, Protocol, 
and Framework ................................................ 27 
3.1 Concept ................................................... 27 
3.1.1 Aims, Goals, and Scope ............................... 27 
3.1.2 Task Definition ...................................... 29 
3.1.3 Relation to Other Tasks ............................... 31 
3.1.4 Challenges .......................................... 33 
3.2 Open-set Recognition: Dataset and Protocol .................... 34 
3.2.1 Dataset by Language ................................. 34 
3.2.2 Metrics ............................................. 36 
3.2.3 Protocols ........................................... 37 
3.3 An OSTR Framework ....................................... 41 
3.3.1 Overall Design and Training Behaviors .................. 42 
3.3.2 Testing Behaviors .................................... 43 
3.4 Modules and Variables ...................................... 44 
3.4.1 Representation Space ................................. 44 
3.4.2 Label-to-Representation Mapping ...................... 45 
3.4.3 Sample-to-Representation Mapping ..................... 46
viiviii Contents
3.4.4 Open-set Predictor ................................... 46 
3.5 Backward Compatibility ..................................... 47 
3.5.1 Part Based .......................................... 47 
3.5.2 Glyph Based ........................................ 48 
References ..................................................... 48 
4 Open-Set Text Recognition Implementations(I): 
Label-to-Representation Mapping ............................... 53 
4.1 Representation Space ....................................... 53 
4.1.1 Gram-based Prototypes ............................... 54 
4.1.2 Character-Based Prototypes ........................... 55 
4.1.3 Part-Based Representation ............................. 57 
4.1.4 Representation Fusing ................................ 58 
4.2 Label-to-Representation Mapping ............................. 58 
4.2.1 Side-Information ..................................... 59 
4.2.2 Common Mapping Patterns ............................ 60 
References ..................................................... 62 
5 Open-Set Text Recognition Implementations(II): 
Sample-to-Representation Mapping .............................. 67 
5.1 Feature Extractor ........................................... 68 
5.1.1 Knowledge-Driven Feature Extractor ................... 68 
5.1.2 Data-Driven Feature Extractor ......................... 69 
5.2 Sampler ................................................... 69 
5.2.1 Gram Based ......................................... 70 
5.2.2 Feature Aggregation .................................. 70 
5.2.3 Label Aggregation ................................... 71 
5.3 Linguistic Information Handling .............................. 72 
5.3.1 Handling Linguistic Information 
in Close-Environment ................................ 72 
5.3.2 Handling Linguistic Information 
in Open-Environment ................................. 73 
References ..................................................... 73 
6 Open-Set Text Recognition Implementations(III): Open-set 
Predictor ...................................................... 79 
6.1 Recognition ............................................... 80 
6.1.1 Simple Distance Function ............................. 80 
6.1.2 Discrete Attribute Matching ........................... 81 
6.1.3 Learnable Comparison Function ........................ 81 
6.2 Rejection of Out-of-Set ...................................... 82 
6.2.1 Centered Approaches ................................. 82 
6.2.2 Non-Centered Approaches ............................ 82 
6.3 Cognition ................................................. 83 
References ..................................................... 83Contents ix
7 Open-Set Text Recognition: Case-Studies ......................... 87 
7.1 OSOCR ................................................... 87 
7.1.1 Framework Implementation Overview .................. 87 
7.1.2 Topology-Preserving Transformation Network ........... 88 
7.1.3 Label-to-Representation Mapping Module ............... 89 
7.1.4 Open-Set Predictor ................................... 90 
7.1.5 Optimization ........................................ 92 
7.2 Character-Context Decoupling ............................... 93 
7.2.1 Character-Context Decoupling Framework ............... 94 
7.2.2 Decoupled Context Anchor Mechanism ................. 95 
7.2.3 OpenCCD Network .................................. 98 
7.2.4 Proof of Theorem 1 .................................. 100 
7.2.5 Proof of Theorem 2 .................................. 101 
7.2.6 Proof of Theorem 3 .................................. 103 
7.3 Performances Overview ..................................... 104 
7.3.1 Open-Set Text Recognition ............................ 104 
7.3.2 Performance on Other Unseen Languages ............... 106 
7.3.3 Standard Close-Set Text Recognition ................... 107 
References ..................................................... 109 
8 Discussions and Future Directions ............................... 113 
8.1 Applications for OSTR ...................................... 113 
8.2 Discussions on MLLM and OSTR ............................ 115 
8.3 Future Directions ........................................... 117 
References ..................................................... 118Acronyms 
OSTR Open-Set Text Recognition 
Table 1 Notations of “numbers” in the book
Notation Type Shape/ 
Value 
Description 
d const 512 # dimensions of the prototype space, 512 in this work 
τ const 0.5 Limits extent of rectification the TPT module can do 
f s const 0.8 The fraction of seen characters for the label sampler 
bmax const 512 The maximum of the training process 
tm const – The maximum sample length the model supports (including 
end-of-speech token). 
λemb const 0.3 Weight of Lemb 
nmax const 256 The maximum iteration of the training process 
t index – Indicates a timestamp (the tth item) 
tm scalar The upper bound of timestamp length 
sunk scalar – Traiable decision boundary radius for all classes 
Lemb loss The embedding loss pusing prototypes too close too close to 
each other away. 
Lmodel loss – The total loss of the proposed method 
Lce loss – The cross entropy of the classification task. 
LA metric Line accuracy 
CA metric Char accuracy 
RE metric Recall of spotting samples containing Cu 
test . 
PR metric Precision of spotting samples containing Cu 
test . 
FM metric F-measure of spotting samples containing Cu 
test . 
c character A character on the C
xixii Acronyms
Table 2 Notations of “sets” in the book 
Notation Type Element 
Shape 
Description 
C space Set of all characters. 
T space R32×32 Template space containing 32 × 32 gray scale patches from the 
Noto-font (or other fonts). 
F space The aligned space where prototype and character feature resides 
on. Again this does not have to be a space of real vectors. 
Ctrain set Set of distinct characters (labels) in the training set 
Ctest set Set of distinct characters (labels) in the testing set 
Ck 
test set Labels in the testing set with side-information. 
Cu 
test set Out-of-set labels in the testing set without side-information. 
Clabel set character The set of characters appear in a batch of training data 
Cbatch set character The set of active labels in each training iteration, 
Cneg ∪ Cpos ∪ {[s], [−]}
Cpos set character Sampled characters that appear in the batch of training data 
Cneg set character Sampled characters that do not appear in the batch of training 
data 
C set character An arbitrary set of characters. 
T[i] set R32×32 All templates of the ith character, each corresponds to a case.Acronyms xiii
Table 3 Other important notations in the book 
Notation Type “Shape” Description 
Same function (str, str ) → {0, 1} Return 1 if the two inputs are the same, 0 otherwise 
ED function (str, str ) → N Edit distance between strings. 
Len function str → N+ Returns the length of the string. 
Rej function str → {0, 1} Indicator function returns whether the string contains 
unknown language or characters in Cu 
test . 
H function Mapping template to a normalized prototype. 
R function Maps each character to corresponding representation 
prototypes. Note a same character may have different 
representations, in terms of both visually and 
semantically. 
E function Mapping template to a normalized prototype. 
P tensor R|C|×d Prototypes for all classes. Note one class can be 
mapped to more than one prototype depending on the 
number of cases. 
M. tensor Rw×h×d
An (intermediate) feature map of the input word clip. 
Dx tensor Rw×h + The foreground density matrix for the X-axis. 
Dy tensor Rw×h + The foreground density matrix for the Y -axis. 
I tensor Rw×h×2 + The coordination mapping for the rectification. 
F Rtm×d The character representation of each character in one 
sample. Could be a feature, but could also be 
“attributes” as well. 
T[i] tensor R32×32 A template on the template space T. 
P[i] tensor Rd 
S tensor Rtm×|P| Similarity of character feature at each timestamp t 
and each prototype.Chapter 1 
Introduction 
Abstract In real-world applications, new data, patterns, and categories that were 
not covered by the training data can frequently emerge, necessitating the capability 
to detect and adapt to novel characters incrementally. Researchers refer to these 
challenges as the Open-Set Text Recognition (OSTR) task, which has, in recent 
years, emerged as one of the prominent issues in the field of text recognition. In 
this chapter, we first introduce the evolution and several main trends of preliminary 
works on novel (unseen) character identification and recognition. Then, we briefly 
discuss three main challenges in OSTR. Finally, we introduce the overall structure 
and main content of our book. 
Keywords Open-set text recognition · OSTR · Novel character · Unseen character 
1.1 Introduction 
A character is a unit of information that roughly corresponds to a grapheme, 
grapheme-like unit, or symbol, such as in an alphabet or syllabary in the written 
form of a natural language. Text is a system of characters used to record, commu￾nicate, or inherit culture. As one of the most influential inventions of humanity, text 
has played an important role in human life. Recognizing text in pictures, images, and 
videos (e.g., document images, natural scenes, and web videos), also known as text 
recognition, is a key step in many content-based image and video applications, such 
as content-based web image search, video information retrieval, and reading in the 
wild. 
Text recognition is gaining popularity among both academic researchers and 
industry fellows due to its vast applications. Currently, many text recognition meth￾ods [ 1– 3] have achieved promising performance on the close-set text recognition 
benchmarks, where characters in the testing set are covered by the training set. Most 
existing methods always fail to handle novel (unseen) characters that do not appear in 
the training set. Specifically, these methods model the prototypes (centers) of classes 
as latent weights of a linear classifier. However, adding weights for novel charac￾ters will be difficult without retraining the model. This caveat makes conventional
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_1
12 1 Introduction
methods unfeasible as collecting and annotating data can be very time-consuming 
and expensive. For example, the annotation of minority languages and ancient docu￾ments could take a long period and yield high personnel costs. Also, novel characters 
may be continuously discovered from the incoming data stream, especially when pro￾cessing historical documents or internet-oriented images. In such cases, retraining 
the model each time a novel character is found would yield considerable time and 
resource costs.
In the literature, a few text recognition methods, always called zero-shot text 
recognition methods, are capable of handling novel characters. These methods can 
be divided into two different schemes. One category [ 4– 6] exploits the radical com￾position of each character, consequentially limiting most of them to the Chinese 
language. Another category is based on weight imprinting [ 7], which generates pro￾totypes with corresponding glyphs [ 8– 10]. Despite showing no language-specific 
limitation, many methods in this category do not scale well on large label sets due to 
the training cost of the imprinting module. For both categories, despite a few methods 
possessing text-line recognition capability [ 6, 9, 11], very few approaches demon￾strate competitive performance on standard close-set benchmarks [ 12], limiting their 
feasibility in practice. 
Furthermore, the aforementioned zero-shot text recognition methods are not capa￾ble of rejecting out-of-set samples like Open-Set Recognition (OSR) methods [ 13, 
14]. The rejection capability can help the users to quickly spot newly emerging 
characters in the incoming data stream, whereas the recognition capability enables 
a quick adaption without retraining the model. Combining these two capabilities 
yields a human-in-the-loop recognition system capable of evolving with the input 
data, which we formulate as the open-set text recognition task. 
Generally speaking, the open-set text recognition task (OSTR) extracts text from 
images that potentially contain novel characters unseen in the training set. Like the 
general text recognition task, OSTR recognizes the seen characters and identifies, 
rejects, or recognizes novel characters. As dealing with novel characters, OSTR faces 
three main challenges. 
First, the model should be able to actively discover new characters that have not 
been seen before and notify the user by rejection, rather than recognizing them as 
some known text. Second, after discovering new characters, the model should be able 
to recognize these new character classes by fast adaptation, i.e., the problem of recog￾nition of new classes. Third, in open environments, along with the emergence of new 
characters and scripts, the text content is affected by language evolution and gradually 
deviates from the trained corpus, which affects the recognition performance. 
This book begins by providing an introduction to the background of the OSTR 
task, covering essential aspects such as open-set identification and recognition, con￾ventional OCR methods, and their applications. Subsequently, the concept and def￾inition of the OSTR task are presented encompassing its objectives, use cases, per￾formance metrics, datasets, and protocols. A general framework for OSTR is then 
detailed, composed of four key components: The Aligned Represented Space, the 
Label-to-Representation Mapping, the Sample-to-Representation Mapping, and the 
Open-set Predictor. In addition, possible implementations of each module withinReferences 3
the framework are discussed. Following this, two specific open-set text recognition 
methods, OSOCR and OpenCCD, are introduced. The book concludes by delving 
into applications and future directions of Open-set text recognition tasks. 
This book offers a comprehensive overview of the open-set text recognition task, 
including concepts, frameworks, and algorithms. It is suitable for graduate students 
and young researchers majoring in pattern recognition and computer science, espe￾cially interdisciplinary research. 
References 
1. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: Large scale system for text detection and 
recognition in images. In: Proceedings of the 24th ACM SIGKDD International Conference 
on Knowledge Discovery Data Mining, KDD 2018, August 19–23, pp. 71–79. ACM, London, 
UK (2018) 
2. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: ASTER: an attentional scene text 
recognizer with flexible rectification. IEEE Trans. Pattern Anal. Mach. Intell. 41(9), 2035– 
2048 (2019) 
3. Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.: Decoupled attention 
network for text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, February 7–12, pp. 12 216–12 224. AAAI Press, New York, NY, USA (2020) 
4. Chen, J., Li, B., Xue, X.: Zero-shot Chinese character recognition with stroke-level decompo￾sition. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 
IJCAI 2021, Virtual Event/Montreal, Canada, August 19–27, pp. 615–621. (2021). www.ijcai. 
org 
5. Wang, T., Xie, Z., Li, Z., Jin, L., Chen, X.: Radical aggregation network for few-shot offline 
handwritten Chinese character recognition. Pattern Recognit. Lett. 125, 821–827 (2019) 
6. Zhang, J., Du, J., Dai, L.: Radical analysis network for learning hierarchies of Chinese char￾acters. Pattern Recognit. 103, 107305 (2020) 
7. Qi, H., Brown, M., Lowe, D.G.: Low-shot learning with imprinted weights. In: 2018 IEEE 
Conference on Computer Vision and Pattern Recognition. CVPR 2018, June 18–22, pp. 5822– 
5830. IEEE Computer Society, Salt Lake City, UT, USA (2018) 
8. Ao, X., Zhang, X., Yang, H., Yin, F., Liu, C.: Cross-modal prototype learning for zero-shot 
handwriting recognition. In: 2019 International Conference on Document Analysis and Recog￾nition. ICDAR 2019, September 20–25, pp. 589–594. IEEE, Sydney, Australia (2019) 
9. Zhang, C., Gupta, A., Zisserman, A.: Adaptive text recognition through visual matching. In: 
Computer Vision-ECCV 2020–16th European Conference, August 23–28,: Proceedings, Part 
XVI, ser. Lecture Notes in Computer Science, vol. 12361, pp. 51–67. Springer, Glasgow, UK 
(2020) 
10. Souibgui, M.A., Fornés, A., Kessentini, Y., Megyesi, B.: Few shots is all you need: A pro￾gressive few shot learning approach for low resource handwriting recognition (2021). [Online] 
Available: https://arxiv.org/abs/2107.10064 
11. Huang, Y., Jin, L., Peng, D.: Zero-shot Chinese text recognition via matching class embed￾ding. In: 16th International Conference on Document Analysis and Recognition, ICDAR 2021, 
September 5–10, 2021, Proceedings, Part III, ser. Lecture Notes in Computer Science, vol. 
12823, pp. 127–141. Springer, Lausanne, Switzerland (2021)4 1 Introduction
12. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision. ICCV 2019, October 27-November 2, pp. 4714– 
4722. IEEE, Seoul, Korea (South) (2019) 
13. Geng, C., Huang, S., Chen, S.: Recent advances in open set recognition: A survey. IEEE Trans. 
Pattern Anal. Mach. Intell. 43(10), 3614–3631 (2021) 
14. Fei, G., Liu, B.: Breaking the closed world assumption in text classification. In: Proceedings 
of the 2016 Conference of the North American Chapter of the Association for Computational 
Linguistics: Human Language Technologies. Association for Computational Linguistics, pp. 
506–514 (2016)Chapter 2 
Background 
Abstract This character offers an introduction to the background of the OSTR 
task, covering essential aspects such as open-set identification and recognition, con￾ventional OCR methods, and their applications. First, we introduce the concept of 
open-set (or open-world). We discuss the different usage of open-set from various 
research areas and declare the OSTR task concern on either identification or recog￾nition capability, with both seen and novel (abnormal) samples. Then, we introduce 
the study on open-set identification and open-set recognition to show how unknown 
instances are dealt with by various tasks involving recognition, including image 
classification, object detection, semantic segmentation, and instance segmentation. 
The introduction focuses on general ideas instead of implementations. Besides, the 
chapter also lays the background of conventional OCR methods, which can be cat￾egorized into three main frameworks: Word Level Prediction, Feature Aggregation, 
and Label Aggregation. Finally, we also introduce some existing studies beyond 
close-set text recognition before the OSTR task. 
Keywords Open-world · Novel identification · Incremental recognition · OCR 
2.1 Agents in Open World 
2.1.1 Open-World Learning Tasks 
In real-world recognition/classification tasks, limited by various objective factors, it 
is usually difficult to collect training samples to exhaust all classes when training a 
recognizer or classifier. However, a more realistic scenario is usually open and non￾stationary such as driverless, fault/medical diagnosis, etc., where unseen situations 
can emerge unexpectedly, which drastically weakens the robustness of these existing 
methods. Open-set recognition (OSR) describes such a scenario where incomplete 
knowledge of the world exists at training time, and unknown classes can be submitted 
to an algorithm during testing, requiring the classifiers to not only accurately classify 
the seen classes but also effectively deal with unseen ones. In such a setting, the 
unknown classes and their risk should be considered in the algorithm. 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_2
56 2 Background
OSR systems require not only to identify and discriminate instances that belong to 
the source domain (i.e., the seen known classes contained in the training dataset) but 
also to reject unknown classes in the target domain (classes used in the testing phase). 
Until recently, the success of almost all machine-learning-based systems has been 
obtained by conducting them on “closed-set” classification tasks. In such systems, 
the source and target domains are assumed to contain the same object classes, and 
the system is only tested on known classes that have been seen during training. 
Therefore, different from the “closed-set” setting, a more realistic scenario is 
solving real-world problems consisting of an “open-set” of objects. With the advent 
of building intelligent systems and utilizing machine-learning-based systems, a wide 
range of applications require robust AI methods. Based on Donald Rumsfeld’s 
famous “There are known knowns” statement [ 1], Gen et al. expand the basic recog￾nition categories of classes asserted by [ 2], where recognition should consider four 
basic categories of classes as follows: 
(1) known known classes (KKCs), i.e., the classes with distinctly labeled posi￾tive training samples (also serving as negative samples for other KKCs), and 
even have the corresponding side-information like semantic/attribute informa￾tion, etc.; 
(2) known unknown classes (KUCs), i.e., labeled negative samples, not necessarily 
grouped into meaningful classes, such as the background classes [ 3], etc.; 
(3) unknown known classes (UKCs), i.e., classes with no available samples in train￾ing, but available side-information (e.g., semantic/attribute information) of them 
during training; 
(4) unknown unknown classes (UUCs), i.e., classes without any information regard￾ing them during training: not only unseen but also having no side-information 
(e.g., semantic/attribute information, etc.) during training. 
Open-set recognition task [ 4] describes such a scenario where new classes (UUCs) 
unseen in training appear in testing and require the classifiers to not only accurately 
classify KKCs but also effectively deal with UUCs. Besides, the classifiers need to 
have a corresponding reject option when a testing sample comes from some UUC. 
Consequently, when presented with a test sample in OSR (Open-Set Recognition), 
our task involves not only novel identification (which includes identifying KKC and 
UKC while rejecting UUC) but also novel recognition (recognizing KKC and UKC). 
As there are various definitions of open-set from different research areas, in the 
rest of this section, we will first discuss the scope of the concept of “Open”. Then, we 
will introduce two typical kinds of tasks in Open-set learning: Open-Set Identification 
and Open-Set Recognition. 
2.1.1.1 The Scopes of “Open” 
In this part, we will first discuss the scopes of the open-set concept, then discuss 
commonly related capabilities, and finally talk about what capability the “open-set”2.1 Agents in Open World 7
in the proposed “Open-Set Text Recognition” refers to and its relation with other 
open-set related tasks. 
The concept of open-set dates back to [ 5], in the early days, it referred to rejecting 
samples that not be seen during training [ 5, 6]. Later, [ 4] tries to differentiate OCR 
from anomaly detection and extend the concept to “cognition” of novel classes. 
Formulating that open-set recognition would retain the recognition capability on 
KKCs and the rejection capability on UUCs. 
However, the definition has been relaxed by much today, “open” in “open-set”, 
“open-world”, or “open vocabulary” could refer to applying recognition, rejection, 
or cognition upon “anything” that is not covered by the training set. Here, anything 
includes novel categories, potentially not a vital part of the task itself. It can also 
refer to the composition of attributes, or domains. 
Specifically, the border between OSR and anomaly has been much-blurred by [ 7, 
8], and so does the border with low-shot learning. Beyond that, the term “open” has 
been covering more topics like generalized zero-shot learning, weak/self-supervised 
learning, ReID, and some recent methods even aiming to implement the cognition 
capabilities. 
However, the three capability categories formulated in [ 4] still seem to hold, i.e., 
identification, recognition, and open-set cognition can still be used to describe model 
behaviors with some extensions. 
Specifically, we extend the open-set recognition with the capability to address 
novel samples in the same way as seen samples. Note the novelty here can either 
refer to labels(i.e., UKCs) and also attribute combinations, domains, 1 etc. Upon 
narrowing to zero-shot image recognition tasks, this capability directly interprets as 
recognizing UKC, matching the unextend definition [ 4]. 
The open-set identification capability refers to the model being aware when an 
abnormal sample appears in the data stream. For anomaly detection, such capability 
can be interpreted as the rejection of KUCs and/or UUCs, localization of novel 
objects in object detection, etc. On the other hand, the capability also includes sample 
exclusion during training. 
The cognition capability refers to extracting structural knowledge from the UUCs, 
which covers clustering behavior, or knowledge extraction where implements as 
describing each novel instance by natural language. 
Moreover, this book focuses on the Open-Set Text Recognition Task. The task 
demands both identification capability and recognition capability, on both seen and 
novel (abnormal) samples. Like other tasks, the novel identification capability helps 
the model to spot novel instances from the datastream so the admin can know and 
avoid silent recognition errors, and the novel recognition helps the model to incre￾mentally adapt to novel classes without an expensive retraining process. The model 
also demands a high accuracy on seen categories for it to be practical in applications. 
Uniquely, the OSTR task also demands the capability to reject seen samples when 
their side information gets removed, since text recognition features a huge label
1 For tracking, and/or ReID, the category label is not a direct part of the task formulation, so it is 
considered as a domain here. 8 2 Background
set especially handling CJK characters. This allows the implementation of a cache 
mechanism where low-frequency characters can be temporally removed, only to be 
loaded back upon rejection. 
2.1.1.2 Open-Set Identification 
As previously mentioned, the open-set identification capability refers to the model’s 
awareness when an abnormal sample appears in the data stream. 
Numerous approaches exist concerning classification with a reject option that 
can be adjusted to support open sets. However, simply rejecting uncertain inputs 
in such classifiers protects against misclassification but is insufficient for handling 
unknowns. As mentioned by Mahdavi et al. [ 9], one prevalent rejection-adapted 
approach involves variants of Support Vector Machine (SVM) classifiers with the 
ability to reject observations, along with one-class classifiers based on support vec￾tors. Although these techniques relate to OSR by rejecting inputs, they have different 
rationales for rejection actions. Classifiers with rejection options focus on the ambi￾guity between classes to reject an uncertain input of one class as a member of another 
and minimize the distribution mismatch between training and testing domains. 
In contrast, OSR rejects an input because it doesn’t belong to any of the known 
classes. In these techniques, unknowns often appear uncertainly and are labeled 
with confidence. OSR supports rejecting the unknown object by determining an 
acceptable level of uncertainty and searching among any known classes to identify 
if the true class exists. Put differently, the set of possible outcomes of predictions 
is a significant distinction between an open-set recognition classifier and a typical 
multi-class classifier. In margin classifiers like SVMs, confidence is assessed in terms 
of the associated distance to the decision boundary provided for each example. The 
objective of SVMs is to find an optimal hyperplane to classify and separate the classes 
of training samples. An unknown far from the boundary is incorrectly labeled and 
will be incorrectly classified with very strong evidence. 
On the other hand, some people argue that identifying novel classes and detecting 
anomalies sometimes can solve the OSR problem. In such cases, open-set pattern 
recognition tasks are related to other tasks such as out-of-distribution detection, 
anomaly detection, outlier detection, and novelty detection. Yang et al. [ 10] proposed 
a generalized out-of-distribution detection framework and compared and summarized 
the above tasks within this framework. 
In the survey, they first present a unified framework known as generalized out-of￾distribution (OOD) detection, which encompasses the five aforementioned problems: 
anomaly detection (AD), novelty detection (ND), open-set recognition (OSR), out￾of-distribution (OOD) detection, and outlier detection (OD). Concerning the OSR 
task, they align OSR within a generalized OOD detection framework, where “known 
known classes” and “unknown unknown classes” correspond to in-distribution (ID) 
and OOD, respectively. Formally, OSR addresses the scenario where OOD sam￾ples during testing exhibit semantic shifts. The objective of OSR is largely shared2.1 Agents in Open World 9
with that of multi-class novelty detection (ND)—the key difference being that OSR 
additionally requires accurate classification of ID samples. 
2.1.1.3 Open-Set Recognition 
The open-set recognition capability refers to recognizing seen classes and unseen 
classes with knowledge. 
When recognizing seen classes, it is commonly approached as a traditional multi￾class classification task. In a conventional multi-class classifier, due to the closed￾set assumption, all inputs are labeled and classified into one of the known classes 
observed during training. More precisely, in the closed-set classification task, the 
learner only has access to a fixed set of known classes .C = {L1, L2, ...L M } and 
constructs an M-class classifier during the training phase. The resulting classifier 
is tested on the data from only the M classes. However, a problem arises with the 
appearance of a test sample from an unknown class that does not belong to any of 
the known classes. Thus, the most likely class for an input observation is always 
provided, and an unknown will wrongly be recognized as a sample belonging to one 
of those predefined classes. 
However, within the realm of OSR, it’s not feasible to consider knowledge of the 
complete set of potential classes during the training phase. The classifier is allowed 
to predict classes from the set of.C = {c1, c2, ..., cM , cM+1, ..., cM+Ω}, where classes 
.cM+1 through.cM+Ω cover all unknown classes not observed during training but which 
appeared at query time. A test sample may be predicted to belong either to one of the 
known classes c. ∈ C or to an unknown one. with an incomplete understanding of the 
entire range of potential classes, a traditional multi-class classifier assigns class labels 
from the closed training set to an expansive region. Consequently, during classifica￾tion, unseen inputs within the open space may be incorrectly classified. In contrast, 
the open-set recognition method distinguishes known samples and constrains the 
decision scope based on the support of the training data. 
In recent years, open-set pattern recognition has gradually become one of the 
research hotspots in the field of pattern recognition. When recognizing novel cate￾gories, As discussed by Geng et al. [ 11], existing open-set pattern recognition tech￾niques can be broadly categorized into discriminative methods and generative meth￾ods. 
Discriminative Methods-Based Open-Set Pattern Recognition Techniques: 
Before the advent of deep-learning techniques, most discriminative methods relied 
on traditional machine learning models such as Support Vector Machines (SVM), 
nearest neighbors classifiers, and sparse representation as baseline models, which 
were then extended to open-set pattern recognition methods. The effectiveness of 
these methods depends on the design and selection of features. With the rapid devel￾opment of deep-learning techniques, discriminative methods based on deep learn￾ing [ 12– 14] have shown better performance in handling open-set pattern recognition 
tasks. Yoshihashi [ 14] proposed a classification-reconstruction learning algorithm for 
open-set recognition (CROSR), which uses latent representations for reconstruction10 2 Background
and can robustly detect unknown classes without compromising known classification 
accuracy. Chen et al. [ 12] introduced the Reciprocal Points Learning framework, 
where Reciprocal Points are latent representations of out-of-class spaces for each 
known class. By introducing unknown information through exchange points, neural 
networks can learn more compact and robust feature spaces, effectively separating 
known and unknown classes. Shu et al. [ 13] incorporated prototype learning into 
open-set recognition tasks, guiding deep models to obtain more discriminative fea￾tures by simultaneously learning prototype representations and prototypes. Detection 
of unknown classes is achieved through distance measurement between features and 
prototypes. 
Generative Methods-Based Open-Set Pattern Recognition Techniques: Gen￾erative methods are mostly based on deep learning. Based on whether they rely on 
training samples, generative methods can be divided into instance-based methods [ 6, 
15] and non-instance-based methods [ 16]. Instance-based generative methods typi￾cally use Generative Adversarial Networks (GAN) to generate samples of unknown 
classes, assisting the model in learning the boundaries between known (KKC) and 
unknown classes (UUC). Ge et al. [ 6] proposed the G-OpenMax (generative open 
max) algorithm, which uses conditional GAN to generate samples of unknown 
classes. Yu et al. [ 15] introduced an adversarial sample generation framework for 
open-set pattern recognition (ASG). Non-instance-based methods are mostly based 
on Dirichlet processes. Dirichlet processes do not overly rely on training samples 
and can adapt to changes in data, making them naturally suitable for open-set pattern 
recognition scenarios. Geng and Chen [ 16] proposed a collective decision-based 
Open-Set Recognition model (CD-OSR) by modifying hierarchical Dirichlet pro￾cesses. This method does not require defining thresholds to differentiate between 
known and unknown classes. 
2.1.2 Open-World Learning Approaches 
In this section, we introduce two learning approaches (i.e., Class-incremental learn￾ing, Foundation Model) for OSR. 
2.1.2.1 Incremental Learning 
In real-world open applications, datasets are dynamic, requiring continuous detection 
and the addition of novel categories. Therefore, an approach enabling the learner to 
incrementally incorporate knowledge about new classes is necessary, allowing the 
construction of a universal classifier encompassing all observed classes—referred to 
as Incremental Learning(or Class-Incremental Learning, CIL). 
Class-incremental learning aims to learn from an evolutive stream with incoming 
new classes. The fatal problem in CIL is called catastrophic forgetting, i.e., directly 
optimizing the network with new classes will erase the knowledge of former ones2.1 Agents in Open World 11
and result in irreversible performance degradation. Hence, how to effectively resist 
catastrophic forgetting becomes the core problem in building CIL models. 
A good model should strike a balance between depicting the characteristics of 
new classes and preserving the pattern of formerly learned old classes. This trade-off 
is also known as the “stability-plasticity dilemma” in neural systems, where stability 
denotes the ability to maintain former knowledge and plasticity represents the ability 
to adapt to new patterns. 
Zhang et al. [ 17] propose an open-set pattern recognition process based on class￾incremental learning. This process involves three stages: (1) identification of known 
classes and placement of unknown class samples into a buffer; (2) manual or auto￾matic labeling of unknown class samples; (3) using new data and patterns for the 
class-incremental learning of the model, followed by employing the updated model 
to either reject or recognize samples within the buffer. Since it is hard to enumerate 
all categories at once, how to smoothly update the system to learn more and more 
concepts over time is therefore an important and challenging task. 
Zhou et al. [ 18] have summarized several works related to class-incremental 
learning in recent years, igniting substantial debate within the machine learning and 
computer vision community. They categorize existing CIL methods systematically, 
focusing on three key aspects: data-centric, model-centric, and algorithm-centric 
approaches. 
Data-centric methods tackle CIL using exemplars and can be divided into data 
replay and data regularization strategies. 
Data replay entails using past data for rehearsal, allowing the model to revisit 
earlier classes and mitigate forgetting. Conversely, data regularization constructs 
regularization terms using additional data, intending to guide the optimization pro￾cess to prevent catastrophic forgetting. 
Data-centric CIL algorithms mainly concentrate on resisting forgetting with the 
help of former data. However, due to the exemplar set capturing only a small fraction 
of the training data, both methods may run the risk of encountering overfitting prob￾lems. Besides, a challenge arises in the form of data imbalance, stemming from the 
disparity between the few-shot exemplars and the many-shot training set. Addressing 
this issue, Castro et al. [ 19] tackle it through balanced sampling techniques. On the 
other hand, data regularization methods hinge on specific assumptions, e.g., treat￾ing the loss of exemplars as the indicator of forgetting. However, these assumptions 
might not universally hold, leading to subpar performance in certain scenarios. 
Moreover, Both these methodologies depend on preserving historical exemplars, 
potentially raising concerns about user privacy. Consequently, in privacy-sensitive 
class-incremental learning settings, it’s imperative to explore exemplar-free learning 
approaches as a viable alternative. 
Model-centric methods primarily focus on two main aspects: preventing model 
parameter drift and enhancing network structures to improve representation, which 
are categorized into Dynamic networks and parameter regularization. 
Dynamic networks expand as needed to address finite network capacity and 
enhance representation. In contrast, parameter regularization evaluates parameter 
significance, regulating essential parameters to prevent divergence.12 2 Background
Model-centric CIL methods concentrate on evolving models during learning, 
involving adjustments to model structure or the creation of generalization terms. 
However, their activation often requires task identifiers or auxiliary task classifiers. 
Similarly, certain methods propose designing specific modules for incremental tasks. 
However, crafting these modules manually demands heuristic designs or task-specific 
priors. 
Learning dynamic networks, especially backbone expansion methods, demon￾strate promising performance but require expandable memory budgets, limiting their 
suitability for edge devices. Additionally, their dependence on pretrained models for 
rapid expansion restricts their application in tasks lacking pretrained models. 
On the other hand, Parameter regularization encounters challenges in estimating 
parameter importance and managing memory scaling issues. Moreover, conflicting 
importance matrices across incremental stages pose optimization challenges for this 
method. 
Algorithm-centric methods focus on designing algorithms to maintain the 
model’s knowledge in former tasks. These methods fall into two categories: knowl￾edge distillation and Model Rectify. 
Knowledge distillation-based CIL methods aim to build the mapping between 
old and new models and reflect the characteristics of the old model in the updating 
process. On the other hand, Model Rectify-based methods aim to discover and reduce 
the bias in incremental models. 
Algorithm-centric methods design task-specific algorithms to resist forgetting. 
However, since the knowledge distillation term aims to strike a balance between 
learning the new and remembering the old, it will undoubtedly suffer catastrophic for￾getting compared to dynamic networks. Specifically, knowledge distillation methods 
show stronger performance given limited memory, while dynamic networks require 
an adequate memory budget to perform competitively. 
On the other hand, model rectification methods target mitigating the biased induc￾tive tendencies within the CIL model, aligning it with the ideal oracle model. This 
approach aids in comprehending the inherent causes of catastrophic forgetting. Some 
studies [ 20, 21] assert that the bias in the CIL model stems from an imbalanced data 
stream. Additionally, Pham et al. [ 22] identify biases in batch normalization lay￾ers within CIL and propose the re-normalization of layer outputs. Exploring other 
potential factors contributing to catastrophic forgetting and devising corresponding 
solutions remains an important area for future investigation.” 
In conclusion, the continuous expansion of concepts poses a significant challenge 
in updating systems to accommodate evolving categories. Class-incremental learn￾ing stands as a solution to this challenge, enabling the system to smoothly acquire 
and integrate new concepts over time. It addresses the difficulty of enumerating all 
categories at once.2.1 Agents in Open World 13
2.1.2.2 Foundation Model 
Since the release of ChatGPT, in less than two months, it has gathered over a hundred 
million monthly active users, ushering the AI industry into the era of foundation 
models. Thanks to the powerful representational capabilities of foundation models, 
it is a potentially effective approach to handling open-set recognition tasks using 
these models. 
Bommasani et al. [ 23] define a foundation model as any model trained on expan￾sive data (typically through self-supervision at scale) and adaptable (e.g., fine-tuned) 
for various downstream tasks. Notable instances include CLIP, GPT-4, and LLaMA. 
These models are rooted in deep neural networks and self-supervised learning, both 
long-established techniques. 
The sheer magnitude and breadth of recent foundation models have expanded our 
conception of their capabilities. For example, GPT-3, boasting 175 billion parame￾ters, demonstrates decent performance across a diverse array of tasks through natural 
language prompts, despite not being explicitly trained for many of them [ 24]. Yet, 
these existing models hold the potential to exacerbate issues, and their inherent char￾acteristics remain poorly comprehended. With their imminent widespread adoption, 
they’ve become the subject of intense scrutiny [ 25]. 
According to different data types, Foundational Models can be categorized into 
Large Language Model(LLM)s, Visual Large Model(VLM)s, and Multi-modal Large 
Language Model(MLLM)s. 
LLMs and VLMs have shown remarkable performance in classification tasks by 
learning rich visual and/or linguistic representations. Thus, LLMs and VLMs can 
serve as the foundation for open-world learning in various tasks, aiming to distill 
or apply the knowledge from the foundation model into their respective domains. 
Techniques focus on distilling the knowledge acquired from Vision-and-Language 
Models (VLMs) into closed-set methods. 
Recently, Multi-modal Large Language Models (MLLMs), trained on image-text 
pairs, showcase impressive zero-shot capabilities across diverse vision tasks. These 
models effectively align images and language vocabularies into a unified feature 
space, bridging the gap between visual and linguistic data. Leveraging this alignment, 
various open-vocabulary methods blur the distinction between close-set and open￾set scenarios, enhancing their adaptability for practical applications. Some methods 
involve incorporating learned prompts into foundational models, facilitating easier 
knowledge transfer to downstream tasks. Text embeddings for category names are 
generated by inputting prompts into the text encoder of pretrained MLLMs. 
In summary, due to the powerful representational capabilities of foundation mod￾els, it is considered a potentially effective method for handling open-set recognition. 
Further discussion about MLLM and open-set text recognition (OSTR) can be found 
in Chap. 7.14 2 Background
2.1.2.3 Text Recognition, LLMs, and Open World 
Recent research shows a merging trend of QA, text recognition, and multi-modal￾large models. This trend shows in three aspects: 
First, multi-modal language attains increasing capabilities on text recognition [ 26, 
27]. One thing to notice is these foundation model may include testing images as their 
pretraining data, so evaluation and comparison need to proceed with extra caution, 
especially when applied to open-world use cases or when hallucination can cause 
damage. 
Second, novel characters in open environments are also reshaping language mod￾els. Some language models start using rendered text images to replace text-based 
tokens as input, which demonstrates some extent of generalization capability over 
unseen characters [ 28]. 
Third, some OCR frameworks are taking the form of VQA/MLM frameworks. 
This indicates that text recognition, which used to be upstream is taking a position 
as one of the downstream tasks. 
2.1.3 Typical Applications 
The recent two years have witnessed a rapid increase of research interest in open-set 
related fields. In all, the concept of open-world/open-set/open-vocabulary refers to 
categories that are not covered by the training data. The (desirable) action. The rapid 
growth of the community, however, brings differences in actions. In this chapter, we 
focus on introducing these concepts and applications. Note some recent methods 
apply to more than one application [ 29], and even go further than that [ 30]. 
2.1.3.1 Novel Sample Spotting 
Open-set is first seen in image recognition [ 5, 6], aiming to spot then text classifica￾tion [ 31]. These methods focus on rejecting unseen categories (which is one approach 
to spotting them), worth mentioning recognition capability back then is denoted as 
few-shot or zero-shot learning. This behavior is adopted to localize unknown targets, 
including objects, actions, etc. Another main application in this field is anomaly 
detection, where the model detects seen and unseen abnormal patterns in an open 
environment. 
2.1.3.2 Novel Sample Recognizing 
Recognition novel samples first come from few-shot and zero-shot [32], like rejection 
concepts are gradually adopted by other downstream tasks like object detection [ 33] 
and segmentation communities [ 34]. Due to the fast development of large-scale pre-2.2 Text Recognition 15
training models like Bert [ 35], CLIP [ 36], zero-shot learning methods got magnificent 
performance and generalization capability improvements [ 37– 39]. 
Despite incurring data-leaking concerns, these methods begin to include “open” 
in their names [ 40], despite they are more closely affinities with zero-shot learning 
methods. Another front is matching applications, starting from ReID and face recog￾nition, which later expands into tracking and content matching-based fact-checking. 
2.1.3.3 Novel Sample Avoiding 
Unlike the above use cases that aim to address novel samples during training, some 
tasks focus on avoiding novel classes during training. Open-set domain adaption [ 41] 
aims to alleviate the impact caused by unlabeled samples caused novel classes, similar 
use cases are also seen in generative models [ 42, 43], which also excludes samples 
from novel classes to prevent preformance degrading. This approach is also included 
in active learning scenarios [ 44]. 
2.2 Text Recognition 
2.2.1 Conventional Close-Set Text Recognition 
A text recognition task translates each image.img to a partially ordered set of charac￾ters.{Yˆ , ≤}. Except very rare languages, 2 .{Yˆ , ≤} is a chain, where we omit the rela￾tion .≤ and simply denote the prediction as . Yˆ . Generally, a generic text recognition 
framework includes three blocks, namely the label representation block, the sample 
representation block, and the prediction block. Specifically, the label representation 
maps each character in the character set.Ck
test to prototypes, the sample representation 
block extracts character features from the input image, and the predictor. For some 
recent methods, an extra language model may be used as a post-processing module 
to correct the recognition module. 
Noteworthy, different frameworks may call different modules in different order. 
Currently, the mainstream text recognition framework mainly transcribes the input 
image (and auxiliary information) into predicted text results through a predictor. 
According to the information processing granularity of the predictor, the text recog￾nition framework can be mainly divided into three types [ 45]: (a) word recognition, 
(b) character sequence recognition based on feature aggregation, and (c) character 
sequence recognition based on label aggregation. Figure 2.1 illustrates the overall 
process of text recognition and the three types of text recognition frameworks.
2 https://commons.wikimedia.org/wiki/File:Egypt_Hieroglyphe4.jpg. 16 2 Background
Fig. 2.1 Main framework for text recognition technology [ 45] 
2.2.1.1 Word Level Prediction 
Inspired by the principle of “the whole is greater than the sum of its parts”, methods 
for whole-word recognition define text recognition tasks as either word spotting, 
which is a matching problem based on image features, or a multi-class recognition 
problem. Such methods represent the input image as a whole and transcribe it into 
text. 
For closed-set text recognition tasks, Manmatha et al. [ 46] proposed a word￾spotting method for retrieving the same word text image from historical handwritten 
documents. Wang and Belongie applied word-spotting techniques to natural scene 
text recognition. This method improved 16% on Google’s Street View Text (SVT) 
compared to traditional OCR models, demonstrating that word-spotting techniques 
can also be applied to complex background natural scene text recognition. Almazan et 
al. [ 47] interpreted the recognition and matching tasks as nearest neighbor problems. 
They embedded word images and text strings into a common vector subspace or 
Euclidean space, and combined label embedding with attribute learning to achieve 
text recognition based on the assumption that image features and string features 
representing the same word should be similar. Jaderberg [ 48] defined text recognition 
as a multi-class classification problem, using a deep convolutional neural network to 
predict the character sequence and bigram of text images, and using dynamic search 
to obtain word text recognition results. 
For open-set text recognition tasks, Chanda [ 49] introduced the zero-shot learning 
paradigm to solve the problem of unknown class word recognition in the digitization 
of historical handwritten documents. They used a deep-learning framework to extract 
discriminative image features, predict custom stroke attributes of handwriting, and 
complete word recognition. Also for historical document text recognition, Rai et al. 
[ 50] predicted 11 main shape attributes based on whole image features and proposed 
the mixed-model Photoshop (SC)Net to achieve zero-shot recognition of handwritten 
words. Chanda [ 51] predicted 13 basic shape/stroke attributes based on whole image2.2 Text Recognition 17
features and applied the zero-shot learning framework to achieve recognition of 
handwritten Bengali characters. 
Since whole-word recognition methods typically view the text label as a whole 
rather than a character sequence, the models and language information in such meth￾ods are strongly correlated and it is difficult to recognize out-of-vocabulary words. 
Because of this limitation, researchers have begun to focus on character sequence 
recognition methods, namely feature-based character sequence recognition and label￾based character sequence recognition. 
2.2.1.2 Feature Aggregation 
The character sequence recognition method based on feature aggregation first divides 
and collects the features of the entire image to obtain the feature representation corre￾sponding to each character; secondly, predicts each character category independently; 
and finally, merges all character prediction results to obtain the prediction result of 
the character sequence. This method does not require rule-based post-processing 
steps and is easy to constrain the character features [ 52, 53]. 
According to the decoding framework, the character sequence recognition method 
based on feature aggregation can be divided into (1) serial decoding framework rep￾resented by ASTER(attentional scene text recognizer) [ 54] and (2) parallel decoding 
framework represented by ABINet (autonomous, bidirectional and iterative language 
modeling network) [ 55]. 
Serial decoding 
The naive idea for text recognition is to predict each character independently in 
order and then arrange the predicted results to obtain the text prediction result. For 
the task of recognizing irregular-shaped text, Shi et al. [ 54] proposed the ASTER 
network. The ASTER network consists of a rectification network and a recognition 
network. The rectification network adaptively corrects the text in the input image 
and inputs the rectified text image into a sequence model based on the attention 
mechanism. It sequentially extracts the regions of interest for each character and 
predicts the current character until the prediction result is the end of the sentence 
(EoS). ASTER has good recognition performance on irregular-shaped text datasets. 
To solve the problems of slow RNN training speed and high computation com￾plexity of stacked convolutional layers, Sheng et al. [ 56] proposed a non-recurrent 
sequence-to-sequence text recognizer (NRTR). NRTR uses the complete Trans￾former structure to encode the input image and predicts the current character using 
the encoding features and previous prediction results. NRTR has higher parallelism 
and lower complexity in training. 
Among the existing open-set text recognition methods, Zhang et al. [ 53] fully 
utilize the inherent hierarchical structure of Chinese characters to deconstruct a Chi￾nese character into an independent tree, in which strokes are used as leaf nodes 
and a hierarchical stroke spatial structure is used as the root node. They use an 
encoder-decoder network combined with an attention mechanism to achieve serial 
transcription of Chinese text.18 2 Background
Parallel decoding 
Due to the need to predict each character in order, serial decoding is slow. In 
recent years, researchers have proposed parallel decoding strategies to speed up the 
decoding process. 
Fang et al. [ 55] pointed out that current natural scene text recognition methods 
that incorporate language models have defects such as implicit language modeling, 
undirected feature representation, and noisy inputs, which do not fully exploit the 
language model’s capability. Therefore, they proposed an autonomous, bidirectional, 
iterative natural scene text recognition model called ABINet. The visual model of 
ABINet consists of a backbone network and a positional attention module. The 
method uses the positional attention module, based on the query paradigm, to tran￾scribe visual features in parallel into character probabilities. 
Wang et al. [ 57] proposed a decoupled attention network (DAN) to address the 
attention alignment problem that arises with attention-based methods using auto￾regressive structures. The model mainly consists of a feature collector, a convolu￾tional alignment module, and a decoder based on decoupling. The convolutional 
alignment module aligns the features based on the encoder’s output, and the text 
decoder predicts each character in parallel by jointly using the feature map and the 
attention map, effectively addressing the attention alignment problem. 
Yu et al. [ 58] believed that many existing scene text recognition models use RNN 
structures to decode semantic information, making the decoding process difficult to 
parallelize, and limiting computational efficiency. At the same time, the propaga￾tion direction is unidirectional, and information cannot be effectively transmitted, 
leading to the spread of useless and erroneous semantic information. Therefore, they 
proposed an SRN network that uses a GSRM module to achieve multi-path parallel 
propagation of global semantic information in the network, thereby fully exploiting 
contextual semantic information. 
Beak et al. [ 59] believed that many current scene text recognition models use 
different training and testing sets for evaluation, resulting in a lack of comprehensive 
and fair comparison in the field. They proposed a text recognition unified framework 
that transcribes characters in parallel to obtain text prediction results. Also, under 
the training and testing set settings, [ 59]analyzed the contributions of each module 
to the model’s performance in terms of accuracy, speed, and memory requirements. 
In open-set text recognition tasks, Liu et al. [ 60, 61] used a parallel decoding 
framework. These methods first use an attention mechanism to locate each character 
and extract feature representations. Then, they classify each character based on its 
match with the standard character shape. Finally, the results are merged into text 
prediction results. 
2.2.1.3 Label Aggregation 
Different from feature collection methods, the label-based collection method first 
classifies features and then collects them based on the classification results to obtain 
the final text prediction result. Shi et al. [ 62] proposed an end-to-end training model,2.2 Text Recognition 19
CRNN, for text recognition. The model integrates feature extraction, sequence mod￾eling, and transcription processes, and predicts text sequences from input text-line 
images. Under the condition of no character segmentation or horizontal scale nor￾malization, the model can handle texts of any length. Furthermore, the model is not 
limited to any predefined lexicon, making it highly practical in real-world applica￾tions. 
Borisyuk et al. [ 63] proposed a text extraction and recognition method for large￾scale images called Rosetta. Rosetta follows the current mainstream architecture of 
text recognition systems and is divided into two parts: text detection and text recog￾nition. The text detection part uses the Faster-RCNN model to detect regions in the 
image containing text. The text recognition part uses a fully convolutional character 
recognition model to transcribe the detected text-line image into text content. In the 
paper [ 63], the authors provide a practical reference for achieving high-performance 
text recognition tasks by balancing performance and effect in industrial-grade text 
recognition applications. 
Cheng et al. [ 64] proposed a novel recognition method, AON, for irregular-shaped 
text recognition tasks. This method divides text directions into four categories: from 
left to right (Left2Right), from right to left (Right2Left), from bottom to top (Bot￾tom2Top), and from top to bottom (Top2Bottom). First, the model extracts character 
sequence features and weight vectors for each of these four directions. Then, the 
four-character sequence features and weights are combined to form the final charac￾ter sequence. Finally, the input decoder with an attention mechanism generates the 
final text prediction result. 
Most current research considers scene text recognition as a one-dimensional 
sequence prediction problem. However, text in images is distributed in two￾dimensional space. In principle, compressing text features directly into one￾dimensional form may lose useful information and introduce additional noise. Liao 
et al. [ 65] approached scene text recognition from a two-dimensional perspective and 
proposed a Character Attention Fully Convolutional Network (CA-FCN) for recog￾nizing text of different shapes. Scene text recognition is achieved using a semantic 
segmentation network and a character attention module is designed. By adding a 
morphological module, CA-FCN can simultaneously recognize text and predict the 
position of each character, achieving good performance on both regular and irregular 
text datasets. 
2.2.2 Beyond Close-Set Text Recognition 
Most current zero-shot text recognition methods focus on Chinese Character recog￾nition due to its challenging large character set. However, many methods [ 66– 69] 
require detailed annotations on the radical composition of each character (mainly the 
radical composition tree or stroke sequence of a character). Such knowledge is mostly 
used at the decoder side and implemented as an RNN which is hard to parallelize [68]. 
Some recent methods are seen to use it on the encoder side [ 67, 68], which can avoid20 2 Background
the RNN radical decoding process during evaluation. A few radical-based methods 
like [ 53, 68] are capable of performing Chinese text-line recognition tasks on private 
datasets. However, mapping labels to corresponding component sequences requires 
strong domain knowledge and the annotating process is also a tedious job. Further￾more, text components are mainly specific to Chinese characters, which practically 
limits the feasibility in multi-language scenarios. Recently, Zhang et al. [ 70] pro￾posed a visual-matching-based method that can handle novel characters in text lines 
without language limitations. However, this method would yield a significant com￾putation burden during training, for encoding a large “glyph-line image” caused by 
the large charset. Also, this method shows very limited performance on conventional 
close-set benchmarks, which renders it less feasible for real-world applications. 
References 
1. Naylor, A.R.: Known knowns, known unknowns and unknown unknowns: a 2010 update on 
carotid artery disease (2010). [Online]. https://api.semanticscholar.org/CorpusID:196394883 
2. Scheirer, W.J., Jain, L.P., Boult, T.E.: Probability models for open set recognition. IEEE 
Trans. Pattern Anal. Mach. Intell. 36(11), 2317–2324 (2014). [Online]. https://doi.org/10.1109/ 
TPAMI.2014.2321392 
3. Dhamija, A.R., Günther, M., Boult, T.E.: Reducing network agnostophobia. In: Bengio, 
S., Wallach, H.M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) 
Advances in Neural Information Processing Systems 31: Annual Conference on Neu￾ral Information Processing Systems 2018, NeurIPS 2018, Dec 3–8, 2018, Montréal, 
Canada (2018), pp. 9175–9186. [Online]. https://proceedings.neurips.cc/paper/2018/hash/ 
48db71587df6c7c442e5b76cc723169a-Abstract.html 
4. Geng, C., Huang, S., Chen, S.: Recent advances in open set recognition: a survey. IEEE Trans. 
Pattern Anal. Mach. Intell. 43(10), 3614–3631 (2021) 
5. Scheirer, W.J., de Rezende Rocha, A., Sapkota, A., Boult, T.E.: Toward open set recognition. 
IEEE Trans. Pattern Anal. Mach. Intell. 35(7), 1757–1772 (2013) 
6. Ge, Z., Demyanov, S., Garnavi, R.: Generative openmax for multi-class open set classification. 
In: British Machine Vision Conference 2017, BMVC 2017, London, UK, Sept 4–7, 2017. 
BMVA Press (2017) 
7. Ding, C., Pang, G., Shen, C.: Catching both gray and black swans: open-set supervised anomaly 
detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 
2022, New Orleans, LA, USA, June 18–24, 2022. IEEE (2022), pp. 7378–7388 
8. Acsintoae, A., Florescu, A., Georgescu, M., Mare, T., Sumedrea, P., Ionescu, R.T., Khan, 
F.S., Shah, M.: Ubnormal: new benchmark for supervised open-set video anomaly detection. 
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022. IEEE (2022), pp. 20 111–20 121 
9. Mahdavi, A., Carvalho, M.: A survey on open set recognition. In: Fourth IEEE Interna￾tional Conference on Artificial Intelligence and Knowledge Engineering, AIKE 2021, Laguna 
Hills, CA, USA, Dec 1–3, 2021. IEEE (2021), pp. 37–44. [Online]. https://doi.org/10.1109/ 
AIKE52691.2021.00013 
10. Yang, J., Zhou, K., Li, Y., Liu, Z.: Generalized out-of-distribution detection: a survey. CoRR, 
vol. abs/2110.11334 (2021). [Online]. https://arxiv.org/abs/2110.11334References 21
11. Geng, C., Huang, S., Chen, S.: Recent advances in open set recognition: a survey. IEEE 
Trans. Pattern Anal. Mach. Intell. 43(10), 3614–3631 (2021). [Online]. https://doi.org/10.1109/ 
TPAMI.2020.2981604 
12. Chen, G., Qiao, L., Shi, Y., Peng, P., Li, J., Huang, T., Pu, S., Tian, Y.: Learning open set network 
with discriminative reciprocal points. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J. (eds.) 
Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, Aug 23–28, 2020, 
Proceedings, Part III, ser. Lecture Notes in Computer Science, vol. 12348. Springer (2020), 
pp. 507–522. [Online]. https://doi.org/10.1007/978-3-030-58580-8_30 
13. Shu, Y., Shi, Y., Wang, Y., Huang, T., Tian, Y.: P-odn: prototype-based open deep network for 
open set recognition. Sci. Rep. 10 (2019). [Online]. https://api.semanticscholar.org/CorpusID: 
146120506 
14. Yoshihashi, R., Shao, W., Kawakami, R., You, S., Iida, M., Naemura, T.: Classification￾reconstruction learning for open-set recognition. In: IEEE Conference on Computer Vision 
and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16–20, 2019. Computer 
Vision Foundation/IEEE (2019), pp. 4016–4025. [Online]. http://openaccess.thecvf.com/ 
content_CVPR_2019/html/Yoshihashi_Classification-Reconstruction_Learning_for_Open￾Set_Recognition_CVPR_2019_paper.html 
15. Yu, Y., Qu, W., Li, N., Guo, Z.: Open category classification by adversarial sample generation. 
In: Sierra, C. (ed.) Proceedings of the Twenty-Sixth International Joint Conference on Artificial 
Intelligence, IJCAI 2017, Melbourne, Australia, Aug 19–25, 2017, ijcai.org (2017), pp. 3357– 
3363. [Online]. https://doi.org/10.24963/ijcai.2017/469 
16. Geng, C., Chen, S.: Collective decision for open set recognition. IEEE Trans. Knowl. Data 
Eng. 34(1), 192–204 (2022). [Online]. https://doi.org/10.1109/TKDE.2020.2978199 
17. Zhang, X., Liu, C., Suen, C.Y.: Towards robust pattern recognition: a review. Proc. IEEE 108(6), 
894–922 (2020). [Online]. https://doi.org/10.1109/JPROC.2020.2989782 
18. Zhou, D., Wang, Q., Qi, Z., Ye, H., Zhan, D., Liu, Z.: Deep class-incremental learning: a survey. 
CoRR, vol. abs/2302.03648 (2023). [Online]. https://doi.org/10.48550/arXiv.2302.03648 
19. Castro, F.M., Marín-Jiménez, M.J., Guil, N., Schmid, C., Alahari, K.: End-to-end incremental 
learning. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) Computer Vision -
ECCV 2018 - 15th European Conference, Munich, Germany, Sept 8–14, 2018, Proceedings, 
Part XII, ser. Lecture Notes in Computer Science, vol. 11216. Springer (2018), pp. 241–257. 
[Online]. https://doi.org/10.1007/978-3-030-01258-8_15 
20. Ahn, H., Kwak, J., Lim, S., Bang, H., Kim, H., Moon, T.: SS-IL: separated softmax for incre￾mental learning. In: 2021 IEEE/CVF International Conference on Computer Vision, ICCV 
2021, Montreal, QC, Canada, Oct 10–17, 2021. IEEE (2021), pp. 824–833. [Online]. https:// 
doi.org/10.1109/ICCV48922.2021.00088 
21. He, C., Wang, R., Chen, X.: A tale of two cils: the connections between class incre￾mental learning and class imbalanced learning, and beyond. In: IEEE Conference on 
Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2021, vir￾tual, June 19–25, 2021. Computer Vision Foundation/IEEE (2021), pp. 3559–3569. 
[Online]. https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/He_A_Tale_ 
of_Two_CILs_The_Connections_Between_Class_Incremental_CVPRW_2021_paper.html 
22. Pham, Q., Liu, C., Hoi, S.C.H.: Continual normalization: rethinking batch normalization for 
online continual learning. In: The Tenth International Conference on Learning Representa￾tions, ICLR 2022, Virtual Event, April 25–29, 2022. OpenReview.net (2022). [Online]. https:// 
openreview.net/forum?id=vwLLQ-HwqhZ 
23. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R.B., Arora, S., von Arx, S., Bernstein, 
M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., 
Chatterji, N.S., Chen, A.S., Creel, K., Davis, J.Q., Demszky, D., Donahue, C., Doumbouya, M., 
Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, 
L., Goel, K., Goodman, N.D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., 
Ho, D.E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, 
S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M.S., Krishna, R., Kuditipudi, R.,22 2 Background
et al.: On the opportunities and risks of foundation models. CoRR, vol. abs/2108.07258 (2021). 
[Online]. https://arxiv.org/abs/2108.07258 
24. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., 
Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, 
T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, 
E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., 
Sutskever, I., Amodei, D.: Language models are few-shot learners. In: Larochelle, H., Ran￾zato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing 
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 
2020, Dec 6–12, 2020, Virtual (2020). [Online]. https://proceedings.neurips.cc/paper/2020/ 
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html 
25. Bender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: On the dangers of stochas￾tic parrots: can language models be too big? In: Elish, M.C., Isaac, W., Zemel, R.S. (eds.) 
FAccT ’21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual 
Event/Toronto, Canada, March 3–10, 2021, ACM (2021), pp. 610–623. [Online]. https://doi. 
org/10.1145/3442188.3445922 
26. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., 
Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, 
F.: Language is not all you need: aligning perception with language models (2023). [Online]. 
http://arxiv.org/abs/2302.14045 
27. Shi, Y., Peng, D., Liao, W., Lin, Z., Chen, X., Liu, C., Zhang, Y., Jin, L.: Exploring ocr capabil￾ities of gpt-4v (ision): a quantitative and in-depth evaluation. arXiv preprint arXiv:2310.16809 
(2023) 
28. Rust, P., Lotz, J.F., Bugliarello, E., Salesky, E., de Lhoneux, M., Elliott, D.: Language modelling 
with pixels. In: The Eleventh International Conference on Learning Representations, ICLR 
2023, Kigali, Rwanda, May 1–5, 2023. OpenReview.net (2023) 
29. Liu, C., Yang, C., Yin, X.: Open-set text recognition via shape-awareness visual reconstruction. 
In: Document Analysis and Recognition - ICDAR 2023–17th International Conference, San 
José, CA, USA, Aug 21–26, 2023, Proceedings, Part VI, ser. Lecture Notes in Computer 
Science, vol. 14192. Springer (2023), pp. 89–105 
30. Long, Y., Wen, Y., Han, J., Xu, H., Ren, P., Zhang, W., Zhao, S., Liang, X.: Capdet: unifying 
dense captioning and open-world detection pretraining. In: IEEE/CVF Conference on Com￾puter Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17–24, 2023. 
IEEE (2023), pp. 15 233–15 243 
31. Fei, G., Liu, B.: Breaking the closed world assumption in text classification. In: Proceedings of 
the 2016 Conference of the North American Chapter of the Association for Computational Lin￾guistics: Human Language Technologies. Association for Computational Linguistics (2016), 
pp. 506–514 
32. Pourpanah, F., Abdar, M., Luo, Y., Zhou, X., Wang, R., Lim, C.P., Wang, X., Wu, Q.M.J.: 
A review of generalized zero-shot learning methods. IEEE Trans. Pattern Anal. Mach. Intell. 
45(4), 4051–4070 (2023) 
33. Bansal, A., Sikka, K., Sharma, G., Chellappa, R., Divakaran, A.: Zero-shot object detection. In: 
Proceedings of the European Conference on Computer Vision (ECCV) (2018), pp. 384–400 
34. Zheng, Y., Wu, J., Qin, Y., Zhang, F., Cui, L.: Zero-shot instance segmentation. In: IEEE 
Conference on Computer Vision and Pattern Recognition, CVPR 2021, Virtual, June 19–25, 
2021. Computer Vision Foundation/IEEE (2021), pp. 2593–2602 
35. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional 
transformers for language understanding. In: Proceedings of the 2019 Conference of the North 
American Chapter of the Association for Computational Linguistics: Human Language Tech￾nologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics (2019), 
pp. 4171–4186 
36. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 
A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from 
natural language supervision. In: Proceedings of the 38th International Conference on MachineReferences 23
Learning, ICML 2021, 18–24 July 2021, Virtual Event, ser. Proceedings of Machine Learning 
Research, vol. 139. PMLR (2021), pp. 8748–8763 
37. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, 
P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. 
Learn. Res. 21, 140:1–140:67 (2020) 
38. Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, 
A., Raja, A., Dey, M., Bari, M.S., Xu, C., Thakker, U., Sharma, S.S., Szczechla, E., Kim, 
T., Chhablani, G., Nayak, N.V., Datta, D., Chang, J., Jiang, M.T., Wang, H., Manica, M., 
Shen, S., Yong, Z.X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., 
Santilli, A., Févry, T., Fries, J.A., Teehan, R., Scao, T.L., Biderman, S., Gao, L., Wolf, T., 
Rush, A.M.: Multitask prompted training enables zero-shot task generalization. In: The Tenth 
International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25–29, 
2022. OpenReview.net (2022) 
39. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, 
S., Berg, A.C., Lo, W.-Y., et al.: Segment anything (2023) 
40. Ma, Z., Luo, G., Gao, J., Li, L., Chen, Y., Wang, S., Zhang, C., Hu, W.: Open-vocabulary 
one-stage detection with hierarchical visual-language knowledge distillation. In: IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, 
USA, June 18–24, 2022. IEEE (2022), pp. 14 054–14 063 
41. Panareda Busto, P., Gall, J.: Open set domain adaptation. In: Proceedings of the IEEE Interna￾tional Conference on Computer Vision (2017), pp. 754–763 
42. Shi, J., Xu, N., Zheng, H., Smith, A., Luo, J., Xu, C.: Spaceedit: learning a unified editing 
space for open-domain image color editing. In: IEEE/CVF Conference on Computer Vision 
and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18–24, 2022. IEEE (2022), 
pp. 19 698–19 707 
43. Katsumata, K., Vo, D.M., Nakayama, H.: OSSGAN: open-set semi-supervised image gener￾ation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, 
New Orleans, LA, USA, June 18–24, 2022. IEEE (2022), pp. 11 175–11 183 
44. Ning, K., Zhao, X., Li, Y., Huang, S.: Active learning for open-set annotation. In: IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, 
USA, June 18–24, 2022. IEEE (2022), pp. 41–49 
45. Yang, C., Liu, C., Fang, Z.-Y., Han, Z., Liu, C.-L., Yin, X.-C.: Open set text recognition 
technology. J. Image Graph. 28, 1767–1791 (2023) 
46. Manmatha, R., Han, C., Riseman, E.M.: Word spotting: a new approach to indexing hand￾writing. In: 1996 Conference on Computer Vision and Pattern Recognition (CVPR ’96), June 
18–20, 1996 San Francisco, CA, USA. IEEE Computer Society (1996), pp. 631–637 
47. Almazán, J., Gordo, A., Fornés, A., Valveny, E.: Word spotting and recognition with embedded 
attributes. IEEE Trans. Pattern Anal. Mach. Intell. 36(12), 2552–2566 (2014) 
48. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Reading text in the wild with con￾volutional neural networks. Int. J. Comput. Vis. 116(1), 1–20 (2016) 
49. Chanda, S., Baas, J., Haitink, D., Hamel, S., Stutzmann, D., Schomaker, L.: Zero-shot learning 
based approach for medieval word recognition using deep-learned features. In: 16th Interna￾tional Conference on Frontiers in Handwriting Recognition, ICFHR 2018, Niagara Falls, NY, 
USA, Aug 5–8, 2018. IEEE (2018), pp. 345–350 
50. Rai, A., Krishnan, N.C., Chanda, S.: Pho(sc)net: an approach towards zero-shot word image 
recognition in historical documents. In: 16th International Conference on Document Analysis 
and Recognition, ICDAR 2021, Lausanne, Switzerland, Sept 5–10, 2021, Proceedings, Part I, 
ser. Lecture Notes in Computer Science, vol. 12821. Springer (2021), pp. 19–33 
51. Chanda, S., Haitink, D., Prasad, P.K., Baas, J., Pal, U., Schomaker, L.: Recognizing Bengali 
word images - A zero-shot learning perspective. In: 25th International Conference on Pattern 
Recognition, ICPR 2020, Virtual Event/Milan, Italy, Jan 10–15, 2021. IEEE (2020), pp. 5603– 
5610 
52. Li, B., Tang, X., Qi, X., Chen, Y., Xiao, R.: Hamming OCR: a locality sensitive hashing neural 
network for scene text recognition (2020). [Online]. https://arxiv.org/abs/2009.1087424 2 Background
53. Zhang, J., Du, J., Dai, L.: Radical analysis network for learning hierarchies of Chinese char￾acters. Pattern Recognit. 103, 107305 (2020) 
54. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: ASTER: an attentional scene text 
recognizer with flexible rectification. IEEE Trans. Pattern Anal. Mach. Intell. 41(9), 2035– 
2048 (2019) 
55. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: autonomous, bidirectional 
and iterative language modeling for scene text recognition. In: IEEE Conference on Computer 
Vision and Pattern Recognition, CVPR 2021, Virtual, June 19–25, 2021. Computer Vision 
Foundation/IEEE (2021), pp. 7098–7107 
56. Sheng, F., Chen, Z., Xu, B.: NRTR: a no-recurrence sequence-to-sequence model for scene 
text recognition. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, Sept 20–25, 2019. IEEE (2019), pp. 781–786 
57. Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.: Decoupled attention 
network for text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, New York, NY, USA, Feb 7–12, 2020. AAAI Press (2020), pp. 12 216–12 224 
58. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate scene text 
recognition with semantic reasoning networks. In: 2020 IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020. IEEE 
(2020), pp. 12 110–12 119 
59. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), Oct 27–Nov 
2, 2019. IEEE (2019), pp. 4714–4722 
60. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022. IEEE (2022), pp. 4513–4522 
61. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
62. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence 
recognition and its application to scene text recognition. IEEE Trans. Pattern Anal. Mach. 
Intell. 39(11), 2298–2304 (2017) 
63. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: large scale system for text detection and 
recognition in images. In: Proceedings of the 24th ACM SIGKDD International Conference 
on Knowledge Discovery Data Mining, KDD 2018, London, UK, Aug 19–23, 2018. ACM 
(2018), pp. 71–79 
64. Cheng, Z., Xu, Y., Bai, F., Niu, Y., Pu, S., Zhou, S.: AON: towards arbitrarily-oriented text 
recognition. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 
2018, Salt Lake City, UT, USA, June 18–22, 2018. IEEE Computer Society (2018), pp. 5571– 
5579 
65. Liao, M., Zhang, J., Wan, Z., Xie, F., Liang, J., Lyu, P., Yao, C., Bai, X.: Scene text recogni￾tion from two-dimensional perspective. In: The Thirty-Third AAAI Conference on Artificial 
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence 
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial 
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, Jan 27–Feb 1, 2019. AAAI Press (2019), 
pp. 8714–8721 
66. Wang, T., Xie, Z., Li, Z., Jin, L., Chen, X.: Radical aggregation network for few-shot offline 
handwritten Chinese character recognition. Pattern Recognit. Lett. 125, 821–827 (2019) 
67. Cao, Z., Lu, J., Cui, S., Zhang, C.: Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recognit. 107, 107488 (2020) 
68. Huang, Y., Jin, L., Peng, D.: Zero-shot Chinese text recognition via matching class embedding. 
In: 16th International Conference on Document Analysis and Recognition, ICDAR 2021, Lau￾sanne, Switzerland, Sept 5–10, 2021, Proceedings, Part III, ser. Lecture Notes in Computer 
Science, vol. 12823. Springer (2021), pp. 127–141References 25
69. Chen, J., Li, B., Xue, X.: Zero-shot Chinese character recognition with stroke-level decompo￾sition. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 
IJCAI 2021, Virtual Event/Montreal, Canada, 19–27 Aug 2021. ijcai.org (2021), pp. 615–621 
70. Zhang, C., Gupta, A., Zisserman, A.: Adaptive text recognition through visual matching. In: 
Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, Aug 23–28, 2020, 
Proceedings, Part XVI, ser. Lecture Notes in Computer Science, vol. 12361. Springer (2020), 
pp. 51–67Chapter 3 
Open-Set Text Recognition: Concept, 
Dataset, Protocol, and Framework 
Abstract This chapter gives a clear and detailed definition of the OSTR task. First, 
we describe the aim, goal, and scope, and formulate and define the OSTR task and the 
relation between OSTR and other tasks. Secondly, we narrow down to the specific 
protocols used to measure model performances on the task. Before reaching the 
specific protocol, we introduce the commonly used protocols and datasets to lay out 
the background. Finally, this chapter presents a general framework of OSTR as an 
implementation guideline to build models for the OSOCR task. Here, we first describe 
the overall design, including the modules, and how they interact with each other. 
Specifically, we discuss the training workflow of the framework, its testing workflow, 
and how it may cope with an open-world data stream in a production environment. We 
then formulate the key variables of individual modules, giving a brief introduction to 
the functionality. More implementations will be introduced in detail in later chapters. 
Open-Set Text Recognition: Concept, Dataset, and Protocol In addition, we explain 
the backward compatibility by examples, i.e., how the framework also fits zero-shot 
and close-set text recognition methods. 
Keywords Open-set text recognition task · Data set and protocol · Framework 
3.1 Concept 
3.1.1 Aims, Goals, and Scope 
Text recognition is a process that transcripts text content from input images, widely 
used in scene understanding and document digitization. Albeit the development of 
Multi-modal LLMs [ 1] and image-based token [ 2] considerably reduced the sig￾nificance of OCR in image understanding, the task is there to remain for a while. 
Scene understanding aside, OCR is still essential to document digitization, as cur-
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_3 
2728 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
Fig. 3.1 Novel characters in the open environments 
rently, LLMs are still not capable of performing efficient loss-less compress or string 
matching. 
Ideally, the data used to train an OCR method will sufficiently cover the data 
during inference, which SOTA text recognition methods [ 3, 4] can handle pretty 
well. However, decent coverage is often a luxury in open-word applications, which 
forms the dilemma the OSTR task aims to solve. 
The coverage problem presents in two folds, for the training data may fail to 
cover all words, yielding the OOV challenge [ 5]. The situation can worsen when 
the characters-set is not properly covered, yielding the open-set text recognition 
problem [ 6]. Figure 3.1 illustrates the novel characters in the open environments. 
The general causes of the coverage problem are mainly insufficient knowledge 
of the language, or simply the evolving nature of languages. The lack of knowledge 
happens to language with a vast character set like the CJK family, which also happens 
to cultures amassing ligatures, which makes it to exhaust possible character up-front 
a tedious work. Also, building synthetic data could be expensive, as one needs to 
build fonts of various styles with a large coverage range. 
For historical documents, collecting all characters beforehand would be very dif￾ficult as a character would mostly only be known to exist until archeologists actually 
find it somewhere. 
Worse still, languages evolve unpredictably over time, which means words from 
foreign languages are constantly adopted, yielding new words. Characters from for￾eign characters can also be seen in scene images, resulting in a constantly changing 
open character set. Emojis also contribute to the evolution of characters and make the 
problem even more challenging due to their fast-evolving speed. One would never 
tell what kind of emojis will be created in the next month. 
The above challenges are extremely expensive to address for conventional close￾set text recognition methods, if not impossible. Thus, OCR models that can evolve 
alongside the language itself are then necessary, which forms the goal of the OSTR 
task. 
Specifically, the task aims to support the third workflow shown in Fig. 3.2, reduc￾ing the tedious retraining process and the usually delayed and unfeasible user com￾plaints. To achieve the open-set text recognition goal, a model shall possess the fol￾lowing capabilities besides recognition of seen characters: novel character spotting3.1 Concept 29
Fig. 3.2 Text recognition works build upon different type of text recognition methods 
and novel character recognition capabilities. Specifically, novel character recognition 
is extensively studied by the zero-shot text recognition methods [ 7– 9]. Novel char￾acter spotting [ 6, 10] is currently less studied in the text recognition field, however, 
is a popular topic in other domains [ 11, 12]. 
Beyond the two required capabilities, the capability of novel character cognition, 
which requires the model to cluster spotted novel instances into classes and generate 
corresponding side-information, would be helpful to further reduce human workloads 
and remains a rarely explored domain in and beyond the text recognition field. 
We hence define the scope of the OSTR task, a method can be called an OSTR 
method if it 1) achieves reasonable recognition capability on conventional close-set 
text recognition tasks, and 2) possesses at least one of the aforementioned capabili￾ties, i.e., novel character spotting, novel character recognition, and novel character 
cognition. 
We also limit the scope of OSTR to single-line crop word recognition. Specifi￾cally, we exclude the online character recognition tasks [ 13] and end-to-end spotting 
tasks [ 14, 15]. We also leave the effects and interaction with upstream tasks like text 
localization, or downstream tasks like QA out of the scope of the task. 
3.1.2 Task Definition 
Generally speaking, the open-set text recognition task (OSTR, illustrated in Fig. 3.3) 
transcribes each input sample image that potentially contains open-set characters 
into corresponding predictions, with regards to a given character set provided with 
corresponding side-information.30 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
Fig. 3.3 Open-set text recognition task: During the evaluation process, some samples may include 
characters not appearing in the training set. The task requires the model to recognize both seen and 
novel characters in.Ck
test (blue arrow) and reject samples with “out-of-set” characters in.Cu
test (red 
arrow). [ 16] 
We first define the following character sets: .Ctrain, .Ctest , .Ck
test , and .Cu
test . The 
training character set .Ctrain denotes all characters in the training samples, while 
the testing character set .Ctest denotes all characters in the testing samples. . Ctest
can be further disjointly divided into two subsets .Ck
test and .Cu
test . Here, the known 
character set .Ck
test contains characters to be recognized, which are provided with 
corresponding side-information. On the other hand, the unknown character set. Cu
test
includes characters with no information and subject to rejection. Note in real-life 
applications, .Cu
test is usually not fully known to the users because, by definition it 
includes unknown characters yet to be spotted or even haven’t appeared in the data 
stream. 
Given the definition of character sets, a character. e can thus be categorized with 
regard to its novelty and side-information availability. For addressing convenience, 
we denote the four categories shown in Fig. 3.3 as: “Seen In-set Character (SIC)”, 
“Seen Out-of-set Character (SOC)”, “Novel In-set Character (NIC)”, and the “Novel 
Out-of-set Character (NOC)”. The first term indicates whether the character is “seen” 
or “novel” to training samples, and the second tells whether its side-information 
presents for evaluation. Specifically, a character . e is considered as “novel” if it 
does not appear in training samples, i.e., .c ∈/ Ctrain. Otherwise, it is considered 
as “seen”. Side-information wise, a character with side-information is considered 
as “in-set”(.e ∈ Ck
test) and subject to recognition, otherwise considered as “out-of￾set”(.e ∈ Cu
test) and subject to rejection.3.1 Concept 31
Fig. 3.4 Converting relation 
of characters 
Formally, an open-set text recognition model takes a sample image .img of label 
.Y∗, and a character set .C as inputs, and produces the prediction .Yˆ as output. The 
model is expected to recognize in-set characters and reject out-of-set characters, i.e., 
.Yˆ [l] =
{
Y∗
[l], if Y∗
[l] ∈ C
[−], otherwise (3.1) 
During the evaluation, .Ck
test is used as . C, as in open-word applications . Ctest
describes all potential characters, which is intractable. Specifically, the subset 
.Ck
test models the “known” 1 part of characters, which refers to the correspond￾ing side-information being available. These characters can be either seen (SIC) or 
unseen(NIC) during training. The rest naturally goes to the .Cu
test , which includes 
SOC which is seen but excluded due to long absence, and NIC which is still not 
spotted or again excluded. 
As shown in Fig. 3.4, the user may adjust .Ck
test according to the input data, 
computation load, and demands. Specifically, the user can audit the rejected samples 
and add missing characters to .Ck
test by providing corresponding side-information. 
Alternatively, the user can make the model “forget” rare characters from .Ck
test by 
removing side-information in exchange for speed. 
3.1.3 Relation to Other Tasks 
3.1.3.1 Similarities and Differences with Other Text Recognition Tasks 
The proposed task is related to several existing text recognition tasks as summarized 
in Fig. 3.5. Compared to the close-set text recognition task solely focusing on the 
SIC, the OSTR task relaxes the.Ctest ⊆ Ctrain assumption by introducing novel labels 
which do not appear in the training samples. On the other hand, compared to the zero￾shot character recognition task [ 18, 19] which can also be regarded as a special case 
of OSTR when.Ctest ∩ Ctrain = Φ,.Cu
test = Φ, and all samples have a constant length 
of. 1. Furthermore, compared to both tasks, the OSTR task introduces the rejection of
1 As of the second “known” in [ 17]. 32 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
Fig. 3.5 The relation between the Open-Set Text Recognition task and other text recognition tasks 
out-of-set characters, allowing two folds of capability. First, the rejection capability 
would allow the user to get a notification when novel characters appear in the data 
stream. Second, it allows the user to optionally skip recognition of rare characters 
to speed up, while keeping track of the rejected samples, which may include the 
skipped characters, for later checking. 
3.1.3.2 Relation with Other Tasks 
In a broader sense, the open-set text recognition (OSTR) task can be roughly con￾sidered as a combination of the open-set recognition task (OSR) [ 17, 20] and the 
generalized zero-shot learning (GZSL) task [21]. Specifically, the OSTR task requires 
recognizing both seen and novel characters in .Ck
test with side-information like the 
GZSL tasks [ 21]. Furthermore, the OSTR task requires the rejection capability on 
character instances not covered by.Ck
test , seen or novel. Despite the subjects of rejec￾tion being slightly different, Both OSTR and OSR imply closed decision boundaries 
for each class. Figure 3.6 illustrates concept comparison with other OSR related 
tasks. 
Fig. 3.6 Concept comparison with other OSR related tasks3.1 Concept 33
In summary, compared to the GZSL tasks, the OSTR task further adds the require￾ment on rejection capability, which enables actively spotting unknown samples in 
applications (characters not covered by .Ck
test , i.e., “out-of-set” characters). Com￾pared to the OSR task, besides the capability to spot samples from novel classes 
from the data stream, the OSTR introduced the capability to recognize samples from 
novel classes once the corresponding side-information is provided, allowing fast and 
incremental model adaption without retraining the model. 
Noteworthy, the recognition capability of novel classes with closed decision 
boundaries implies that the samples of each novel class shall gather closely to its 
class center(s). Since this “gather” property is not affected by whether or not the 
side-information is known to the model, “cognition” on “unknown classes” is to 
some extent implied in the OSTR task. Hence, the OSTR task can be considered as 
a variant of the generalized open-set recognition task [ 17] as well. 
3.1.4 Challenges 
3.1.4.1 Common OCR Challenges 
This section introduces the common challenge of text recognition tasks in both close 
and open environments. The challenges come from three aspects, namely image 
quality, style variation, and language-specific challenges. 
The image quality challenges are related to image quality degradation. For scene 
text recognition, degrades usually include focus failure [ 22], blurs [ 23], background 
variations [ 24], letter blocking [ 4], etc. For handwriting recognition, the image quality 
challenges mainly come from document corruption [25], stains [ 26], overlapping with 
background and other elements [ 27], etc. 
The style variation shares similar sources between scene text and handwritten 
text, mainly coming from font and size differences, writer-specific style [ 28], color, 
and writing orientations [ 29]. 
The language-specific challenge comes from specific traits directly from the lan￾guage, like huge intra-class variance [ 30], morphology [ 31], complex writing order 
(e.g., hieroglyphics in a line do not always write in 1-D order [ 32]), etc. 
3.1.4.2 Unique Challenges 
Open-set text recognition task yields additional challenges due to new categories and 
new data compared to closed-set text recognition. New categories refer to, that data 
streams may contain characters that are not able to be anticipated during training 
(e.g., emojis [ 2], and undiscovered historical ligatures [ 33], etc). New data, on the 
other hand, refers to the linguist shift due to language evolving through time, or 
regional differences. 
The challenges can introduce the following three technical problems.34 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
First, models should be able to actively discover novel characters that are not 
covered by training samples and notify the users, denoted as the unknown sample 
discovery problem. Specifically, models need to reject the testing samples that include 
characters that fail to match all characters in .Ck
test , instead of recognizing them as 
some known ones in.Ck
test . Albeit not yet extensively studied in the text recognition 
field [ 6, 33, 34], this problem is one of the main scientific issues that the open-set 
recognition tasks [ 20, 35– 37] and anomaly [ 12, 38] detection tasks aim to solve. 
The task faces two key implementation challenges, namely (1) how to model the 
unknown classes and (2) how to use data from known classes to model the unknown 
classes in training. 
Second, after discovering new characters, the model should be able to adapt and 
recognize them without retraining, which we summarize as the problem of recog￾nizing new classes. This challenge is recently studied as few-shot [ 19] and zero-shot 
text recognition task [ 18, 39, 40], and is also a main topic of both few-shot [ 19] and 
zero-shot learning tasks [ 41]. The implementation of such a mechanism involves 
constructing a robust mapping from side-information to representation prototype 
that generalizes to novel classes. 
Third, in open environments, along with the emergence of new characters and 
scripts, linguistic information is also affected by language evolution and gradually 
deviates from the trained corpus, which affects the recognition performance [ 42], 
i.e., the problem of contextual information bias. Contextual information bias is also 
one of the common problems widely found in different fields of machine learning 
[ 43– 46]. Due to the specificity of text, contextual information plays a relatively larger 
role in text recognition, which also leads to the prediction results of models for new 
characters being more prominently affected by contextual bias. 
3.2 Open-set Recognition: Dataset and Protocol 
In order to evaluate a text recognition method as fairly as possible, several evaluation 
protocols have been proposed for different researchers to train, test, and evaluate their 
methods. An evaluation protocol consists of two parts: data (training set, testing set, 
and character set) and evaluation metrics. This subsection provides an overview of 
the data, metrics, and protocols, respectively. 
3.2.1 Dataset by Language 
We provide an overview of some common publicly available datasets, the details of 
which are given in Table 3.1, for private data [ 7] we do not discuss it here. 
The common real datasets can be divided into English datasets and other language 
datasets according to language, with English datasets being the most used. For the 
English language, two synthetic datasets, MJ [ 47] and ST [ 48], are used for training,3.2 Open-set Recognition: Dataset and Protocol 35 Table 3.1 Common text recognition dataset. “Label” indicates the whether annotation information include the corresponding granularity (character location, the text content of the word image, and the language of the image). Cictionary refers to whether the dataset is accompanied by predefined ground truth candidates for each image, and its size if available Datasets Type Language Images Label Shape Lexicon Instances Training Testing Char Word Language IIIT5K Real, scene English 5000 2000 3000 ./ ./ ✕ Regular 50/1k SVT Real, scene English 725 211 514 ✕ ./ ✕ Regular 50 IC03 Real, scene English 2268 1157 1111 ./ ./ ✕ Regular 50/1k/full IC13 Real, scene English 5003 3564 1439 ./ ./ ✕ Regular ✕ SVTP Real, scene English 639 0 639 ✕ ./ ✕ Irregular 50/full CUTE80 Real, scene English 288 0 288 ✕ ./ ✕ Irregular ✕ IC15 Real, scene English 6545 4468 2077 ✕ ./ ✕ Irregular ✕ Synth90k Synthetic English . ∼9000000 – – ✕ ./ ✕ Regular ✕ SynthText Synthetic English . ∼6000000 – – ./ ./ ✕ Regular ✕ SVHN Real, scene Digits 600000 573968 26032 ./ ./ ✕ Regular ✕ COCO-Text Real, scene English 145859 118309 27550 ✕ ./ ✕ Irregular ✕ Total-Text Real, scene English 11459 11166 293 ✕ ./ ✕ Irregular ✕ Verisimilar Synthesis Synthetic English . ∼5000000 – – ✕ ./ ✕ Regular ✕ UnrealText Synthetic English . ∼12000000 – – ./ ./ ✕ Regular ✕ MTWI Real, web Chinese+English 290206 141476 148730 ✕ ./ ✕ Regular ✕ RCTW-17 Real, scene Chinese+English – – – ✕ ./ ✕ Regular ✕ CTW Real, scene Chinese+English 1018402 812872 103519 ./ ./ ✕ Regular ✕ SCUT-CTW1500 Real, scene Chinese+English 10751 7683 3068 ✕ ./ ✕ Irregular ✕ LSVT Real, scene Chinese+English – – – ✕ ./ ✕ Irregular ✕ ArT Real, scene Chinese+English 98455 50029 48426 ✕ ./ ✕ Irregular ✕ ReCTS-25k Real, scene Chinese+English 119713 108924 10789 ./ ./ ✕ Irregular ✕ MLT Real, scene Multilingual 191639 89177 102462 – ./ ./ Irregular – OLHWDB1 Real, hand written Chinese ./ – – – – IAM Real, hand written English36 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
while real datasets are commonly used only for testing. Commen real testings set are 
IIIT5k [ 49], CUTE [ 50], SVT [ 51] , SVTP [ 29] , IC03 [ 52], IC13 [ 53], and IC15 [ 22]. 
There are also Google book sets [ 8] , Coco-text [ 54], Uber-text [ 55] and others that 
are occasionally used for training and testing [ 8, 14]. In recent years, researchers 
have also proposed datasets in other languages, of which a large proportion are 
Chinese-English datasets covering natural scenes, web images, and offline/online 
handwriting data. Datasets from other languages are also proposed, however, receive 
comparatively less attention. We list the attributes of popular datasets in Table 3.1. 
3.2.2 Metrics 
Open-set text recognition methods can be measured with the recognition performance 
and the rejection performance. 
3.2.2.1 Recognition Performance 
Line Accuracy is the most frequently used metric in measuring scene text recognition 
performance [ 56, 57], defined as 
.L A = 1
N
Σ
N
i=1
Same(Gt[i], Pred[i]), (3.2) 
where .N is the size of the dataset, .Gt[i] and .Pred[i] correspondingly indicate the 
label and the prediction of the .ith sample. Line accuracy is also referred to as word 
accuracy, or just accuracy for short, in literature. 
Character Accuracy (CA) is another common metric, also known as accuracy 
rate (AR), the same as 1-NED (normalized edit distance). This metric has two dif￾ferent variants, which are 
.C A = 1 −
ΣN
i=1 ED(Pred[i], Gt[i])
ΣN
i=1 Len(Gt[i])
(3.3) 
and 
.C A = 1 −Σ
N
i=1
ED(Pred[i], Gt[i])
max(Len(Pred[i]), Len(Gt[i])). (3.4) 
Note that the denominators of these two definitions are different; Eq. 3.3 first 
averages the edit distance for each word and then for the dataset [ 58]. Equation 3.4, 
on the other hand, takes all strings of the entire dataset together to find the edit 
distance [ 59], and thus this metric highlights the recognition performance of long 
texts more. Another variant, called Correct Rate (CR) [ 52], excludes the insertion 
errors from the edit distance count from the character accuracy.3.2 Open-set Recognition: Dataset and Protocol 37
Character Accuracy is less used in recent text recognition protocols due to the 
improvement of model performance, and probably its vast number of variety and 
ambiguity. For character recognition protocols where all samples are of length 1, the 
line accuracy and the two character accuracy algorithms are equivalent. 
3.2.2.2 Rejection Performance 
The capability to reject characters in.Cu
test is measured with word-level recall (.RE), 
precision (.P R), and F-measure (.F M). 
.
RE =
ΣN
i=1 Rej(Pred[i])Rej(Gt[i])
ΣN
i=1 Rej(Gt[i]) ,
P R =
ΣN
i=1 Rej(Pred[i])Rej(Gt[i])
ΣN
i=1 Rej(Pred[i]) .
(3.5) 
Here, .Rej is an indicator function that returns whether the string contains “out￾of-set” characters (SOC and NOC). .Pred[i] and .Gt[i] denote the prediction result 
and the ground truth value of the.i-th text-line sample, respectively. Specifically, the 
recall describes the likelihood for the system to reject a sample containing out-of-set 
characters and alert the user to adjust.Ck
test . The precision measures the ratio of fake 
positive warnings that require manual dismissal. Finally, the F-measure is given as 
an overall metric of recall and precision, 
.F M = 2RE ∗ P R
RE + P R . (3.6) 
This metric uses line-level measures instead of character-level measures, thus 
simplifying the calculation of metrics and avoiding ambiguities arising from the 
temporal misalignment, where the length of prediction mismatches with the length 
of the ground truth. Also, due to the fact that a sample image containing out-of-set 
characters will require human review regardless of whether the sample contains other 
in-set characters, word-level measurement still matches the actual feasibility of the 
model in question. 
3.2.3 Protocols 
3.2.3.1 Close-shot Text Recognition 
Since seen text makes an important part of most real-life applications, a referenced 
comparison to the conventional methods is needed to validate a method’s feasibility 
in a real-life production environment. Currently, most methods honor the English38 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
Benchmark [ 56]. Also, an emerging Chinese Language benchmark [ 57] is pro￾posed to model the measure the performance on languages with large character sets. 
Table 3.2 illustrates common Text Recognition Protocols for benchmarking, speci￾fying the training set, test set, character set, and evaluation metrics used. 
For closed-set text recognition, the most common protocols are trained on two 
synthetic datasets, MJ [ 47] and ST [ 48], and tested on real datasets like IIIT5k [ 49], 
CUTE [ 50], SVT [ 51], SVTP [ 29], IC03 [ 52], IC13 [ 53], and IC15 [ 22] datasets, and 
the results are measured with the Line Accuracy metric. We denote this evaluation 
protocol as www [ 56]. Note that the protocol can be subject to some changes in 
some methods, for example, introducing annotations for character localization [ 60], 
utilizing external language models [ 3, 61], or adding new training data [ 62]. The 
potential annotation cost and performance impact of these changes should be noted 
during model picking. 
Chen et al. [ 57] propose a protocol for evaluating under four Chinese scenarios 
which we call Fudan 2 for short, in which they test typical text recognition methods 
using line accuracy and character accuracy as a measure of recognition effectiveness. 
The two evaluation protocols can to some extent measure a model’s ability to rec￾ognize in-set characters. In most open environments, in-set characters still constitute 
the majority of samples, so the selection of open-set text recognition methods should 
take the closed-set character recognition performance into account. 
3.2.3.2 Zero-shot Text Recognition 
For zero-shot text recognition, the most popular benchmarks focus on character￾level training and testing, forming two evaluation protocols, which here we refer to 
as HDE [ 18] and Fudan [ 40]. Note that although the two protocols utilize the same 
dataset and split size, the differences in splitting rules cause a significant difference 
in difficulty. Specifically, The two protocols are trained and tested with two datasets, 
HWDB and CTW, respectively. Both protocols first delineate the test set and delineate 
the training set of different sizes among the remaining characters. The difference is 
that HDE, on the other hand, performs the partitioning randomly, while Fudan’s 
partitioning is done according to some specific rules. There is currently no obvious 
consensus on word-level zero-shot text recognition, where methods [ 7– 9, 63] are 
using different training sets, testing sets, and evaluation metrics to measure their 
model performances. 
3.2.3.3 Open-set Text Recognition 
For open-set text recognition, OSOCR [ 6] constructs an open-set text recognition 
dataset with a collection of existing openly available datasets. Specifically, they 
choose Chinese scene text images for training while Japanese scene text images
2 The proposed is proposed by a team in Fudan university. 3.2 Open-set Recognition: Dataset and Protocol 39 Table 3.2 Common Text Recognition Protocols for benchmarking, specifying the training set, test set, character set, and evaluation metrics used Task Type Name Variants Training Testing Validation Datasets Character set Extra annotation Datasets Character set (Types) Metric SIC SOC NIC NOC Close-set www Common MJ, ST English, digit – IIIT5k, CUTE80, SVT, SVTP, IC03, IC13, IC15 English, digit – – – LA N Semantic MJ, ST Language model IIIT5k, CUTE80, SVT, SVTP, IC03, IC13, IC15 English, digit – – – Character MJ, ST Character boxes IIIT5k,CUTE80, SVT, SVTP, IC03, IC13, IC15 English, digit – – – FudanVI Scene RCTW, ReCTS, LSVT, ArT, CTW 7938 Characters Radical composition RCTW, ReCTS, LSVT, ArT, CTW 7938 Characters – – – LA/1-NED Y Web MTWI MTWI – – – Document FudanVI FudanVI – – – Handwriting SCUT- HCCDoc SCUT-HCCDoc – – – Zero-shot HDE HWDB HWDB subsets (500, 1000, 1500, 2000, 2755) Chinese Characters in subsets (500, 1000, 1500, 2000, 2755) Radical composition HWDB subsets (remaining 1000 unseen) – – 1000 Unseen Characters LA N (continued) 40 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework Table 3.2 (continued) Task Type Name Variants Training Testing Validation Datasets Character set Extra annotation Datasets Character set (Types) Metric SIC SOC NIC NOC CTW CTW subsets (500, 1000, 1500, 2000, 2755) Chinese Characters in subsets (500, 1000, 1500, 2000, 2755) CTW subsets (remaining 500 unseen) 
– – 500 Unseen 
Characters 
– 
FudanVI HWDB HWDB 
subsets (500, 
1000, 1500, 
2000, 2755) 
LA N 
CTW CTW subsets 
(500, 1000, 
1500, 2000, 
2755) 
Open-set OSOCR GZSL-JAP RCTW, 
LSVT, ArT, 
CTW, MLT 
(Latin, 
Chinese) 
Chinese Tire1, 
English, digit 
– MLT-Japanese – – – – LA/1-NED N 
GZSL-KR – MLT-Korean – – – – 
OSR – MLT-Japanese Shared Kanji, 
English, digit 
– – Unique Kanji, 
Kana 
LA/R/P/F 
GOSR – MLT-Japanese Shared Kanji, 
English, digit 
– Unique Kanji Kana 
OSTR – MLT-Japanese Shared Kanji English, digits Unique Kanji Kana 3.3 An OSTR Framework 41
Fig. 3.7 Spaces and mappings of OSOCR framework 
for evaluation, due to the abundant amount of samples that can be found in these 
two languages. The training set contains Chinese and Latin samples collected from 
ART [ 59], RCTW [ 64], LSVT [ 65], CTW [ 66], and the Latin-Chinese subset of the 
MLT dataset. 3 The character set.Ctrain contains Tier-1 simplified Chinese characters, 
English letters(“A”-“z”), and digits (“0”-“9”). Samples with characters not covered 
by .Ctrain are excluded from the training set. The list of upstream datasets and the 
processing scripts are also released. 4
The testing dataset contains Japanese samples drawn from the MLT dataset. The 
character set.Ctest contains all.1460 characters that appear in this subset. 
The evaluation is performed under five setups using different splits over .Ctest . 
The first setup is closer to the generalized zero-shot learning tasks (GZSL) where all 
.1460 characters go to.Ck
test . The second split all novel characters to.Cu
test to match the 
open-set recognition (OSR) setup. The third introduces SOC to the second setup. The 
fourth split the Hiragana and Katakana into.Cu
test , imitating the GOSR setup. Finally, 
the fifth introduces SOC to.Cu
test of task four, implementing the full OSTR task. To 
simplify the task and focus on the open-set problem, we remove all vertical texts in 
the training and testing sets. The testing set contains a total of.4009 text lines. Note 
that because the character sets of Japanese and Chinese overlap, the conventional 
close-set methods do not yield zero accuracies. 
3.3 An OSTR Framework 
The OSTR framework (shown in Fig. 3.7) transcribes the contents in .img, where 
characters not in .C are predicted as “unknown”. It takes a sample image .img, an 
arbitrary character set .C and according Side-Information as input, and produce a 
prediction result prediction. Yˆ as output.
3 Crops with language annotated as Latin and Chinese. 
4 https://github.com/lancercat/OSOCR-data. 42 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
The framework can be formally defined as the following function.OSTR: 
.Yˆ = OSTR(img), (3.7) 
where the predicted string.Yˆ : (Yˆ [1], ...Yˆ [tm ]), Yˆ [t] ∈ C ∪ {[−]} is expected to match 
the ground truth, 
.Yˆ [1] =
{
Y∗
[l], if Y∗
[l] ∈ C
[−], otherwise. (3.8) 
Albeit the implementation of open-set text recognition can take various forms, 
we summarize the majority of OSTR methods into the open-set framework (shown 
in Fig. 3.7), which also covers close-set [ 56] and zero-shot [ 7, 8] text recogni￾tion methods as well. In all, the framework maps the elements on the image and 
the side-information onto representation space. F, and then performs matching. The 
framework consists of three main modules, the label-to-representation mapping mod￾ule . H, the visual-to-representation mapping module . S, and the open-set predictor 
module. C. 
3.3.1 Overall Design and Training Behaviors 
Specifically, the framework first defines a representation space. F, where the (proto￾types) and features are aligned on. 
The label-to-representation mapping module maps each element, mostly char￾acter, 
5 .e ∈ C to one or more corresponding representation prototypes, where each 
representation prototype .P resides on the representation space . F. Note that, one 
label can be mapped to more than one prototype, as one character may have more 
than one form, e.g., cases in Latin characters, contextual shapes in Arabic, simpli￾fied/traditional forms in Chinese and Japanese, etc. 
The sample-to-representation mapping module maps character in input sample 
image to the aligned representation space via the sample-to-representation mapping 
module, resulting character features.Fc. 
Finally, the open-set predictor module is used to compare each feature instance 
to prototypes, performing recognition, or rejection. The module may also involve an 
aggregation on the predicted labels to obtain final results [ 7, 67]. 
Compared to close-set text recognition methods, the OSTR framework can be 
regarded as a modification with an explicit label-to-representation mapping mod￾ule and the open-set predictor module, which implements the two extra capabili￾ties OSTR task demands. Specifically, the label-to-representation mapping module 
implements the capability to incrementally get the representations of unseen labels, 
while the rejection is implemented with the open-set predictor module.
5 We refer elements of various granularity as character to keep the concept less confusing. 3.3 An OSTR Framework 43
Fig. 3.8 Steps and typical actions of the framework to handle open environments 
During training, the supervision could be enforced on the final prediction or the 
sample-to-representation mapping if the “correct” representation is known [ 68, 69]. 
The training process may also need to involve a label sampler [ 6] to produce instances 
that fail to match. The sampler also helps to control training complexity yielded by 
the label-to-representation mapping module. 
3.3.2 Testing Behaviors 
The testing processes are shown in Fig. 3.8. 
Before putting it into production, one is advised to cache the prototypes for cur￾rently in-set characters (.Ck
test) beforehand, as illustrated in step 0 in Fig. 3.8. Since 
the weights of label-to-representation mapping module are generally frozen, upon 
addition, prototypes can be cached for each character. Thus, the computation cost of 
the label-to-representation mapping module is often negligible. 
Then the framework would be ready to recognize text utilizing the cached proto￾types and the remaining modules, as shown in Step 1. 
When a sample with an out-of-set instance 6 occurs like in Step 2, the model would 
produce “reject” labels for those novel characters that fail to match any characters in 
.Ck
test . 
Such samples will raise alerts to the system admins, who will manually go through 
these samples as illustrated in Step 3. The admin needs to pick out the unseen char￾6 In .Cu
test , could mostly be novel, or could be also explicitly swapped-out characters. Note . Cu
test
describes anything NOT in.Ck
test , hence is possibly unknown to the user..Ck
test models characters 
with side-information provided. 44 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework 
acters and decide whether they should be added for further recognition. Once the 
admin decides to do so, the person simply grabs the corresponding side-information, 
maps them to prototypes with the feature extractor and .Attn module, appends the 
generated prototypes to the cached prototypes, and the model is ready to recognize 
novel characters as shown in Step 4. 
However, the admin may find some characters rarely occur in the data, and decide 
to remove or swap them out to speed up the inference process, as shown in Step. 5. 
This is also the reason for the OSTR task to include training characters as part of 
.Cu 
test (SOC, excluded from .Ck 
test hence subject to rejection) in the full OSTR split. 
Finally, if the characters are swapped out instead of removed, rejection will trigger 
a cache miss first, which invokes the model to rematch the swapped-out prototypes 
for such classes. The admins will be notified if and only if the rematch fails. 
3.4 Modules and Variables 
This section focuses on providing a formal definition of the key modules and variables 
in the framework. For modules, we introduce their inputs and outputs, and for vari￾ables, we introduce their semantic and typical datatypes. Specific implementations 
will be introduced in the next chapters. 
3.4.1 Representation Space 
The representation space. F(Fig. 3.9) is a space where aligned representation proto￾type and character representation resides on. To be more specific, each element in 
the space represents a “einstance”. Note that all instances of one character can take 
multiple disjoint regions of the space. 
Elements of. F can take various forms, depending on specific designs. For example, 
it could simply be a feature space.Rd [ 8, 10, 18]. 
Fig. 3.9 Potential elements in the aligned representation space 3.4 Modules and Variables 45
Also, its elements could be a sequence of discrete artificially designed com￾ponents, i.e., .{0, 1}d×n, which could be either knowledge driven [ 19, 63] or data 
driven [ 69]. 
There are also methods jointing different representations, like [ 39, 40, 70, 71]. 
3.4.2 Label-to-Representation Mapping 
The label-to-representation mapping module .H maps each character . e to a set of 
representation prototypes, i.e., 
.Pe = H(e). (3.9) 
The module maps each character. e to its corresponding prototypes(class centers) 
. Pe. The framework models the mapping targets as sets, due to a character may have 
different visual forms [ 6]. Figure 3.10 illustrates the label-to-representation mapping. 
Generally, the module breaks down to a composition of the label-to-side￾information function. R and information-to-prototype function. E, 
.
Te = R(e),
T = ∪e∈CTe,
P[j] = E(T[j]),T[j] ∈ T .
(3.10) 
Upon the choice of side-information and the design of representation space, . R
and. E could take different forms, including identity functions. Specifically, in some 
radical-based methods [ 19],. R maps label to corresponding radical sequence,. E is an 
identity function, while in conventional close-set methods [ 56],. R are identity and. E
is an embedding function. 
Fig. 3.10 The label-to-representation mapping46 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework
3.4.3 Sample-to-Representation Mapping 
The function extract character representation.F from the input sample image.img, 
.F = B(img,C). (3.11) 
The module may wrap the open-set prediction module, which means the aggre￾gation sub-module may depend on the (partial) prediction results from the open-set 
prediction. Figure 3.11 illustrates general genres of implementations to map samples 
to the representation space. 
Specifically, some recurrent attention decoding-based methods [ 62, 72] depend 
on previous timestamp predictions to sample the next character from the feature 
map or perform aggregation based on dense prediction results [ 60, 67]. However, 
the wrapping does not always happen [ 3, 6, 58]. Single character recognition meth￾ods [ 18, 40] are generally simpler on the aggregation part, which essentially aligns 
features or predictions to timestamps. 
3.4.4 Open-set Predictor 
Figure 3.12 illustrates the open-set predictor module. The Open-set Predictor module 
.C matches character representation .F and representation prototype .P to produce 
prediction. Yˆ , 
.Yˆ [t] = C(F[t], P). (3.12) 
Specifically, the module matches each erepresentation .F[t] to prototypes in . P. 
Upon successful matches, the module predicts .Yˆ as the label associated with the 
matched prototype. For the instances that failed to match any prototype instance 
would be rejected and assigned with the unknown label, 
Fig. 3.11 General genres of implementations to map samples to the representation space3.5 Backward Compatibility 47
Fig. 3.12 The open-set predictor module 
.Yˆ [i] =
{
Y∗
[i], if Y∗
[i] ∈ C
[−], otherwise. (3.13) 
For close-set text recognition methods and zero-shot text recognition methods, 
the matching would always “succeed”. This trait makes spotting novel characters 
a manual process and can yield silent recognition errors. Hence, implementing a 
rejection mechanism is essential in open-world use cases. 
3.5 Backward Compatibility 
This section gives examples of typical methods and illustrates how they fit into the 
previously introduced framework. 
3.5.1 Part Based 
Part-based methods represent each character with a composition of parts, mostly 
radical. Depending on the choice of representation space, there are basically two 
types of part-based text recognition methods. 
One genre, represented by Zhang et. al. [ 63], uses the radical knowledge rep￾resentation as elements of representation space. For these methods, the Label-to￾Representation Mapping is simply an identity function, as no conversion is needed. 
The sample-to-representation mapping involves a recognition network that decodes 
the image to a radical sequence, splitting predicted radicals with a separator token. 
The Open-set Predictor is simply implemented with the nearest neighbor matching. 
The other genre, represented by Huang et. al. [ 7], uses feature vectors as elements 
of representation space. For these methods, the Label-to-Representation Mapping 
takes the form of an encoder converting the radical information of each character 
into a corresponding feature vector. Depending on the strategy of aggregation, the 
sample-to-representation mapping module could take the form of a dense feature48 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework 
extractor [ 60, 67], or timestamp aligned feature extractor [ 58, 62]. Open-set Predictor 
would be typically implemented as a simple linear classifier. 
3.5.2 Glyph Based 
Glyph-based methods [ 6, 8– 10] are generally similar to the second genre of Part￾based methods, despite replacing the side-information from radical to glyphs. Some 
methods of this type are seen to use multiple prototypes for one single label to handle 
different cases of characters. 
References 
1. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., 
Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, 
F.: Language is not all you need: Aligning perception with language models (2023). [Online]. 
Available: http://arxiv.org/abs/2302.14045 
2. Rust, P., Lotz, J.F., Bugliarello, E., Salesky, E., de Lhoneux, M., Elliott, D.: Language modelling 
with pixels. In: The Eleventh International Conference on Learning Representations, ICLR 
2023, Kigali, Rwanda, May 1–5, 2023 (2023) www.OpenReview.net 
3. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: Autonomous, bidirectional 
and iterative language modeling for scene text recognition. In: IEEE Conference on Com￾puter Vision and Pattern Recognition, CVPR 2021, virtual, June 19–25, 2021, pp. 7098–7107. 
Computer Vision Foundation/IEEE (2021) 
4. Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: A new scene 
text recognizer with visual language modeling network. In: 2021 IEEE/CVF International 
Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10–17, 2021, 
pp. 14 174–14 183. IEEE (2021) 
5. Garcia-Bordils, S., Mafla, A., Biten, A.F., Nuriel, O., Aberdam, A., Mazor, S., Litman, R., 
Karatzas, D.: Out-of-vocabulary challenge report. In: Computer Vision–ECCV,: Workshops– 
Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IV, ser. Lecture Notes in Computer 
Science, vol. 13804, pp. 359–375. Springer (2022) 
6. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
7. Huang, Y., Jin, L., Peng, D.: Zero-shot Chinese text recognition via matching class embed￾ding. In: 16th International Conference on Document Analysis and Recognition, ICDAR 2021, 
Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part III, ser. Lecture Notes in 
Computer Science, vol. 12823, pp. 127–141. Springer (2021) 
8. Zhang, C., Gupta, A., Zisserman, A.: Adaptive text recognition through visual matching. In: 
Computer Vision–ECCV 2020–16th European Conference, Glasgow, UK, August 23–28: Pro￾ceedings, Part XVI, ser. Lecture Notes in Computer Science, vol. 12361, pp. 51–67. Springer 
(2020) 
9. Souibgui, M.A., Fornés, A., Kessentini, Y., Megyesi, B.: Few shots is all you need: A progres￾sive few shot learning approach for low resource handwriting recognition (2021). [Online]. 
Available: https://arxiv.org/abs/2107.10064 
10. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022, pp. 4513–4522. IEEE (2022) References 49 
11. Du, Y., Wei, F., Zhang, Z., Shi, M., Gao, Y., Li, G.: Learning to prompt for open-vocabulary 
object detection with vision-language model. In: IEEE/CVF Conference on Computer Vision 
and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18–24, 2022, pp. 14 064– 
14 073. IEEE (2022) 
12. Ding, C., Pang, G., Shen, C.: Catching both gray and black swans: Open-set supervised anomaly 
detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 
2022, New Orleans, LA, USA, June 18–24, 2022, pp. 7378–7388. IEEE (2022) 
13. Ao, X., Zhang, X., Yang, H., Yin, F., Liu, C.: Cross-modal prototype learning for zero-shot 
handwriting recognition. In: 2019 International Conference on Document Analysis and Recog￾nition, ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 589–594. IEEE (2019) 
14. Aberdam, A., Bensaid, D., Golts, A., Ganz, R., Nuriel, O., Tichauer, R., Mazor, S., Litman, 
R.: Clipter: Looking at the bigger picture in scene text recognition. In: Proceedings of the 
IEEE/CVF International Conference on Computer Vision (ICCV), October, pp. 21 706–21 717 
(2023) 
15. Li, H., Wang, P., Shen, C.: Towards end-to-end text spotting with convolutional recurrent neural 
networks. In: IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, 
October 22–29, 2017. IEEE Computer Society, pp. 5248–5256 (2017) 
16. Yang, C., Liu, C., Fang, Z.-Y., Han, Z., Liu, C.-L., Yin, X.-C.: Open set text recognition 
technology. J. Image Graph. 28, 1767–1791 (2023) 
17. Geng, C., Huang, S., Chen, S.: Recent advances in open set recognition: A survey. IEEE Trans. 
Pattern Anal. Mach. Intell. 43(10), 3614–3631 (2021) 
18. Cao, Z., Lu, J., Cui, S., Zhang, C.: Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recognit. 107, 107488 (2020) 
19. Wang, T., Xie, Z., Li, Z., Jin, L., Chen, X.: Radical aggregation network for few-shot offline 
handwritten Chinese character recognition. Pattern Recognit. Lett. 125, 821–827 (2019) 
20. Scheirer, W.J., de Rezende Rocha, A., Sapkota, A., Boult, T.E.: Toward open set recognition. 
IEEE Trans. Pattern Anal. Mach. Intell. 35(7), 1757–1772 (2013) 
21. Pourpanah, F., Abdar, M., Luo, Y., Zhou, X., Wang, R., Lim, C.P., Wang, X., Wu, Q.M.J.: 
A review of generalized zero-shot learning methods. IEEE Trans. Pattern Anal. Mach. Intell. 
45(4), 4051–4070 (2023) 
22. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S.K., Bagdanov, A.D., Iwamura, M., 
Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S., Valveny, E.: 
ICDAR 2015 competition on robust reading. In: 13th International Conference on Document 
Analysis and Recognition, ICDAR 2015, Nancy, France, August 23–26, 2015, pp. 1156–1160. 
IEEE Computer Society (2015) 
23. Chen, J., Li, B., Xue, X.: Scene text telescope: Text-focused scene image super-resolution. 
In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 
19–25, 2021, pp. 12 026–12 035. Computer Vision Foundation/IEEE (2021) 
24. Zhao, L., Wu, Z., Wu, X., Wilsbacher, G., Wang, S.: Background-insensitive scene text recog￾nition with text semantic segmentation. In: Computer Vision–ECCV 2022–17th European 
Conference, Tel Aviv, Israel, October 23–27: Proceedings, Part XXV, ser. Lecture Notes in 
Computer Science, vol. 13685, pp. 163–182. Springer (2022) 
25. Amin, J., Siddiqi, I., Moetesum, M.: Reconstruction of broken writing strokes in greek papyri. 
In: International Conference on Document Analysis and Recognition, pp. 253–266. Springer 
(2023) 
26. Phillips, I.T.: Methodologies for using uw databases for ocr and image-understanding systems. 
In: Document Recognition V, vol. 3305, pp. 112–127. SPIE (1998) 
27. Huang, B., Lin, J., Liu, J., Chen, J., Zhang, J., Hu, Y., Chen, E., Yan, J.: Separating Chinese 
character from noisy background using GAN. Wireless Commun. Mobile Comput. 2021, 1–13 
(2021) 
28. Liu, C., Yin, F., Wang, D., Wang, Q.: CASIA online and offline Chinese handwriting databases. 
In: 2011 International Conference on Document Analysis and Recognition, ICDAR 2011, 
Beijing, China, September 18–21, 2011, pp. 37–41. IEEE Computer Society (2011) 50 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework 
29. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with perspective distortion 
in natural scenes. In: IEEE International Conference on Computer Vision, ICCV 2013, Sydney, 
Australia, December 1–8, 2013, pp. 569–576. IEEE Computer Society (2013) 
30. Huang, S., Wang, H., Liu, Y., Shi, X., Jin, L.: OBC306: A large-scale oracle bone character 
recognition dataset. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 681–688. IEEE (2019) 
31. Elsaid, A., Mohammed, A., Ibrahim, L.F., Sakre, M.M.: A comprehensive review of arabic text 
summarization. IEEE Access 10, 38 012–38 030 (2022) 
32. Nederhof, M.-J., Berti, M.: Ocr of handwritten transcriptions of ancient egyptian hieroglyphic 
text, Altertumswissenschaften in a Digital Age: Egyptology. Papyrology and beyond, Leipzig 
(2015) 
33. Kordon, F., Weichselbaumer, N., Herz, R., Mossman, S., Potten, E., Seuret, M., Mayr, 
M., Christlein, V.: Classification of incunable glyphs and out-of-distribution detection with 
joint energy-based models. In: International Journal on Document Analysis and Recognition 
(IJDAR), pp. 1–18 (2023) 
34. Liu, C., Yang, C., Yin, X.: Open-set text recognition via shape-awareness visual reconstruction, 
in Document Analysis and Recognition–ICDAR 2023–17th International Conference, San 
José, CA, USA, August 21–26: Proceedings, Part VI, ser. Lecture Notes in Computer Science, 
vol. 14192, pp. 89–105. Springer (2023) 
35. Fei, G., Liu, B.: Breaking the closed world assumption in text classification. In: Proceedings 
of the 2016 Conference of the North American Chapter of the Association for Computational 
Linguistics: Human Language Technologies, pp. 506–514. Association for Computational Lin￾guistics (2016) 
36. Zhang, H., Xu, H., Lin, T.: Deep open intent classification with adaptive decision boundary. In: 
Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference 
on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on 
Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2–9, 2021, 
pp. 14 374–14 382. AAAI Press (2021) 
37. Han, J., Ren, Y., Ding, J., Pan, X., Yan, K., Xia, G.: Expanding low-density latent regions 
for open-set object detection. In: IEEE/CVF Conference on Computer Vision and Pattern 
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 9581–9590. IEEE 
(2022) 
38. Acsintoae, A., Florescu, A., Georgescu, M., Mare, T., Sumedrea, P., Ionescu, R.T., Khan, 
F.S., Shah, M.: Ubnormal: New benchmark for supervised open-set video anomaly detection. 
In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022, pp. 20 111–20 121. IEEE (2022) 
39. Zu, X., Yu, H., Li, B., Xue, X.: Chinese character recognition with augmented character pro￾file matching. In: MM ’22: The 30th ACM International Conference on Multimedia, Lisboa, 
Portugal, October 10–14, 2022, pp. 6094–6102. ACM (2022) 
40. Chen, J., Li, B., Xue, X.: Zero-shot Chinese character recognition with stroke-level decompo￾sition. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 
IJCAI 2021, Virtual Event / Montreal, Canada, 19–27 August 2021, pp. 615–621 (2021). www. 
ijcai.org 
41. Pourpanah, F., Abdar, M., Luo, Y., Zhou, X., Wang, R., Lim, C.P., Wang, X., Wu, Q.M.J.: 
A review of generalized zero-shot learning methods. IEEE Trans. Pattern Anal. Mach. Intell. 
45(4), 4051–4070 (2023) 
42. Wan, Z., Zhang, J., Zhang, L., Luo, J., Yao, C.: On vocabulary reliance in scene text recognition. 
In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, 
Seattle, WA, USA, June 13-19, 2020, pp. 11 422–11 431. IEEE (2020) 
43. Wang, T., Huang, J., Zhang, H., Sun, Q.: Visual commonsense R-CNN. In: 2020 IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, 
June 13–19, 2020, pp. 10 757–10 767. IEEE (2020) 
44. Yue, Z., Zhang, H., Sun, Q., Hua, X.: Interventional few-shot learning. In: Advances in Neural 
Information Processing Systems 33: Annual Conference on Neural Information Processing 
Systems 2020, NeurIPS 2020, December 6–12, 2020, virtual (2020) References 51 
45. Su, Y., Sun, R., Lin, G., Wu, Q.: Context decoupling augmentation for weakly supervised 
semantic segmentation. In: 2021 IEEE/CVF International Conference on Computer Vision, 
ICCV 2021, Montreal, QC, Canada, October 10–17, 2021, pp. 6984–6994. IEEE (2021) 
46. Liu, R., Liu, H., Li, G., Hou, H., Yu, T., Yang, T.: Contextual debiasing for visual recognition 
with causal mechanisms. In: IEEE/CVF Conference on Computer Vision and Pattern Recog￾nition, CVPR 2022, New Orleans, LA, USA, June 18–24, 2022, pp. 12 745–12 755. IEEE 
(2022) 
47. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and artificial neu￾ral networks for natural scene text recognition. In: NIPS Deep Learning Workshop. Neural 
Information Processing Systems (2014) 
48. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images. In: 
2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, 
NV, USA, June 27–30, 2016, pp. 2315–2324. IEEE Computer Society (2016) 
49. Mishra, A., Alahari, K., Jawahar, C.V.: Scene text recognition using higher order language 
priors. In: British Machine Vision Conference, BMVC,: Surrey, UK, September 3–7, 2012. 
BMVA Press 2012, 1–11 (2012) 
50. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary text detection 
system for natural scene images. Expert Syst. Appl. 41(18), 8027–8048 (2014) 
51. Wang, K., Babenko, B., Belongie, S.J.: End-to-end scene text recognition. In: IEEE Interna￾tional Conference on Computer Vision, ICCV 2011, Barcelona, Spain, November 6–13, 2011, 
pp. 1457–1464. IEEE Computer Society (2011) 
52. Lucas, S.M., Panaretos, A., Sosa, L., Tang, A., Wong, S., Young, R., Ashida, K., Nagai, H., 
Okamoto, M., Yamamoto, H., Miyao, H., Zhu, J., Ou, W., Wolf, C., Jolion, J., Todoran, L., 
Worring, M., Lin, X.: ICDAR 2003 robust reading competitions: entries, results, and future 
directions. Int. J. Document Anal. Recognit. 7(2–3), 105–122 (2005) 
53. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R., Mas, J., 
Mota, D.F., Almazán, J., de las Heras, L.: ICDAR 2013 robust reading competition. In: 12th 
International Conference on Document Analysis and Recognition, ICDAR 2013, Washington, 
DC, USA, August 25–28, 2013, pp. 1484–1493. IEEE Computer Society (2013) 
54. Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.J.: Coco-text: Dataset and benchmark 
for text detection and recognition in natural images (2016). [Online]. Available: http://arxiv. 
org/abs/1601.07140 
55. Zhang, Y., Gueguen, L., Zharkov, I., Zhang, P., Seifert, K., Kadlec, B.: Uber-text: A large￾scale dataset for optical character recognition from street-level imagery. In: SUNw: Scene 
Understanding Workshop-CVPR, vol. 2017, , p. 5 (2017) 
56. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 
27–November 2, 2019, pp. 4714–4722. IEEE (2019) 
57. Chen, J., Yu, H., Ma, J., Guan, M., Xu, X., Wang, X., Qu, S., Li, B., Xue, X.: Benchmark￾ing Chinese text recognition: Datasets, baselines, and an empirical study (2021). [Online]. 
Available: https://arxiv.org/abs/2112.15093 
58. Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.: Decoupled attention 
network for text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, New York, NY, USA, February 7–12, 2020, pp. 12 216–12 224. AAAI Press 
(2020) 
59. Chng, C.K., Ding, E., Liu, J., Karatzas, D., Chan, C.S., Jin, L., Liu, Y., Sun, Y., Ng, C.C., Luo, 
C., Ni, Z., Fang, C., Zhang, S., Han, J.: ICDAR2019 robust reading challenge on arbitrary￾shaped text–rrc-art. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 1571–1576. IEEE (2019) 
60. Liao, M., Zhang, J., Wan, Z., Xie, F., Liang, J., Lyu, P., Yao, C., Bai, X.: Scene text recogni￾tion from two-dimensional perspective. In: The Thirty-Third AAAI Conference on Artificial 52 3 Open-Set Text Recognition: Concept, Dataset, Protocol, and Framework 
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence 
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial 
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27–February 1, 2019, pp. 8714– 
8721. AAAI Press (2019) 
61. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: SEED: semantics enhanced encoder-decoder 
framework for scene text recognition. In: 2020 IEEE/CVF Conference on Computer Vision and 
Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020, pp. 13 525–13 534. 
IEEE (2020) 
62. Li, H., Wang, P., Shen, C., Zhang, G.: Show, attend and read: A simple and strong baseline for 
irregular text recognition. In: The Thirty-Third AAAI Conference on Artificial Intelligence, 
AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2019, Honolulu, Hawaii, USA, January 27–February 1, 2019, pp. 8610–8617. AAAI 
Press (2019) 
63. Zhang, J., Du, J., Dai, L.: Radical analysis network for learning hierarchies of Chinese char￾acters. Pattern Recognit. 103, 107305 (2020) 
64. Shi, B., Yao, C., Liao, M., Yang, M., Xu, P., Cui, L., Belongie, S.J., Lu, S., Bai, X.: ICDAR2017 
competition on reading Chinese text in the wild (RCTW-17). In: 14th IAPR International 
Conference on Document Analysis and Recognition, ICDAR 2017, Kyoto, Japan, November 
9–15, 2017, pp. 1429–1434. IEEE (2017) 
65. Sun, Y., Karatzas, D., Chan, C.S., Jin, L., Ni, Z., Chng, C.K., Liu, Y., Luo, C., Ng, C.C., 
Han, J., Ding, E., Liu, J.: ICDAR 2019 competition on large-scale street view text with partial 
labeling–RRC-LSVT. In: 2019 International Conference on Document Analysis and Recogni￾tion, ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 1557–1562. IEEE (2019) 
66. Yuan, T., Zhu, Z., Xu, K., Li, C., Mu, T., Hu, S.: A large Chinese text dataset in the wild. J. 
Comput. Sci. Technol. 34(3), 509–521 (2019) 
67. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence 
recognition and its application to scene text recognition. IEEE Trans. Pattern Anal. Mach. 
Intell. 39(11), 2298–2304 (2017) 
68. Wang, W., Zhang, J., Du, J., Wang, Z., Zhu, Y.: DenseRAN for offline handwritten Chinese 
character recognition. In: 16th International Conference on Frontiers in Handwriting Recogni￾tion, ICFHR 2018, Niagara Falls, NY, USA, August 5–8, 2018, pp. 104–109. IEEE Computer 
Society (2018) 
69. Li, B., Tang, X., Qi, X., Chen, Y., Xiao, R.: Hamming OCR: A locality sensitive hashing 
neural network for scene text recognition (2020). [Online]. Available: https://arxiv.org/abs/ 
2009.10874 
70. He, S., Schomaker, L.: Open set Chinese character recognition using multi-typed attributes 
(2018). [Online]. Available: http://arxiv.org/abs/1808.08993 
71. Wang, P., Da, C., Yao, C.: Multi-granularity prediction for scene text recognition. In: European 
Conference on Computer Vision, pp. 339–355. Springer (2022) 
72. Sheng, F., Chen, Z., Xu, B.: NRTR: A no-recurrence sequence-to-sequence model for scene 
text recognition. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 781–786. IEEE (2019) Chapter 4 
Open-Set Text Recognition 
Implementations(I): 
Label-to-Representation Mapping 
Abstract This chapter describes the possible approaches to implement the 
representation-related variable and module in the framework discussed above, i.e., 
the representation space and the label-to-representation mapping module. First, this 
chapter introduces how characters, or other corresponding granularities, are repre￾sented in different methods, i.e., the representation space, where class centers (pro￾totypes) and features extracted from input images reside. Second, we discuss choices 
of human representation of labels (side-information) and different approaches in the 
literature to implement the label-to-representation mapping module. The module, 
which maps the side information to prototypes residing in the representation space, 
is the key to implementing class incremental learning functionality. 
Keywords Representation space · Label-to-Representation mapping · Prototypes · Side-information 
This chapter describes the possible approaches to implementing the representation￾related variable and module in the framework discussed in Sect. 3.2.3.3, i.e., the 
representation space and the label-to-representation mapping module (Fig. 4.1). 
First, this chapter introduces how characters, or other corresponding granularities, 
are represented in different methods, i.e., the representation space, where class centers 
(prototypes) and features extracted from input images reside. 
Second, we discuss choices of human representation of labels (side-information) 
and different approaches in the literature to implement the label-to-representation 
mapping module. The module, which maps the side-information to prototypes resid￾ing in the representation space, is the key to implementing class-incremental learning 
functionality. 
4.1 Representation Space 
This section discusses how elements (usually characters, could be words, grams, or 
parts) in a word are described, as in word feature, character feature sequence, char￾acter sparse code sequence, and radical tree sequence (Fig. 4.2). In other words, we
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_4
5354 4 Open-Set Text Recognition Implementations(I): Label-to-Representation Mapping
Fig. 4.1 This section focuses on the aligned representation space 
Fig. 4.2 Common design of elements in the representation space 
discuss the choices of elements and their representation in open-set text recognition. 
Note mapping class labels or samples into the representation space is out of the scope 
of this section and will be discussed in later sections.
4.1.1 Gram-based Prototypes 
Gram-based prototypes are seen mostly in early-day methods and are now less com￾mon due to flexibility or performance issues. We briefly introduce them in this book 
as some backgrounds. 
4.1.1.1 Whole Word 
Some early-age OCR methods models conduct whole word-level representation, 
which means the input samples are matched to word representations directly. Goel 
et al. [ 1– 3]. Specifically, each word in the dictionary is associated with a dedicated4.1 Representation Space 55
prototype that resides in the representation space. Implementation-wise, the proto￾types are implemented as corresponding rows in a feature vector. The core issue of 
this representation is the model cannot recognize any out-of-dictionary words, let 
alone new characters. Also, each word in the dictionary will need a decent number 
of training instances, which is less data efficient compared to character-based meth￾ods. These properties limit their feasibility in many real-life applications, hence few 
recent methods have adopted this approach. 
4.1.1.2 Part Histogram 
To break away from the fixed dictionaries, part histogram representation [ 4] was 
introduced in 2014, where words are represented as the histogram of parts. This 
category is frequently seen for a short while, later replaced by individual character 
representations [ 5, 6]. However, the ideology of part histogram also passes down 
to the ACELoss [ 7]. The histogram methods also pass to zero-shot recognition by 
counting stroke parts and characters, in whole words or different segments [ 8– 10] 
to achieve mainly OOV recognition. The methods would theoretically have some 
extent of capability to be applied to novel characters but are rarely seen in practice. 
4.1.2 Character-Based Prototypes 
In modern days, the majority of OCR methods utilize character-level classification. 
Generally, these methods map labels to associated phototypes (class centers), usually 
implemented as feature vectors. All character instances will be mapped to the same 
representation space, and classification is implemented by comparing the instance 
features and prototypes. 
4.1.2.1 Single-Centered Embedding 
The most common approaches are single centered, where each center or instance 
is modeled as one latent feature vector (Fig. 4.3). One major example would be 
the majority of close-set text recognition methods [ 11]. Mostly, each class center 
is modeled to one row of the linear classifier, and each character in the input word 
samples is modeled as features (timestamp aligned [ 11] or not [ 5]). The feature 
models a mixture of character visual information and contextual information [ 12]. 
Methods with RNNs [ 6, 11, 13] tend to include more contextual information in the 
representation, while RNN-free tends to model less [ 14]. 
For close-set text recognition tasks, contextual information can get explicitly 
modeled [ 15] and enhanced with a dedicated semantic feature vector probably trained 
on an external corpus [ 16]. Since each character instance (either a class center or a 
character feature from the input sample) is still represented with one point on the56 4 Open-Set Text Recognition Implementations(I): Label-to-Representation Mapping
Fig. 4.3 Feature vector(s) based characters representation, where a character is represented with a 
feature vector. Some methods may allow a class to have more than one center 
joint space of the “visual 1” and semantic space, we still consider these methods to 
be single centered. 
The aforementioned close-set text recognition methods model class centers as 
latent weights, which makes generating such centers without retraining the model 
exceptionally difficult for novel classes. To resolve this problem and get the represen￾tation for unknown characters, zero-shot and open-set text methods use an encoder to 
map side-information [ 17– 21] to this space, which will be later discussed in Sect. 4.2 
in details. 
4.1.2.2 Multi-Centered Embedding 
One dilemma of low-shot text recognition is the drastic visual shape difference 
between different forms of a character (illustrated in Fig. 4.4), due to style or different 
“cases”. Although some languages use different Unicode characters for each form, 
the differences are not usually honored during the annotation process. For close-set 
applications, aligning different forms to the same center does not yield significant 
problems. This is because both mappings, specifically label-to-representation map￾ping and visual-to-representation mapping, do not need to be generalized to unseen 
characters. However, in open-set scenarios where mappings need to generalize, both 
mappings are preferably distance-preserving, which means characters with close
Fig. 4.4 Characters with various forms
1 Might have some semantic mixed in as well without explicit isolation. 4.1 Representation Space 57
shapes stay close on the representation space, and vice versa. However, forcing align￾ment on different cases with different shapes would explicitly break such property, 
hence unfeasible.
To alleviate this problem, some decide to use radical sequences instead of 
images [ 22, 23], where each case of a character corresponds to a prototype, effec￾tively making the decision boundary of a class includes one or more disjoint regions, 
depending on the number of forms the character possesses. For such methods, similar￾ity to a certain class is obtained by max reducing the similarity of all of its associated 
prototypes. 
4.1.3 Part-Based Representation 
Instead of modeling characters holistically, a lot of methods model each character 
as an explicit composition of parts (Fig. 4.5). Prediction results come from directly 
matching compositions. The representation could be knowledge driven or data driven. 
Different from part-histograms from the previous section, the parts of this category 
are composed of single characters. Alike methods in generic low-shot learning date 
back to 10 years ago [ 26]. 
4.1.3.1 Knowledge Driven 
One trivial choice would be to represent each character with a human-defined knowl￾edge. Specifically, each character is represented with a set of or an ordered sequence 
of attributes designed by experts. 
The most known design would be the Radical Sequences [ 27– 30], which is mostly 
seen in zero-shot Chinese text recognition methods, the majority represents each 
character with a traverse of its radical tree. Radical representations are also seen to
Fig. 4.5 Various component-based representations. Note such representation is not limited to the 
Chinese language. Some examples are a Korean [ 24], b Japanese [ 25] c Bangladish [ 9]58 4 Open-Set Text Recognition Implementations(I): Label-to-Representation Mapping
represent Japanese [ 31] characters in other tasks. Besides the traverse representation, 
a bag of radicals [ 32] can theoretically be used as well, however, a side representation 
would be needed to resolve coding conflicts.
Stroke sequence [ 24, 33– 35] is also a popular representation. The representa￾tion sometimes yields coding conflicts. To alleviate this problem, some methods are 
seen to be jointly used stroke sequence with the whole word representation. More 
abstracted codings like different IME codes [ 36] are also sometimes seen as a rep￾resentation. 
Beyond CJK languages, despite knowledge driven whole word representations 
are seen in Bengali [ 9] and Latin [ 8, 10], character-level representations are not 
common, perhaps due to the high coding conflict rate, less demands, or lack of 
domain knowledge for some languages. 
4.1.3.2 Data Driven 
Hamming representation [ 37] breaks free from the reliance on domain knowledge, 
featuring a full data-driven character decomposition. Also given the data-driven trait, 
the methods can be applied to multiple languages, specifically, the model is known 
to work with a mixture of Chinese language and English language. 
4.1.4 Representation Fusing 
Also, one can fuse different representations to alleviate coding conflicts, or just to 
improve model performance. For example, Chen et. al. use image-oriented feature 
vectors as a de-conflict representation to stroke-based representation [ 24], this strat￾egy also passes down to their following work [ 33]. 
Fusing, and aligning multiple representations are also proven to enhance perfor￾mance [ 33, 36, 38]. However, fusing representation has its downsides, mainly due to 
the increasing annotation cost of the training data and increase in annotation cost. It 
is also unknown whether or not introducing language reliance representation hinders 
cross-language transferring capabilities. 
4.2 Label-to-Representation Mapping 
This chapter describes how labels are mapped from the human side to the model 
side. We discuss choices on different types of side-information and how the chosen 
human side-information is mapped to the model side, i.e., the prototypes on the 
representation space (Fig. 4.6).4.2 Label-to-Representation Mapping 59
Fig. 4.6 This section focuses on the Label-to-Representation Mapping 
4.2.1 Side-Information 
This section discusses side-information, which bridges from labels to prototypes. 
Specifically, side-information allows humans to provide discriminative descriptions 
to each label, with the potential to generalize to unseen classes. Typical side￾information choices can be categorized into compositional and holistic ones depend￾ing on whether or not compositional knowledge is exploited, some methods may 
also choose to fuse different side-information forms to improve performance, or just 
to remove representation conflicts. 
4.2.1.1 Compositional 
Compositional information is one most common side-information in low-shot learn￾ing. Most composition descriptions include attributes (parts), sometimes also the 
structural relation of different attributes [ 24, 27]. Side-Informationin this category 
usually can be directly used as elements in aligned represented space [ 24, 27, 39], 
or sometimes encoded into feature vectors [ 20]. As the form of such representation 
has been introduced in great detail in Sect. 4.1.3.1, we keep this part brief. 
Radicals are the most common side-information used for zero-shot Chinese text 
recognition[ 20, 21, 27, 39]. Radical used as side-information usually takes the form 
of embedding of a radical tree [ 20, 21], or simply the bag of radicals [ 32]. Like￾wise strokes are another commonly used traits [ 24, 33, 40, 41], which are also 
seen to work with Korean characters. Strokes are more reused in tail classes and 
head classes, and hence may show some generalization advantages. However, it 
also faces representation conflicts and hence needs auxiliary representations like 
glyph information to resolve such conflicts [ 24, 33]. Structure bonded-IME coding, 
which are different variant of radical representation, is less common but also seen in 
practice [ 36].60 4 Open-Set Text Recognition Implementations(I): Label-to-Representation Mapping
Noteworthy, the aforementioned representations demonstrate a heavy reliance 
on language-specific knowledge and hence can face difficulty if the task demands 
cross-language recognition capability. 
4.2.1.2 Holistic 
Holistic information refers to descriptions where each label is described with a non￾structural description. Unlike the compositional category, this representation is rarely 
seen used directly in representation space. 
In text recognition, glyph description is one most common holistic information, 
which dates back to template matching [ 42]. These methods only need one sample 
extracted from a font [ 17, 22, 23] or from real samples [ 18], hence generally do not 
demand much knowledge of labels. However, the sample can introduce irrelevant 
information, like style, background, size, and other information that is not related to 
classification. 
However rare in text recognition, generic object recognition tasks (instance seg￾mentation/ object detection) sometimes use natural language descriptions, which 
come from Wikipedia [ 43], WordNet [ 44], or other sources. These approaches are 
currently usually not used in text recognition, due to a general lack of effective char￾acter shapes/ descriptions. However, with the emergence of emojis, which come with 
ALT descriptions, we may see this approach in future OCR methods. 
4.2.1.3 Combined 
Recent like Stroke [ 24], ACPM [ 33], STAR [ 40], He et. al [ 36], and REZCR [ 41], 
choose to utilize multiple representations to either avoid conflicts or provide more 
information about each class for improvement. However, the more information used, 
the more cost or domain knowledge would be needed to annotate a novel or see a 
label. 
4.2.2 Common Mapping Patterns 
As we have already mentioned, some composition-based methods [29, 30, 39] simply 
use side-information as elements for both side-information and the representation 
space, hence the mapping would be a simple identity function. Other methods chose 
different representations on the human side and the model side, hence would need a 
conversion. The most common choices are encoding the side-information of choice 
into feature vectors that reside on representation space, which generally involves an 
encoding process. 
Most mapping processes would map each label to a single center on the represen￾tation space [ 11, 29], some methods [ 22, 23] also implement a multi-center scheme4.2 Label-to-Representation Mapping 61
to tackle intra-class diversity. This section describes common implementations of 
mapping processes used to encode side-information, though the implementations 
are diverse, they can be summarized into the following four categories. Although 
the mapping heavily depends on the choice of side-information, still the same side￾information can be modeled in different forms and consequentially yields encoding 
methods in different categories. 
It is possible, however yet to be seen, to force convert holistic side-information to 
compositional representation, which in turn yields a decoder. The design may help 
generalize radical to other languages not originally supported. 
4.2.2.1 Image-Based Side-info 
Image-based side-information is generally mapped to feature vectors via convolution 
networks [ 17, 18, 22, 23]. The approach is also common in (cross-domain) few-shot 
learning tasks [ 45]. Depending on the domain gap between the images that serve as 
side-information and actual samples, the encoder may or may not share parameters 
with the sample feature encoder. For example, Zhang et. al [ 17] and Souibgui et. 
al [ 18] adopt Siamese networks [ 46] which share all parameters with the backbone, 
as their side-information forms a glyph-line image close to samples. Liu et. al., 
however, use characters glyphs from Noto fonts, hence opt for dedicated encoders 
sharing no parameters [ 22] or convolution kernels only [ 23] to tackle the domain 
bias between single Noto glyphs and scene text word images. 
Worth mentioning, that this type of encoder is also used to encode glyphs in stroke￾based methods [ 24, 34, 35] where glyphs are used as secondary side-information to 
alleviate coding conflicts. 
4.2.2.2 Sequence-Based Side-Info 
Though rare in text recognition, sequence-based side-information encoding is worth 
mentioning due to its wide application in the generic open-set learning community. 
These types of side-information modeling especially after the emergence of MLLMs 
like CLIP [ 47], which provides visual-aligned text embedding trained on large-scale 
datasets 2. 
In essence, these methods tend to encode the attribute sequence (usually descrip￾tion in natural language) into class centers using a sequence encoder. For natural 
languages, pretrained language models [ 47– 49] could be used [ 44, 50].
2 Note these methods also raise fairness concerns, for they use much more data during pretraining, 
which may or may not include the test images. 62 4 Open-Set Text Recognition Implementations(I): Label-to-Representation Mapping
4.2.2.3 Histogram-Based Side-Info 
Some side-information is treated as a frequency histogram [ 32, 43]. For this category 
of side-information modeling, Liu et. al. [ 32] propose to use a basic bag of a word 
as the embedding. Specifically, each character is represented with binary vectors, 
where each element marks the presence or absence of a corresponding radical in the 
characters. Huang et. al. [ 51] uses a position-wise histogram to count the occurrence 
number for each radical at each position and supervise with ACE loss [ 7]. ZEST [ 43], 
though not a text recognition method, uses TF-IDF [ 52] to further take the relative 
frequency into account. 
4.2.2.4 Hierarchical-Based Side-Info 
Some side-information, for example, radical representation, contains hierarchical 
information. While some methods [ 32] simply strip the composition part. reducing 
to the basic histogram scheme. Others choose to keep the structural information to 
the encoder. HDE [ 20] for example, proposed to encode the depth and path leading 
to each node with an “impact value”, then perform a weighted frequency counting 
of all nodes in the tree. Though encoding the traverse of the tree is discouraged 
in HDE [ 20], the option remains to be explored by the maturing of long-sequence 
modeling methods in language models [ 48, 53]. 
References 
1. Goel, V., Mishra, A., Alahari, K., Jawahar, C.V.: Whole is greater than sum of parts: Recognizing 
scene text words. In: 12th International Conference on Document Analysis and Recognition. 
ICDAR 2013, August 25–28, pp. 398–402. IEEE Computer Society, Washington, DC, USA 
(2013) 
2. Manmatha, R., Han, C., Riseman, E.M.: Word spotting: A new approach to indexing hand￾writing. In: 1996 Conference on Computer Vision and Pattern Recognition (CVPR ’96). June 
18–20, 1996, pp. 631–637. IEEE Computer Society, San Francisco, CA, USA (1996) 
3. Wang, K., Belongie, S.J.: Word spotting in the wild. In: Computer Vision–ECCV 2010, 11th 
European Conference on Computer Vision, Heraklion, Crete, Greece, September 5–11, 2010, 
Proceedings, Part I, ser. Lecture Notes in Computer Science, vol. 6311, pp. 591–604. Springer 
(2010) 
4. Almazán, J., Gordo, A., Fornés, A., Valveny, E.: Word spotting and recognition with embedded 
attributes. IEEE Trans. Pattern Anal. Mach. Intell. 36(12), 2552–2566 (2014) 
5. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence 
recognition and its application to scene text recognition. IEEE Trans. Pattern Anal. Mach. 
Intell. 39(11), 2298–2304 (2017) 
6. Li, H., Wang, P., Shen, C., Zhang, G.: Show, attend and read: A simple and strong baseline for 
irregular text recognition. In: The Thirty-Third AAAI Conference on Artificial Intelligence, 
AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2019, Honolulu, Hawaii, USA, January 27–February 1, 2019, pp. 8610–8617. AAAI 
Press (2019)References 63
7. Xie, Z., Huang, Y., Zhu, Y., Jin, L., Liu, Y., Xie, L.: Aggregation cross-entropy for sequence 
recognition. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, 
Long Beach, CA, USA, June 16–20, 2019, pp. 6538–6547. Computer Vision Foundation/IEEE 
(2019) 
8. Chanda, S., Baas, J., Haitink, D., Hamel, S., Stutzmann, D., Schomaker, L.: Zero-shot learning 
based approach for medieval word recognition using deep-learned features. In: 16th Interna￾tional Conference on Frontiers in Handwriting Recognition, ICFHR 2018, Niagara Falls, NY, 
USA, August 5–8, 2018, pp. 345–350. IEEE Computer Society (2018) 
9. Chanda, S., Haitink, D., Prasad, P.K., Baas, J., Pal, U., Schomaker, L.: Recognizing Bengali 
word images–A zero-shot learning perspective. In: 25th International Conference on Pattern 
Recognition, ICPR 2020, Virtual Event/Milan, Italy, January 10–15, 2021, pp. 5603–5610. 
IEEE (2020) 
10. Rai, A., Krishnan, N.C., Chanda, S.: Pho(sc)net: An approach towards zero-shot word image 
recognition in historical documents. In: 16th International Conference on Document Analysis 
and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5–10, 2021, Proceedings, 
Part I, ser. Lecture Notes in Computer Science, vol. 12821, pp. 19–33. Springer (2021) 
11. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 
27–November 2, 2019, pp. 4714–4722. IEEE (2019) 
12. Wan, Z., Zhang, J., Zhang, L., Luo, J., Yao, C.: On vocabulary reliance in scene text recognition. 
In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, 
Seattle, WA, USA, June 13–19, 2020, pp. 11 422–11 431. IEEE (2020) 
13. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: ASTER: an attentional scene text 
recognizer with flexible rectification. IEEE Trans. Pattern Anal. Mach. Intell. 41(9), 2035– 
2048 (2019) 
14. Liao, M., Zhang, J., Wan, Z., Xie, F., Liang, J., Lyu, P., Yao, C., Bai, X.: Scene text recogni￾tion from two-dimensional perspective. In: The Thirty-Third AAAI Conference on Artificial 
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence 
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial 
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27–February 1, 2019, pp. 8714– 
8721. AAAI Press (2019) 
15. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate scene text 
recognition with semantic reasoning networks. In: 2020 IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020, pp. 12 110– 
12 119. IEEE (2020) 
16. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: Autonomous, bidirectional 
and iterative language modeling for scene text recognition. In: IEEE Conference on Com￾puter Vision and Pattern Recognition, CVPR 2021, virtual, June 19–25, 2021, pp. 7098–7107. 
Computer Vision Foundation/IEEE (2021) 
17. Zhang, C., Gupta, A., Zisserman, A.: Adaptive text recognition through visual matching. In: 
Computer Vision–ECCV 2020–16th European Conference, Glasgow, UK, August 23–28, Pro￾ceedings, Part XVI, ser. Lecture Notes in Computer Science, vol. 12361, pp. 51–67. Springer 
(2020) 
18. Souibgui, M.A., Fornés, A., Kessentini, Y., Megyesi, B.: Few shots is all you need: A progres￾sive few shot learning approach for low resource handwriting recognition (2021). [Online]. 
Available: https://arxiv.org/abs/2107.10064 
19. Ao, X., Zhang, X., Yang, H., Yin, F., Liu, C.: Cross-modal prototype learning for zero-shot 
handwriting recognition. In: 2019 International Conference on Document Analysis and Recog￾nition, ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 589–594. IEEE (2019) 
20. Cao, Z., Lu, J., Cui, S., Zhang, C.: Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recognit. 107, 107488 (2020) 
21. Huang, Y., Jin, L., Peng, D.: Zero-shot Chinese text recognition via matching class embed￾ding. In: 16th International Conference on Document Analysis and Recognition, ICDAR 2021,64 4 Open-Set Text Recognition Implementations(I): Label-to-Representation Mapping
Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part III, ser. Lecture Notes in 
Computer Science, vol. 12823, pp. 127–141. Springer (2021) 
22. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
23. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022, pp. 4513–4522. IEEE (2022) 
24. Chen, J., Li, B., Xue, X.: Zero-shot Chinese character recognition with stroke-level decompo￾sition. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 
IJCAI 2021, Virtual Event / Montreal, Canada, 19–27 August 2021, pp. 615–621 (2021). www. 
ijcai.org 
25. Zhang, J., Matsumoto, T.: Improving character-level japanese-chinese neural machine transla￾tion with radicals as an additional input feature. In: 2017 International Conference on Asian 
Language Processing (IALP), pp. 172–175. IEEE (2017) 
26. Lampert, C.H., Nickisch, H., Harmeling, S.: Attribute-based classification for zero-shot visual 
object categorization. IEEE Trans. Pattern Anal. Mach. Intell. 36(3), 453–465 (2014) 
27. Wang, T., Xie, Z., Li, Z., Jin, L., Chen, X.: Radical aggregation network for few-shot offline 
handwritten Chinese character recognition. Pattern Recognit. Lett. 125, 821–827 (2019) 
28. Wang, W., Zhang, J., Du, J., Wang, Z., Zhu, Y.: DenseRAN for offline handwritten Chinese 
character recognition. In: 16th International Conference on Frontiers in Handwriting Recogni￾tion, ICFHR 2018, Niagara Falls, NY, USA, August 5–8, 2018, pp. 104–109. IEEE Computer 
Society (2018) 
29. Zhang, J., Zhu, Y., Du, J., Dai, L.: Trajectory-based radical analysis network for online handwrit￾ten Chinese character recognition. In: 24th International Conference on Pattern Recognition, 
ICPR 2018, Beijing, China, August 20–24, 2018, pp. 3681–3686. IEEE Computer Society 
(2018) 
30. Zhang, J., Zhu, Y., Du, J., Dai, L.: Radical analysis network for zero-shot learning in printed 
Chinese character recognition. In: 2018 IEEE International Conference on Multimedia and 
Expo, ICME 2018, San Diego, CA, USA, July 23–27, 2018, pp. 1–6. IEEE Computer Society 
(2018) 
31. Ke, Y., Hagiwara, M.: Cnn-encoded radical-level representation for Japanese processing. Trans. 
Japanese Soc. Artif. Intell. 33(4), D–I23 (2018) 
32. Liu, Y., Liu, Q., Chen, J., Wang, Y.: Reading chinese in natural scenes with a bag-of-radicals 
prior. In: 33rd British Machine Vision Conference,: BMVC 2022, London, UK, November 
21–24, 2022, p. 969. BMVA Press (2022) 
33. Zu, X., Yu, H., Li, B., Xue, X.: Chinese character recognition with augmented character pro￾file matching. In: MM ’22: The 30th ACM International Conference on Multimedia, Lisboa, 
Portugal, October 10–14, 2022, pp. 6094–6102. ACM (2022) 
34. Chen, Z., Yang, W., Li, X.: Stroke-based autoencoders: Self-supervised learners for efficient 
zero-shot Chinese character recognition (2022). [Online]. Available: http://arxiv.org/abs/2207. 
08191 
35. Yu, H., Chen, J., Li, B., Xue, X.: Chinese character recognition with radical-structured stroke 
trees (2022). [Online]. Available: http://arxiv.org/abs/2211.13518 
36. He, S., Schomaker, L.: Open set Chinese character recognition using multi-typed attributes 
(2018). [Online]. Available: http://arxiv.org/abs/1808.08993 
37. Li, B., Tang, X., Qi, X., Chen, Y., Xiao, R.: Hamming OCR: A locality sensitive hashing 
neural network for scene text recognition (2020). [Online]. Available: https://arxiv.org/abs/ 
2009.10874 
38. Chen, J., Yu, H., Ma, J., Guan, M., Xu, X., Wang, X., Qu, S., Li, B., Xue, X.: Benchmark￾ing Chinese text recognition: Datasets, baselines, and an empirical study (2021). [Online]. 
Available: https://arxiv.org/abs/2112.15093 
39. Zhang, J., Du, J., Dai, L.: Radical analysis network for learning hierarchies of Chinese char￾acters. Pattern Recognit. 103, 107305 (2020)References 65
40. Zeng, J., Xu, R., Wu, Y., Li, H., Lu, J.: STAR: zero-shot chinese character recognition with 
stroke- and radical-level decompositions (2022). [Online]. Available: http://arxiv.org/abs/2210. 
08490 
41. Diao, X., Shi, D., Tang, H., Wu, L., Li, Y., Xu, H.: REZCR: A zero-shot character recognition 
method via radical extraction (2022). [Online]. Available: https://arxiv.org/abs/2207.05842 
42. Brunelli, R.: Template Matching Techniques in Computer Vision: Theory and Practice. John 
Wiley & Sons (2009) 
43. Paz-Argaman, T., Tsarfaty, R., Chechik, G., Atzmon, Y.: ZEST: Zero-shot learning from text 
descriptions using textual similarity and visual summarization. In: Findings of the Association 
for Computational Linguistics: EMNLP. Association for Computational Linguistics 2020, 569– 
579 (2020) 
44. Yang, C., Liu, C., Yin, X.: Weakly correlated knowledge integration for few-shot image clas￾sification. Int. J. Autom. Comput. 19(1), 24–37 (2022) 
45. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., Wierstra, D.: Matching networks 
for one shot learning. In: Advances in Neural Information Processing Systems 29: Annual 
Conference on Neural Information Processing Systems 2016, December 5–10, 2016, pp. 3630– 
3638. Barcelona, Spain (2016) 
46. Koch, G., Zemel, R., Salakhutdinov, R., et al.: Siamese neural networks for one-shot image 
recognition. In: ICML Deep Learning Workshop, vol. 2, 1st edn. Lille (2015) 
47. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 
A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from 
natural language supervision. In: Proceedings of the 38th International Conference on Machine 
Learning, ICML 2021, 18–24 July 2021, Virtual Event, ser. Proceedings of Machine Learning 
Research, vol. 139, pp. 8748–8763. PMLR (2021) 
48. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional 
transformers for language understanding. In: Proceedings of the 2019 Conference of the North 
American Chapter of the Association for Computational Linguistics: Human Language Tech￾nologies, Volume 1 (Long and Short Papers), pp. 4171–4186. Association for Computational 
Linguistics (2019) 
49. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding 
with unsupervised learning (2018) 
50. Ma, Z., Luo, G., Gao, J., Li, L., Chen, Y., Wang, S., Zhang, C., Hu, W.: Open-vocabulary 
one-stage detection with hierarchical visual-language knowledge distillation. In: IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, 
USA, June 18–24, 2022, pp. 14 054–14 063. IEEE (2022) 
51. Huang, G., Luo, X., Wang, S., Gu, T., Su, K.: Hippocampus-heuristic character recognition 
network for zero-shot learning in Chinese character recognition. Pattern Recognit. 130, 108818 
(2022) 
52. Salton, G., Buckley, C.: Term-weighting approaches in automatic text retrieval. Inf. Process. 
Manag. 24(5), 513–523 (1988) 
53. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., 
Metzler, D.: Long range arena : A benchmark for efficient transformers. In: 9th International 
Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3–7, 2021. 
OpenReview.net (2021)Chapter 5 
Open-Set Text Recognition 
Implementations(II): 
Sample-to-Representation Mapping 
Abstract This chapter introduces how representations are encoded and extracted 
from samples, i.e., the sample-to-representation mapping module in the framework 
discussed above (Fig. 5.1). The contents include three parts: feature extraction, repre￾sentation aggregation, and context handling. Since most mapping approaches break 
down into a feature extractor and a sampler module, specifically, the feature extractor 
maps input images to feature maps, whereas the sampler sample character features 
from the feature maps. Note for some methods classification modules may be called 
interleaved, before the sampling procedure, or after the sampling procedure. Despite 
the execution, the main functionality of the sampler is to aggregate the corresponding 
regions in the feature map to timestamp aligned targets, so we consider it executes a 
part of the Sample-to-Representation Mapping functionality. In addition, we discuss 
the context handling part of the definition of linguistic information and introduce its 
impact and handling of linguistic information under closed and open environments. 
Keywords Sample-to-Representation Mapping · Feature extraction · Representation aggregation · Context handling 
This chapter introduces how representations are encoded and extracted from samples, 
i.e., the sample-to-representation mapping module in the framework discussed in 
Sect. 3.2.3.3. The contents including three parts: feature extraction, representation 
aggregation, and context handling. 
Since most mapping approaches break down into a feature extractor and a sampler 
module, specifically, the feature extractor maps input images to feature maps, whereas 
the sampler sample character features from the feature maps. Note for some methods 
classification modules may be called interleaved with [ 1– 3], before the sampling 
procedure [ 4, 5], or after the sampling procedure [ 6, 7]. Despite the execution, the 
main functionality of the sampler is to aggregate the corresponding regions in the 
feature map to timestamp aligned targets, so we consider it executes a part of the 
Sample-to-Representation Mapping functionality. 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_5
6768 5 Open-Set Text Recognition Implementations(II): Sample-to-Representation …
Fig. 5.1 This section focuses on the Sample-to-Representation Mapping 
In addition, we discuss the context handling part of the definition of linguistic 
information and introduce its impact and handling of linguistic information under 
closed and open environments (Fig. 5.1). 
5.1 Feature Extractor 
5.1.1 Knowledge-Driven Feature Extractor 
In this section, we retrospectively take a glance at feature extraction algorithms 
before the advent of deep learning. More detailed and comprehensive coverage of 
these works can be found in [ 8, 9]. 
For handwritten text image recognition tasks, researchers focused on low-level 
feature extraction methods. These encompassed representing the global text profile, 
edge-based feature representation, and methods for extracting local text image fea￾tures using the Bag-of-Feature (BoF) approach [ 10]. Additionally, to mitigate the 
impact of text segmentation results on feature extraction effectiveness, researchers 
replaced text image segmentation processing with sliding window scanning. Almazan 
et al. [ 11] employed Histogram of Gradient (HOG) descriptors and a sample SVM 
matching framework for document retrieval. They introduced the Pyramidal His￾togram of Characters (PHOC) and utilized SVM to extract features based on PHOC 
encoding. Because PHOC encoding contains spatial information about characters in 
the image, the extracted features are more suitable for text retrieval and recognition 
tasks. 
For nature scene text recognition tasks, researchers utilized mid-level image fea￾tures instead of SIFT descriptors to explore BoF features. They proposed methods 
like Stroke Width Transform (SWT) [ 12], Sketch tokens [ 13], Bag of Words (BoW) 
[ 14], and Strokelets [ 15]. Strokelets involve randomly partitioning images containing 
the same letters, and then clustering the resulting local blocks to obtain cluster cen￾ters representing strokelets. These strokelets effectively represent the local aspects of5.2 Sampler 69
character images. Using strokelets to encode character images allows for character 
recognition even when parts of the character image are blurred or obscured, leverag￾ing the clear parts’ recognition results. Gordo et al. [ 16] also proposed a mid-level 
feature extraction method and mapped this feature to a semantically associated space. 
The difference from strokelets is that the mid-level features proposed by Gordo et 
al. include spatial localization information of characters. However, this information 
inclusion is implicit and does not directly correspond to the precise positioning of 
each character. Experimental results indicate a significant performance improve￾ment compared to using solely strokelets and Hough graphs, emphasizing the equal 
importance of the semantic association of labels with image feature extraction. 
In summary, during the pre-deep learning era, text recognition methods primarily 
extracted low-level or mid-level handcrafted image features, involving demanding 
and repetitive pre-processing and post-processing steps. Constrained by the limited 
representation ability of handcrafted features and the complexity of pipelines, those 
methods struggled to handle intricate circumstances. 
5.1.2 Data-Driven Feature Extractor 
The deep-learning era introduces various convolutional networks [ 17, 18], which are 
used by the majority of OCR methods [ 6]. Some methods choose to append Sequence 
modeling modules like RNNs [ 5, 19] behind the CNN backbone to better model the 
contextual information. Instead of vanilla CNNs, some decide to reuse layers to 
reduce the number of parameters [ 20]. Given the success of visual transformer in 
a range of tasks, recent OCR methods are seen adopting it as their backbones [ 21, 
22]. Among these some steps further seek direct content understanding [ 23] and 
self-supervision [ 24]. 
Some methods add a preceding alignments module to reduce the impact of camera 
skew or irregular shape. Typical methods involve STN [ 25], TPS [ 26], Symmetry￾based Rectification [ 27], or (constrained) mesh-grid transform [ 28, 29] for more 
fine-grained control. 
5.2 Sampler 
The sampler, however, is diverse from method to method and can be generally cat￾egorized into three genres, i.e., the gram-based sampler, feature aggregation-based, 
and label aggregation-based samplers. Note although most methods contain only one 
sampling method, different samplers can be ensembled to further improve perfor￾mance. Two typical examples are Robust Scanner [ 30] and Text Spotter [ 31].70 5 Open-Set Text Recognition Implementations(II): Sample-to-Representation …
5.2.1 Gram Based 
Gram-based methods refer to methods where labels are associated with more than 
one character, which are generally used with word-level methods and gram-level 
representations in the early days. Due to the limited flexibility, most such methods 
happened before or at the beginning of the deep-learning era. 
5.2.2 Feature Aggregation 
These methods align features to timestamp, before classification. Some RNN meth￾ods depend on previously predicted labels for the current timestamp, but the feature 
for the current timestamp is still extracted before classification, so we put them here. 
The recurrent decoder (illustrated in Fig. 5.2) refers to decoders that sample one 
character representation at a time, where the output represented at each timestamp 
depends on previous states. Generally, the implementations form two main cate￾gories, which are the RNN-Attention methods [ 1, 26] and the transformer decoder 
category [ 3, 32]. 
The RNN-Attention category utilizes an attention date back to 2016 [ 33], and was 
soon adapted to handle skew scene text on a 2-D feature map [ 34]. SAR [ 1] adapts 
the pipeline by initializing the hidden state (which they call a “holistic feature”) 
with an LSTM encoder applied to horizontal features. Alternatively, ASTER [ 26] 
attaches a rectification network, which aligns the text to the x-axis. Some methods 
involve more complex rectification networks like symmetric-based rectification [ 27] 
and Moran [ 28]. 
Instead of using RNN cells [ 35, 36], some methods use a transformer decoder [ 37] 
to sample features for prediction. Much like the aforementioned RNN-Attention 
pipelines, these methods call the attention decoder at each timestamp, with text from 
previous time stamps as query, and extracted feature map as keys and values. Alter￾native to the attention mechanism, Chen et al. [ 38] propose a single-point sampling 
for better efficiency. 
Note for a few methods, the decoder does not follow the chronology order shown 
in Fig. 5.2, e.g., [ 39] augment the training by introducing random permutation to the 
decoding order. 
Fig. 5.2 Recurrent Sample-to-Representation Mapping5.2 Sampler 71
Fig. 5.3 Parallel Sample-to-Representation Mapping 
Though the iterative methods demonstrate decent performance, the recurrent 
structure limits their potential to be parallelized. The parallel decoders [ 6, 7] (shown 
in Fig. 5.3) are proposed to address this problem by removing the dependencies 
between timestamps. These methods generally sample features at each timestamp 
with a query associated with the timestamp, for those who don’t actively work in 
the OCR domain, these methods resemble the DETR [ 40] pipeline. The quires are 
mostly implemented as trainable query parameters [ 6, 29, 41– 43], For DAN [ 7], the 
queries are implemented as convolution kernels in the CAM module. 
5.2.3 Label Aggregation 
As shown in Fig. 5.4, these methods first produce dense prediction results, and aggre￾gate the predictions according to their spatial layout, specifically by joining adjacent 
similar predictions on 1-D or 2-D space. Functionality-wise, these submodules still 
align regions on the feature map to timestamp due to the correspondence between 
the dense prediction and the features. Hence, despite this genre of aggregation sub￾module is always called after the open-set predictor modules, they are still treated as 
a part of the sample-to-representation mapping module. 
The most common category would be the CTC methods [ 5, 44, 45]. Inference￾wise, these methods employ a Top-1 beam search, which is de-facto a 1-D Connec￾tion component algorithm. With the help of CTCLoss [ 46], the optimization can be 
achieved without knowing the location of individual characters in the sample. How￾ever, the 1-D methods come with a strong assumption, that the contents are aligned 
Fig. 5.4 The Label Aggregation methods72 5 Open-Set Text Recognition Implementations(II): Sample-to-Representation …
to the x-axis and read from left to right, hence demonstrating difficulties in handling 
curvy or skew text samples. 
To address this limitation, 2-D connection component-based methods are pro￾posed to tackle this problem. Instance segmentation-based methods like CA-FCN [ 4, 
30, 31, 47] directly employ instance segmentation approaches to text recognition. 
This solution is straightforward, however, would require character localization anno￾tations. AON [ 44] tries aggregation in all four possible directions, breaking free from 
the character annotation necessity. 2-D-CTC [ 19] expands CTC to 2-D space [ 19], 
further saves time trying different orientations. 
Instead of using rule-based score aggregation, Zhang et al. [48] propose a trainable 
module to achieve more flexible aggregation. 
5.3 Linguistic Information Handling 
As the subject of text recognition is usually natural language, the occurrence of 
characters in a word sample is usually not random. Specifically, the occurrence of 
characters usually forms a long-tail distribution, further, the co-occurrence correla￾tion of characters, adjacent or not, can be modeled and exploited by the models [ 49]. 
This part surveys how such information is modeled for Close-set environments 
and Open-set Environments. 
5.3.1 Handling Linguistic Information in Close-Environment 
Linguistic Information is frequently exploited in close environments to deduct char￾acters that are visually difficult to distinguish due to blur, obstruction, artistic fonts, 
or irregular text shapes. 
Early methods usually utilize RNNs like (Bidirectional) LSTMs [ 5, 44] or 
GRUs [ 36] upon extracted feature maps. Later methods [ 1, 3] utilize characters 
from timestamp.0....t − 1 to generate the character prediction. 
Another recent typical trend is to use post-recognition language models to 
model linguistic information and refine the recognition results [ 6, 41, 50]. Recently, 
several methods [ 51– 53] think that the text modal itself is not sufficient, hence intro￾ducing the visual modal to the language model. 
Recently, there are also a variety of unique approaches [ 54– 56] that model con￾textual information. For example, [ 54] fuses linguistic information within the con￾volution backbone, using cbow-like [ 57] approach [ 56]. 
Though linguistic informationhelps recognition under close-set environments 
some methods like [ 4, 45] choose to not model for faster speed.References 73
5.3.2 Handling Linguistic Information in Open-Environment 
In Open environments, despite a few methods [ 58] found Linguistic Information 
helpful when handling unseen words from a seen language, most methods still view 
Linguistic Information as a potentially harmful confounder [ 42, 49, 59]. One most 
trivial approaches is to build contextual invariant data [ 60]. A similar approach would 
be to align character features under different contexts [ 59]. Decoupling visual and 
contextual representations is another approach [ 42, 61]. 
However, given an open environment does not necessarily mean all words are 
out-of-vocabulary, which is also not the majority part of real-life applications, many 
methods [ 49, 60– 62] choose to adaptively fuse context-free and context-based pre￾diction accordingly instead of completely discarding contextual information. 
On the other end, Clipter [ 55] decided to expand training data to get the testing 
environment better covered. 
References 
1. Li, H., Wang, P., Shen, C., Zhang, G.: Show, attend and read: a simple and strong baseline for 
irregular text recognition. In: The Thirty-Third AAAI Conference on Artificial Intelligence, 
AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2019, Honolulu, Hawaii, USA, Jan 27–Feb 1, 2019. AAAI Press (2019), pp. 8610–8617 
2. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), Oct 27–Nov 
2, 2019. IEEE (2019), pp. 4714–4722 
3. Sheng, F., Chen, Z., Xu, B.: NRTR: a no-recurrence sequence-to-sequence model for scene 
text recognition. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, Sept 20–25, 2019. IEEE (2019), pp. 781–786 
4. Liao, M., Zhang, J., Wan, Z., Xie, F., Liang, J., Lyu, P., Yao, C., Bai, X.: Scene text recogni￾tion from two-dimensional perspective. In: The Thirty-Third AAAI Conference on Artificial 
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence 
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial 
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, Jan 27–Feb 1, 2019. AAAI Press (2019), 
pp. 8714–8721 
5. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence 
recognition and its application to scene text recognition. IEEE Trans. Pattern Anal. Mach. 
Intell. 39(11), 2298–2304 (2017) 
6. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: autonomous, bidirectional 
and iterative language modeling for scene text recognition. In: IEEE Conference on Computer 
Vision and Pattern Recognition, CVPR 2021, virtual, June 19–25, 2021. Computer Vision 
Foundation/IEEE (2021), pp. 7098–7107 
7. Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.: Decoupled attention 
network for text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, New York, NY, USA, Feb 7–12, 2020. AAAI Press (2020), pp. 12 216–12 22474 5 Open-Set Text Recognition Implementations(II): Sample-to-Representation …
8. Yin, X., Zuo, Z., Tian, S., Liu, C.: Text detection, tracking and recognition in video: a compre￾hensive survey. IEEE Trans. Image Process. 25(6), 2752–2773 (2016). [Online]. https://doi. 
org/10.1109/TIP.2016.2554321 
9. Long, S., He, X., Yao, C.: Scene text detection and recognition: the deep learning era. Int. J. 
Comput. Vis. 129(1), 161–184 (2021). [Online]. https://doi.org/10.1007/s11263-020-01369-
0 
10. Rothacker, L., Rusiñol, M., Fink, G.A.: Bag-of-features hmms for segmentation-free word 
spotting in handwritten documents. In: 12th International Conference on Document Analysis 
and Recognition, ICDAR 2013, Washington, DC, USA, Aug 25–28, 2013. IEEE Computer 
Society (2013), pp. 1305–1309. [Online]. https://doi.org/10.1109/ICDAR.2013.264 
11. Almazán, J., Gordo, A., Fornés, A., Valveny, E.: Handwritten word spotting with corrected 
attributes. In: IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Aus￾tralia, Dec 1–8, 2013. IEEE Computer Society (2013), pp. 1017–1024. [Online]. https://doi. 
org/10.1109/ICCV.2013.130 
12. Epshtein, B., Ofek, E., Wexler, Y.: Detecting text in natural scenes with stroke width transform. 
In: The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 
2010, San Francisco, CA, USA, 13–18 June 2010. IEEE Computer Society (2010), pp. 2963– 
2970. [Online]. https://doi.org/10.1109/CVPR.2010.5540041 
13. Lim, J.J., Zitnick, C.L., Dollár, P.: Sketch tokens: a learned mid-level representation for contour 
and object detection. In: 2013 IEEE Conference on Computer Vision and Pattern Recogni￾tion, Portland, OR, USA, June 23–28, 2013. IEEE Computer Society (2013), pp. 3158–3165. 
[Online]. https://doi.org/10.1109/CVPR.2013.406 
14. Fei-Fei, L., Perona, P.: A bayesian hierarchical model for learning natural scene categories. 
In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition 
(CVPR 2005), 20–26 June 2005, San Diego, CA, USA. IEEE Computer Society (2005), pp. 
524–531. [Online]. https://doi.org/10.1109/CVPR.2005.16 
15. Bai, X., Yao, C., Liu, W.: Strokelets: a learned multi-scale mid-level representation for scene 
text recognition. IEEE Trans. Image Process. 25(6), 2789–2802 (2016). [Online]. https://doi. 
org/10.1109/TIP.2016.2555080 
16. Gordo, A.: Supervised mid-level features for word image representation. In: IEEE Conference 
on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7–12, 
2015. IEEE Computer Society (2015), 2956–2964. [Online]. https://doi.org/10.1109/CVPR. 
2015.7298914 
17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: 2016 
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, 
USA, June 27–30, 2016. IEEE Computer Society (2016), pp. 770–778 
18. Postadjian, T., Bris, A.L., Mallet, C., Sahbi, H.: Superpixel partitioning of very high resolution 
satellite images for large-scale classification perspectives with deep convolutional neural net￾works. In: IEEE International Geoscience and Remote Sensing Symposium, IGARSS 2018, 
Valencia, Spain, July 22–27, 2018. IEEE (2018), pp. 1328–1331 
19. Wan, Z., Xie, F., Liu, Y., Bai, X., Yao, C.: 2d-ctc for scene text recognition (2019). [Online]. 
http://arxiv.org/abs/1907.09705 
20. Lai, S., Xu, L., Liu, K., Zhao, J.: Recurrent convolutional neural networks for text classification. 
In: Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, Jan 25–30, 
2015, Austin, Texas, USA. AAAI Press (2015), pp. 2267–2273 
21. Atienza, R.: Vision transformer for fast and efficient scene text recognition. In: 16th Interna￾tional Conference on Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzer￾land, Sept 5–10, 2021, Proceedings, Part I, ser. Lecture Notes in Computer Science, vol. 12821. 
Springer (2021), pp. 319–334 
22. Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florêncio, D.A.F., Zhang, C., Li, Z., Wei, F.: Trocr: 
transformer-based optical character recognition with pre-trained models. In: Thirty-Seventh 
AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innova￾tive Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational 
Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, Feb 7–14, 2023. AAAI 
Press (2023), pp. 13 094–13 102References 75
23. Rust, P., Lotz, J.F., Bugliarello, E., Salesky, E., de Lhoneux, M., Elliott, D.: Language modelling 
with pixels. In: The Eleventh International Conference on Learning Representations, ICLR 
2023, Kigali, Rwanda, May 1–5, 2023. OpenReview.net (2023) 
24. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., 
Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, 
F.: Language is not all you need: aligning perception with language models (2023). [Online]. 
http://arxiv.org/abs/2302.14045 
25. Luo, H., Jiang, W., Fan, X., Zhang, C.: Stnreid: deep convolutional networks with pairwise 
spatial transformer networks for partial person re-identification. IEEE Trans. Multim. 22(11), 
2905–2913 (2020) 
26. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: ASTER: an attentional scene text 
recognizer with flexible rectification. IEEE Trans. Pattern Anal. Mach. Intell. 41(9), 2035– 
2048 (2019) 
27. Yang, M., Guan, Y., Liao, M., He, X., Bian, K., Bai, S., Yao, C., Bai, X.: Symmetry-constrained 
rectification network for scene text recognition. In: 2019 IEEE/CVF International Conference 
on Computer Vision, ICCV 2019, Seoul, Korea (South), Oct 27–Nov 2, 2019. IEEE (2019), 
pp. 9146–9155 
28. Luo, C., Jin, L., Sun, Z.: MORAN: a multi-object rectified attention network for scene text 
recognition. Pattern Recognit. 90, 109–118 (2019) 
29. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
30. Yue, X., Kuang, Z., Lin, C., Sun, H., Zhang, W.: Robustscanner: dynamically enhancing posi￾tional clues for robust text recognition. In: Computer Vision - ECCV 2020–16th European 
Conference, Glasgow, UK, August 23–28: Proceedings, Part XIX, ser. Lecture Notes in Com￾puter Science, vol. 12364. Springer (2020), 135–151 
31. Wan, Z., He, M., Chen, H., Bai, X., Yao, C.: Textscanner: reading characters in order for robust 
scene text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, New York, NY, USA, Feb 7–12, 2020. AAAI Press (2020), pp. 12 120–12 127 
32. Zhu, Y., Wang, S., Huang, Z., Chen, K.: Text recognition in images based on transformer with 
hierarchical attention. In: 2019 IEEE International Conference on Image Processing, ICIP 
2019, Taipei, Taiwan, Sept 22–25, 2019. IEEE (2019), pp. 1945–1949 
33. Lee, C., Osindero, S.: Recursive recurrent nets with attention modeling for OCR in the wild. In: 
2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, 
NV, USA, June 27–30, 2016. IEEE Computer Society (2016), pp. 2231–2239 
34. Yang, X., He, D., Zhou, Z., Kifer, D., Giles, C.L.: Learning to read irregular text with attention 
mechanisms. In: Proceedings of the Twenty-Sixth International Joint Conference on Artificial 
Intelligence, IJCAI 2017, Melbourne, Australia, Aug 19–25, 2017. ijcai.org (2017), pp. 3280– 
3286 
35. Vo, V., Le, B.S., Vo, H.P., Nguyen, H.T.C., Lam, P.H.K.: Build A module for improvement 
real time speech enhancement using long short-term memory approach: improvement real time 
speech enhancement using long short-term memory. In: Proceedings of the 2023 8th Interna￾tional Conference on Intelligent Information Technology, ICIIT 2023, Da Nang, Vietnam, Feb 
24–26, 2023. ACM (2023), pp. 259–264 
36. Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, 
Y.: Learning phrase representations using RNN encoder–decoder for statistical machine trans￾lation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language 
Processing (EMNLP). Association for Computational Linguistics (2014), pp. 1724–1734 
37. Zhang, X., Yang, H., Young, E.F.Y.: Attentional transfer is all you need: Technology-aware 
layout pattern generation. In: 58th ACM, IEEE Design Automation Conference, DAC, San 
Francisco, CA, USA, Dec 5–9, 2021. IEEE (2021), pp. 169–174 
38. Chen, L., Qin, H., Zhang, S., Yang, C., Yin, X.: Scene text recognition with single-point 
decoding network. In: Artificial Intelligence - Second CAAI International Conference, CICAI76 5 Open-Set Text Recognition Implementations(II): Sample-to-Representation …
2022, Beijing, China, Aug 27–28, 2022, Revised Selected Papers, Part I, ser. Lecture Notes in 
Computer Science, vol. 13604. Springer (2022), pp. 142–153 
39. Bautista, D., Atienza, R.: Scene text recognition with permuted autoregressive sequence mod￾els. In: Computer Vision - ECCV 2022–17th European Conference, Tel Aviv, Israel, Oct 23–27, 
2022, Proceedings, Part XXVIII, ser. Lecture Notes in Computer Science, vol. 13688. Springer 
(2022), pp. 178–196 
40. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object 
detection with transformers. In: Computer Vision - ECCV 2020–16th European Conference, 
Glasgow, UK, August 23–28, 2020, Proceedings, Part I, ser. Lecture Notes in Computer Science, 
vol. 12346. Springer (2020), pp. 213–229 
41. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate scene text 
recognition with semantic reasoning networks. In: 2020 IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020. IEEE 
(2020), pp. 12 110–12 119 
42. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022. IEEE (2022), pp. 4513–4522 
43. Aberdam, A., Ganz, R., Mazor, S., Litman, R.: Multimodal semi-supervised learning for text 
recognition (2022). [Online]. https://arxiv.org/abs/2205.03873 
44. Cheng, Z., Xu, Y., Bai, F., Niu, Y., Pu, S., Zhou, S.: AON: towards arbitrarily-oriented text 
recognition. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 
2018, Salt Lake City, UT, USA, June 18–22, 2018. IEEE Computer Society (2018), pp. 5571– 
5579 
45. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: large scale system for text detection and 
recognition in images. In: Proceedings of the 24th ACM SIGKDD International Conference 
on Knowledge Discovery Data Mining, KDD 2018, London, UK, Aug 19–23, 2018. ACM 
(2018), pp. 71–79 
46. Graves, A., Fernández, S., Gomez, F.J., Schmidhuber, J.: Connectionist temporal classification: 
labelling unsegmented sequence data with recurrent neural networks. In: Machine Learning, 
Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Penn￾sylvania, USA, June 25–29, 2006, ser. ACM International Conference Proceeding Series, vol. 
148. ACM (2006), pp. 369–376 
47. Liao, M., Lyu, P., He, M., Yao, C., Wu, W., Bai, X.: Mask textspotter: an end-to-end trainable 
neural network for spotting text with arbitrary shapes. IEEE Trans. Pattern Anal. Mach. Intell. 
43(2), 532–548 (2021) 
48. Zhang, C., Gupta, A., Zisserman, A.: Adaptive text recognition through visual matching. In: 
Computer Vision - ECCV 2020–16th European Conference, Glasgow, UK, Aug 23–28, 2020, 
Proceedings, Part XVI, ser. Lecture Notes in Computer Science, vol. 12361. Springer (2020), 
pp. 51–67 
49. Wan, Z., Zhang, J., Zhang, L., Luo, J., Yao, C.: On vocabulary reliance in scene text recognition. 
In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, 
Seattle, WA, USA, June 13–19, 2020. IEEE (2020), pp. 11 422–11 431 
50. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: SEED: semantics enhanced encoder-decoder 
framework for scene text recognition. In: 2020 IEEE/CVF Conference on Computer Vision 
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020. IEEE (2020), pp. 
13 525–13 534 
51. Diao, L., Tang, X., Wang, J., Fang, R., Xie, G., Chen, W.: Visual-semantic transformer for scene 
text recognition. In: 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, 
Nov 21–24, 2022. BMVA Press (2022), p. 772 
52. Bhunia, A.K., Sain, A., Kumar, A., Ghose, S., Chowdhury, P.N., Song, Y.: Joint visual semantic 
reasoning: multi-stage decoder for text recognition. In: 2021 IEEE/CVF International Confer￾ence on Computer Vision, ICCV 2021, Montreal, QC, Canada, Oct 10–17, 2021. IEEE (2021), 
pp. 14 920–14 929References 77
53. He, Y., Chen, C., Zhang, J., Liu, J., He, F., Wang, C., Du, B.: Visual semantics allow for textual 
reasoning better in scene text recognition. In: Thirty-Sixth AAAI Conference on Artificial 
Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial 
Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial 
Intelligence, EAAI 2022 Virtual Event, Feb 22–March 1, 2022. AAAI Press (2022), pp. 888– 
896 
54. Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: a new scene 
text recognizer with visual language modeling network. In: 2021 IEEE/CVF International 
Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, Oct 10–17, 2021. IEEE 
(2021), pp. 14 174–14 183 
55. Aberdam, A., Bensaid, D., Golts, A., Ganz, R., Nuriel, O., Tichauer, R., Mazor, S., Litman, R.: 
Clipter: looking at the bigger picture in scene text recognition. In: Proceedings of the IEEE/CVF 
International Conference on Computer Vision (ICCV), Oct 2023, pp. 21 706–21 717 
56. Wang, P., Da, C., Yao, C.: Multi-granularity prediction for scene text recognition. In: Computer 
Vision - ECCV 2022–17th European Conference, Tel Aviv, Israel, Oct 23–27, 2022, Proceed￾ings, Part XXVIII, ser. Lecture Notes in Computer Science, vol. 13688. Springer (2022), pp. 
339–355 
57. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations 
in vector space. In: 1st International Conference on Learning Representations, ICLR 2013, 
Scottsdale, Arizona, USA, May 2–4, 2013, Workshop Track Proceedings (2013) 
58. Bhunia, A.K., Chowdhury, P.N., Sain, A., Song, Y.: Towards the unseen: iterative text recog￾nition by distilling from errors. In: 2021 IEEE/CVF International Conference on Computer 
Vision, ICCV 2021, Montreal, QC, Canada, Oct 10–17, 2021. IEEE (2021), pp. 14 930–14 939 
59. Zhang, X., Zhu, B., Yao, X., Sun, Q., Li, R., Yu, B.: Context-based contrastive learning for 
scene text recognition. In: Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 
2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 
2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 
2022 Virtual Event, Feb 22–March 1, 2022. AAAI Press (2022), pp. 3353–3361 
60. Park, S., Chung, S., Lee, J., Choo, J.: Improving scene text recognition for character-level 
long-tailed distribution (2023). [Online]. http://arxiv.org/abs/2304.08592 
61. Cheng, C., Li, B., Zheng, Q., Wang, Y., Liu, W.: Decoupling visual-semantic feature learning 
for robust scene text recognition (2021). [Online]. https://arxiv.org/abs/2111.12351 
62. Tian, F., Wu, H., Xu, B.: Financial ticket intelligent recognition system based on deep learning 
(2020) [Online]. https://arxiv.org/abs/2010.15356Chapter 6 
Open-Set Text Recognition 
Implementations(III): Open-set Predictor 
Abstract This chapter discusses the approaches of the representation-prototype 
matching process, which is used to recognize or reject the corresponding samples in 
question. For each query instance representation extracted from the sample image, 
the open-set predictor matches it to representation prototype and determines whether 
the character belongs to a corresponding class or not. If the instance representation 
successfully matches a representation prototype, the module yields the label asso￾ciated with the prototype, otherwise the open-set predictor is rejected, yielding an 
unknown token. This section studies the inference stage behavior on recognition and 
rejection, and optimization approaches, specifically loss design and training tricks, 
that are used to implement the desired behavior. 
Keywords Open-set predictor · Representation-prototype matching · Representation prototype · Template matching · Recognition · Rejection 
This chapter discusses the approaches of the representation-prototype matching pro￾cess (see Fig. 6.1), which is used to recognize or reject the corresponding samples 
in question. 
For each query instance representation.F extracted from sample image .img, the 
open-set predictor matches it to representation prototypes. P and determines whether 
the character belongs to a corresponding class or not. If .F successfully matches a 
representation prototype, the module yields the label associated with the prototype, 
otherwise, the open-set predictor is rejected, yielding an unknown token “.[−]”. 
This section studies the inference stage behavior on recognition and rejection, and 
optimization approaches, specifically loss design and training tricks, that are used to 
implement the desired behavior. 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_6
7980 6 Open-Set Text Recognition Implementations(III): Open-set Predictor
Fig. 6.1 This section focuses on the open-set predictor 
6.1 Recognition 
6.1.1 Simple Distance Function 
Dot Production refers to model query instance representation F and representation 
prototypes with feature vectors and weight vectors, and matches them with a linear 
classifier, 
. S˜[t,j] =F[t]P[J ]. (6.1) 
This approach dates [ 1] way before the deep-learning era, and most close-set 
learning methods [ 2, 3] adopt this approach, where each category corresponds to 
a row in the classifier weight. However, since the mapping from label to weight 
is implicit, close-set methods need to undergo a retraining or fine-tuning process 
depending on the risk of forgetting [ 4]. 
Then some (generalized) low-shot learning methods decide to change the implicit 
nature of the mapping, thus they propose to generate the “weight vectors” from the 
given description of the label. The description ranges from attributes [ 5, 6], domain 
knowledge [ 7], real samples [ 8], and descriptions [ 9], to prompts [ 10, 11]. 
Cosine Similarity is a common special case of dot production when features 
and prototypes are normalized, the dot product similarity becomes the cosine sim￾ilarity [ 12], which then becomes a member of the metric learning family. Another 
typical example is MatchNet [ 13], which measures the prototypes extracted with 
“embedding functions. g” and character representation extracted with another embed￾ding function . f , which uses an LSTM to utilize information from the full support 
set to enhance the sample representation. The cosine similarity can be scaled by a 
constant [ 14], or by feature norm as well [ 15, 16]. 
Euclidean Distances is another common member metric learning family, which 
reduces to cosine similarity, 
.|P − F|
2 = 2 − 2cos(P, F), (6.2)6.1 Recognition 81
when both. P and. F are normalized unit vectors. Besides the Euclidean metric, some 
methods may use other distance functions like Mahalanobis distance [ 17]. 
6.1.2 Discrete Attribute Matching 
Attribution matching [ 18] is another major genre to measure the similarity between 
prototypes and sample representations. The main advantage of this genre is that it 
provides an interpretable representation, this kind of method is very popular in char￾acter recognition, because characters are directly generated from a set of structural 
knowledge [ 19] (e.g., Radicals [ 5, 20], Strokes [ 21, 22]), or can be sufficiently cov￾ered by a sequence of predefined visually grounded codes [ 23– 25], without coding 
conflicts. 
For example, FewRAN [ 6] adopts radical sequence a the representation space and 
uses a CNN-RNN structure to map the input character image to the predicted radical 
sequence. The recognition is conducted by matching the predicted radical sequence 
to the ground truth sequence. Zhang et. al. [ 26] apply this method to the character 
by adding a separation token to split the predicted token sequence into characters in 
the image. 
However, due to representation conflicts or overlapping decision boundaries, the 
matching process may end up with multiple results, which usually are solved with 
an auxiliary classifier [ 21, 22]. 
As of today, this genre is less used in generic zero-shot learning tasks other 
than the OCR field, possibly due to its demand on structural, discrete, and mostly 
confliction-free demands on knowledge-based encoding. A future way to overcome 
this limitation may lie in the automatic creation of structural knowledge, one example 
would be the HammingOCR [ 27]. 
6.1.3 Learnable Comparison Function 
Sometimes people design a more complex learnable function to resolve the matching 
problem. Specifically, they leave the measurement to the data, by concating the 
prototype and the sample together, and then sending them to a potentially non-linear 
classifier [ 28]. However, complexity comes with a huge cost, growing linearly or 
quadratically by the amount of categories. This, we think, is the main reason these 
methods are not usually seen in text recognition methods. 
Some decide that simply looking at each individual sample-prototype pair is not 
enough. This genre looks back and forth between the prototypes and features, and 
then decides which samples match which prototypes. An example is to use of graph 
networks to adjust similarity scores [ 29– 31]. For example, TPN [ 31] constructs a82 6 Open-Set Text Recognition Implementations(III): Open-set Predictor
graph with initial one-hot labels as nodes, and pair-wise visual similarity as edges. 
The labels are then propagated from the representation prototype (representation of 
support samples) to query instance representation (representation of support sam￾ples). 
6.2 Rejection of Out-of-Set 
In this section, we talk about the modeling of unknown, i.e., what means the sample 
is not known to the model. Generally speaking, the approach can be divided into 
centered approaches and non-centered approaches, depending on whether or not the 
unknown samples are supposed to be aligned upon a common center. 
6.2.1 Centered Approaches 
This genre aligns all unknown samples to one, or a few class centers. This approach 
is also commonly found in object detection methods [ 32, 33]. Unknown instances 
are usually mined from the unannotated regions from the training image by pseudo￾labeling. For example, ORE [ 32] labels top k background proposals as unknown 
classes in terms of objectiveness score and then aligns them to the prototype associ￾ated with the unknown label. However, this setup implies that all unknown classes 
must have something in common, which limits the feasibility in the open world where 
“unknown” samples possess a large variety. 
6.2.2 Non-Centered Approaches 
These methods find hints of uncertainty from distributions, including distribution of 
confidence, sample representation, and data itself. 
By Confidence Distribution People have long observed that the confidence 
response of an unknown category is much less “one-hot” than those that come from 
seen classes. It’s practical enough to use hard thresholding directly, therefore Open￾Max [ 34] proposes to normalize the confidence distribution of the top. k classes with 
a Weibull distribution, then apply a threshold for rejection. 
In recent days, open-word object detection methods [ 35, 36], action recogni￾tion [ 37], and anomaly detection methods [ 38] widely adopted this genre. 
By Representation Distribution The methods [ 15, 16, 39– 42] first measure 
sample representation similarities, defined by how close the sample representation 
falls to the prototypes of each class, or falls within the “distributions” centered at the 
prototypes. Whether or not the sample is considered unknown is usually decided by 
comparing the similarities to a fixed or learned threshold.References 83
Here, we take [ 39] as an example. The method first measures the cosine similarity 
between the prototypes and features of character instances, and then appends a learn￾able unknown score to the scores as the threshold. When all similarities are below 
the threshold, the max score falls upon the unknown label, yielding a rejection. Oth￾erwise, the model predicts the label associated with the prototype yields the highest 
score. The score-appending approach allows the model to be trained end-to-end with 
a simple cross-entropy loss. 
By Data Distribution People also make perceptions by how familiar the data is 
to the model, i.e., measuring how likely the data appear in the distribution that the 
model is trained on. 
One typical example would be Logbret [ 43], the method first trains a masked 
language model, specifically BERT [ 44], on normal logs. Then during inference, 
the method randomly masks out a portion of the testing log and has the language 
model to reconstruct the missing parts, those logs that failed during reconstruction 
are considered abnormal ones. 
Despite being less frequently seen, this category has one unique advantage: they 
do not need ANY annotations, hence these methods have cost advantages for those 
with sufficient data and computation resources. 
6.3 Cognition 
Well, some works decide to go a little beyond rejection and categorize novel classes 
without side-information, i.e., have the model, instead of humans, find different 
novel categories and provide definitions. This is a new domain with increasing meth￾ods [ 45– 47] exploiting. Specifically, [ 45] categorizes the novel classes by their 
responses to meta classes, while [ 46] seeks to describe unseen classes with natural 
language. 
References 
1. Yuan, G., Ho, C., Lin, C.: Recent advances of large-scale linear classification. Proc. IEEE 
100(9), 2584–2603 (2012) 
2. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence 
recognition and its application to scene text recognition. IEEE Trans. Pattern Anal. Mach. 
Intell. 39(11), 2298–2304 (2017) 
3. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 
27–November 2, 2019, pp. 4714–4722. IEEE (2019) 
4. Lu, J., Cao, Z., Wu, K., Zhang, G., Zhang, C.: Boosting few-shot image recognition via domain 
alignment prototypical networks. In: IEEE 30th International Conference on Tools with Artifi￾cial Intelligence, ICTAI 2018, 5–7 November 2018, Volos, Greece, pp. 260–264. IEEE (2018)84 6 Open-Set Text Recognition Implementations(III): Open-set Predictor
5. Zhang, J., Zhu, Y., Du, J., Dai, L.: Trajectory-based radical analysis network for online handwrit￾ten Chinese character recognition. In: 24th International Conference on Pattern Recognition, 
ICPR 2018, Beijing, China, August 20–24, 2018, pp. 3681–3686. IEEE Computer Society 
(2018) 
6. Wang, T., Xie, Z., Li, Z., Jin, L., Chen, X.: Radical aggregation network for few-shot offline 
handwritten Chinese character recognition. Pattern Recognit. Lett. 125, 821–827 (2019) 
7. Cao, Z., Lu, J., Cui, S., Zhang, C.: Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recognit. 107, 107488 (2020) 
8. Snell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning. In: Advances 
in Neural Information Processing Systems 30: Annual Conference on Neural Information 
Processing Systems 2017, December 4–9, 2017, pp. 4077–4087. Long Beach, CA, USA (2017) 
9. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 
A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from 
natural language supervision. In: Proceedings of the 38th International Conference on Machine 
Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning 
Research, vol. 139, pp. 8748–8763. PMLR (2021) 
10. Sanh, V., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, 
A., Raja, A., Dey, M., Bari, M.S., Xu, C., Thakker, U., Sharma, S.S., Szczechla, E., Kim, T., 
Chhablani, G., Nayak, N.V., Datta, D., Chang, J., Jiang, M.T., Wang, H., Manica, M., Shen, 
S., Yong, Z.X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, 
A., Févry, T., Fries, J.A., Teehan, R., Scao, T.L., Biderman, S., Gao, L., Wolf, T., Rush, A.M.: 
Multitask prompted training enables zero-shot task generalization. In: The Tenth International 
Conference on Learning Representations, ICLR 2022, Virtual Event, April 25–29, 2022 (2022) 
www.OpenReview.net 
11. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, 
P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. 
Learn. Res. 21(140, 67), 1–140 (2020) 
12. Qi, H., Brown, M., Lowe, D.G.: Low-shot learning with imprinted weights. In: 2018 IEEE 
Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, 
USA, June 18–22, 2018, pp. 5822–5830. IEEE Computer Society (2018) 
13. Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., Wierstra, D.: Matching networks 
for one shot learning. In: Advances in Neural Information Processing Systems 29: Annual 
Conference on Neural Information Processing Systems 2016, December 5–10, 2016, pp. 3630– 
3638. Barcelona, Spain (2016) 
14. Chen, W., Liu, Y., Kira, Z., Wang, Y.F., Huang, J.: A closer look at few-shot classification. 
In: 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, 
USA, May 6–9, 2019 (2019) www.OpenReview.net 
15. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
16. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022, pp. 4513–4522. IEEE (2022) 
17. Bateni, P., Barber, J., van de Meent, J., Wood, F.: Enhancing few-shot image classification with 
unlabelled examples. In: IEEE/CVF Winter Conference on Applications of Computer Vision, 
WACV 2022, Waikoloa, HI, USA, January 3–8, 2022, pp. 1597–1606. IEEE (2022) 
18. Lampert, C.H., Nickisch, H., Harmeling, S.: Attribute-based classification for zero-shot visual 
object categorization. IEEE Trans. Pattern Anal. Mach. Intell. 36(3), 453–465 (2014) 
19. Huang, G., Luo, X., Wang, S., Gu, T., Su, K.: Hippocampus-heuristic character recognition 
network for zero-shot learning in Chinese character recognition. Pattern Recognit. 130, 108818 
(2022) 
20. Ke, Y., Hagiwara, M.: Cnn-encoded radical-level representation for Japanese processing. Trans. 
Japanese Soc. Artif. Intell. 33(4), D–I23 (2018) 
21. Zu, X., Yu, H., Li, B., Xue, X.: Chinese character recognition with augmented character pro￾file matching. In: MM ’22: The 30th ACM International Conference on Multimedia, Lisboa, 
Portugal, October 10–14, 2022, pp. 6094–6102. ACM (2022)References 85
22. Chen, J., Li, B., Xue, X.: Zero-shot Chinese character recognition with stroke-level decompo￾sition. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 
IJCAI 2021, Virtual Event/Montreal, Canada, 19–27 August 2021, pp. 615–621 (2021). www. 
ijcai.org 
23. He, S., Schomaker, L.: Open set Chinese character recognition using multi-typed attributes 
(2018). [Online]. Available: http://arxiv.org/abs/1808.08993 
24. Chanda, S., Haitink, D., Prasad, P.K., Baas, J., Pal, U., Schomaker, L.: Recognizing Bengali 
word images–A zero-shot learning perspective. In: 25th International Conference on Pattern 
Recognition, ICPR 2020, Virtual Event / Milan, Italy, January 10–15, 2021, pp. 5603–5610. 
IEEE (2020) 
25. Chanda, S., Baas, J., Haitink, D., Hamel, S., Stutzmann, D., Schomaker, L.: Zero-shot learning 
based approach for medieval word recognition using deep-learned features. In: 16th Interna￾tional Conference on Frontiers in Handwriting Recognition, ICFHR 2018, Niagara Falls, NY, 
USA, August 5–8, 2018, pp. 345–350. IEEE Computer Society (2018) 
26. Zhang, J., Du, J., Dai, L.: Radical analysis network for learning hierarchies of Chinese char￾acters. Pattern Recognit. 103, 107305 (2020) 
27. Li, B., Tang, X., Qi, X., Chen, Y., Xiao, R.: Hamming OCR: A locality sensitive hashing 
neural network for scene text recognition (2020). [Online]. Available: https://arxiv.org/abs/ 
2009.10874 
28. Xu, X., Cao, H., Yang, Y., Yang, E., Deng, C.: Zero-shot metric learning. In: Proceedings of the 
Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, 
China, August 10–16, 2019, pp. 3996–4002 (2019) www.ijcai.org 
29. Kim, J., Kim, T., Kim, S., Yoo, C.D.: Edge-labeling graph neural network for few-shot learning. 
In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, 
CA, USA, June 16–20, 2019, pp. 11–20. Computer Vision Foundation/IEEE (2019) 
30. Yang, C., Liu, C., Yin, X.: Weakly correlated knowledge integration for few-shot image clas￾sification. Int. J. Autom. Comput. 19(1), 24–37 (2022) 
31. Liu, Y., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S.J., Yang, Y.: Learning to propagate 
labels: Transductive propagation network for few-shot learning. In: 7th International Confer￾ence on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6–9 (2019) www. 
OpenReview.net 
32. Joseph, K.J., Khan, S.H., Khan, F.S., Balasubramanian, V.N.: Towards open world object 
detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, 
virtual, June 19–25, 2021, pp. 5830–5840. Computer Vision Foundation/IEEE (2021) 
33. Ma, S., Wang, Y., Wei, Y., Fan, J., Li, T.H., Liu, H., Lv, F.: CAT: localization and identification 
cascade detection transformer for open-world object detection. In: IEEE/CVF Conference on 
Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17–24, 
2023, pp. 19 681–19 690. IEEE (2023) 
34. Bendale, A., Boult, T.E.: Towards open set deep networks. In: 2016 IEEE Conference on 
Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27–30, 
2016, pp. 1563–1572. IEEE Computer Society (2016) 
35. Bao, W., Yu, Q., Kong, Y.: Opental: Towards open set temporal action localization. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022, pp. 2969–2979. IEEE (2022) 
36. Du, Y., Wei, F., Zhang, Z., Shi, M., Gao, Y., Li, G.: Learning to prompt for open-vocabulary 
object detection with vision-language model. In: IEEE/CVF Conference on Computer Vision 
and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18–24, 2022, pp. 14 064– 
14 073. IEEE (2022) 
37. Chen, M., Gao, J., Xu, C.: Cascade evidential learning for open-world weakly-supervised tem￾poral action localization. In: IEEE/CVF Conference on Computer Vision and Pattern Recog￾nition, CVPR 2023, Vancouver, BC, Canada, June 17–24, 2023, pp. 14 741–14 750. IEEE 
(2023) 
38. Ding, C., Pang, G., Shen, C.: Catching both gray and black swans: Open-set supervised anomaly 
detection. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 
2022, New Orleans, LA, USA, June 18–24, 2022, pp. 7378–7388. IEEE (2022)86 6 Open-Set Text Recognition Implementations(III): Open-set Predictor
39. Liu, C., Yang, C., Yin, X.: Open-set text recognition via shape-awareness visual reconstruction. 
In: Document Analysis and Recognition–ICDAR 2023–17th International Conference, San 
José, CA, USA, August 21–26: Proceedings, Part VI, ser. Lecture Notes in Computer Science, 
vol. 14192, pp. 89–105. Springer (2023) 
40. Han, J., Ren, Y., Ding, J., Pan, X., Yan, K., Xia, G.: Expanding low-density latent regions 
for open-set object detection. In: IEEE/CVF Conference on Computer Vision and Pattern 
Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 9581–9590. IEEE 
(2022) 
41. Huang, S., Ma, J., Han, G., Chang, S.: Task-adaptive negative envision for few-shot open-set 
recognition. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 
2022, New Orleans, LA, USA, June 18–24, 2022, pp. 7161–7170. IEEE (2022) 
42. Ning, K., Zhao, X., Li, Y., Huang, S.: Active learning for open-set annotation. In: IEEE/CVF 
Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, 
USA, June 18–24, 2022, pp. 41–49. IEEE (2022) 
43. Guo, H., Yuan, S., Wu, X.: Logbert: Log anomaly detection via BERT. In: International Joint 
Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18–22, 2021, pp. 1–8. 
IEEE (2021) 
44. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional 
transformers for language understanding. In: Proceedings of the 2019 Conference of the North 
American Chapter of the Association for Computational Linguistics: Human Language Tech￾nologies, vol. 1 (Long and Short Papers), pp. 4171–4186. Association for Computational Lin￾guistics (2019) 
45. Kim, G., Kang, J., Han, B.: Open-set representation learning through combinatorial embed￾ding. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, 
Vancouver, BC, Canada, June 17–24, 2023, pp. 19 744–19 753. IEEE (2023) 
46. Long, Y., Wen, Y., Han, J., Xu, H., Ren, P., Zhang, W., Zhao, S., Liang, X.: Capdet: Unifying 
dense captioning and open-world detection pretraining. In: IEEE/CVF Conference on Com￾puter Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17–24, 2023, 
pp. 15 233–15 243. IEEE (2023) 
47. Cao, T., Wang, Y., Xing, Y., Xiao, T., He, T., Zhang, Z., Zhou, H., Tighe, J., PSS: progressive 
sample selection for open-world visual representation learning. In: Computer Vision–ECCV 
2022–17th European Conference, Tel Aviv, Israel, October 23–27,: Proceedings, Part XXXI, 
ser. Lecture Notes in Computer Science, vol. 13691, pp. 278–294. Springer (2022)Chapter 7 
Open-Set Text Recognition: Case-Studies 
Abstract This chapter surveys two specific open-set text recognition methods, 
OSOCR and OpenCCD, which support the full feature set of the open-set text recog￾nition tasks, specifically novel character spotting and novel character recognition. 
The two approaches serve as examples to make case studies of the framework, and 
may also serve as baseline solutions for the interested readers with the code and 
models publicly available. For each method, we introduce its motivation, overall 
design, implementation of individual modules, and training techniques including 
regularization terms and practical label sampling tricks. This chapter also includes a 
summarization of the performances of the two models, together with some additional 
results from the released code. 
Keywords Case-study · Novel character spotting · Novel character recognition 
7.1 OSOCR 
7.1.1 Framework Implementation Overview 
In OSOCR [ 1], Liu et al. propose a label-to-representation mapping framework 
(Fig. 7.1) as a baseline of the proposed OSTR task, while retaining competitive per￾formance on standard close-set [ 2] and zero-shot Chinese character [ 3] benchmarks. 
The proposed framework is composed of four main parts, namely the Recognition 
Backbone (Sect. 7.1.2), the Label-to-prototype Module (Sect. 7.1.3), the Open-set￾Predictor (Sect. 7.1.4), and the Label Sampler (Sect. 7.1.5). 
Specifically, the Recognition Backbone locates, sorts, and extracts visual features 
.F : Rtm×d of each character in the input sample. Most conventional text recognition 
methods can be used for this part after removing the linear classifier. 1
The label-to-representation mapping module provides a tractable mapping. H from 
side-information of a character label to its corresponding prototypes. P, each defining 
the center of a corresponding “case” of the character. E.g.,.H(“a”. ) returns two proto￾1 Methods with a recurrent decoder [ 4, 5] need an extra change on the “hidden state” to remove the 
dependency on history predictions. 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_7 
8788 7 Open-Set Text Recognition: Case-Studies
Fig. 7.1 Evaluation phase and optimization phase of our proposed framework. During evaluation, 
novel and seen characters are mapped corresponding prototypes via the label-to-prototype module, 
which is cached for all evaluation samples. During optimization, a label sampler is introduced to 
produce unknown labels by mini-batching the seen characters. The unknown labels “[-]” are used to 
train the rejection threshold.s− for “out-of-set” characters. The image feature is then extracted and 
serialized with the text recognition backbone. Finally, the open-set predictor computes the similarity 
.S˜
t between the extracted feature .Ft and prototypes . P, and then reduces to the character score 
vector. St
types, one for uppercase “A” and another for lowercase “a”. The Open-set Predictor 
is then used to categorize or reject each input character feature.F[L] according to the 
decision boundaries, defined with corresponding prototypes and the globally shared 
radius.sunk . Finally, the Label Sampler is proposed for the training phase to produce 
out-of-set samples and reduce the training burden of the label-to-representation map￾ping module by sampling a reasonable portion of labels from all training characters 
.Ctrain at each iteration. 
7.1.2 Topology-Preserving Transformation Network 
For the Recognition Backbone, Liu et al. implement a Topology-Preserving Transfor￾mation Network (TPTNet) adapted from the Decoupled Attention Network (DAN). 
Wang et al. [ 6] Like DAN, the TPTNet first extracts visual feature.M with a ResNet 
backbone, then a Concurrent Attention Module (CAM) is adopted to sample the 
character feature sequence.F : (F1, ..., Ft, ..., Ftm ) from. M. The RNN used to model 
the contextual information is removed for speed-performance trade-off. 
In OSOCR [ 1], a fused-in feature-level mesh-grid rectification is proposed to alle￾viate the common vulnerability of perspective transformations and various irregular 
distortions. The fused block (Fig. 7.2) takes a feature map.Mi as input, and produces 
a processed and rectified feature map.Mo as output. Specifically, the feature map is 
first processed into a local feature map.Ml with a convolutional block, 
.Ml = Conv(Mi). (7.1)7.1 OSOCR 89
Fig. 7.2 Topology-preserving transformation: Each input feature map is first processed into the 
local feature map.Ml with a convolutional block (FE-Conv). Next, Liu et al. estimate the effective 
feature density with .DX and .DY modules. Then, Liu et al. integrate the density into destination 
coordinates. Finally, Liu et al. rectify.Ml with a grid sampler according to the destination coordinates 
Next, Liu et al. estimate the feature density .Dx ∈ Rw×h
+ on the .X direction and 
.Dy ∈ Rw×h
+ on the. Y direction with corresponding functions.Dx and.Dy, 
.
Dx = Sigmoid(FCx(Ml)) + b := Dx(Ml),
Dy = Sigmoid(FCy(Ml)) + b := Dy(Ml), (7.2) 
where.FC indicates.1 × 1 convolution layers and. τ is a constant number controlling 
the extent of rectification that the module can perform. The densities.Dx and.Dy are 
then integrated and normalized into the coordination mapping.I ∈ Rw×h×2
+ , 
.I[H,w] = [w
∑w
∑
i=0 Dx[H,i]
wm
i=0 Dx[H,i]
, h
∑h
j=0 Dy[J,w]
∑hm
j=0 Dy[J,w]
]. (7.3) 
Specifically,. I maps the coordination from the rectified feature map.Mo to the source 
feature map .Ml , and the mesh-grid transformation is implemented by sampling . Ml
into .Mo according to . I with a linear sampler. As the density is always greater than 
zero, the integration is able to preserve the input topology, i.e., lines without inter￾sections will not intersect after the transformation and vice versa. The gradients of 
coordinates are computed similarly to the deformable convolution network in [ 7]. 
The estimation of transformation parameters . I shares most computation with the 
feature extraction, avoiding costly dedicated rectification networks in conventional 
approaches [ 8, 9]. 
7.1.3 Label-to-Representation Mapping Module 
The key point of open-set text recognition is to construct a tractable mapping . H
from the label space . C to the representation space . F, which can be applied to both 
seen and novel characters. Note .F could potentially be a one-to-many mapping as 
a character may have multiple cases. In OSOCR [ 1], Liu et al. propose the label￾to-representation mapping module decomposing .H into two mappings, namely .R90 7 Open-Set Text Recognition: Case-Studies
and. E, by introducing the intermediate template space.T : R32×32 containing glyphs 
drawn from the Noto-font. 2
Here, .R : C → T maps each character in the character set . C to the glyphs of all 
its cases. “Case” here refers to different forms of a character, e.g., upper case and 
lower case, simplified Chinese and traditional Chinese, Hiragana and Katakana, and 
different contextual shapes as in Arabic. As the Noto-font covers most characters 
in many languages with a uniform style, .R is highly likely to generalize to novel 
characters not appearing in the training set. Also,. R requires little domain knowledge 
of the specific languages, e.g., part information and composition information for each 
character. 
.E : T → F is the mapping from the template space to the prototype space. The 
mapping is implemented with the ProtoCNN module, which includes a ResNet18 net￾work, a normalization layer, and a trainable latent prototype.P[s] for the end-of-speech 
“character” (“[s]”). Prototypes of normal characters are generated by encoding and 
normalizing the corresponding glyphs with the ResNet, 
. P[j] = Res18(T[j])
|Res18(T[j])|
. (7.4) 
The normalization is conducted to alleviate the effect of the potential bias of character 
frequency between the training set and the testing set. For . E to be generalizable 
to novel classes, the templates of seen characters need to fill the template space 
.T densely enough, which requires the framework to be trained on languages with 
large character sets. As the normalized prototypes can be cached before evaluation 
and updated incrementally on character set change, the evaluation-time overhead is 
negligible. 
7.1.4 Open-Set Predictor 
In OSOCR [ 1], an open-set predictor is proposed to replace the linear classifiers 
used in conventional text recognition methods [ 2]. Like the classifiers in the weight 
imprinting methods [ 10], the open-set predictor performs classification for charac￾ters in .Ck
test by comparing the generated prototypes to visual features. The module 
further adopts closed decision boundaries [ 11, 12] to achieve rejection on out-of-set 
characters (in.Cu
test) not similar to any provided prototypes. 
Specifically, the open-set predictor first computes the “case-specific” similarity 
scores .S˜ ∈ Rtm×|P| via the scaled product of the visual feature .F ∈ Rtm×d and the 
normalized prototype matrix.P ∈ R|C|×d , 
.S˜ = αF P. (7.5)
2 The Noto fonts for most languages are available at https://noto-Liuet.al.bsite-2.storage.googleapis. 
com/pkgs/Noto-unhinted.zip. 7.1 OSOCR 91
Here, the similarity score of the feature at timestamp. t and the. nth prototype can be 
written as 
.
S˜[t,j] =αF[t]P[J ] = α|P[J ]||F[t]|cos(P[J ], F[t])
=α|F[t]|cos(P[J ], F[t]), ∀ j |P[J ]| = 1. (7.6) 
Since.α|F[t]| is a constant number given a certain timestamp. t,.S˜[t] can be interpreted 
as the scaled cosine similarities between all prototypes in .P and the visual feature 
.F[t]. Thus,. S˜ is detonated as the similarity score. Due to the exponential operator in 
the Softmax function,.α|F[t]| controls how much the predicted probability would be 
close to one-hot. Hence,.|F[t]| is interpreted as the confidence of timestamp. t, and. α
interprets as the overall model confidence. Also, since .|P[j]| can be interpreted as 
the overall “preference” of prototype . j, the prototypes are normalized in the label￾to-representation mapping module to alleviate potential frequency-related bias. 
The module then applies max reduction on case-wise similarity. S˜, producing the 
label-wise classification scores.Sc ∈ Rtm×(|C|+1)
, i.e., 
.Sc
[t,i] = max
{j|φ(j)=i}
(S˜[t,j]), (7.7) 
where . φ is the label function that reduces all .|P| prototypes to .|e| + 1 labels, and 
the extra one comes from the “end-of-speech” token. Specifically, .φ(j) returns the 
character which has the . jth template as one of its cases. Combining this strategy 
with the mapping operation in the label-to-prototype module allows us to train the 
framework with case agnostic annotations, without explicitly aligning cases with 
drastic appearance differences to the same region on the prototype space . F, e.g., 
‘A’ and ‘a’. The module rejects samples not similar to any provided prototypes by 
predicting them into the out-of-set label “[-]”. The out-of-set label is not associated 
with a dedicated prototype as it covers many characters that do not share common 
visual traits. Instead, the module uses a trainable score .sunk attached to .S˜
t as the 
similarity score for the out-of-set label at each timestamp. t, yielding the final score 
vector.S ∈ Rtm×(|C|+2)
, 
.S[t] = [Sc
[t,1], Sc
[t,2], ..., Sc
[t,|e|+1],sunk ]. (7.8) 
During training, the classification loss pushes .s− above other scores on out-of-set 
characters, for “in-set” characters, .s− is pushed below the score of correct classes. 
During evaluation, out-of-set characters would yield similarities less than.s− to any 
prototypes due to the shape difference, yielding a rejection. Hence, .s− can also be 
interpreted as the radius of the decision boundary [ 11], or a similarity threshold.92 7 Open-Set Text Recognition: Case-Studies
Fig. 7.3 The proposed label 
sampler. The module 
samples a subset of the seen 
characters according to the 
labels for each training 
iteration 
7.1.5 Optimization 
In OSOCR [ 1], a label sampler (illustrated in Fig. 7.3) is proposed to introduce out￾of-set samples and control complexity during training. For each batch of training 
data, the sampler constructs a character set .Cbatch ⊂ C, composed of three disjoint 
subsets: the “positive” characters.Cpos, the “negative” characters.Cneg, and the special 
tokens. 3
Here, the module first collects all characters that appear in the batch, denoted as 
.Clabel . Then, it samples a fraction of the .Clabel as the “positive” subset .Cpos sub￾jecting to two conditions. First, a fraction of characters shall be left unsampled as 
the out-of-set classes, i.e., .|Cpos|≤|Clabel| ∗ fs. Second, the number of all proto￾types corresponding to.Cpos should be smaller than the batch size limitation.bmax . To 
cover more characters in the label space. C, characters that don’t appear in the labels 
(.Ctrain − Clabel) are sampled into the “negative” subset .Cneg, with its size limited 
under.bmax − |Cpos| to keep the computation cost under control. Finally,.Cpos,.Cneg, 
and special labels are merged as the final character set .Cbatch for the iteration. The 
label of each sample is then filtered with .Cbatch , by replacing all characters not in 
.Cbatch with the out-of-setlabel “.[−]”. 
The framework is trained with the images, filtered labels, and templates associated 
with .Cbatch . Like most attention-based word-level text recognition methods [ 2, 6, 
13], a cross-entropy loss.Lce is used for the classification task, 
.Lce = − 1
tm
∑tm
t
log(
|
∑
C|+2
i
Y[t,i]eS[L,i]
∑|C|+2
j eS[t,j]
), (7.9) 
where.Y[t] ∈ {0, 1}|C|+2 is the one-hot encoded label at timestamp. l..tm indicates the 
number of characters in the sample (including the EOS character “[s]”), and the 
maximum length allowed by the model is denoted as. tm.
3 The end-of-speech label “[s]” and the unknown label “.[−]”. 7.2 Character-Context Decoupling 93
A regularization term.Lemb is adopted to improve the margins between classes, 
. Lemb =
∑
|P|
i,j
Relu((PT P)[I,j] − mp). (7.10) 
Here.mp is the cosine similarity between the nearest pairs from. n evenly distributed 
prototypes on a. d dimension space. “Evenly distributed” means the distance between 
each prototype and its nearest neighbor equals the same number. Here, the margin, 
.mp, is used to preserve space for novel classes and can be approximately solved via 
gradient descent. We estimate.mp by optimizing the following formula, 
.
Q∗ = arg minQ
max
(i,j)
(QT Q − 2I)i,j,
mp = max
(i,j)
(Q∗T Q∗ − 2I)i,j, (7.11) 
where.Q is a.d × n matrix with the random initialization, each column in.Q is a unit 
vector, . n is the estimated number of prototypes in the open-set, . d is the dimension 
of the prototypes, and . I is an identity matrix. In OSOCR [ 1], . d is set to .512 and N 
to.50, 000. That is to say, Liu et al. estimate that character space. C contains. 50, 000
distinct “cases”. The optimization result of Eq. 14 suggests.mp is around.0.14. 
The object function .Lmodel is the combination of the classification loss .Lce and 
the regularization term.Lemb, and there is 
.Lmodel = Lmodel + λembLemb. (7.12) 
7.2 Character-Context Decoupling 
In literature [ 14], a character-context decoupling framework (shown in Fig. 7.4) to 
reduce the impact of contextual information bias under open-set scenarios, by sep￾arating and isolating character visual information and contextual information with 
Fig. 7.4 The OpenCCD framework [ 14]94 7 Open-Set Text Recognition: Case-Studies
the Detached Temporal Attention module and the Decoupled Context Anchor mech￾anism. The framework and its optimization are first formulated in Sect. 7.2.1. Then, 
a detailed explanation of the less intuitive Decoupled Context Anchor mechanism 
is presented in Sect. 7.2.2. Finally, the Open-set Character-Context Decoupling net￾work (OpenCCD) is given as an example implementation of OpenCCD in Sect. 7.2.3. 
In literature [ 14]. 
7.2.1 Character-Context Decoupling Framework 
The framework takes a sample (word-level image).img and a character set. E as input, 
and outputs the predicted word .Yˆ : (Yˆ [0], ..., Yˆ [t]) with the maximum probability 
given the sample and character set, 
. Yˆ = arg maxY
P(Y|x, E; θ ), (7.13) 
where. x is the visual feature representation of all characters in the sample. We omit 
the .E and . θ in the following part for writing convenience. In OpenCCD, Liu et al. 
expand.P(Y|x) with a predicted length. l, using the law of total probability, 
. P(Y|x) =
maxL
∑
l=1
P(l|x)P(Y|x,l), (7.14) 
where .maxL is the maximum length of a word. Different from most existing text 
recognition frameworks using end-of-speech [ 6, 13], segmentation [ 15, 16], or 
blanks [ 17, 18] to handle lengths, OpenCCD explicitly predicts the length.. P(Y|x,l)
can be further decomposed to contextual prediction and visual prediction via the pro￾posed Decoupled Context Anchor mechanism (detailed in Sect. 7.2.2), 
.
P(Y|x,l)
=
∏t
i=1
Pr(Yˆ [i]|x[i])
∏t
i=1
{ c∈C[t]
Pr(Yˆ [t]|c)Pr(c|x, t). (7.15) 
Here, . c is the common “context” (linguistic information) of characters, . x models 
the visual information of all characters in the input image, and .x[t] corresponds to 
the character visual information of the . tth character. Hence, the optimization goal 
would be maximizing the log-likelihood .log P(Y∗|x) of the ground truth label 
sequence.Y∗,7.2 Character-Context Decoupling 95
.
log P(Y∗|x)
=log(
maxL
∑
l=1
P(l|x)P(Y∗|x,l))
(a)
= log P(l
∗|x) + log P(Y∗|x,l
∗)
=log P(l
∗|x) +
l
∑∗
t=1
(log P(Y∗
[t]|x[t]))
+
l
∑∗
t=1
log(
{ c∈C[t]
P(Y∗
[t]|c)P(c|x,l
∗))
:= − (Llen + Lvis + Lctx ),
(7.16) 
where .Llen, .Lvis, and .Lctx are the corresponding cross-entropy losses of the three 
log-likelihood terms. Step (a) holds because the correct label can only be predicted 
when the length is correctly predicted. 
7.2.2 Decoupled Context Anchor Mechanism 
In OpenCCD, Liu et al. propose a Decoupled Context Anchor mechanism to model 
and separate the effect of linguistic information. c over character.Yˆ [t] at each times￾tamp. t. 
Assumption 1 (A1) 
We assume the linguistic information functions as a common cause of the input visual 
information and the prediction outputs at all timestamps (See Fig. 7.5). We model 
the sample image as a “rendered” result of the label . Y. Also, Liu et al. assume the 
label (words) is generated according to linguistic information . c, making the label 
.Y a causal result of . c. Hence, linguistic context . c and the character-level visual 
information .x[t] are the only two direct factors affecting the probability of .Yˆ [t] at 
timestamp. t, 
.P(Yˆ [t]|x[t], x, Yˆ [t−1]...Yˆ [0],l, c) = P(Yˆ [t]|x[t], c). (7.17)96 7 Open-Set Text Recognition: Case-Studies
Fig. 7.5 Causal graph of the Decoupled Context Anchor mechanism [ 14] 
Assumption 2 (A2) 
The shape (character visual information) of a character and its context (linguistic 
information) are independent given the character.y[t], i.e., 
.
P(x[t]|Yˆ [t], c) = P(x[t]|Yˆ [t])
⇐⇒ P(x[t], c|Yˆ [t]) = P(x[t]|Yˆ [t])P(c|Yˆ [t]).
(7.18) 
This assumption implies that the linguistic information does not affect the “style” 
(font face, color, background, etc.) of the word, which generally holds in most syn￾thetic datasets where styles and contents are randomly matched. 
Theorem 1: The Anchor Property of Context 
Given assumption A1, the probability of a predicted word .Yˆ given image . x and its 
length . l, .P(Yˆ |x,l), can be written as the product of the “anchored predictions” of 
all timestamps, i.e., 
. P(Yˆ |x,l) = ∏
l
t=1
{ c∈C[t]
P(Yˆ [t]|x[t], c)P(c|x,l), (7.19) 
and the proof is detailed in Sect. 7.2.4. Here, the integral term can be interpreted 
as an ensemble of “anchored prediction” .P(Yˆ [t]|x[t], c) over all possible contexts 
. c, which is similar to the hidden anchor mechanism [ 19]. Hence, Liu et al. call this 
theorem the anchor property of context.7.2 Character-Context Decoupling 97
Theorem 2: The Separable Property of Linguistic information and Character 
Visual Information 
Given Assumption A2, the effect of character visual information over the label 
.P(Yˆ [t]|x[t]) and the effect of linguistic information.P(Yˆ [t]|c) is separable from con￾textual prediction.P(Yˆ [t]|x[t], c), 
. P(Yˆ [t]|x[t], c) ∝
P(Yˆ [t]|x[t])P(Yˆ [t]|c)
P(Yˆ [t]) . (7.20) 
Here,.P(Yˆ [t]|x[t])represents the predicted probability of.Yˆ [t] with regard to character 
visual information.x[t],.P(Yˆ [t]|c) models the effect caused by linguistic information, 
and .P(Yˆ [t]) models the character frequency on the training set. The proof of this 
theorem is given in Sect. 7.2.5. This theorem suggests that the effect of character 
visual information and linguistic information over the prediction can be explicitly 
separated under specific conditions. 
Intuitively, .P(Yˆ [t]|c) “explains away” [ 20] the linguistic information from the 
visual-based prediction.P(Yˆ [t]|x[t]). This behavior happens in the backpropagation 
pass of OpenCCD during training, where the gradients of .Lctx and .Lvis are accu￾mulated to update the feature extractor. This is the reason that .Lctx needs to be 
backpropagated, and also makes .Lctx a regularization term, in terms of enforcing 
certain properties of the network via backpropagation. This property differentiates it 
from the “look-twice” mechanisms [ 21, 22] that cut gradients. 
Theorem 3: Decoupled Context Anchor Mechanism 
Combining Theorems 1 and 2, Liu et al. have the Decoupled Context Anchor mech￾anism, 
.
P(Yˆ |x,l)
=
∏t
i=1
Pr(Yˆ [i]|x[i])
∏t
i=1
{ c∈C[t]
Pr(Yˆ [t]|c)Pr(c|x, t). (7.21) 
Proof of this theorem can be found in Sect. 7.2.6. The mechanism further allows 
explicit separation of linguistic information and character visual information on the 
word level, which provides a way to model and separate linguistic information 
learned on the training set, resulting in a feature extractor focusing more on char￾acter visual information and less affected by the training set linguistic information. 
Considering the anchor property revealed in Theorem 1 and the decoupling nature of 
Theorem 2, Liu et al. call this mechanism the Decoupled Context Anchor mechanism.98 7 Open-Set Text Recognition: Case-Studies
7.2.3 OpenCCD Network 
In this section, the Open-set Character-Context Decoupling network (OpenCCD, 
Fig. 7.2) is given as an example implementation. Here, character set . E : (Ev, Ec)
consists of glyphs from the Noto-font.Ev and semantic embeddings of the characters 
.Ec. The network first extracts visual features of the word images.img and the glyphs 
.Ev with the 45-layer ResNet built with DSBN [ 23] layers (Res45-DSBN). It shares 
the convolutional layers between the glyphs and word images while keeping task￾specific batch statistics. Three levels of word features .(Fl, Fm, Fh) and the latest 
feature map.Fg
h of glyphs are used. The prototypes (class centers).Wv are generated 
by applying geometric attention to .Fg
h . During training, Liu et al. mini-batch . Ev
at each iteration to achieve a reasonable training speed. During the evaluation, the 
visual prototypes.Wv for the whole dataset are cached beforehand, hence prototype 
generation yields little extra costs. 
Next, the Detached Temporal Attention (DTA) module is used to predict the length 
of the word .P(l|x), and the max probable length is detonated as . ˆl. Then the DTA 
module samples ordered character-level visual features .x : (x[0], ..., x[ˆl]
) from the 
feature map.Fh. 
The visual-based prediction.P(Yˆ [t]|x[t]) is then produced by the open-set classi￾fier. For close-set scenarios, the linguistic information oriented prediction. P(Yˆ [t]|c)
is produced via the Decoupled Context Anchor (DCA) module. For open-set sce￾narios where linguistic information is intractable, .P(Yˆ [t]|c) is treated as a uniform 
distribution, which is equivalent to only using the visual prediction. 
Detached Temporal Attention 
In OpenCCD, the detached temporal attention module (Fig. 7.6) is proposed to pre￾dict the sequence length.P(l|x). It also sorts and samples characters in feature map 
.Fh via the attention map . A. The module utilizes an FPN to model global tempo￾ral information from the input feature maps and decodes them into .A and .P(l|x). 
Since temporal information is not related to individual character shapes (character 
visual information), the module novelly isolates it from input visual feature maps 
by cutting the gradients w.r.t..P(l|x) and. A. The module then segments input visual 
feature map.Fh into visual features. x for individual characters according to attention 
map .A and the most probable length . ˆl, allowing only character visual information 
backpropagating to.Fh via. x. 
In OpenCCD,.P(Yˆ [t]|x[t])is produced by comparing the prototypes with character￾level features.x[t], 
.P(Yˆ [t]|x[t]) ∝
{
α|x[t]| Yˆ [t] is [UNK]
|x[t]|Sim(x[t], Yˆ [t]) otherwise, (7.22)7.2 Character-Context Decoupling 99
Fig. 7.6 The proposed detached temporal attention module. We isolate sequence modeling within 
the Temporal Attention module, and zero the gradient of convolution features, w.r.t., the temporal 
attention map. Here, GAP indicates a global average pooling [ 14] 
where.|x[t]| is the L2-Norm of.x[t],“[UNK]” indicates unknown characters, and. α is 
a trainable similarity threshold for rejection..Sim(x[t], Yˆ [t]) is defined as 
.Sim(x[t], Yˆ [t]) := max
wv∈ψ(Yˆ [t])
(cos(wv, x[t])), (7.23) 
where .ψ returns all prototypes .ψ(Yˆ [t]) ⊂ Wv associated with label .Yˆ [t], and each 
individual prototype.wv corresponds to a “case” of character.Yˆ [t]. 
Decoupled Context Anchor 
Instead of implementing a Variational Auto Encoder [ 24] to estimate the distribu￾tion of linguistic information and estimate the integral with Monte-Carlo, Liu et al. 
approximate the integral with predicted context. cˆ, which is similar to the conventional 
anchor mechanisms using only the anchor with maximum prediction likelihood [ 25, 
26], 
.
{ c∈C[t]
P(Yˆ [t]|c)P(c|x,l) ≈P(Yˆ [t]| ˆc). (7.24) 
Combined with Eq. 7.33, the probability of a predicted character at timestamp. t can 
be approximated as 
. P(Yˆ [t]|x) ≈ P(Yˆ [t]|x[t])P(Yˆ [t]| ˆc). (7.25) 
As linguistic information is mostly related to labels, Liu et al. estimate the lin￾guistic information . cˆ from the predicted label instead of the feature map. More 
specifically, the module reuses the estimated character probability distribution 
.Y ∈ (0, 1)
l×M : (P(Y[0]|x[0]), ..., P(Y[l]|x[l])) with regard to character visual infor-100 7 Open-Set Text Recognition: Case-Studies
mation at each timestamp . t, and .P(Y[t]|x[t]) : (P(Yˆ 0
[t]|x[t]), ..., P(Yˆ M
[t]|x[t])) is the 
probability distribution of all characters at timestamp. t. Then. cˆ is estimated with a 
4-layer transformer encoder [ 27] applied on the expectation of character embeddings, 
. cˆ =Trans(Y Ec), (7.26) 
where .Ec ∈ RM×C is the semantic embedding of seen characters in the training 
set, hence .Y Ec interprets as expectation. .Trans indicates the 4-layer transformer 
encoder. Finally,.P(Y[t]| ˆc) is estimated by comparing character embedding.Ec to. cˆ, 
.P(Y[t]| ˆc) = σ (cEˆ T
c )[t], (7.27) 
where. σ is the softmax function. 
Optimization 
With Eq.7.24 reducing the integral down to a standard classification problem, . Lctx
in Eq. 7.16 can be implemented as a cross-entropy loss like .Llen and .Lvis. Hence, 
OpenCCD can be optimized with the three equally weighted cross-entropy losses. 
7.2.4 Proof of Theorem 1 
Assumption 1 (A1) 
We assume the linguistic information functions as a common cause of the input visual 
information and the prediction outputs at all timestamps (See Fig. 7.5). We model 
the sample image as a “rendered” result of the label . Y. Also, Liu et al. assume the 
label (words) is generated according to linguistic information . c, making the label 
.Y a causal result of . c. Hence, linguistic context . c and the character-level visual 
information .x[t] are the only two direct factors affecting the probability of .Yˆ [t] at 
timestamp. t, 
.
Pr(Yˆ [t]|x[t], x, Yˆ [t−1]...Yˆ [0],l, c)
(a)
:=Pr(Yˆ [t]|x[t], x, Pre[t],l, c)
=Pr(Yˆ [t]|x[t], c),
(7.28) 
where Liu et al. denote prefix of.Yˆ [t] as.Pre[t] in (a).7.2 Character-Context Decoupling 101
Proof 
.
Pr(Yˆ |x,l)
=
∏
l
t=1
Pr(Yˆ [t]|x,l, Yˆ [t−1]...Yˆ [0])
=
∏
l
t=1
{ c∈C[t]
Pr(Yˆ [t]|x, Pre[t],l, c)Pr(c|x,l)
(a)
=∏
l
t=1
{ c∈C[t]
Pr(Yˆ [t]|x[t], c)Pr(c|x,l).
(7.29) 
Here, (a) is derived by applying Eq. 7.28 of Assumption A1. The integral term 
can be interpreted as an ensemble of “anchored prediction” .Pr(Yˆ [t]|x[t], c) over all 
possible contexts . c, which is similar to the hidden anchor mechanism [ 19]. Hence, 
Liu et al. call this theorem the anchor property of context. 
7.2.5 Proof of Theorem 2 
Assumption 2 (A2) 
The shape (character visual information) of a character and its context (linguistic 
information) are independent given the character.y[t], i.e., 
.
Pr(x[t]|Yˆ [t], c) = Pr(x[t]|Yˆ [t])
⇐⇒ Pr(x[t], c|Yˆ [t]) = Pr(x[t]|Yˆ [t])Pr(c|Yˆ [t]).
(7.30) 
Theorem 2: The Separable Property of Linguistic Information and Character Visual 
Information 
Given assumption A2 holds, the effect of character visual information over label 
.Pr(Yˆ [t]|x[t]) and the effect of .Pr(Yˆ [t]|c) can be separated from contextual prediction 
.Pr(Yˆ [t]|x[t], c): 
.
Pr(Yˆ [t]|x[t], c)
∝
Pr(Yˆ [t]|x[t])Pr(Yˆ [t]|c)
Pr(Yˆ [t])
:=likelihood(Yˆ [t], x[t], c)
(7.31)102 7 Open-Set Text Recognition: Case-Studies
Proof 
.
Pr(Yˆ [t]|x[t], c)
=Pr(x[t], Yˆ [t], c)
Pr(x[t], c)
=Pr(x[t], c|Yˆ [t])Pr(Yˆ [t])
Pr(x[t], c)
(a)
=Pr(x[t]|Yˆ [t])Pr(c|Yˆ [t])Pr(Yˆ [t])
Pr(x[t], c)
= Pr(Yˆ [t])
Pr(x[t], c)
Pr(x[t]|Yˆ [t])Pr(c|Yˆ [t])
= Pr(Yˆ [t])
Pr(x[t], c)
Pr(Yˆ [t]|x[t])Pr(x[t])
Pr(Yˆ [t])
Pr(Yˆ [t]|c)Pr(c)
Pr(Yˆ [t])
=Pr(Yˆ [t]|c)Pr(Yˆ [t]|x[t])
Pr(x[t])Pr(c)
Pr(x[t], c)Pr(Yˆ [t])
=Pr(Yˆ [t]|x[t])Pr(Yˆ [t]|c)
Pr(Yˆ [t])
Pr(x[t])
Pr(x[t]|c)
=likelihood(Yˆ [t], x[t], c)
Pr(x[t])
Pr(x[t]|c)
=likelihood(Yˆ [t], x[t], c)
Pr(x[t])
∑C
Yˆ
'
[t]
Pr(x[t]|Yˆ
'
[t], c)Pr(Yˆ
'
[t]|c)
(b)
=likelihood(Yˆ [t], x[t], c)
Pr(x[t])
∑C
Yˆ
'
[t]
Pr(x[t]|Yˆ
'
[t])Pr(Yˆ
'
[t]|c)
(c)
= likelihood(Yˆ [t], x[t], c)
∑C
Yˆ
'
[t]
likelihood(Yˆ
'
[t], x[t], c)
(d)
∝likelihood(Yˆ [t], x[t], c).
(7.32) 
Here,.charset is the character set. Steps (a) and (b) are derived using Eq. 7.30 in 
assumption A2. Step (c) is derived by applying Bayesian rule over.P(x[t]|Yˆ
'
[t]) and 
canceling.x[t]. Although.
∑charset
Yˆ
'
[t]
Pr(Yˆ
'
[t], x[t], c) is not a constant number and may 
vary with timestamp . t, but it is the same for all label .Yˆ
'
[t] at a certain timestamp . t, 
hence Step (d) holds, despite the constant factor can change.7.2 Character-Context Decoupling 103
7.2.6 Proof of Theorem 3 
Combining Theorems 1 and 2, Liu et al. have the decoupled context anchor mecha￾nism, 
.
Pr(Yˆ |x,l)
=
∏t
i=1
Pr(Yˆ [i]|x[i])
∏t
i=1
{ c∈C[t]
Pr(Yˆ [t]|c)Pr(c|x, t). (7.33) 
Proof 
.
Pr(Yˆ |x,l) = ∏
l
t=1
{ c∈C[t]
Pr(Yˆ [t]|x[t], c)Pr(c|x,l)
∝
∏
l
t=1
{ c∈C[t] Pr(Yˆ [t]|x[t])Pr(Yˆ [t]|c)
Pr(Yˆ [t])
P(c|x,l)
=α
∏
l
t=1
Pr(Yˆ [t]|x[t])
Pr(Yˆ [t])
∏
l
t=1
{ c∈C[t]
Pr(Yˆ [t]|c)Pr(c|x,l).
=β(y)
∏
l
i=1
Pr(Yˆ [i]|x[i])
∏t
i=1
{ c∈C[t]
Pr(Yˆ [t]|c)Pr(c|x, t),
:=∏t
i=1
Pr(Yˆ [i]|x[i])
∏t
i=1
{ c∈C[t]
Pr(Yˆ [t]|c)Pr(c|x, t),
(7.34) 
The visual prediction can be taken out of the integral as it is not affected by the 
linguistic information, i.e.,. c. 
Here, 
.β(y) = α
∏l
1 P(yt)
, (7.35) 
and it is a character frequency term related to the word. During training,.β(y∗) only 
associates with the label, hence would be constant for a certain label and won’t pro￾duce gradients. During the evaluation, as the dictionary and character frequency are 
unknown, character frequency would be assumed as uniform, resulting in.β(y) being 
a constant number . α
|Ceval|
l for all words with length . l. Hence, despite varying from 
word to word, treating it as a constant does not affect either training or evaluation. 
As a result,. β is omitted for writing convenience.104 7 Open-Set Text Recognition: Case-Studies
7.3 Performances Overview 
We summarize the current performances of open-set text recognition methods on 
open-set text recognition benchmarks and close-set text recognition benchmarks. 
7.3.1 Open-Set Text Recognition 
In this section, we survey the performances of the example open-set text recognition 
models [ 1, 14] under four different splits, namely the GZSL mode, the OSR mode, the 
GOSR mode, and the OSTR mode. The results and specifics of each split are shown 
in Table 7.1, and qualitative results are shown in Fig. 7.7. Here, “Shared Kanji” 
refers to Kanjis covered by the Tier-1 Simplified Chinese characters [ 28], while 
“Unique Kanji” means unseen Kanjis that are novel to the model. The term “Latin” 
indicates digits and English alphabets, and the term “Kana” indicates Hiraganas and 
Katakanas. 
The models show overall acceptable recognition capability for seen and novel 
characters under most scenarios. Overall, OpenCCD has better performance than 
OSOCR, however, the gap is not substantial. For the rejection capability, the perfor￾Table 7.1 Results for different open-set text recognition splits. LA stands for the Line Accuracy for 
samples containing only.Ck
eval characters. R, P, and F stand for the Recall, Precision, and F-measure 
performance spotting samples with out-of-set characters. “Shared Kanji” refers to Kanjis covered 
by the Tier-1 Simplified Chinese characters [ 28], while “Unique Kanji” means unseen Kanjis that 
are novel to the model 
Split .Ck
test .Cu
test Name LA Recall Precision F-measure 
GZSL Unique 
Kanji 
.∅ OSOCR [ 1] 28.11 – – – 
Shared 
Kanji, 
OSOCR-Large [ 1] 30.83 – – – 
Kana, 
Latin, 
OpenCCD [ 14] 36.57 – – – 
OpenCCD-Large [ 14] 41.31 – – – 
OSR Shared 
Kanji 
Unique 
Kanji 
OSOCR-Large [ 1] 74.35 11.27 98.28 20.23 
Latin Kana OpenCCD-Large 84.76 30.63 98.90 46.78 
GOSR Shared 
Kanji 
Kana OSOCR-Large [ 1] 56.03 3.03 63.52 5.78 
Unique 
Kanji 
OpenCCD-Large 68.29 3.47 86.11 6.68 
OSTR Shared 
Kanji 
Latin OSOCR-Large [ 1] 58.57 24.46 93.78 38.80 
Unique 
Kanji 
Kana OpenCCD-Large 69.82 35.95 97.03 52.477.3 Performances Overview 105
Fig. 7.7 Sample results from the open-set text recognition task. The top figure includes the results 
from OSOCR [ 1] and the bottom figure contains the results from OpenCCD [ 14]. The figure shows 
qualitative performance under the “Hangul”, “Kana”, “Unique Kanji”, and “Shared Kanji (close￾set)” scenarios. The results for each group are represented with two rows, where the top row shows 
the success cases and the bottom shows failure cases. Text in white indicates seen characters, yellow 
indicates novel characters, red indicates recognition error, green indicates correct results, and purple 
block indicates rejected results 
mance goes well with seen characters (SOC). The performance is also acceptable for 
some novel characters that are structural-wise close to characters in the training set. 
However, the model demonstrates major limitations on the scalability over the class 
number in.Ck
test . However, the precisions are above 60% on all setups, meaning the 
human labor necessary to dismiss fake warnings is low. Considering most applica￾tions involve a large amount of data and each novel character only needs to be spotted 
once, the weakness in recall is not of much importance. Hence, the framework is still 
feasible for finding novel characters in the data stream. 
Note the size of the character set.|Ck
test| can have a serious impact on recognition 
performance. This phenomenon suggests the decision boundaries of different pro￾totypes may overlap with each other despite being bounded by the threshold .sunk . 
Worth mentioning, that the result is not directly comparable to the zero-shot character 
recognition results on the CTW dataset due to the vast differences among datasets, 
character sets, and metrics.106 7 Open-Set Text Recognition: Case-Studies
7.3.2 Performance on Other Unseen Languages 
Aside from the standard benchmark, we quantitatively and qualitatively measure the 
performances of the framework in various languages. 
To further validate the generalization capability of the large model, the methods 
conducted evaluations on word-level samples from Korean, Russian, and Greek. 
The Korean words are drawn from the MLT dataset like the Japanese language, and 
qualitative results are shown in Figs. 7.8 and 7.9. Results indicate that the main 
limitation comes from confusion over characters with close shapes. Quantitative 
results on.5, 171 Korean words are shown in Table 7.2. 
Performances on the Russian and the Greek samples collected from the SIW￾13 [ 29] dataset are also included. As the dataset does not provide annotation on 
contents, only demonstrates the qualitative results in Figs. 7.10 and 7.11. 
Fig. 7.8 Qualitative results of OSOCR [ 1] on the Korean language. Ground truth is annotated with 
“GT”, and prediction is annotated with “PR”. Green indicates correct predictions red for wrong 
ones 
Fig. 7.9 Qualitative results of OpenCCD [ 1] on the Korean language. Ground truth is annotated 
with “GT”, and prediction is annotated with “PR”. Green indicates correct predictions red for wrong 
ones 
Table 7.2 Model performances on the Korean language 
Name Line accuracy Character accuracy 
OSOCR-Large [ 1] 1.35 9.07 
OpenCCD-Large [ 14] 19.16 42.107.3 Performances Overview 107
Fig. 7.10 Qualitative results of OSOCR [ 1] on the Russian scripts and Greek scripts from the 
SIW-13 [ 29] dataset. Recognition results are cast to lower case. Blue block means the character is 
rejected by the model 
Fig. 7.11 Qualitative results of OpenCCD [ 14] on the Russian scripts and Greek scripts from the 
SIW-13 [ 29] dataset. Recognition results are cast to lower case. Blue block means the character is 
rejected by the model 
7.3.3 Standard Close-Set Text Recognition 
Experiments on popular close-set benchmarks are also an important factor, where 
both examples suffice as a feasible lightweight method on conventional close-set text 
recognition tasks 
Following the majority of methods in this community, the models are trained on 
synthetic samples from Jaderberg (MJ) [ 30] and Gupta (ST) [ 31]. The evaluation 
utilizes the IIIT5K-Words (IIIT5K), Street View Text (SVT), ICDAR 2003 (IC03), 
ICDAR 2013 (IC13), and CUTE80 (CUTE) as our testing sets. 
Among the testing sets, IIIT5K, IC03, IC13, and SVT focus on regular-shaped 
texts, and CUTE focuses on irregular-shaped text. IIIT5K contains .3, 000 testing 
images collected from the web. SVT has.647 testing images from Google Street View. 
IC03 includes.867 words from scene text, while IC13 extends IC03 and contains. 1015
images. CUTE80 includes.288 curved samples. All models are trained for. 5 epochs 
for close-set experiments, and the results are shown in Table 7.3. 
On the close-set benchmarks, the regular methods are performance-wise better 
than early lightweight methods like Rosetta [ 35]. The method is also comparable to 
the state-of-the-art RNN-free method CA-FCN [ 15], while being significantly faster. 
Moreover, our method does not require character-level annotations for training. 
Larger models, on the other hand, demonstrate around the speed of CA-FCN [ 15] 
yielding better performances close to heavy RNN-based methods [ 6]. Plus, our meth-108 7 Open-Set Text Recognition: Case-Studies
Table 7.3 Performance on conventional close-set benchmarks. * indicates character-level annota￾tion and. 
+ for multi-batch evaluation 
Methods Venue Training Set RNN FPS IIIT5K SVT IC03 IC13 CUTE 
Comb.Best [ 2] ICCV’19 MJ+ST Y 36.23 87.9 87.5 94.4 92.3 71.8 
SAR [ 4] AAAI’19 MJ+ST Y – 91.5 84.5 – – 83.3 
ESIR [ 32] CVPR’19 MJ+ST Y – 93.3 90.2 – – 83.3 
SCATTER [ 33] CVPR’20 MJ+ST+Extra Y – 93.7 92.7 96.3 93.9 87.5 
SEED [ 34] CVPR’20 MJ+ST Y – 93.8 89.6 – 92.8 83.6 
DAN [ 6] AAAI’20 MJ+ST Y – 94.3 89.2 95.0 93.9 84.4 
Rosetta [ 2, 35] KDD’18 MJ+ST N 212.76 84.3 84.7 92.9 89.0 69.2 
CA-FCN* [ 15] AAAI’19 ST N 45 92.0 82.1 – 91.4 78.1 
TextScanner* [ 16] AAAI’20 MJ+ST+Extra N – 93.9 90.1 – 92.9 83.3 
OSOCR-Large PR23 MJ+ST N 70 92.63 88.25 93.42 93.79 86.80 
OpenCCD-Large CVPR22 MJ+ST N 66 91.90 85.93 92.38 92.21 83.68 
Fig. 7.12 Recognition examples on the close-set benchmarks: Ground truth is annotated with “GT”, 
prediction is annotated with “PR”, and wrong predictions are indicated with the red color 
ods are RNN-free, hence do not require batching up to margin out the latency caused 
by RNNs. 
Representative samples from OSOCR [ 1] are illustrated in Fig. 7.12 and the sam￾ples from OpenCCD are shown in Fig. 7.13. The models demonstrate some extent 
of robustness for samples with blur, irregular shapes, and different styles. 
The speeds of the example method are shown in Table 7.4. Speeds reported by 
other methods are also listed as a rough reference. These models run with FP32 
datatype on an RTX2070 Mobile GPU, and the speeds are computed on the IIIT5k 
dataset. For latency critique tasks, these methods can manage a . 9 to .15 ms latency 
under single-batched mode on a laptop GPU of around . 7 TFLOPS. When tested 
in multi-batch mode, both can reach more Space-wise, both models can run with 
less than 2.5GiB in multi-batch testing, and less than 2Gib in single-batch tests. InReferences 109
Fig. 7.13 Recognition examples on the close-set benchmarks: Ground truth is annotated with “GT”, 
prediction is annotated with “PR”, and wrong predictions are indicated with the red color [ 14] 
Table 7.4 Single batch speed on close-set benchmarks, where “*” means the results are derived 
from [ 2] 
Method Batch size IIIT5K CUTE GPU TFlops Speed (ms) Vram (MB) 
CRNN* [ 18] 1 78.2 – P40 12 4.4 – 
Rosetta* [ 35] 1 84.3 69.2 P40 12 4.7 – 
Comb.Best* [ 2] 1 87.9 74.0 P40 12 27.6 – 
CA-FCN [ 15] 1 92.0 79.9 Titan XP 12 22.2 – 
OSOCR [ 1] 1 90.40 82.29 RTX2070M 6.6 9.19 1227 
OSOCR-Large [ 1] 1 92.63 86.80 RTX2070M 6.6 14.18 1377 
OpenCCD-Large [ 14] 1 91.90 83.68 RTX2070M 6.6 14.94 – 
summary, these models are reasonably small and fast, thus friendly to smaller devices 
like laptops, phones, and single-boards. 
In conclusion, despite showing an acceptable margin against heavy SOTA meth￾ods, the methods are comparable to, or better than many popular RNN-free methods 
for close-set text recognition, demonstrating its readiness for industrial deployments. 
References 
1. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
2. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with 
scene text recognition model comparisons? dataset and model analysis. In: 2019 IEEE/CVF 
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), Oct 27–Nov 
2, 2019. IEEE (2019), pp. 4714–4722 
3. Cao, Z., Lu, J., Cui, S., Zhang, C.: Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recognit. 107, 107488 (2020)110 7 Open-Set Text Recognition: Case-Studies
4. Li, H., Wang, P., Shen, C., Zhang, G.: Show, attend and read: a simple and strong baseline for 
irregular text recognition. In: The Thirty-Third AAAI Conference on Artificial Intelligence, 
AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, 
IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2019, Honolulu, Hawaii, USA, Jan 27–Feb 1, 2019. AAAI Press (2019), pp. 8610–8617 
5. Sheng, F., Chen, Z., Xu, B.: NRTR: a no-recurrence sequence-to-sequence model for scene 
text recognition. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, Sept 20–25, 2019. IEEE (2019), pp. 781–786 
6. Wang, T., Zhu, Y., Jin, L., Luo, C., Chen, X., Wu, Y., Wang, Q., Cai, M.: Decoupled attention 
network for text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, New York, NY, USA, Feb 7–12, 2020. AAAI Press (2020), pp. 12 216–12 224 
7. Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks. 
In: IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, Oct 22–29, 
2017. IEEE Computer Society (2017), pp. 764–773 
8. Luo, C., Jin, L., Sun, Z.: MORAN: a multi-object rectified attention network for scene text 
recognition. Pattern Recognit. 90, 109–118 (2019) 
9. Yang, M., Guan, Y., Liao, M., He, X., Bian, K., Bai, S., Yao, C., Bai, X.: Symmetry-constrained 
rectification network for scene text recognition. In: 2019 IEEE/CVF International Conference 
on Computer Vision, ICCV 2019, Seoul, Korea (South), Oct 27–Nov 2, 2019. IEEE (2019), 
pp. 9146–9155 
10. Qi, H., Brown, M., Lowe, D.G.: Low-shot learning with imprinted weights. In: 2018 IEEE 
Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, 
USA, June 18–22, 2018. IEEE Computer Society (2018), pp. 5822–5830 
11. Fei, G., Liu, B.: Breaking the closed world assumption in text classification. In: Proceedings of 
the 2016 Conference of the North American Chapter of the Association for Computational Lin￾guistics: Human Language Technologies. Association for Computational Linguistics (2016), 
pp. 506–514 
12. Zhang, H., Xu, H., Lin, T.: Deep open intent classification with adaptive decision boundary. In: 
Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference 
on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on 
Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, Feb 2–9, 2021. 
AAAI Press (2021), pp. 14 374–14 382 
13. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: ASTER: an attentional scene text 
recognizer with flexible rectification. IEEE Trans. Pattern Anal. Mach. Intell. 41(9), 2035– 
2048 (2019) 
14. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022. IEEE (2022), pp. 4513–4522 
15. Liao, M., Zhang, J., Wan, Z., Xie, F., Liang, J., Lyu, P., Yao, C., Bai, X.: Scene text recogni￾tion from two-dimensional perspective. In: The Thirty-Third AAAI Conference on Artificial 
Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence 
Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial 
Intelligence, EAAI 2019, Honolulu, Hawaii, USA, Jan 27–Feb 1, 2019. AAAI Press (2019), 
pp. 8714–8721 
16. Wan, Z., He, M., Chen, H., Bai, X., Yao, C.: Textscanner: reading characters in order for robust 
scene text recognition. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, 
AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, 
IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, 
EAAI 2020, New York, NY, USA, Feb 7–12, 2020. AAAI Press (2020), pp. 12 120–12 127 
17. Cheng, Z., Xu, Y., Bai, F., Niu, Y., Pu, S., Zhou, S.: AON: towards arbitrarily-oriented text 
recognition. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPRReferences 111
2018, Salt Lake City, UT, USA, June 18–22, 2018. IEEE Computer Society (2018), pp. 5571– 
5579 
18. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence 
recognition and its application to scene text recognition. IEEE Trans. Pattern Anal. Mach. 
Intell. 39(11), 2298–2304 (2017) 
19. Hou, J., Zhu, X., Liu, C., Sheng, K., Wu, L., Wang, H., Yin, X.: HAM: hidden anchor mechanism 
for scene text detection. IEEE Trans. Image Process. 29, 7904–7916 (2020) 
20. Wellman, M.P., Henrion, M.: Explaining ’explaining away. IEEE Trans. Pattern Anal. Mach. 
Intell. 15(3), 287–292 (1993) 
21. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: autonomous, bidirectional 
and iterative language modeling for scene text recognition. In: IEEE Conference on Computer 
Vision and Pattern Recognition, CVPR 2021, virtual, June 19–25, 2021. Computer Vision 
Foundation/IEEE (2021), pp. 7098–7107 
22. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate scene text 
recognition with semantic reasoning networks. In: 2020 IEEE/CVF Conference on Computer 
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020. IEEE 
(2020), pp. 12 110–12 119 
23. Chang, W., You, T., Seo, S., Kwak, S., Han, B.: Domain-specific batch normalization for unsu￾pervised domain adaptation. In: IEEE Conference on Computer Vision and Pattern Recognition, 
CVPR 2019, Long Beach, CA, USA, June 16–20, 2019. Computer Vision Foundation/IEEE 
(2019), pp. 7354–7362 
24. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. In: 2nd International Conference 
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14–16, 2014, Conference 
Track Proceedings (2014) 
25. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with 
region proposal networks. In: Advances in Neural Information Processing Systems 28: Annual 
Conference on Neural Information Processing Systems 2015, Dec 7–12, 2015, Montreal, Que￾bec, Canada (2015), pp. 91–99 
26. Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: 2017 IEEE Conference on 
Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21–26, 
2017. IEEE Computer Society (2017), pp. 6517–6525 
27. Zhang, X., Yang, H., Young, E.F.Y.: Attentional transfer is all you need: technology-aware 
layout pattern generation. In: 58th ACM, IEEE Design Automation Conference, DAC, San 
Francisco, CA, USA, Dec 5–9, 2021. IEEE 2021, pp. 169–174 (2021) 
28. Liu, C., Yin, F., Wang, D., Wang, Q.: CASIA online and offline Chinese handwriting databases. 
In: 2011 International Conference on Document Analysis and Recognition, ICDAR 2011, 
Beijing, China, Sept 18–21, 2011. IEEE Computer Society (2011), pp. 37–41 
29. Shi, B., Bai, X., Yao, C.: Script identification in the wild via discriminative convolutional neural 
network. Pattern Recognit. 52, 448–458 (2016) 
30. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and artificial neu￾ral networks for natural scene text recognition. In: NIPS Deep Learning Workshop. Neural 
Information Processing Systems (2014) 
31. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images. In: 
2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, 
NV, USA, June 27–30, 2016. IEEE Computer Society (2016), pp. 2315–2324 
32. Zhan, F., Lu, S.: ESIR: end-to-end scene text recognition via iterative image rectification. In: 
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, 
CA, USA, June 16–20, 2019. Computer Vision Foundation/IEEE (2019), pp. 2059–2068 
33. Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: SCATTER: selective 
context attentional scene text recognizer. In: 2020 IEEE/CVF Conference on Computer Vision 
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020. IEEE (2020), pp. 
11 959–11 969 
34. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: SEED: semantics enhanced encoder-decoder 
framework for scene text recognition. In: 2020 IEEE/CVF Conference on Computer Vision112 7 Open-Set Text Recognition: Case-Studies
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13–19, 2020. IEEE (2020), pp. 
13 525–13 534 
35. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: large scale system for text detection and 
recognition in images. In: Proceedings of the 24th ACM SIGKDD International Conference 
on Knowledge Discovery Data Mining, KDD 2018, London, UK, Aug 19–23, 2018. ACM 
(2018), pp. 71–79Chapter 8 
Discussions and Future Directions 
Abstract This chapter introduces the applications and future directions of Open￾set text recognition tasks. First, this chapter introduces some existing applications 
in OSTR tasks, from different perspectives: recognized language, text recognition 
granularity, the overall technical route, and open-set classifier design and model 
implementation. Then we discuss the influence of the Multi-modal Large Language 
Model on the Open-Set Text Recognition (OSTR) task. We introduce some typical 
Large Language Models and Multi-modal Large Language Models, and their applica￾tion to Text Recognition. We also discuss some potentials to leverage the capabilities 
of MLLMs for the OSTR task: Auto/semi-auto data construction, Downstream task 
fine-tuning, and Semantic understanding enhancement. In the end, this chapter dis￾cusses four main trends in the development of open-set text recognition techniques: 
(a) Cross-language text recognition technology, (b) Character fine-grained analysis 
techniques, (c) New character induction discovery techniques, and (d) Incremental 
language model evolution techniques. 
Keywords Application · Future directions 
8.1 Applications for OSTR 
Currently, most of the work on open-set text recognition is carried out by domestic 
research teams. In general, there is little difference between domestic and foreign 
research goals and application scenarios specifically. These studies are all aimed at 
addressing rapid adaptation to multiple languages [ 1– 4], or ancient text recognition 
[ 5– 8] or new character processing problems encountered in ancient text recognition 
[ 5– 7]. 
From the perspective of recognized languages, domestic studies focus on CJK 
characters [ 5, 9]; abroad, languages of different linguistic families are studied. He 
et al. [ 10] focus on Chinese. Chanda et al. [ 6] focus on Medieval Latin, [ 7] on 
Bengali and Ancient Korean. Zhang et al. [ 1] focus on Latin. Souibgui et al. [ 4] 
focus on some other minor languages. 
© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024 
X.-C. Yin et al., Open-Set Text Recognition, SpringerBriefs in Computer Science, 
https://doi.org/10.1007/978-981-97-0361-6_8
113114 8 Discussions and Future Directions
From the perspective of text recognition granularity, character level and line level 
have been studied both domestically and internationally. When researching character 
level recognition both domestically and internationally, this task is often regarded 
as an open-set classification problem, or more broadly, as a fine-grained long-tail 
classification problem. The line level, on the other hand, introduces the problem of 
processing sequence information on the basis of the character level. Accordingly, 
there are national and international examples of validation of generic open sets, few 
(zero) samples, or long-tail recognition methods using character recognition [ 11]. 
There is no significant difference in the task definition of open-set identification 
in the national and international context. From the overall technical route perspec￾tive, there is no clear demarcation between domestic and international research. The 
open-set classification techniques have both component-based and matching-based 
approaches, both domestically and internationally. In terms of frameworks, most of 
the approaches proposed by domestic research teams are extensions for some kind 
of closed-set recognition framework; foreign research teams are less likely to adopt 
the mainstream character sequence-based closed-set text recognition framework, but 
follow the approach of the earlier whole-word classification framework [ 6, 7] ; there 
are also some foreign research teams that propose new closed-set text recognition 
framework and extend it into an open-set text recognition model [ 1, 4]. 
From the perspective of the overall technical route, there is no clear demarcation 
between domestic and international research. In terms of open-set classification tech￾niques, both domestic and foreign approaches are based on components and based 
on matching. In terms of frameworks, most of the approaches proposed by domestic 
research teams are extensions for some kind of closed-set recognition framework; 
foreign research teams are less likely to adopt the mainstream character sequence￾based closed-set text recognition framework, but follow the approaches of earlier 
whole-word classification frameworks [ 6, 7], or propose new closed-set text recog￾nition framework [ 1, 4] and extend it into an open-set text recognition model. 
From the perspective of open-set classifier implementation, domestic and inter￾national work has focused on two types of approaches, attribute-based and visual 
matching-based. On the one hand, the general idea of both domestic and international 
teams studying component-based approaches is to obtain generalization capability to 
unknown categories through the reuse of components. In the research work, different 
recognition languages usually lead to different component designs and implementa￾tions. On the other hand, the overall differences between domestic [ 2, 3] and foreign 
[ 1, 4] studies targeting visual matching-based approaches are not significant. For the 
rejection recognition task, the open-set text recognition task is still a new concept 
due to the complete feature, and only a few teams in China are currently working 
on it in the field of text recognition [ 2] . In the open-set recognition task for targets 
other than text, there are related works by both domestic and foreign research teams. 
From the implementation perspective, domestic research work has been imple￾mented in a relatively diverse manner. For attribute-based matching methods [ 6– 8]: 
foreign research teams [ 6– 8] count attributes without order in a training sense much 
like ACE loss [ 12] ; domestic research teams [ 13– 15] use a direct attribute compari￾son approach, using sequences of components as comparison objects; some domestic8.2 Discussions on MLLM and OSTR 115
research works [ 5, 9] also encode attribute information as feature vectors and com￾pare them directly with image features, which are less common in foreign research 
works related to text recognition, but more common in foreign generic zero-sample 
studies. For the visual matching-based methods, both domestic and foreign research 
works have been involved. In particular, for the phenomenon of large CJK charac￾ter sets, when training open-set text recognition methods based on visual matching, 
domestic research teams often introduce a sampling mechanism for labels as a way 
to reduce computational resource overhead. 
8.2 Discussions on MLLM and OSTR 
The emergence of ChatGPT signifies a monumental achievement in the realm of Arti￾ficial Intelligence (AI). Large Language Models (e.g., CLIP [ 16], GPT-4 [ 17], and 
LLaMA [ 18]) have demonstrated language comprehension and generation abilities 
that closely resemble human levels. This is attributed to their exceptional capabil￾ities in answering questions, generating content, and even writing code. Simulta￾neously, the field of multi-modal large language models (MLLMs) is experiencing 
rapid growth due to their proficiency in comprehending and processing a variety 
of data types, encompassing both images and text. Their adeptness in addressing a 
broad spectrum of multi-modal tasks, spanning from image captioning to visual ques￾tion answering and beyond, has garnered considerable attention from the academic 
community. Multi-modal Large Language Models (MLLMs), trained on image-text 
pairs, showcase impressive zero-shot capabilities across diverse vision tasks [ 19, 
20]. Noteworthy models like LLaVAR [ 21] and Qwen-VL [ 22] exemplify the rapid 
progress within this domain. 
As the text recognition task can take the form of a Visual Question Answering Task 
(VQA, [ 23– 25]), Multi-modal large language models can be utilized to accomplish 
tasks associated with text detection and recognition. Multi-modal large language 
models, such as UDOP [ 26] or LLaVAR [ 21], possess the capability to comprehend 
and generate both textual and non-textual data, including images. They demonstrate 
effective performance, particularly in close-set text recognition tasks, underscoring 
the versatility and potential of these models. 
Tang et al. [ 26] proposed Universal Document Processing (UDOP), serving as 
a foundational Document AI model that integrates text, image, and layout modali￾ties across diverse task formats, including document understanding and generation. 
UDOP capitalizes on the spatial correlation between textual content and document 
images, unifying image, text, and layout representations into a single unified model. 
Through a novel Vision-Text-Layout Transformer, UDOP consolidates pretraining 
and multi-domain downstream tasks within a prompt-based sequence generation 
framework. The model underwent evaluation across various document AI tasks, 
including document classification, layout analysis, information extraction, and ques￾tion answering.116 8 Discussions and Future Directions
Feng et al. [ 27] proposed UniDoc, a novel multi-modal model equipped with text 
detection and recognition capabilities, addressing deficiencies present in existing 
approaches. To implement UniDoc, they conducted unified multi-modal instruction 
tuning using substantial instruction-following datasets. The concatenated embedding 
sequence of visual and instruction tokens is processed through Vicuna [ 28]. The 
model underwent evaluation across tasks such as text detection, recognition, spotting, 
and comprehension. 
Li et al. [ 29] identified limitations arising from supported input resolution con￾straints (e.g., 448 .× 448) and incomplete descriptions in training image-text pairs, 
leading these models to encounter challenges in handling intricate scene understand￾ings and narratives. Consequently, they proposed a multi-level description generation 
method aimed at automatically providing comprehensive information. This approach 
guides the model in learning contextual associations between scenes and objects, 
addressing complexities in scene understanding and narratives. 
MLLMs have made notable strides in closed-set text recognition tasks focused 
on Latin scripts. However, open-set text recognition continues to pose a significant 
challenge. 
When it comes to open-set text recognition, where the goal is to identify and 
differentiate text data that falls within known categories from text data that doesn’t, 
these multi-modal models can be used in various ways. Here are some potentials to 
leverage the capabilities of MLLMs for the OSTR task: 
Auto/semi-auto data construction: Leveraging Language Model Machines 
(LMMs) for the automatic or semi-automatic annotation and generation of data holds 
the promise of significantly diminishing the expenses associated with manual label￾ing. This strategic approach stands as an effective means to address the challenges 
inherent in data acquisition. By employing these models, the process of annotat￾ing and creating datasets becomes more streamlined, potentially reducing time and 
resources spent on laborious manual labeling. As a result, it not only offers cost￾efficiency but also enhances the scalability and accessibility of generating labeled 
data, fostering advancements in various fields reliant on robust datasets. 
Downstream task fine-tuning: Another effective strategy that maximizes the 
existing knowledge embedded in Language Model Machines (LMMs) is the process 
of fine-tuning, which proves particularly valuable in situations characterized by lim￾ited data availability. Fine-tuning serves as a mechanism through which the model can 
tailor its capabilities to address specific tasks or domains, thereby enhancing over￾all performance. This adaptive process involves the refinement of the pre-trained 
LMM on a narrower dataset related to the target task, allowing the model to learn 
task-specific nuances and nuances in the absence of extensive data. By fine-tuning, 
the LMM becomes more specialized and attuned to the intricacies of the particular 
domain, leading to improved accuracy and effectiveness in addressing the challenges 
posed by limited data scenarios. This method not only optimizes model performance 
but also demonstrates the versatility and adaptability of LMMs in various practical 
applications. 
Semantic understanding enhancement: According to “known known class” text 
recognition, one effective strategy is the enhancement of semantic understanding. The8.3 Future Directions 117
remarkable strength of Language Model Machines (LMMs) resides in their profound 
semantic comprehension, achieved through extensive training on vast datasets. This 
innate capability to grasp and interpret semantics stands as a pivotal asset in document 
comprehension and associated tasks. By harnessing the semantic potential embedded 
within LMMs, there emerges a tremendous opportunity to significantly elevate per￾formance across these tasks. The profound understanding of context, meaning, and 
relationships between words and concepts, attained through the model’s exposure 
to diverse linguistic patterns, empowers LMMs to excel in tasks reliant on nuanced 
semantic comprehension. Leveraging this strength effectively could lead to more 
accurate and nuanced document understanding, improved natural language process￾ing, and enhanced performance in various applications that hinge on deep semantic 
understanding. Hence, tapping into the semantic prowess of LMMs presents a path￾way toward substantial advancements in document analysis and related domains. 
In summary, the combination of multi-modal large language models and open-set 
text recognition presents an intriguing approach to tackle the challenges involved 
in identifying and categorizing text data into known and unknown categories. These 
models harness their profound understanding of text and their capacity to process non￾textual data, offering crucial context for decision-making in open-set recognition. 
However, there are still some challenges that require thoughtful consideration in 
practical applications. 
8.3 Future Directions 
Although some research progress has been made in zero-sample text recognition [ 1, 
4, 5] and open-set text recognition methods [ 2, 3], there are still major limitations 
in the performance of the existing methods. The main trends in the development of 
open-set text recognition techniques include the following: 
(1) Cross-language text recognition technology 
The existing work is not robust enough to handle the differences in shape of 
different language characters and cross-language character recognition. Here, it is 
mainly reflected in the difficulty of open-set classifiers to perform cross-language 
character migration and also has a large impact on the localization ability of attention 
mechanisms. This kind of problem belongs to the domain gap, which is one of the 
popular research directions in pattern recognition and machine learning in recent 
years. In open-set text recognition tasks, this problem can be alleviated by increasing 
the number of trained languages on the one hand, and by borrowing from upstream 
Domain Generalization [ 30] or Unsupervised Domain adaption [ 31] to solve the 
problem of recognizing new characters across languages and even languages. 
(2) Character fine-grained analysis techniques 
The performance of existing models is affected by the character set .Ck
test which 
means that the model is not sensitive enough to character detail differences. This 
problem can be considered as a Fine-grained classification of characters [ 32]. In118 8 Discussions and Future Directions
the future, it is necessary to consider the application of fine-grained classification to 
improve the scalability of open-set character recognition methods. 
(3) New character induction discovery techniques 
Existing methods cannot automatically generalize new out-of-set (NOC) char￾acters in the data. In the process of ancient book recognition, induction capability 
can be used to perform semi-automatic or automatic glyph analysis of new charac￾ters, thus reducing the workload of experts. Existing open-set character recognition 
methods can only achieve preliminary rejection recognition, and cannot yet achieve 
automatic induction of new characters. At present, new sample induction [ 33] is a 
relatively new research direction and there is no mature research yet, that is to be 
discussed jointly in different fields. 
(4) Incremental language model evolution techniques 
Most current open-set text recognition methods [ 3, 34] attempt to exclude the 
influence of language models. In fact, in many application settings, language mod￾els evolve incrementally asymptotically, and thus unsupervised training of language 
models [ 35] can be performed to build semantic models based on incremental learn￾ing and make them evolve with test data to improve open-set text recognition per￾formance. 
In addition, the existing zero-sample and open-set text recognition methods have 
some performance gaps in handling known characters compared to the mainstream 
closed-set text recognition algorithms. In addition to the incremental evolution of the 
language model, this problem can be improved by replacing the feature extraction 
network [ 36] or using correction modules [ 37– 39]. 
References 
1. Zhang, C., Gupta, A., Zisserman, A., Adaptive text recognition through visual matching. In: 
Computer Vision–ECCV 2020–16th European Conference, Glasgow, UK, August 23–28: Pro￾ceedings, Part XVI, ser. Lecture Notes in Computer Science, vol. 12361, pp. 51–67. Springer 
(2020) 
2. Liu, C., Yang, C., Qin, H., Zhu, X., Liu, C., Yin, X.: Towards open-set text recognition via 
label-to-prototype learning. Pattern Recognit. 134, 109109 (2023) 
3. Liu, C., Yang, C., Yin, X.: Open-set text recognition via character-context decoupling. In: 
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New 
Orleans, LA, USA, June 18–24, 2022, pp. 4513–4522. IEEE (2022) 
4. Souibgui, M.A., Fornés, A., Kessentini, Y., Megyesi, B.: Few shots is all you need: A pro￾gressive few shot learning approach for low resource handwriting recognition (2021) [Online]. 
Available: https://arxiv.org/abs/2107.10064 
5. Huang, Y., Jin, L., Peng, D.: Zero-shot Chinese text recognition via matching class embed￾ding. In: 16th International Conference on Document Analysis and Recognition, ICDAR 2021, 
Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part III, ser. Lecture Notes in 
Computer Science, vol. 12823, pp. 127–141. Springer (2021) 
6. Chanda, S., Baas, J., Haitink, D., Hamel, S., Stutzmann, D., Schomaker, L.: Zero-shot learning 
based approach for medieval word recognition using deep-learned features. In: 16th Interna￾tional Conference on Frontiers in Handwriting Recognition, ICFHR 2018, Niagara Falls, NY, 
USA, August 5–8, 2018, pp. 345–350. IEEE (2018)References 119
7. Chanda, S., Haitink, D., Prasad, P.K., Baas, J., Pal, U., Schomaker, L.: Recognizing Bengali 
word images–A zero-shot learning perspective. In: 25th International Conference on Pattern 
Recognition, ICPR 2020, Virtual Event/Milan, Italy, January 10–15, 2021, pp. 5603–5610. 
IEEE (2020) 
8. Rai, A., Krishnan, N.C., Chanda, S.: Pho(sc)net: An approach towards zero-shot word image 
recognition in historical documents. In: 16th International Conference on Document Analysis 
and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5–10, 2021, Proceedings, 
Part I, ser. Lecture Notes in Computer Science, vol. 12821, pp. 19–33. Springer (2021) 
9. Cao, Z., Lu, J., Cui, S., Zhang, C.: Zero-shot handwritten Chinese character recognition with 
hierarchical decomposition embedding. Pattern Recognit. 107, 107488 (2020) 
10. He, M., Liu, Y., Yang, Z., Zhang, S., Luo, C., Gao, F., Zheng, Q., Wang, Y., Zhang, X., Jin, 
L.: ICPR2018 contest on robust reading for multi-type web images. In: 24th International 
Conference on Pattern Recognition, ICPR 2018, Beijing, China, August 20–24, 2018, pp. 7– 
12. IEEE Computer Society (2018) [Online]. Available: https://doi.org/10.1109/ICPR.2018. 
8546143 
11. Bertinetto, L., Henriques, J.F., Valmadre, J., Torr, P.H.S., Vedaldi, A.: Learning feed-forward 
one-shot learners. In: Advances in Neural Information Processing Systems 29: Annual Con￾ference on Neural Information Processing Systems 2016, December 5–10, 2016, pp. 523–531. 
Barcelona, Spain (2016) 
12. Xie, Z., Huang, Y., Zhu, Y., Jin, L., Liu, Y., Xie, L.: Aggregation cross-entropy for sequence 
recognition. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, 
Long Beach, CA, USA, June 16-20, 2019, pp. 6538–6547. Computer Vision Foundation/IEEE 
(2019) 
13. Zhang, J., Du, J., Dai, L.: Radical analysis network for learning hierarchies of Chinese char￾acters. Pattern Recognit. 103, 107305 (2020) 
14. Chen, J., Li, B., Xue, X.: Zero-shot Chinese character recognition with stroke-level decompo￾sition. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 
IJCAI 2021, Virtual Event/Montreal, Canada, 19–27 August 2021, pp. 615–621 (2021) www. 
ijcai.org 
15. Zhang, J., Zhu, Y., Du, J., Dai, L.: Radical analysis network for zero-shot learning in printed 
Chinese character recognition. In: 2018 IEEE International Conference on Multimedia and 
Expo, ICME 2018, San Diego, CA, USA, July 23-27, 2018, pp. 1–6. IEEE Computer Society 
(2018) 
16. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 
A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from 
natural language supervision. In: Proceedings of the 38th International Conference on Machine 
Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning 
Research, vol. 139, pp. 8748–8763. PMLR (2021) 
17. OpenAI.: GPT-4 technical report. CoRR, vol. arXiv:2303.08774 (2023). [Online]. Available: 
https://doi.org/10.48550/arXiv.2303.08774 
18. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, 
N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open 
and efficient foundation language models. CoRR, vol. arXiv:2302.13971 (2023). [Online]. 
Available: https://doi.org/10.48550/arXiv.2302.13971 
19. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, 
A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from 
natural language supervision. In: Proceedings of the 38th International Conference on Machine 
Learning, ICML 2021, 18-24 July 2021, Virtual Event, ser. Proceedings of Machine Learning 
Research, M. Meila and T. Zhang, Eds., vol. 139, pp. 8748–8763. PMLR (2021) [Online]. 
Available: http://proceedings.mlr.press/v139/radford21a.html 
20. Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le, Q.V., Sung, Y., Li, Z., Duerig, 
T.: Scaling up visual and vision-language representation learning with noisy text supervision. 
In: Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-
24 July 2021, Virtual Event, ser. Proceedings of Machine Learning Research, M. Meila and120 8 Discussions and Future Directions
T. Zhang, Eds., vol. 139, pp. 4904–4916. PMLR (2021) [Online]. Available: http://proceedings. 
mlr.press/v139/jia21b.html 
21. Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., Sun, T.: Llavar: Enhanced 
visual instruction tuning for text-rich image understanding. CoRR, vol. abs/2306.17107 (2023). 
[Online]. Available: https://doi.org/10.48550/arXiv.2306.17107 
22. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: 
A frontier large vision-language model with versatile abilities. CoRR, vol. abs/2308.12966 
(2023). [Online]. Available: https://doi.org/10.48550/arXiv.2308.12966 
23. Kim, G., Yokoo, S., Seo, S., Osanai, A., Okamoto, Y., Baek, Y.: On text localization in end￾to-end OCR-Free document understanding transformer without text localization supervision. 
In: International Conference on Document Analysis and Recognition, pp. 215–232. Springer 
(2023) 
24. Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florêncio, D.A.F., Zhang, C., Li, Z., Wei, F.: Trocr: 
Transformer-based optical character recognition with pre-trained models. In: Thirty-Seventh 
AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innova￾tive Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational 
Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7–14, 2023, 
pp. 13 094–13 102. AAAI Press (2023) 
25. Huang, Y., Lv, T., Cui, L., Lu, Y., Wei, F.: Layoutlmv3: Pre-training for document AI with 
unified text and image masking. In: MM ’22: The 30th ACM International Conference on 
Multimedia, Lisboa, Portugal, October 10–14, 2022, pp. 4083–4091. ACM (2022) 
26. Tang, Z., Yang, Z., Wang, G., Fang, Y., Liu, Y., Zhu, C., Zeng, M., Zhang, C., Bansal, M.: 
Unifying vision, text, and layout for universal document processing. In: IEEE/CVF Conference 
on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 
17-24, 2023, pp. 19 254–19 264. IEEE (2023) [Online]. Available: https://doi.org/10.1109/ 
CVPR52729.2023.01845 
27. Feng, H., Wang, Z., Tang, J., Lu, J., Zhou, W., Li, H., Huang, C.: Unidoc: A universal large 
multimodal model for simultaneous text detection, recognition, spotting and understanding. 
CoRR vol. abs/2308.11592 (2023). [Online]. Available: https://doi.org/10.48550/arXiv.2308. 
11592 
28. Zheng, L., Chiang, W., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, 
E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot 
arena. CoRR, vol. abs/2306.05685 (2023). [Online]. Available: https://doi.org/10.48550/arXiv. 
2306.05685 
29. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.: Monkey: 
Image resolution and text label are important things for large multi-modal models. CoRR, vol. 
abs/2311.06607 (2023). [Online]. Available: https://doi.org/10.48550/arXiv.2311.06607 
30. Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., Yu, P.S.: Generalizing 
to unseen domains: A survey on domain generalization. IEEE Trans. Knowl. Data Eng. 35(8), 
8052–8072 (2023) 
31. Wilson, G., Cook, D.J.: A survey of unsupervised deep domain adaptation. ACM Trans. Intell. 
Syst. Technol. 11(5) (2020) 
32. Wei, X., Song, Y., Aodha, O.M., Wu, J., Peng, Y., Tang, J., Yang, J., Belongie, S.J.: Fine-grained 
image analysis with deep learning: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 44(12), 
8927–8948 (2022) 
33. Huang, S., Wang, H., Liu, Y., Shi, X., Jin, L.: OBC306: A large-scale oracle bone character 
recognition dataset. In: 2019 International Conference on Document Analysis and Recognition, 
ICDAR 2019, Sydney, Australia, September 20–25, 2019, pp. 681–688. IEEE (2019) 
34. Wan, Z., Zhang, J., Zhang, L., Luo, J., Yao, C.: On vocabulary reliance in scene text recognition. 
In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, 
Seattle, WA, USA, June 13-19, 2020, pp. 11 422–11 431. IEEE (2020) 
35. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional 
transformers for language understanding. In: Proceedings of the 2019 Conference of the NorthReferences 121
American Chapter of the Association for Computational Linguistics: Human Language Tech￾nologies, Volume 1 (Long and Short Papers), pp. 4171–4186. Association for Computational 
Linguistics (2019) 
36. Atienza, R.: Vision transformer for fast and efficient scene text recognition. In: 16th Interna￾tional Conference on Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzer￾land, September 5-10, 2021, Proceedings, Part I, ser. Lecture Notes in Computer Science, vol. 
12821, pp. 319–334. Springer (2021) 
37. Luo, H., Jiang, W., Fan, X., Zhang, C.: Stnreid: Deep convolutional networks with pairwise 
spatial transformer networks for partial person re-identification. IEEE Trans. Multim. 22(11), 
2905–2913 (2020) 
38. Luo, C., Jin, L., Sun, Z.: MORAN: A multi-object rectified attention network for scene text 
recognition. Pattern Recognit. 90, 109–118 (2019) 
39. Yang, M., Guan, Y., Liao, M., He, X., Bian, K., Bai, S., Yao, C., Bai, X.: Symmetry-constrained 
rectification network for scene text recognition. In: 2019 IEEE/CVF International Conference 
on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27–November 2, 2019, pp. 
9146–9155. IEEE (2019)
