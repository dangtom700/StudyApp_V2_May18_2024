Optimization for Data Analysis
Optimization techniques are at the core of data science, including data analysis and
machine learning. An understanding of basic optimization techniques and their
fundamental properties provides important grounding for students, researchers, and
practitioners in these areas This text covers the fundamentals of optimization
algorithms in a compact, self-contained way, focusing on the techniques most relevant
to data science An introductory chapter demonstrates that many standard problems in
data science can be formulated as optimization problems Next, many fundamental
methods in optimization are described and analyzed, including gradient and
accelerated gradient methods for unconstrained optimization of smooth (especially
convex) functions; the stochastic gradient method, a workhorse algorithm in machine
learning; the coordinate descent approach; several key algorithms for constrained
optimization problems; algorithms for minimizing nonsmooth functions arising in data
science; foundations of the analysis of nonsmooth functions and optimization duality;
and the back-propagation approach, relevant to neural networks.
stephen j. wright holds the George B. Dantzig Professorship, the Sheldon
Lubar Chair, and the Amar and Balinder Sohi Professorship of Computer Sciences at
the University of Wisconsin–Madison. He is a Discovery Fellow in the Wisconsin
Institute for Discovery and works in computational optimization and its applications to
data science and many other areas of science and engineering. Wright is also a fellow
of the Society for Industrial and Applied Mathematics (SIAM) and recipient of the
2014 W. R. G. Baker Award from IEEE for most outstanding paper, the 2020
Khachiyan Prize by the INFORMS Optimization Society for lifetime achievements in
optimization, and the 2020 NeurIPS Test of Time award. He is the author and coauthor
of widely used textbooks and reference books in optimization, including Primal Dual
Interior-Point Methods and Numerical Optimization
benjamin recht is Associate Professor in the Department of Electrical
Engineering and Computer Sciences at the University of California, Berkeley His
research group studies how to make machine learning systems more robust to
interactions with a dynamic and uncertain world by using mathematical tools from
optimization, statistics, and dynamical systems Recht is the recipient of a Presidential
Early Career Award for Scientists and Engineers, an Alfred P Sloan Research
Fellowship, the 2012 SIAM/MOS Lagrange Prize in Continuous Optimization, the
2014 Jamon Prize, the 2015 William O Baker Award for Initiatives in Research, and
the 2017 and 2020 NeurIPS Test of Time awards.Optimization for Data Analysis
STEPHEN J. WRIGHT
University of Wisconsin–Madison
BENJAMIN RECHT
University of California, BerkeleyUniversity Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314 321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre,
New Delhi 110025, India
103 Penang Road, #05–06/07, Visioncrest Commercial, Singapore 238467
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge org
Information on this title: www.cambridge.org/9781316518984
DOI: 10 1017/9781009004282
© Stephen J. Wright and Benjamin Recht 2022
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2022
Printed in the United Kingdom by TJ Books Ltd, Padstow Cornwall
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging-in-Publication Data
Names: Wright, Stephen J , 1960– author | Recht, Benjamin, author
Title: Optimization for data analysis / Stephen J. Wright and Benjamin Recht.
Description: New York : Cambridge University Press, [2021] | Includes
bibliographical references and index.
Identifiers: LCCN 2021028671 (print) | LCCN 2021028672 (ebook) |
ISBN 9781316518984 (hardback) | ISBN 9781009004282 (epub)
Subjects: LCSH: Big data | Mathematical optimization. | Quantitative
research. | Artificial intgelligence. | BISAC: MATHEMATICS / General |
MATHEMATICS / General
Classification: LCC QA76.9.B45 W75 2021 (print) | LCC QA76.9.B45 (ebook)
| DDC 005.7–dc23
LC record available at https://lccn.loc.gov/2021028671
LC ebook record available at https://lccn.loc.gov/2021028672
ISBN 978-1-316-51898-4 Hardback
Cambridge University Press has no responsibility for the persistence or accuracy of
URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.
Cover image courtesy of © Isaac SparksContents
Preface page ix
1 Introduction 1
1.1 Data Analysis and Optimization 1
1.2 Least Squares 4
1.3 Matrix Factorization Problems 5
1.4 Support Vector Machines 6
1.5 Logistic Regression 9
1.6 Deep Learning 11
1.7 Emphasis 13
2 Foundations of Smooth Optimization 15
2.1 A Taxonomy of Solutions to Optimization Problems 15
2.2 Taylor’s Theorem 16
2.3 Characterizing Minima of Smooth Functions 18
2.4 Convex Sets and Functions 20
2.5 Strongly Convex Functions 22
3 Descent Methods 26
3.1 Descent Directions 27
3.2 Steepest-Descent Method 28
3.2.1 General Case 28
3.2.2 Convex Case 29
3.2.3 Strongly Convex Case 30
3.2.4 Comparison between Rates 32
3.3 Descent Methods: Convergence 33
3.4 Line-Search Methods: Choosing the Direction 36
3.5 Line-Search Methods: Choosing the Steplength 38
vvi Contents
3.6 Convergence to Approximate Second-Order Necessary Points 42
3.7 Mirror Descent 44
3.8 The KL and PL Properties 51
4 Gradient Methods Using Momentum 55
4.1 Motivation from Differential Equations 56
4.2 Nesterov’s Method: Convex Quadratics 58
4.3 Convergence for Strongly Convex Functions 62
4.4 Convergence for Weakly Convex Functions 66
4.5 Conjugate Gradient Methods 68
4.6 Lower Bounds on Convergence Rates 70
5 Stochastic Gradient 75
5.1 Examples and Motivation 76
5.1.1 Noisy Gradients 76
5.1.2 Incremental Gradient Method 77
5.1.3 Classification and the Perceptron 77
5.1.4 Empirical Risk Minimization 78
5.2 Randomness and Steplength: Insights 80
5.2.1 Example: Computing a Mean 80
5.2.2 The Randomized Kaczmarz Method 82
5.3 Key Assumptions for Convergence Analysis 85
5.3.1 Case 1: Bounded Gradients: Lg = 0 86
5.3.2 Case 2: Randomized Kaczmarz: B = 0, Lg > 0 86
5.3.3 Case 3: Additive Gaussian Noise 86
5.3.4 Case 4: Incremental Gradient 87
5.4 Convergence Analysis 87
5.4.1 Case 1: Lg = 0 89
5.4.2 Case 2: B = 0 90
5.4.3 Case 3: B and Lg Both Nonzero 92
5.5 Implementation Aspects 93
5.5.1 Epochs 93
5.5.2 Minibatching 94
5.5.3 Acceleration Using Momentum 94
6 Coordinate Descent 100
6.1 Coordinate Descent in Machine Learning 101
6.2 Coordinate Descent for Smooth Convex Functions 103
6.2.1 Lipschitz Constants 104
6.2.2 Randomized CD: Sampling with Replacement 105
6.2.3 Cyclic CD 110Contents vii
6.2.4 Random Permutations CD: Sampling without
Replacement 112
6.3 Block-Coordinate Descent 113
7 First-Order Methods for Constrained Optimization 118
7.1 Optimality Conditions 118
7.2 Euclidean Projection 120
7.3 The Projected Gradient Algorithm 122
7.3.1 General Case: A Short-Step Approach 123
7.3.2 General Case: Backtracking 124
7.3.3 Smooth Strongly Convex Case 125
7.3.4 Momentum Variants 126
7.3.5 Alternative Search Directions 126
7.4 The Conditional Gradient (Frank–Wolfe) Method 127
8 Nonsmooth Functions and Subgradients 132
8.1 Subgradients and Subdifferentials 134
8.2 The Subdifferential and Directional Derivatives 137
8.3 Calculus of Subdifferentials 141
8.4 Convex Sets and Convex Constrained Optimization 144
8.5 Optimality Conditions for Composite Nonsmooth Functions 146
8.6 Proximal Operators and the Moreau Envelope 148
9 Nonsmooth Optimization Methods 153
9.1 Subgradient Descent 155
9.2 The Subgradient Method 156
9.2.1 Steplengths 158
9.3 Proximal-Gradient Algorithms for Regularized Optimization 160
9.3.1 Convergence Rate for Convex f 162
9.4 Proximal Coordinate Descent for Structured Nonsmooth
Functions 164
9.5 Proximal Point Method 167
10 Duality and Algorithms 170
10.1 Quadratic Penalty Function 170
10.2 Lagrangians and Duality 172
10.3 First-Order Optimality Conditions 174
10.4 Strong Duality 178
10.5 Dual Algorithms 179
10.5.1 Dual Subgradient 179
10.5.2 Augmented Lagrangian Method 180viii Contents
10.5.3 Alternating Direction Method of Multipliers 181
10.6 Some Applications of Dual Algorithms 182
10.6.1 Consensus Optimization 182
10.6.2 Utility Maximization 184
10.6.3 Linear and Quadratic Programming 185
11 Differentiation and Adjoints 188
11.1 The Chain Rule for a Nested Composition of Vector Functions 188
11.2 The Method of Adjoints 190
11.3 Adjoints in Deep Learning 191
11.4 Automatic Differentiation 192
11.5 Derivations via the Lagrangian and Implicit Function Theorem 195
11.5.1 A Constrained Optimization Formulation of the
Progressive Function 195
11.5.2 A General Perspective on Unconstrained and
Constrained Formulations 197
11.5.3 Extension: Control 197
Appendix 200
A.1 Definitions and Basic Concepts 200
A.2 Convergence Rates and Iteration Complexity 203
A.3 Algorithm 3.1 Is an Effective Line-Search Technique 204
A.4 Linear Programming Duality, Theorems of the Alternative 205
A.5 Limiting Feasible Directions 208
A.6 Separation Results 209
A.7 Bounds for Degenerate Quadratic Functions 213
Bibliography 216
Index 223Preface
Optimization formulations and algorithms have long played a central role in
data analysis and machine learning. Maximum likelihood concepts date to
Gauss and Laplace in the late 1700s; problems of this type drove developments
in unconstrained optimization in the latter half of the 20th century. Man￾gasarian’s papers in the 1960s on pattern separation using linear programming
made an explicit connection between machine learning and optimization in the
early days of the former subject. During the 1990s, optimization techniques
(especially quadratic programming and duality) were key to the development
of support vector machines and kernel learning. The period 1997–2010 saw
many synergies emerge between regularized / sparse optimization, variable
selection, and compressed sensing. In the current era of deep learning, two
optimization techniques—stochastic gradient and automatic differentiation
(a.k.a. back-propagation)—are essential.
This book is an introduction to the basics of continuous optimization, with
an emphasis on techniques that are relevant to data analysis and machine
learning. We discuss basic algorithms, with analysis of their convergence
and complexity properties, mostly (though not exclusively) for the case of
convex problems. An introductory chapter provides an overview of the use of
optimization in modern data analysis, and the final chapter on differentiation
provides several perspectives on gradient calculation for functions that arise in
deep learning and control. The chapters in between discuss gradient methods,
including accelerated gradient and stochastic gradient; coordinate descent
methods; gradient methods for problems with simple constraints; theory and
algorithms for problems with convex nonsmooth terms; and duality based
methods for constrained optimization problems. The material is suitable for a
one-quarter or one-semester class at advanced undergraduate or early graduate
level. We and our colleagues have made extensive use of drafts of this material
in the latter setting.
ixx Preface
This book has been a work in progress since about 2010, when we began
to revamp our optimization courses, trying to balance the viewpoints of
practical optimization techniques against renewed interest in non-asymptotic
analyses of optimization algorithms. At that time, the flavor of analysis of
optimization algorithms was shifting to include a greater emphasis on worst￾case complexity. But algorithms were being judged more by their worst-case
bounds rather than by their performance on practical problems in applied
sciences. This book occupies a middle ground between analysis and practice.
Beginning with our courses CS726 and CS730 at University of Wisconsin,
we began writing notes, problems, and drafts. After Ben moved to UC Berkeley
in 2013, these notes became the core of the class EECS227C. Our material
drew heavily from the evolving theoretical understanding of optimization
algorithms. For instance, in several parts of the text, we have made use of the
excellent slides written and refined over many years by Lieven Vandenberghe
for the UCLA course ECE236C. Our presentation of accelerated methods
reflects a trend in viewing optimization algorithms as dynamical systems,
and was heavily influenced by collaborative work with Laurent Lessard and
Andrew Packard. In choosing what material to include, we tried to not be
distracted by methods that are not widely used in practice but also to highlight
how theory can guide algorithm selection and design by applied researchers.
We are indebted to many other colleagues whose input shaped the material
in this book. Moritz Hardt initially inspired us to try to write down our views
after we presented a review of optimization algorithms at the bootcamp for
the Simons Institute Program on Big Data in Fall 2013. He has subsequently
provided feedback on the presentation and organization of drafts of this
book. Ashia Wilson was Ben’s TA in EECS227C, and her input and notes
helped us to clarify our pedagogical messages in several ways. More recently,
Martin Wainwright taught EECS227C and provided helpful feedback, and
Jelena Diakonikolas provided corrections for the early chapters after she
taught CS726. Andre Wibisono provided perspectives on accelerated gradient ´
methods, and Ching pei Lee gave useful advice on coordinate descent. We are
also indebted to the many students who took CS726 and CS730 at Wisconsin
and EECS227C at Berkeley who found typos and beta tested homework
problems, and who continue to make this material a joy to teach. Finally,
we would like to thank the Simons Institute for supporting us on multiple
occasions, including Fall 2017 when we both participated in their program
on Optimization.
Madison, Wisconsin, USA
Berkeley, California, USA1
Introduction
This book is about the fundamentals of algorithms for solving continuous
optimization problems, which involve minimizing functions of multiple real￾valued variables, possibly subject to some restrictions or constraints on the
values that those variables may take. We focus particularly (though not
exclusively) on convex problems, and our choice of topics is motivated by
relevance to data science. That is, the formulations and algorithms that we
discuss are useful in solving problems from machine learning, statistics, and
data analysis.
To set the stage for subsequent chapters, the rest of this chapter outlines
several paradigms from data science and shows how they can be formulated
as continuous optimization problems. We must pay attention to particular
properties of these formulations their smoothness properties and structure
when we choose algorithms to solve them.
1.1 Data Analysis and Optimization
The typical optimization problem in data analysis is to find a model that agrees
with some collected data set but also adheres to some structural constraints that
reflect our beliefs about what a good model should be. The data set in a typical
analysis problem consists of m objects:
D := {(aj ,yj ), j = 1,2,...,m}, (1.1)
where aj is a vector (or matrix) of features and yj is an observation or label.
(We can assume that the data has been cleaned so that all pairs (aj ,yj ), j =
1,2,...,m have the same size and shape.) The data analysis task then consists
of discovering a function φ such that φ(aj ) ≈ yj for most j = 1,2,...,m. The
process of discovering the mapping φ is often called “learning” or “training.”
12 1 Introduction
The function φ is often defined in terms of a vector or matrix of parameters,
which we denote in what follows by x or X (and occasionally by other
notation). With these parametrizations, the problem of identifying φ becomes
a traditional data-fitting problem: Find the parameters x defining φ such that
φ(aj ) ≈ yj , j = 1,2,...,m in some optimal sense. Once we come up with
a definition of the term “optimal” (and possibly also with restrictions on the
values that we allow to parameters to take), we have an optimization problem.
Frequently, these optimization formulations have objective functions of the
finite sum type
LD(x) := 1
m
m
j=1
(aj ,yj ;x). (1.2)
The function (a,y;x) here represents a “loss” incurred for not properly
aligning our prediction φ(a) with y. Thus, the objective LD(x) measures the
average loss accrued over the entire data set when the parameter vector is
equal to x.
Once an appropriate value of x (and thus φ) has been learned from the data,
we can use it to make predictions about other items of data not in the set D
(1.1). Given an unseen item of data aˆ of the same type as aj , j = 1,2,...,m,
we predict the label yˆ associated with aˆ to be φ(a)ˆ . The mapping φ may also
expose other structures and properties in the data set. For example, it may
reveal that only a small fraction of the features in aj are needed to reliably
predict the label yj . (This is known as feature selection.) When the parameter
x is a matrix, it could reveal a low-dimensional subspace that contains most of
the vectors aj , or it could reveal a matrix with particular structure (low-rank,
sparse) such that observations of X prompted by the feature vectors aj yield
results close to yj .
The form of the labels yj differs according to the nature of the data analysis
problem.
• If each yj is a real number, we typically have a regression problem.
• When each yj is a label, that is, an integer drawn from the set {1,2,...,M}
indicating that aj belongs to one of M classes, this is a classification
problem. When M = 2, we have a binary classification problem, whereas
M > 2 is multiclass classification. (In data analysis problems arising in
speech and image recognition, M can be very large, of the order of
thousands or more.)
• The labels yj may not even exist; the data set may contain only the feature
vectors aj , j = 1,2,...,m. There are still interesting data analysis
problems associated with these cases. For example, we may wish to group1.1 Data Analysis and Optimization 3
the aj into clusters (where the vectors within each cluster are deemed to be
functionally similar) or identify a low-dimensional subspace (or a
collection of low-dimensional subspaces) that approximately contains the
aj . In such problems, we are essentially learning the labels yj alongside the
function φ. For example, in a clustering problem, yj could represent the
cluster to which aj is assigned.
Even after cleaning and preparation, the preceding setup may contain many
complications that need to be dealt with in formulating the problem in rigorous
mathematical terms. The quantities (aj ,yj ) may contain noise or may be
otherwise corrupted, and we would like the mapping φ to be robust to such
errors. There may be missing data: Parts of the vectors aj may be missing,
or we may not know all the labels yj . The data may be arriving in streaming
fashion rather than being available all at once. In this case, we would learn φ
in an online fashion.
One consideration that arises frequently is that we wish to avoid overfitting
the model to the data set D in (1.1). The particular data set D available to us
can often be thought of as a finite sample drawn from some underlying larger
(perhaps infinite) collection of possible data points, and we wish the function φ
to perform well on the unobserved data points as well as the observed subset D.
In other words, we want φ to be not too sensitive to the particular sample D that
is used to define empirical objective functions such as (1.2). One way to avoid
this issue is to modify the objective function by adding constraints or penalty
terms, in a way that limits the “complexity” of the function φ. This process is
typically called regularization. An optimization formulation that balances fit
to the training data D, model complexity, and model structure is
min
x∈
LD(x) + λ pen(x), (1.3)
where  is a set of allowable values for x, pen(·) is a regularization function or
regularizer, and λ ≥ 0 is a regularization parameter. The regularizer usually
takes lower values for parameters x that yield functions φ with lower complex￾ity. (For example, φ may depend on fewer of the features in the data vectors
aj or may be less oscillatory.) The parameter λ can be “tuned” to provide an
appropriate balance between fitting the data and lowering the complexity of φ:
Smaller values of λ tend to produce solutions that fit the training data D more
accurately, while large values of λ lead to less complex models.1
1 Interestingly, the concept of overfitting has been reexamined in recent years, particularly in the
context of deep learning, where models that perfectly fit the training data are sometimes
observed to also do a good job of classifying previously unseen data. This phenomenon is a
topic of intense current research in the machine learning community.4 1 Introduction
The constraint set  in (1.3) may be chosen to exclude values of x that are
not relevant or useful in the context of the data analysis problem. For example,
in some applications, we may not wish to consider values of x in which one
or more components are negative, so we could set  to be the set of vectors
whose components are all greater than or equal to zero.
We now examine some particular problems in data science that give rise to
formulations that are special cases of our master problem (1.3). We will see that
a large variety of problems can be formulated using this general framework, but
we will also see that within this framework, there is a wide range of structures
that must be taken into account in choosing algorithms to solve these problems
efficiently.
1.2 Least Squares
Probably the oldest and best known data analysis problem is linear least
squares. Here, the data points (aj ,yj ) lie in Rn × R, and we solve
min
x
1
2m
m
j=1

aT
j x yj
2
= 1
2mAx y2
2, (1.4)
where A the matrix whose rows are aT
j , j = 1,2,...,m and y =
(y1,y2,...,ym)T . In the preceding terminology, the function φ is defined
by φ(a) := aT x. (We can introduce a nonzero intercept by adding an extra
parameter β ∈ R and defining φ(a) := aT x + β.) This formulation can
be motivated statistically, as a maximum-likelihood estimate of x when the
observations yj are exact but for independent identically distributed (i.i.d.)
Gaussian noise. We can add a variety of penalty functions to this basic least
squares problem to impose desirable structure on x and, hence, on φ. For
example, ridge regression adds a squared 2-norm penalty, resulting in
min
x
1
2mAx y2
2 + λx2
2, for some parameter λ > 0.
The solution x of this regularized formulation has less sensitivity to perturba￾tions in the data (aj ,yj ). The LASSO formulation
min
x
1
2mAx y2
2 + λx1 (1.5)
tends to yield solutions x that are sparse – that is, containing relatively
few nonzero components (Tibshirani, 1996). This formulation performs
feature selection: The locations of the nonzero components in x reveal those1.3 Matrix Factorization Problems 5
components of aj that are instrumental in determining the observation yj .
Besides its statistical appeal – predictors that depend on few features are
potentially simpler and more comprehensible than those depending on many
features – feature selection has practical appeal in making predictions about
future data. Rather than gathering all components of a new data vector aˆ, we
need to find only the “selected” features because only these are needed to make
a prediction.
The LASSO formulation (1.5) is an important prototype for many problems
in data analysis in that it involves a regularization term λx1 that is non￾smooth and convex but has relatively simple structure that can potentially be
exploited by algorithms.
1.3 Matrix Factorization Problems
There are a variety of data analysis problems that require estimating a low-rank
matrix from some sparse collection of data. Such problems can be formulated
as natural extension of least squares to problems in which the data aj are
naturally represented as matrices rather than vectors.
Changing notation slightly, we suppose that each Aj is an n×p matrix, and
we seek another n × p matrix X that solves
min
X
1
2m
m
j=1
(Aj ,X yj )
2
, (1.6)
where A,B := trace(AT B). Here we can think of the Aj as “probing” the
unknown matrix X. Commonly considered types of observations are random
linear combinations (where the elements of Aj are selected i.i.d. from some
distribution) or single element observations (in which each Aj has 1 in a
single location and zeros elsewhere). A regularized version of (1.6), leading
to solutions X that are low rank, is
min
X
1
2m
m
j=1
(Aj ,X yj )
2 + λX∗, (1.7)
where X∗ is the nuclear norm, which is the sum of singular values of X
(Recht et al., 2010). The nuclear norm plays a role analogous to the 1 norm in
(1.5), where as the 1 norm favors sparse vectors, the nuclear norm favors low￾rank matrices. Although the nuclear norm is a somewhat complex nonsmooth
function, it is at least convex so that the formulation (1.7) is also convex. This
formulation can be shown to yield a statistically valid solution when the true6 1 Introduction
X is low rank and the observation matrices Aj satisfy a “restricted isometry
property,” commonly satisfied by random matrices but not by matrices with
just one nonzero element. The formulation is also valid in a different context,
in which the true X is incoherent (roughly speaking, it does not have a few
elements that are much larger than the others), and the observations Aj are of
single elements (Candes and Recht, ` 2009).
In another form of regularization, the matrix X is represented explicitly as
a product of two “thin” matrices L and R, where L ∈ Rn×r and R ∈ Rp×r,
with r 	 min(n,p). We set X = LRT in (1.6) and solve
min
L,R
1
2m
m
j=1
(Aj ,LRT  − yj )
2. (1.8)
In this formulation, the rank r is “hard wired” into the definition of X, so
there is no need to include a regularizing term. This formulation is also
typically much more compact than (1.7); the total number of elements in
(L,R) is (n + p)r, which is much less than np. However, this function is
nonconvex when considered as a function of (L,R) jointly. An active line of
current research, pioneered by Burer and Monteiro (2003) and also drawing on
statistical sources, shows that the nonconvexity is benign in many situations
and that, under certain assumptions on the data (Aj ,yj ), j = 1,2,...,m and
careful choice of algorithmic strategy, good solutions can be obtained from the
formulation (1.8). A clue to this good behavior is that although this formulation
is nonconvex, it is in some sense an approximation to a tractable problem: If we
have a complete observation of X, then a rank-r approximation can be found
by performing a singular value decomposition of X and defining L and R in
terms of the r leading left and right singular vectors.
Some applications in computer vision, chemometrics, and document clus￾tering require us to find factors L and R like those in (1.8) in which all elements
are nonnegative. If the full matrix Y ∈ Rn×p is observed, this problem has the
form
min
L,R
LRT Y 2
F, subject to L ≥ 0, R ≥ 0
and is called nonnegative matrix factorization.
1.4 Support Vector Machines
Classification via support vector machines (SVM) is a classical optimization
problem in machine learning, tracing its origins to the 1960s. Given the input1.4 Support Vector Machines 7
data (aj ,yj ) with aj ∈ Rn and yj ∈ { 1,1}, SVM seeks a vector x ∈ Rn and
a scalar β ∈ R such that
aT
j x β ≥ 1 when yj = +1, (1.9a)
aT
j x β ≤ 1 when yj = 1. (1.9b)
Any pair (x,β) that satisfies these conditions defines a separating hyperplane
in Rn, that separates the “positive” cases {aj | yj = +1} from the “negative”
cases {aj | yj = −1}. Among all separating hyperplanes, the one that
minimizes x2 is the one that maximizes the margin between the two classes –
that is, the hyperplane whose distance to the nearest point aj of either class is
greatest.
We can formulate the problem of finding a separating hyperplane as an
optimization problem by defining an objective with the summation form (1.2):
H(x,β) = 1
m
m
j=1
max(1 − yj (aT
j x − β),0). (1.10)
Note that the j th term in this summation is zero if the conditions (1.9) are
satisfied, and it is positive otherwise. Even if no pair (x,β) exists for which
H(x,β) = 0, a value (x,β) that minimizes (1.2) will be the one that comes
as close as possible to satisfying (1.9) in some sense. A term λx2
2 (for some
parameter λ > 0) is often added to (1.10), yielding the following regularized
version:
H(x,β) = 1
m
m
j=1
max(1 yj (aT
j x β),0) +
1
2
λx2
2. (1.11)
Note that, in contrast to the examples presented so far, the SVM problem has
a nonsmooth loss function and a smooth regularizer.
If λ is sufficiently small, and if separating hyperplanes exist, the pair
(x,β) that minimizes (1.11) is the maximum-margin separating hyperplane.
The maximum-margin property is consistent with the goals of generalizability
and robustness. For example, if the observed data (aj ,yj ) is drawn from
an underlying “cloud” of positive and negative cases, the maximum-margin
solution usually does a reasonable job of separating other empirical data
samples drawn from the same clouds, whereas a hyperplane that passes close
to several of the observed data points may not do as well (see Figure 1.1).
Often, it is not possible to find a hyperplane that separates the positive
and negative cases well enough to be useful as a classifier. One solution is
to transform all of the raw data vectors aj by some nonlinear mapping ψ and1.5 Logistic Regression 9
and Vapnik, 1995). This is the so-called kernel trick. (The kernel function K
can also be used to construct a classification function φ from the solution of
(1.14).) A particularly popular choice of kernel is the Gaussian kernel:
K(ak,al) := exp 1
2σ ak al2

,
where σ is a positive parameter.
1.5 Logistic Regression
Logistic regression can be viewed as a softened form of binary support vector
machine classification in which, rather than the classification function φ giving
a unqualified prediction of the class in which a new data vector a lies, it returns
an estimate of the odds of a belonging to one class or the other. We seek an
“odds function” p parametrized by a vector x ∈ Rn,
p(a;x) := (1 + exp(aT x)) 1
, (1.15)
and aim to choose the parameter x in so that
p(aj ;x) ≈ 1 when yj = +1; (1.16a)
p(aj ;x) ≈ 0 when yj = 1. (1.16b)
(Note the similarity to (1.9).) The optimal value of x can be found by
minimizing a negative-log likelihood function:
L(x) := − 1
m
⎡
⎣ 
j :yj=−1
log(1 − p(aj ;x)) + 
j :yj=1
log p(aj ;x)
⎤
⎦ . (1.17)
Note that the definition (1.15) ensures that p(a;x) ∈ (0,1) for all a and x;
thus, log(1 p(aj ;x)) < 0 and log p(aj ;x) < 0 for all j and all x. When the
conditions (1.16) are satisfied, these log terms will be only slightly negative,
so values of x that satisfy (1.17) will be near optimal.
We can perform feature selection using the model (1.17) by introducing a
regularizer λx1 (as in the LASSO technique for least squares (1.5)),
min
x
1
m
⎡
⎣ 
j :yj=−1
log(1 p(aj ;x)) + 
j :yj=1
log p(aj ;x)
⎤
⎦ + λx1,
(1.18)
where λ > 0 is a regularization parameter. As we see later, this term has
the effect of producing a solution in which few components of x are nonzero,10 1 Introduction
making it possible to evaluate p(a;x) by knowing only those components of a
that correspond to the nonzeros in x.
An important extension of this technique is to multiclass (or multinomial)
logistic regression, in which the data vectors aj belong to more than two
classes. Such applications are common in modern data analysis. For example,
in a speech recognition system, the M classes could each represent a phoneme
of speech, one of the potentially thousands of distinct elementary sounds
that can be uttered by humans in a few tens of milliseconds. A multinomial
logistic regression problem requires a distinct odds function pk for each class
k ∈ {1,2,...,M}. These functions are parametrized by vectors x[k] ∈ Rn,
k = 1,2,...,M, defined as follows:
pk(a;X) := exp(aT x[k])
M
l=1 exp(aT x[l])
, k = 1,2,...,M, (1.19)
where we define X := {x[k] | k = 1,2,...,M}. As in the binary case, we
have pk(a) ∈ (0,1) for all a and all k = 1,2,...,M and, in addition, that
M
k=1 pk(a) = 1. The functions (1.19) perform a “softmax” on the quantities
{aT x[l] | l = 1,2,...,M}.
In the setting of multiclass logistic regression, the labels yj are vectors in
RM whose elements are defined as follows:
yjk =

1 when aj belongs to class k,
0 otherwise. (1.20)
Similarly to (1.16), we seek to define the vectors x[k] so that
pk(aj ;X) ≈ 1 when yjk = 1 (1.21a)
pk(aj ;X) ≈ 0 when yjk = 0. (1.21b)
The problem of finding values of x[k] that satisfy these conditions can again be
formulated as one of minimizing a negative-log likelihood:
L(X) := 1
m
m
j=1


M
=1
yj(xT
[]aj ) log

M
=1
exp(xT
[]aj )
 . (1.22)
“Group-sparse” regularization terms can be included in this formulation to
select a set of features in the vectors aj , common to each class, that distinguish
effectively between the classes.1.6 Deep Learning 11
1.6 Deep Learning
Deep neural networks are often designed to perform the same function as
multiclass logistic regression – that is, to classify a data vector a into one of M
possible classes, often for large M. The major innovation is that the mapping
φ from data vector to prediction is now a nonlinear function, explicitly
parametrized by a set of structured transformations.
The neural network shown in Figure 1.2 illustrates the structure of a particu
lar neural net. In this figure, the data vector aj enters at the left of the network,
and each box (more often referred to as a “layer”) represents a transformation
that takes an input vector and applies a nonlinear transformation of the data
to produce an output vector. The output of each operator becomes the input
for one or more subsequent layers. Each layer has a set of its own parameters,
and the collection of all of the parameters over all the layers comprises our
optimization variable. The different shades of boxes here denote the fact that
the types of transformations might differ between layers, but we can compose
them in whatever fashion suits our application.
A typical transformation, which converts the vector al−1
j representing
output from layer l 1 to the vector al
j representing output from layer l, is
al
j = σ(Wl
al−1
j + gl
), (1.23)
where Wl is a matrix of dimension |al
j |×|al−1
j | and gl is a vector of length |al
j |.
The function σ is a componentwise nonlinear transformation, usually called an
activation function. The most common forms of the activation function σ act
independently on each component of their argument vector as follows:
- Sigmoid: t → 1/(1 + e−t
);
- Rectified Linear Unit (ReLU): t → max(t,0).
Alternative transformations are needed when the input to box l comes from
two or more preceding boxes (as in the case for some boxes in Figure 1.2).
The rightmost layer of the neural network (the output layer) typically has M
outputs, one for each of the possible classes to which the input (aj , say) could
belong. These are compared to the labels yjk, defined as in (1.20) to indicate
which of the M classes that aj belongs to. Often, a softmax is applied to the
Figure 1.2 A deep neural network, showing connections between adjacent layers,
where each layer is represented by a shaded rectangle.12 1 Introduction
outputs in the rightmost layer, and a loss function similar to (1.22) is obtained,
as we describe now.
Consider the special (but not uncommon) case in which the neural net
structure is a linear graph of D levels, in which the output for layer l 1
becomes the input for layer l (for l = 1,2,...,D) with aj = a0
j , j =
1,2,...,m, and the transformation within each box has the form (1.23). A
softmax is applied to the output of the rightmost layer to obtain a set of odds.
The parameters in this neural network are the matrix vector pairs (Wl
,gl
),
l = 1,2,...,D that transform the input vector aj = a0
j into the output aD
j of
the final layer. We aim to choose all these parameters so that the network does
a good job of classifying the training data correctly. Using the notation w for
the layer to layer transformations, that is,
w := (W1
,g1
,W2
,g2
,...,WD,gD),
we can write the loss function for deep learning as
L(w) = − 1
m
m
j=1


M
=1
yjaD
j,(w) − log

M
=1
exp aD
j,(w), (1.24)
where aD
j,(w) ∈ R is the output of the th element in layer D corresponding to
input vector a0
j . (Here we write aD
j,(w) to make explicit the dependence on the
transformations w as well as on the input vector aj .) We can view multiclass
logistic regression as a special case of deep learning with D = 1, so that
a1
j, = W1
,·
a0
j , where W1
,· denotes row  of the matrix W1.
Neural networks in use for particular applications (for example, in image
recognition and speech recognition, where they have been quite successful)
include many variants on the basic design. These include restricted connectiv￾ity between the boxes (which corresponds to enforcing sparsity structure on the
matrices Wl
, l = 1,2,...,D) and sharing parameters, which corresponds to
forcing subsets of the elements of Wl to take the same value. Arrangements of
the boxes may be quite complex, with outputs coming from several layers, con
nections across nonadjacent layers, different componentwise transformations
σ at different layers, and so on. Deep neural networks for practical applications
are highly engineered objects.
The loss function (1.24) shares with many other applications the finite sum
form (1.2), but it has several features that set it apart from the other applications
discussed before. First, and possibly most important, it is nonconvex in the
parameters w. Second, the total number of parameters in w is usually very
large. Effective training of deep learning classifiers typically requires a great
deal of data and computation power. Huge clusters of powerful computers –1.7 Emphasis 13
often using multicore processors, GPUs, and even specially architected pro￾cessing units – are devoted to this task.
1.7 Emphasis
Many problems can be formulated as in the framework (1.3), and their
properties may differ significantly. They might be convex or nonconvex, and
smooth or nonsmooth. But there are important features that they all share.
• They can be formulated as functions of real variables, which we typically
arrange in a vector of length n.
• The functions are continuous. When nonsmoothness appears in the
formulation, it does so in a structured way that can be exploited by the
algorithm. Smoothness properties allow an algorithm to make good
inferences about the behavior of the function on the basis of knowledge
gained at nearby points that have been visited previously.
• The objective is often made up in part of a summation of many terms,
where each term depends on a single item of data.
• The objective is often a sum of two terms: a “loss term” (sometimes arising
from a maximum likelihood expression for some statistical model) and a
“regularization term” whose purpose is to impose structure and
“generalizability” on the recovered model.
Our treatment emphasizes algorithms for solving these various kinds of
problems, with analysis of the convergence properties of these algorithms. We
pay attention to complexity guarantees, which are bounds on the amount of
computational effort required to obtain solutions of a given accuracy. These
bounds usually depend on fundamental properties of the objective function
and the data that defines it, including the dimensions of the data set and the
number of variables in the problem. This emphasis contrasts with much of
the optimization literature, in which global convergence results do not usually
involve complexity bounds. (A notable exception is the analysis of interior
point methods (see Nesterov and Nemirovskii, 1994; Wright, 1997)).
At the same time, we try as much as possible to emphasize the practical
concerns associated with solving these problems. There are a variety of trade￾offs presented by any problem, and the optimizer has to evaluate which tools
are most appropriate to use. On top of the problem formulation, it is imperative
to account for the time budget for the task at hand, the type of computer
on which the problem will be solved, and the guarantees needed for the14 1 Introduction
solution to be useful in the application that gave rise to the problem. Worst-case
complexity guarantees are only a piece of the story here, and understanding the
various parameters and heuristics that form part of any practical algorithmic
strategy are critical for building reliable solvers.
Notes and References
The softmax operator is ubiquitous in problems involving multiple classes.
Given real numbers z1,z2,...,zM, we define pj = ezj /
M
i=1 ezi and note
that pj ∈ (0,1) for all j , and M
j=1 pj = 1. Moreover, if for some j we have
zj  maxij zi, then pj ≈ 1 while pi ≈ 0 for all i  j .
The examples in this chapter are adapted from an article by one of the
authors (Wright, 2018).2
Foundations of Smooth Optimization
We outline here the foundations of the algorithms and theory discussed in
later chapters. These foundations include a review of Taylor’s theorem and its
consequences that form the basis of much of smooth nonlinear optimization.
We also provide a concise review of elements of convex analysis that will be
used throughout the book.
2.1 A Taxonomy of Solutions to Optimization Problems
Before we can begin designing algorithms, we must determine what it means
to solve an optimization problem. Suppose that f is a function mapping some
domain D = dom (f ) ⊂ Rn to the real line R. We have the following
definitions.
• x∗ ∈ D is a local minimizer of f if there is a neighborhood N of x∗ such
that f (x) ≥ f (x∗) for all x ∈ N ∩ D.
• x∗ ∈ D is a global minimizer of f if f (x) ≥ f (x∗) for all x ∈ D.
• x∗ ∈ D is a strict local minimizer if it is a local minimizer for some
neighborhood N of x∗ and, in addition, f (x) > f (x∗) for all x ∈ N with
x  x∗.
• x∗ is an isolated local minimizer if there is a neighborhood N of x∗ such
that f (x) ≥ f (x∗) for all x ∈ N ∩ D and, in addition, N contains no local
minimizers other than x∗.
• x∗ is the unique minimizer if it is the only global minimizer.
For the constrained optimization problem
min
x∈ f (x), (2.1)
1516 2 Foundations of Smooth Optimization
where  ⊂ D ⊂ Rn is a closed set, we modify the terminology slightly to use
the word “solution” rather than “minimizer.” That is, we have the following
definitions.
• x∗ ∈  is a local solution of (2.1) if there is a neighborhood N of x∗ such
that f (x) ≥ f (x∗) for all x ∈ N ∩ .
• x∗ ∈  is a global solution of (2.1) if f (x) ≥ f (x∗) for all x ∈ .
One of the immediate challenges is to provide a simple means of deter￾mining whether a particular point is a local or global solution. To do so, we
introduce a powerful tool from calculus: Taylor’s theorem. Taylor’s theorem is
the most important theorem in all of continuous optimization, and we review
it next.
2.2 Taylor’s Theorem
Taylor’s theorem shows how smooth functions can be approximated locally by
polynomials that depend on low-order derivatives of f .
Theorem 2.1 Given a continuously differentiable function f: Rn → R, and
given x,p ∈ Rn, we have that
f (x + p) = f (x) +
 1
0
∇f (x + γp)T pdγ, (2.2)
f (x + p) = f (x) + ∇f (x + γp)T p, some γ ∈ (0,1). (2.3)
If f is twice continuously differentiable, we have
∇f (x + p) = ∇f (x) +
 1
0
∇2f (x + γp)p dγ, (2.4)
f (x + p) = f (x) + ∇f (x)T p +
1
2
pT ∇2f (x + γp)p, some γ ∈ (0,1).
(2.5)
(We sometimes call the relation (2.2) the “integral form” and (2.3) the “mean￾value form” of Taylor’s theorem.)
A consequence of (2.3) is that for f continuously differentiable at x, we
have1
f (x + p) = f (x) + ∇f (x)T p + o(p). (2.6)
1 See the Appendix for a description of the order notation O(·) and o(·).2.2 Taylor’s Theorem 17
We prove this claim by manipulating (2.3) as follows:
f (x + p) = f (x) + ∇f (x + γp)T p
= f (x) + ∇f (x)T p + (∇f (x + γp) ∇ f (x))T p
= f (x) + ∇f (x)T p + O(∇f (x + γp) ∇ f (x)p)
= f (x) + ∇f (x)T p + o(p),
where the last step follows from continuity: ∇f (x + γp) ∇ f (x) → 0 as
p → 0, for all γ ∈ (0,1).
As we will see throughout this text, a crucial quantity in optimization is the
Lipschitz constant L for the gradient of f , which is defined to satisfy
∇f (x) ∇ f (y) ≤ Lx y, for all x,y ∈ dom (f ). (2.7)
We say that a continuously differentiable function f with this property is L￾smooth or has L Lipschitz gradients. We say that f is L0 Lipschitz if
|f (x) − f (y)| ≤ L0x − y, for all x,y ∈ dom (f ). (2.8)
From (2.2), we have
f (y) f (x) ∇ f (x)T (y x)
=
 1
0
[∇f (x + γ(y x)) ∇ f (x)]
T (y x)dγ .
By using (2.7), we have
[∇f (x + γ(y x)) ∇ f (x)]
T (y x)
≤ ∇f (x + γ(y x)) ∇ f (x)y x ≤ Lγ y x2.
By substituting this bound into the previous integral, we obtain the following
result.
Lemma 2.2 Given an L smooth function f , we have for any x,y ∈ dom (f )
that
f (y) ≤ f (x) + ∇f (x)T (y x) +
L
2 y x2. (2.9)
Lemma 2.2 asserts that f can be upper-bounded by a quadratic function
whose value at x is equal to f (x).
When f is twice continuously differentiable, we can characterize the
constant L in terms of the eigenvalues of the Hessian ∇2f (x). Specifically,
we have
−LI  ∇2f (x)  LI, for all x, (2.10)
as the following result proves.18 2 Foundations of Smooth Optimization
Lemma 2.3 Suppose f is twice continuously differentiable on Rn. Then if f is
L-smooth, we have ∇2f (x)  LI for all x. Conversely, if LI  ∇2f (x) 
LI , then f is L-smooth.
Proof From (2.9), we have, by setting y = x + αp for some α > 0, that
f (x + αp) − f (x) − α∇f (x)T p ≤
L
2
α2p2.
From formula (2.5) from Taylor’s theorem, we have for some γ ∈ (0,1) that
f (x + αp) − f (x) − α∇f (x)T p = 1
2
α2pT ∇2f (x + γαp)p.
By comparing these two expressions, we obtain
pT ∇2f (x + γαp)p ≤ Lp2.
By letting α ↓ 0, we have that all eigenvalues of ∇2f (x) are bounded by L, so
that ∇2f (x)  LI , as claimed.
Suppose now that LI  ∇2f (x)  LI for all x, so that ∇2f (x) ≤ L
for all x. We have, from (2.4), that
∇f (y) − ∇f (x) =





 1
t=0
∇2f (x + t(y − x))(y − x)dt





≤
 1
t=0
∇2f (x + t(y x))y x dt
≤
 1
t=0
Ly − x dt = Ly − x,
as required. This completes the proof. 
2.3 Characterizing Minima of Smooth Functions
The results of Section 2.2 give us the tools needed to characterize solutions of
the unconstrained optimization problem
min
x∈Rn f (x), (2.11)
where f is a smooth function.
We start with necessary conditions, which give properties of the derivatives
of f that are satisfied when x∗ is a local solution. We have the following
result.2.3 Characterizing Minima of Smooth Functions 19
Theorem 2.4 (Necessary Conditions for Smooth Unconstrained Optimization)
(a) Suppose that f is continuously differentiable. If x∗ is a local minimizer of
(2.11), then ∇f (x∗) = 0.
(b) Suppose that f is twice continuously differentiable. If x∗ is a local
minimizer of (2.11), then ∇f (x∗) = 0 and ∇2f (x∗) is positive
semidefinite.
Proof We start by proving (a). Suppose for contradiction that ∇f (x∗)  0, and
consider a step α∇f (x∗) away from x∗, where α is a small positive number.
By setting p = α∇f (x∗) in formula (2.3) from Theorem 2.1, we have
f (x∗ α∇f (x∗)) = f (x∗) α∇f 
x∗ γα∇f (x∗)
T ∇f (x∗), (2.12)
for some γ ∈ (0,1). Since ∇f is continuous, we have that
∇f 
x∗ γα∇f (x∗)
T ∇f (x∗) ≥
1
2
∇f (x∗)2
,
for all α sufficiently small, and any γ ∈ (0,1). Thus, by substituting into (2.12),
we have that
f (x∗ α∇f (x∗)) = f (x∗)
1
2
α∇f (x∗)2 < f (x∗),
for all positive and sufficiently small α. No matter how we choose the
neighborhood N in the definition of local minimizer, it will contain points
of the form x∗ − α∇f (x∗) for sufficiently small α. Thus, it is impossible to
choose a neighborhood N of x∗ such that f (x) ≥ f (x∗) for all x ∈ N , so x∗
is not a local minimizer.
We now prove (b). It follows immediately from (a) that ∇f (x∗) = 0, so
we need to prove only positive semidefiniteness of ∇2f (x∗). Suppose for
contradiction that ∇2f (x∗) has a negative eigenvalue, so there exists a vector
v ∈ Rn and a positive scalar λ such that vT ∇2f (x∗)v ≤ λ. We set x = x∗
and p = αv in formula (2.5) from Theorem 2.1, where α is a small positive
constant, to obtain
f (x∗ + αv) = f (x∗) + α∇f (x∗)
T v +
1
2
α2vT ∇2f (x∗ + γαv)v, (2.13)
for some γ ∈ (0,1). For all α sufficiently small, we have for λ, defined
previously, that vT ∇2f (x∗+γαv)v ≤ −λ/2, for all γ ∈ (0,1). By substituting
this bound, together with ∇f (x∗) = 0, into (2.13), we obtain
f (x∗ + αv) = f (x∗) − 1
4
α2λ < f (x∗),20 2 Foundations of Smooth Optimization
for all sufficiently small, positive values of α. Thus, there is no neighborhood
N of x∗ such that f (x) ≥ f (x∗) for all x ∈ N , so x∗ is not a local minimizer.
Thus, we have proved by contradiction that ∇2f (x∗) is positive semidefinite. 
Condition (a) in Theorem 2.4 is called the first order necessary condition,
because it involves the first-order derivatives of f . Similarly, condition (b) is
called the second-order necessary condition.
We call any point x satisfying ∇f (x) = 0 a stationary point.
We additionally have the following second-order sufficient condition.
Theorem 2.5 (Sufficient Conditions for Smooth Unconstrained Optimization)
Suppose that f is twice continuously differentiable and that, for some x∗, we
have ∇f (x∗) = 0, and ∇2f (x∗) is positive definite. Then x∗ is a strict local
minimizer of (2.11).
Proof We use formula (2.5) from Taylor’s theorem. Define a radius ρ suf￾ficiently small and positive such that the eigenvalues of ∇2f (x∗ + γp) are
bounded below by some positive number , for all p ∈ Rn with p ≤ ρ,
and all γ ∈ (0,1). (Because ∇2f is positive definite at x∗ and continuous, and
because the eigenvalues of a matrix are continuous functions of the elements
of a matrix, it is possible to choose ρ > 0 and  > 0 with these properties.) By
setting x = x∗ in (2.5), we have for some γ ∈ (0,1)
f (x∗ + p) = f (x∗) + ∇f (x∗)
T p +
1
2
pT ∇2f (x∗ + γp)p
≥ f (x∗) +
1
2
p2
, for all p with p ≤ ρ.
Thus, by setting N = {x∗ + p | p < ρ}, we have found a neighborhood of
x∗ such that f (x) > f (x∗) for all x ∈ N with x  x∗, hence satisfying the
conditions for a strict local minimizer. 
The sufficiency promised by Theorem 2.5 only guarantees a local solution.
We now turn to a special but ubiquitous class of functions and sets for which
we can provide necessary and sufficient guarantees for optimality, using only
information from low-order derivatives. The special property that enables these
guarantees is convexity.
2.4 Convex Sets and Functions
Convex functions take a central role in optimization precisely because these are
the instances for which it is easy to verify optimality and for which such optima
are guaranteed to be discoverable within a reasonable amount of computation.2.4 Convex Sets and Functions 21
A convex set  ⊂ Rn has the property that
x,y ∈  ⇒ (1 − α)x + αy ∈  for all α ∈ [0,1]. (2.14)
For all pairs of points (x,y) contained in , the line segment between x and
y is also contained in . The convex sets that we consider in this book are
usually closed.
The defining property of a convex function is the following inequality:
f ((1 − α)x + αy) ≤ (1 − α)f (x) + αf (y), for all x,y ∈ Rn, all α ∈ [0,1].
(2.15)
The line segment connecting (x,f (x)) and (y,f (y)) lies entirely above the
graph of the function f . In other words, the epigraph of f , defined as
epi f := {(x,t) ∈ Rn × R| t ≥ f (x)}, (2.16)
is a convex set. We sometimes call a function satisfying (2.15) as weakly
convex function, to distinguish it from the special class called strongly convex
functions, defined in Section 2.5.
The concepts of “minimizer” and “solution” for the case of convex objective
function and constraint set become more elementary in the convex case than in
the general case of Section 2.1. In particular, the distinction between “local”
and “global” solutions goes away.
Theorem 2.6 Suppose that, in the general constrained optimization problem
(2.1), the function f is convex, and the set  is closed and convex. We have the
following.
(a) Any local solution of (2.1) is also a global solution.
(b) The set of global solutions of (2.1) is a convex set.
Proof For (a), suppose for contradiction that x∗ ∈  is a local solution but not
a global solution, so there exists a point x¯ ∈  such that f (x) < f (x ¯ ∗). Then,
by convexity, we have for any α ∈ [0,1] that
f (x∗ + α(x¯ − x∗)) ≤ (1 − α)f (x∗) + αf (x) < f (x ¯ ∗).
But for any neighborhood N , we have for sufficiently small α > 0 that x∗ +
α(x¯ − x∗)) ∈ N ∩  and f (x∗ + α(x¯ − x∗)) < f (x∗), contradicting the
definition of a local minimizer.
For (b), we simply apply the definition of convexity for both sets and
functions. Given all global solutions x∗ and x¯, we have f (x)¯ = f (x∗), so
for any α ∈ [0,1], we have
f (x∗ + α(x¯ − x∗)) ≤ (1 − α)f (x∗) + αf (x)¯ = f (x∗).22 2 Foundations of Smooth Optimization
We have also that f (x∗ + α(x¯ x∗)) ≥ f (x∗), since x∗ + α(x¯ x∗) ∈
 and x∗ is a global minimizer. It follows from these two inequalities that
f (x∗+α(x¯ x∗)) = f (x∗), so that x∗+α(x¯ x∗) is also a global minimizer. 
By applying Taylor’s theorem (in particular, (2.6)) to the left hand side of
the definition of convexity (2.15), we obtain
f (x + α(y x)) = f (x)+α∇f (x)T (y x) + o(α) ≤ (1 α)f (x) + αf (y).
By canceling the f (x) term, rearranging, and dividing by α, we obtain
f (y) ≥ f (x) + ∇f (x)T (y x) + o(1),
and when α ↓ 0, the o(1) term vanishes, so we obtain
f (y) ≥ f (x) + ∇f (x)T (y x), for any x,y ∈ dom (f ), (2.17)
which is a fundamental characterization of convexity of a smooth function.
While Theorem 2.4 provides a necessary link between the vanishing of
∇f and the minimizing of f , the first order necessary condition is actually
a sufficient condition when f is convex.
Theorem 2.7 Suppose that f is continuously differentiable and convex. Then
if ∇f (x∗) = 0, then x∗ is a global minimizer of (2.11).
Proof The proof of the first part follows immediately from condition (2.17), if
we set x = x∗. Using this inequality together with ∇f (x∗) = 0, we have, for
any y, that
f (y) ≥ f (x∗) + ∇f (x∗)
T (y x∗) = f (x∗),
so that x∗ is a global minimizer. 
2.5 Strongly Convex Functions
For the remainder of this section, we assume that f is continuously differen
tiable and also convex. If there exists a value m > 0 such that
f ((1 α)x + αy) ≤ (1 α)f (x) + αf (y)
1
2
mα(1 α)x y2
2
(2.18)
for all x and y in the domain of f , we say that f is strongly convex with
modulus of convexity m. When f is differentiable, we have the following2.5 Strongly Convex Functions 23
equivalent definition, obtained by working on (2.18) with an argument similar
to the one leading to (2.17) that
f (y) ≥ f (x) + ∇f (x)T (y − x) + m
2 y − x2. (2.19)
Note that this inequality complements the inequality satisfied by functions with
smooth gradients. When the gradients are smooth, a function can be upper
bounded by a quadratic that takes the value f (x) at x. When the function is
strongly convex, it can be lower-bounded by a quadratic that takes the value
f (x) at x.
We have the following extension of Theorem 2.7, whose proof follows
immediately by setting x = x∗ in (2.19).
Theorem 2.8 Suppose that f is continuously differentiable and strongly
convex. Then if ∇f (x∗) = 0, then x∗ is the unique global minimizer of f .
This approximation of convex f by quadratic functions is a key theme in
continuous optimization.
When f is strongly convex and twice continuously differentiable, (2.5)
implies the following, when x∗ is the minimizer:
f (x) − f (x∗) = 1
2
(x − x∗)
T ∇2f (x∗)(x − x∗) + o(x − x∗2). (2.20)
Thus, f behaves like a strongly convex quadratic function in a neighborhood
of x∗. It follows that we can learn a lot about local convergence properties
of algorithms just by studying convex quadratic functions. We use quadratic
functions as a guide for both intuition and algorithmic derivation throughout.
Just as we could characterize the Lipschitz constant of the gradient in
terms of the eigenvalues of the Hessian, the modulus of convexity provides
a lower bound on the eigenvalues of the Hessian when f is twice continuously
differentiable.
Lemma 2.9 Suppose that f is twice continuously differentiable on Rn. Then
f has modulus of convexity m if and only if ∇2f (x)  mI for all x.
Proof For any x,u ∈ Rn and α > 0, we have from Taylor’s theorem that
f (x + αu) =f (x) + α∇f (x)T+
1
2
α2uT ∇2f (x + γαu)u,for some γ ∈ (0,1).
From the strong convexity property, we have
f (x + αu) ≥ f (x) + α∇f (x)T u + m
2
α2u2.24 2 Foundations of Smooth Optimization
By comparing these two expressions, canceling terms, and dividing by α2, we
obtain
uT ∇2f (x + γαu)u ≥ mu2.
By taking α ↓ 0, we obtain uT ∇2f (x)u ≥ mu2, thus proving that
∇2f (x)  mI .
For the converse, suppose that ∇2f (x)  mI for all x. Using the same form
of Taylor’s theorem as before, we obtain
f (z) = f (x) + ∇f (x)T (z − x)
+
1
2
(z x)T ∇2f (x + γ(z x))(z x), for some γ ∈ (0,1).
We obtain the strong convexity expression when we bound the last term as
follows:
(z x)T ∇2f (x + γ(z x))(z x) ≥ mz x2
,
completing the proof. 
The following corollary is a immediate consequence of Lemma 2.3.
Corollary 2.10 Suppose that the conditions of Lemma 2.3 hold, and in
addition that f is convex. Then 0  ∇2f (x)  LI if and only if f is L￾smooth.
Notation
We use · to denote the Euclidean norm ·2 of a vector in Rn. Other norms,
such as ·1 and ·∞, will be denoted explicitly.
Notes and References
The classic reference on convex analysis remains the text of Rockafellar
(1970), which is still remarkably fresh, with many fundamental results. A
more recent classic by Boyd and Vandenberghe (2003) contains a great
deal of information about convex optimization, especially concerning convex
formulations and applications of convex optimization.2.5 Strongly Convex Functions 25
Exercises
1. Prove that the effective domain of a convex function f (that is, the set of
points x ∈ Rn such that f (x) < ∞) is a convex set.
2. Prove that epi f is a convex subset of Rn × R for any convex function f .
3. Suppose that f: Rn → R is convex and concave. Show that f must be an
affine function.
4. Suppose that f: Rn → R is convex and upper bounded. Show that f must
be a constant function.
5. Suppose f: Rn → R is strongly convex and Lipschitz. Show that no such
f exists.
6. Show rigorously how (2.19) is derived from (2.18) when f is continuously
differentiable.
7. Suppose that f: Rn → R is a convex function with L Lipschitz gradient
and a minimizer x∗ with function value f ∗ = f (x∗).
(a) Show (by minimizing both sides of (2.9) with respect to y) that for any
x ∈ Rn, we have
f (x) f ∗ ≥
1
2L∇f (x)2.
(b) Prove the following co-coercivity property: For any x,y ∈ Rn, we have
[∇f (x) ∇ f (y)]
T (x y) ≥
1
L∇f (x) ∇ f (y)2.
Hint: Apply part (a) to the following two functions:
hx (z) := f (z) − ∇f (x)T z, hy (z) := f (z) − ∇f (y)T z.
8. Suppose that f: Rn → R is an m strongly convex function with
L Lipschitz gradient and (unique) minimizer x∗ with function value
f ∗ = f (x∗).
(a) Show that the function q(x) := f (x) m
2 x2 is convex with
L m-Lipschitz continuous gradients.
(b) By applying the co-coercivity property of the previous question to this
function q, show that the following property holds:
[∇f (x) − ∇f (y)]
T (x − y)
≥
mL
m + Lx y2 +
1
m + L∇f (x) ∇ f (y)2. (2.21)3
Descent Methods
Methods that use information about gradients to obtain descent in the objective
function at each iteration form the basis of all of the schemes studied in this
book. We describe several fundamental methods of this type and analyze their
convergence and complexity properties. This chapter can be read as an intro￾duction both to elementary methods based on gradients of the objective and
to the fundamental tools of analysis that are used to understand optimization
algorithms.
Throughout the chapter, we consider the unconstrained minimization of a
smooth convex function:
min
x∈Rn f (x). (3.1)
The algorithms of this chapter are suited to the case in which f and its gradient
∇f can be evaluated – exactly, in principle – at arbitrary points x. Bearing in
mind that this setup may not hold for many data analysis problems, we focus on
those fundamental algorithms that can be extended to more general situations,
for example:
• Objectives consisting of a smooth convex term plus a nonconvex
regularization term
• Minimization of smooth functions over simple constraint sets, such as
bounds on the components of x
• Functions for which f or ∇f cannot be evaluated exactly without a
complete sweep through the data set, but unbiased estimates of ∇f can be
obtained easily
• Situations in which it is much less expensive to evaluate an individual
component or a subvector of ∇f than the full gradient vector
• Smooth but nonconvex f
263.1 Descent Directions 27
Extensions to the fundamental methods in this chapter to these more general
situations will be considered in subsequent chapters.
3.1 Descent Directions
Most of the algorithms we will consider in this book generate a sequence of
iterates {xk} for which the function values decrease at each iteration that is,
f (xk+1) < f (xk) for each k = 0,1,2,.... Line-search methods proceed by
identifying a direction d from each x such that f decreases as we move in the
direction d. This notion can be formalized by the following definition:
Definition 3.1 d is a descent direction for f at x if f (x + td) < f (x) for all
t > 0 sufficiently small.
A simple, sufficient characterization of descent directions is given by the
following proposition.
Proposition 3.2 If f is continuously differentiable in a neighborhood of x,
then any d such that dT ∇f (x) < 0 is a descent direction.
Proof We use Taylor’s theorem Theorem 2.1. By continuity of ∇f , we can
identify t > 0 such that ∇f (x + td)T d < 0 for all t ∈ [0,t]. Thus, from (2.3),
we have for any t ∈ (0,t] that
f (x + td) = f (x) + t∇f (x + γtd)T d, some γ ∈ (0,1),
from which it follows that f (x + td) < f (x), as claimed. 
Note that, among all directions d with unit norm, the one that minimizes
dT ∇f (x) is d = ∇ f (x)/∇f (x). For this reason, we refer to ∇f (x) as
the steepest-descent direction. Perhaps the simplest method for optimization
of a smooth function makes use of this direction, defining its iterates by
xk+1 = xk − αk∇f (xk), k = 0,1,2,..., (3.2)
for some steplength αk > 0. At each iteration, we are guaranteed that there is
some nonnegative step α that decreases the function value, unless ∇f (xk) = 0.
But note that when ∇f (x) = 0 (that is, x is stationary), we will have found a
point that satisfies a first-order necessary condition for local optimality. (If f is
also convex, this point will be a global minimizer of f .) The algorithm defined
by (3.2) is called the gradient descent method or the steepest-descent method.
(We use the latter term in this chapter.) In the next section, we will discuss the28 3 Descent Methods
choice of steplengths αk and analyze how many iterations are required to find
points where the gradient nearly vanishes.
3.2 Steepest-Descent Method
We focus first on the question of choosing the steplength αk for the steepest
descent method (3.2). If αk is too large, we risk taking a step that increases the
function value. On the other hand, if αk is too small, we risk making too little
progress and thus requiring too many iterations to find a solution.
The simplest steplength protocol is the short step variant of steepest
descent, which can be implemented when f is L-smooth (see (2.7)) with a
known value of the parameter L. By setting αk to be a fixed, constant value α,
the formula (3.2) becomes
xk+1 = xk − α∇f (xk), k = 0,1,2,... . (3.3)
To estimate the amount of decrease in f obtained at each iterate of this method,
we use Lemma 2.2, which is a consequence of Taylor’s theorem (Theorem 2.1).
We obtain
f (x + αd) ≤ f (x) + α∇f (x)T d + α2L
2 d2. (3.4)
For d = ∇ f (x), the value of α that minimizes the expression on the right￾hand side is α = 1/L. By substituting this value into (3.4) and setting x = xk,
we obtain
f (xk+1) = f (xk (1/L)∇f (xk)) ≤ f (xk)
1
2L∇f (xk)2. (3.5)
This expression is one of the foundational inequalities in the analysis of
optimization methods. It quantifies the amount of decrease we can obtain from
the function f to two critical quantities: the norm of the gradient ∇f (xk) at the
current iterate and the Lipschitz constant L of the gradient. Depending on the
other assumptions about f , we can derive a variety of different convergence
rates from this basic inequality, as we now show.
3.2.1 General Case
From (3.5) alone, we can already say something about the rate of convergence
of the steepest-descent method, provided we assume that f has a global lower
bound. That is, we assume that there is a value f that satisfies
f (x) ≥ f,¯ for all x. (3.6)3.2 Steepest-Descent Method 29
(In the case that f has a global minimizer x∗, f¯ could be any value such that
f ≤ f (x∗).) By summing the inequalities (3.5) over k = 0,1,...,T 1, and
canceling terms, we find that
f (xT ) ≤ f (x0)
1
2L
T
−1
k=0
∇f (xk)2.
Since f¯ ≤ f (xT ), we have
T
−1
k=0
∇f (xk)2 ≤ 2L[f (x0) f ]
which implies that limT →∞ ∇f (xT ) = 0. Moreover, we have
min
0≤k≤T −1
∇f (xk)2 ≤
1
T
T

1
k=0
∇f (xk)2 ≤
2L[f (x0) f¯]
T .
Thus, we have shown that after T steps of steepest descent, we can find a point
x satisfying
min
0≤k≤T −1
∇f (xk) ≤ 
2L[f (x0) − f¯]
T . (3.7)
Note that this convergence rate is slow and tells us only that we will find a
point xk that is nearly stationary. We need to assume stronger properties of f
to guarantee faster convergence and global optimality.
3.2.2 Convex Case
When f is also convex, we have the following stronger result for the steepest
descent method.
Theorem 3.3 Suppose that f is convex and L-smooth, and suppose that (3.1)
has a solution x∗. Define f ∗ := f (x∗). Then the steepest-descent method with
steplength αk ≡ 1/L generates a sequence {xk}∞
k=0 that satisfies
f (xT ) − f ∗ ≤
L
2T x0 − x∗2
, T = 1,2,... . (3.8)30 3 Descent Methods
Proof By convexity of f , we have f (x∗) ≥ f (xk) + ∇f (xk)T (x∗ xk), so
by substituting into the key inequality (3.5), we obtain for k = 0,1,2,... that
f (xk+1) ≤ f (x∗) + ∇f (xk)
T (xk x∗)
1
2L∇f (xk)2
= f (x∗) +
L
2

xk − x∗2 − xk − x∗ − 1
L
∇f (xk)2

= f (x∗) +
L
2

xk − x∗2 − xk+1 − x∗2

.
By summing over k = 0,1,2,...,T 1, we have
T
−1
k=0
(f (xk+1) f ∗) ≤
L
2
T
−1
k=0

xk x∗2  xk+1 x∗2

= L
2

x0 x∗2  xT x∗2

≤
L
2 x0 x∗2.
Since {f (xk)} is a nonincreasing sequence, we have
f (xT ) f ∗ ≤
1
T
T
−1
k=0
(f (xk+1) f ∗) ≤
L
2T x0 x∗2
,
as desired. 
3.2.3 Strongly Convex Case
Recall from (2.19) that the smooth function f: Rn → R is strongly convex
with modulus m if there is a scalar m > 0 such that
f (z) ≥ f (x) + ∇f (x)T (z x) + m
2 z x2. (3.9)
Strong convexity asserts that f can be lower bounded by quadratic functions.
These functions change from point to point, but only in the linear term. It also
tells us that the curvature of the function is bounded away from zero. Note that
if f is strongly convex and L-smooth, then f is bounded above and below by
simple quadratics (see (2.9) and (2.19)). This “sandwiching” effect enables us
to prove the linear convergence of the steepest-descent method.
The simplest strongly convex function is the squared Euclidean norm x2.
Any convex function can be perturbed to form a strongly convex function by3.2 Steepest-Descent Method 31
adding any small positive multiple of the squared Euclidean norm. In fact, if f
is any L-smooth function, then
fμ(x) = f (x) + μx2
is strongly convex for μ large enough. (Exercise: Prove this!)
As another canonical example, note that a quadratic function f (x) = 1
2 xT Qx is strongly convex if and only if the smallest eigenvalue of Q is
strictly positive. We saw in Theorem 2.8 that a strongly convex f has a unique
minimizer, which we denote by x∗.
Strongly convex functions are, in essence, the “easiest” functions to opti￾mize by first-order methods. First, the norm of the gradient provides useful
information about how far away we are from optimality. Suppose we minimize
both sides of the inequality (3.9) with respect to z. The minimizer on the left￾hand side is clearly attained at z = x∗, while on the right-hand side, it is
attained at x − ∇f (x)/m. By plugging these optimal values into (3.9), we
obtain
f (x∗) ≥ f (x) ∇ f (x)T
 1
m∇f (x)
+ m
2




1
m∇f (x)




2
= f (x)
1
2m∇f (x)2.
By rearrangement, we obtain
∇f (x)2 ≥ 2m[f (x) f (x∗)]. (3.10)
If ∇f (x) < δ, we have
f (x) f (x∗) ≤ ∇f (x)2
2m ≤
δ2
2m.
Thus, for strongly convex functions, when the gradient is small, we are close
to having found a point with minimal function value.
We can derive an estimate of the distance of x to the optimal point x∗
in terms of the gradient by using (3.9) and the Cauchy Schwarz inequality.
We have
f (x∗) ≥ f (x) + ∇f (x)T (x∗ − x) + m
2 x − x∗2
≥ f (x) ∇ f (x) x∗ x + m
2 x x∗2.
By rearranging terms, we have
x − x∗ ≤
2
m∇f (x). (3.11)
We summarize this discussion in the following lemma.32 3 Descent Methods
Lemma 3.4 Let f be a continuously differentiable and strongly convex
function with modulus m. Then we have
f (x) f (x∗) ≤ ∇f (x)2
2m (3.12)
x x∗ ≤
2
m∇f (x). (3.13)
We can now analyze the convergence of the steepest-descent method on
strongly convex functions. By substituting (3.12) into (3.5), we obtain
f (xk+1) = f

xk 1
L
∇f (xk)

≤ f (xk)
1
2L∇f (xk)2
≤ f (xk) m
L(f (xk) f ∗),
where f ∗ := f (x∗), as before. Subtracting f ∗ from both sides of this
inequality gives the recursion
f (xk+1) f ∗ ≤

1 m
L

(f (xk) f ∗). (3.14)
Thus, the sequence of function values converges linearly to the optimum. After
T steps, we have
f (xT ) f ∗ ≤

1 m
L
T
(f (x0) f ∗). (3.15)
3.2.4 Comparison between Rates
It is straightforward to convert these convergence expressions into complex￾ities using the techniques of Appendix A.2. We have, from (3.7), that an
iteration k will be found such that ∇f (xk) ≤  for some k ≤ T , where
T ≥
2L(f (x0) f ∗)
2 .
For the general convex case, we have from (3.8) that f (xk) f ∗ ≤  when
k ≥
Lx0 x∗2
2
. (3.16)
For the strongly convex case, we have from (3.15) that f (xk) − f ∗ ≤  for all
k satisfying
k ≥
L
m log((f (x0) f ∗)/). (3.17)3.3 Descent Methods: Convergence 33
Note that in all three cases, we can get bounds in terms of the initial distance
to optimality x0 x∗ rather than the initial optimality gap f (x0) f ∗ by
using the inequality
f (x0) f ∗ ≤
L
2 x0 x∗2.
The linear rate (3.17) depends only logarithmically on , whereas the
sublinear rates depend on 1/ or 1/2. When  is small (for example,  =
10−6), the linear rate would appear to be dramatically faster, and, indeed, this
is usually the case. The only exception would be when m is extremely small,
so that m/L is of the same order as . The problem is extremely ill conditioned
in this case, and there is little difference between the linear rate (3.17) and the
sublinear rate (3.16).
All of these bounds depend on knowledge of L. What happens when we do
not know L? Even when we do know it, is the steplength αk ≡ 1/L good in
practice? We have reason to suspect not, since the inequality (3.5) on which it
is based uses the conservative global upper bound L on curvature. (A sharper
bound could be obtained in terms of the curvature in the neighborhood of the
current iterate xk.) In the remainder of this chapter, we expand our view to
more general choices of search directions and steplengths.
3.3 Descent Methods: Convergence
In the previous section, we considered the short-step steepest-descent method
that moved along the negative gradient with a steplength 1/L determined
by the global Lipschitz constant of the gradient. In this section, we prove
convergence results for more general descent methods.
Suppose each step has the form
xk+1 = xk + αkdk
, k = 0,1,2,..., (3.18)
where dk is a descent direction and αk is a positive steplength. What do we
need to guarantee convergence to a stationary point at a particular rate? What
do we need to guarantee convergence of the iterates themselves?
Recall that our analysis of steepest-descent algorithm with fixed steplength
in the previous section was based on the bound (3.5), which showed that the
amount of decrease in f at iteration k is at least a multiple of ∇f (xk)2. In the
discussion that follows, we show that the same estimate of function decrease,
except for a different constant, can be obtained for many line-search methods34 3 Descent Methods
of the form (3.18), provided that dk and αk satisfy certain intuitive properties.
Specifically, we show that the following inequality holds:
f (xk+1) ≤ f (xk) C∇f (xk)2
, for some C > 0. (3.19)
The remainder of the analyses in the previous section used properties about
the function f itself that were independent of the algorithm: smoothness,
convexity, and strong convexity. For a general descent method, we can provide
similar analyses based on the property (3.19).
What can we say about the sequence of iterates {xk} generated by a scheme
that guarantees (3.19)? The following elementary theorem shows one basic
property.
Theorem 3.5 Suppose that f is bounded below, with Lipschitz continuous
gradient. Then all accumulation points x¯ of the sequence {xk} generated by a
scheme that satisfies (3.19) are stationary; that is, ∇f (x) = 0. If, in addition,
f is convex, each such x is a solution of (3.1).
Proof Note first from (3.19) that
∇f (xk)2 ≤ [f (xk) f (xk+1)]/C, k = 0,1,2,...,
and since {f (xk)} is a decreasing sequence that is bounded below, it follows
that limk→∞ f (xk) f (xk+1) = 0. If x¯ is an accumulation point, there is
a subsequence S such that limk∈S,k→∞ xk = ¯x. By continuity of ∇f , we
have ∇f (x)¯ = limk∈S,k→∞ ∇f (xk) = 0, as required. If f is convex, each x¯
satisfies the first-order sufficient conditions to be a solution of (3.1). 
It is possible for the the sequence {xk} to be unbounded and have no
accumulation points. For example, some descent methods applied to the scalar
function f (x) = e−x will generate iterates that diverge to ∞. (This function is
convex and bounded below but does not attain its minimum value.)
We can prove other results about rates of convergence of algorithms (3.18)
satisfying (3.19), using almost identical proofs to those of Section 3.2. For
example, for the case in which f is bounded below by some quantity f¯, we
can show using the techniques of Section 3.2.1 that
min
0≤k≤T −1
∇f (xk) ≤ 
f (x0) − f¯
CT .
For the case in which f is strongly convex with modulus m (and unique
solution x∗), we can combine (3.12) with (3.19) to deduce that3.3 Descent Methods: Convergence 35
f (xk+1) f (x∗) ≤ f (xk) f (x∗) C∇f (xk)2
≤ (1 − 2mC)[f (xk) − f (x∗)],
which indicates linear convergence with rate (1 2mC).
The argument of Section 3.2.2 concerning rate of convergence for the
(non-strongly) convex case cannot be generalized to the setting of (3.19),
though similar results can be obtained by another technique under an additional
assumption, as we show next.
Theorem 3.6 Suppose that f is convex and smooth, where ∇f has Lipschitz
constant L, and that (3.1) has a solution x∗. Assume, moreover, that the level
set defined by x0 is bounded in the sense that R0 < ∞, where
R0 := max{x x∗| f (x) ≤ f (x0)}.
Then a descent method satisfying (3.19) generates a sequence {xk}∞
k=0 that
satisfies
f (xT ) f ∗ ≤
R2
0
CT
T = 1,2,... . (3.20)
Proof Defining k := f (xk) f (x∗), we have that
k = f (xk) f (x∗) ≤ ∇f (xk)
T (xk x∗) ≤ R0∇f (xk).
By substituting this bound into (3.19), we obtain
f (xk+1) ≤ f (xk) − C
R2
0
2
k,
which, after subtracting f (x∗) from both sides and using the definition of k,
becomes
k+1 ≤ k
C
R2
0
2
k = k

1
C
R2
0
k

. (3.21)
By inverting both sides, we obtain
1
k+1
≥
1
k
1
1 C
R2
0
k
.
Since k+1 ≥ 0, we have from (3.21) that C
R2
0
k ∈ [0,1], so using the fact that
1
1− ≥ 1 +  for all  ∈ [0,1], we obtain
1
k+1
≥
1
k

1 +
C
R2
0
k

= 1
k
+
C
R2
0
.36 3 Descent Methods
By applying this formula recursively, we have for any T ≥ 1 that
1
T
≥
1
0
+
T C
R2
0
≥
T C
R2
0
,
and we obtain the result by taking the inverse of both sides in this bound and
using T = f (xT ) f (x∗). 
3.4 Line-Search Methods: Choosing the Direction
In this section, we turn to analysis of generic line-search descent methods,
which take steps of the form (3.18), where αk > 0 and dk is a search direction
that satisfies the following properties, for some positive constants , γ1, γ2:
0 <  ≤ (dk)T ∇f (xk)
∇f (xk)dk
, (3.22a)
0 < γ1 ≤ dk
∇f (xk) ≤ γ2. (3.22b)
Condition (3.22a) says that the angle between ∇f (xk) and dk is acute and
bounded away from π/2 for all k, and condition (3.22b) ensures that dk and
∇f (xk) are not too much different in length. (If xk is a stationary point, we
have ∇f (xk) = 0, so our algorithm will set dk = 0 and terminate.)
For the negative gradient (steepest-descent) search direction dk =
∇f (xk), the conditions (3.22) hold trivially, with  = γ1 = γ2 = 1.
We can use Taylor’s theorem to bound the change in f when we move along
dk from the current iteration xk. By setting x = xk and d = dk in (3.4), we
obtain
f (xk+1) = f (xk + αdk)
≤ f (xk) + α∇f (xk)
T dk + α2L
2 dk2
≤ f (xk) α∇f (xk)dk + α2L
2 dk2
≤ f (xk) − α

¯ − α
L
2 γ2

∇f (xk)dk, (3.23)
where we used (3.22) for the last two inequalities. It is clear from this
expression that for all values of α sufficiently small – to be precise, for
α ∈ (0,2/(Lγ ¯ 2)) – we have f (xk+1) < f (xk) – unless, of course, xk is a
stationary point.3.4 Line-Search Methods: Choosing the Direction 37
We mention a few possible choices of dk apart from the negative gradient
direction ∇f (xk).
• The transformed negative gradient direction dk = Sk∇f (xk), where Sk is
a symmetric positive definite matrix with eigenvalues in the range [γ1,γ2],
where γ1 and γ2 are positive quantities, as in (3.22). The condition (3.22b)
holds, by definition of Sk, and condition (3.22a) holds with ¯ = γ1/γ2,
since
−(dk)
T ∇f (xk) = ∇f (xk)
T Sk∇f (xk) ≥ γ1∇f (xk)2
≥ (γ1/γ2)∇f (xk)dk.
Newton’s method, which chooses Sk = ∇2f (xk)−1, would satisfy this
condition, provided that the Hessian ∇2f (x) has eigenvalues uniformly
bounded in the range [1/γ2,1/γ1] for all x.
• The Gauss–Southwell variant of coordinate descent chooses
dk = −[∇f (xk)]ik eik , where ik = arg maxi=1,2, ,n |[∇f (xk)]i| and eik is
the vector containing all zeros except for a 1 in position ik. (We leave it as an
exercise to show that the conditions (3.22) are satisfied for this choice of dk.)
There does not seem to be an obvious reason to use this search direction.
Since it is defined in terms of the full gradient ∇f (xk), why not use
dk = ∇ f (xk) instead? The answer (as we discuss further in Chapter 6)
is that for some important kinds of functions f , the gradient ∇f (xk) can
be updated efficiently to obtain ∇f (xk+1), provided that xk and xk+1 differ
in only a single coordinate. These cost savings make coordinate descent
methods competitive with, and often faster than, full gradient methods.
• Some algorithms make randomized choices of dk in which the conditions
(3.22) hold in the sense of expectation, rather than deterministically. In one
variant of randomized coordinate descent, we set dk = [∇f (xk)]ik , for ik
chosen uniformly at random from {1,2,...,n} at each k. Taking
expectations over ik, we have
Eik

( dk)
T ∇f (xk)

= 1
n
n
i=1
[∇f (xk)]
2
i = 1
n
∇f (xk)2
≥
1
n
∇f (xk)dk,
where the last inequality follows from dk ≤ ∇f (xk), so the condition
(3.22a) holds in an expected sense. Since E(dk2) = 1
n ∇f (xk)2
2, the
norms of dk and ∇f (xk) are also similar to within a scale factor, so
(3.22b) also holds in an expected sense. Rigorous analysis of these methods
is presented in Chapter 6.38 3 Descent Methods
• Another important class of randomized schemes are the stochastic gradient
methods discussed in Chapter 5. In place of an exact gradient ∇f (xk),
these method typically have access to a vector g(xk,ξk), where ξk is a
random variable, such that Eξkg(xk,ξk) = ∇f (xk). That is, g(xk,ξk) is an
unbiased (but often very noisy) estimate of the true gradient ∇f (xk).
Again, if we set dk = g(xk,ξk), the conditions (3.22) hold in an expected
sense, though the bound E(dk) ≤ γ2∇f (xk) requires additional
conditions on the distribution of g(xk,ξk) as a function of ξk.
3.5 Line-Search Methods: Choosing the Steplength
Assuming now that the search direction dk in (3.18) satisfies the properties
(3.22), we turn to the choice of steplength αk, for which a well-designed
procedure is often used. We describe some methods that make use of the
Lipschitz constant L from (2.7) and other methods that do not assume
knowledge of L, but still satisfy a sufficient decrease, like (3.19).
Fixed Steplength. As we have seen in Section 3.2, fixed steplengths can yield
useful convergence results. One drawback of the fixed steplength approach is
that some prior information is needed to properly choose the steplength.
The first approach to choosing a fixed steplength (one commonly used in
machine learning, where the steplength is often known as the “learning rate”)
is trial and error. Extensive experience in applying gradient (or stochastic
gradient) algorithms to a particular class of problems may reveal that a par￾ticular steplength is reliable and reasonably efficient. Typically, a reasonable
heuristic is to pick α as large as possible such that the algorithm does not
diverge. In some sense, this approach is estimating the Lipschitz constant of the
gradient of f by trial and error. Slightly enhanced variants are also possible;
for example, αk may be held constant for many successive iterations and then
decreased periodically. Since such schemes are highly application and problem
dependent, we cannot say much more about them here.
A second approach, a special case of which was investigated already in
Section 3.2, is to base the choice of αk on knowledge of the global properties
of the function f , particularly on the Lipschitz constant L for the gradient (see
(2.7)) or the modulus of convexity m (see (2.18)). Given the expression (3.23),
for example, and supposing we have estimates of all the quantities ¯, γ2, and L
that appear therein, we could choose α to maximize the coefficient of the last
term. Setting α = ¯/(Lγ2), we obtain from (3.23) and (3.22) that3.5 Line-Search Methods: Choosing the Steplength 39
f (xk+1) ≤ f (xk)
2
2Lγ2
∇f (xk)dk ≥ f (xk)
2γ1
2Lγ2
∇f (xk)2.
(3.24)
Exact Line Search. A second option is to perform a one dimensional line
search along direction dk to find the minimizing value of α; that is,
min
α>0 f (xk + αdk). (3.25)
This technique requires evaluation of f (xk + αdk) (and possibly also its
derivative with respect to α, namely (dk)T ∇f (xk + αdk)) economically,
for arbitrary positive values of α. There are many cases where these line
searches can be computed at low cost. For example, if f is a multivariate
polynomial, the line search amounts to minimizing a univariate polynomial.
Such a minimization can be performed by finding the roots of the gradient
along the search direction, and then testing each root to find the minimum. In
other settings, such as coordinate descent methods of Chapter 6, it is possible
to evaluate f (xk + αdk) cheaply for certain functions f , provided that dk
is a coordinate direction. Convergence analysis for exact line-search methods
tracks that for the preceding short-step methods. Since the exact minimizer
of f (xk + αdk) will achieve at least as much reduction in f as the choice
α = /(Lγ2) used to derive the estimate (3.24), this bound also holds for exact
line searches.
Approximate Line Search. In full generality, exact line searches are expen￾sive and unnecessary. Better empirical performance is achieved by approx￾imate line search. Many line-search methods were proposed in the 1970s
and 1980s for finding conditions that should be satisfied by approximate line
searches so as to guarantee good convergence properties and on identifying
line-search procedures that find such approximate solutions economically. (By
“economically,” we mean that an average of three or less evaluations of f
are required.) One popular pair of conditions that the approximate minimizer
α = αk is required to satisfy, called the weak Wolfe Conditions, is defined as
follows:
f (xk + αdk) ≤ f (xk) + c1α∇f (xk)
T dk
, (3.26a)
∇f (xk + αdk)
T dk ≥ c2∇f (xk)
T dk
. (3.26b)
Here, c1 and c2 are constants that satisfy 0 < c1 < c2 < 1. The condition
(3.26a) is often known as the “sufficient decrease condition,” because it ensures
that the actual amount of decrease in f is at least a multiple c1 of the amount3.5 Line-Search Methods: Choosing the Steplength 41
along with a current guess α ∈ (L,U) of this point. If the sufficient decrease
condition (3.26a) is violated by α, then the current guess is too long, so the
upper bound U is assigned the value α, and the new guess is taken to be the
midpoint of the new interval [L,U]. If the sufficient decrease condition holds
but the condition (3.26b) is violated, the current guess of α is too short. In this
case, we move the lower bound up to α and take the next guess of α to be either
the midpoint of [L,U] (if U is finite) or double the previous guess (if U is still
infinite).
A rigorous proof that Algorithm 3.1 terminates with a value of α satisfying
(3.26) can be found in Section A.3 in the Appendix.
Algorithm 3.1 Extrapolation Bisection Line Search (EBLS)
Given 0 < c1 < c2 < 1, set L ← 0, U ← +∞, α ← 1;
repeat
if f (x + αd) > f (x) + c1α∇f (x)T d then
Set U ← α and α ← (U + L)/2;
else if ∇f (x + αd)T d<c2∇f (x)T d then
Set L ← α;
if U = +∞ then
Set α ← 2L;
else
Set α = (L + U)/2;
end if
else
Stop (Success!);
end if
until Forever
Backtracking Line Search. Another popular approach to determining an
appropriate value for αk is known as “backtracking.” It is widely used in
situations where evaluation of f is economical and practical but evaluation
of the gradient ∇f is more difficult. It is easy to implement (no estimate of the
Lipschitz constant L is required, for example) and still results in reasonably
fast convergence.
In its simplest variant, we first try a value α > 0 as the initial guess of
the steplength, and we choose a constant β ∈ (0,1). The steplength αk is set
to the first value in the sequence α,β ¯ α,β ¯ 2α,β ¯ 3α,... ¯ for which a sufficient
decrease condition (3.26a) is satisfied. Note that backtracking does not require
a condition like (3.26b) to be checked. The purpose of such a condition is to42 3 Descent Methods
ensure that αk is not too short, but this is not a concern in backtracking, because
we know that αk is either the fixed value α¯ or within a factor β of a steplength
that is too long.
Under the preceding assumptions, we can again show that the decrease in
f at iteration k is a positive multiple of ∇f (xk)2. When no backtracking is
necessary that is, αk = α we have from ( 3.22) that
f (xk+1) ≤ f (xk) + c1α∇f (xk)
T dk ≤ f (xk) c1αγ1∇f (xk)2. (3.27)
When backtracking is needed, we have from the fact that the test (3.26a) is not
satisfied for the previously tried value α = β−1αk that
f (xk + β−1αkdk) > f (xk) + c1β−1αk∇f (xk)
T dk.
By a Taylor series argument like the one in (3.23), we have
f (xk + β−1αkdk) ≤ f (xk) + β−1αk∇f (xk)
T dk +
L
2 (β−1αk)
2dk2.
From the last two inequalities and some elementary manipulation, we obtain
that
αk ≥
2
Lβ(1 c1)
∇f (xk)T dk
dk2 .
By substituting into (3.26a) with α = αk (note that this condition is satisfied
for this value of α) and then using (3.22), we obtain
f (xk+1) ≤ f (xk) + c1αk∇f (xk)
T dk
≤ f (xk) − 2
Lβ(1 − c1)c1
(∇f (xk)T dk)2
dk2
≤ f (xk)
2
Lβc1(1 c1)¯
2∇f (xk)2. (3.28)
3.6 Convergence to Approximate Second-Order
Necessary Points
The line search methods that we described so far in this chapter asymptotically
satisfy first-order optimality conditions with certain complexity guarantees.
We now describe an elementary method that is designed to find points
that satisfy the second-order necessary conditions for a smooth, possibly
nonconvex function f , which are
∇f (x∗) = 0, ∇2f (x∗) positive semidefinite (3.29)3.6 Convergence to Approximate Second-Order Necessary Points 43
(see Theorem 2.4). In addition to Lipschitz continuity of the gradient ∇f , we
assume Lipschitz continuity of the Hessian ∇2f . That is, we assume that there
is a constant M such that
∇2f (x) − ∇2f (y) ≤ Mx − y, for all x,y ∈ dom (f ). (3.30)
By extending Taylor’s theorem (Theorem 2.1) to a third-order term and using
the definition of M, we obtain the following cubic upper bound on f :
f (x + p) ≤ f (x) + ∇f (x)T p +
1
2
pT ∇2f (x)p +
1
6
Mp3. (3.31)
As in Section 3.2, we make an additional assumption that f is bounded below
by f¯.
We describe an elementary algorithm that makes use of the expansion
(3.31) as well as the steepest-descent theory of Section 3.2. Our algorithm
aims to identify a point that approximately satisfies the second-order necessary
conditions (3.29) – that is,
∇f (x) ≤ g, λmin(∇2f (x)) ≥ H, (3.32)
where g and H are two small constants.
Our algorithm takes steps of two types: a steepest-descent step, as in
Section 3.2 or a step in a negative-curvature direction for ∇2f . Iteration k
proceeds as follows:
(i) If ∇f (xk) > g, take the steepest-descent step (3.2) with αk = 1/L.
(ii) Otherwise, define λk to be the minimum eigenvalue of ∇2f (xk) – that is,
λk := λmin(∇2f (xk)). If λk < H , choose pk to be the eigenvector
corresponding to the most negative eigenvalue of ∇2f (xk). Choose the
size and sign of pk such that pk = 1 and (pk)T ∇f (xk) ≤ 0, and set
xk+1 = xk + αkpk
, where αk = 2|λk|
M . (3.33)
If neither of these conditions holds, then xk satisfies the necessary conditions
(3.32), so it is an approximate second-order-necessary point.
For the steepest-descent step (i), we have from (3.5) that
f (xk+1) ≤ f (xk) − 1
2L∇f (xk)2 ≤ f (xk) − 2
g
2L. (3.34)44 3 Descent Methods
For a step of type (ii), we have from (3.31) that
f (xk+1) ≤ f (xk) + αk∇f (xk)
T pk +
1
2
α2
k (pk)
T ∇2f (xk)pk +
1
6
Mα3
kpk3
≤ f (xk)
1
2
2|λk|
M
2
|λk| +
1
6
M
2|λk|
M
3
= f (xk)
2
3
|λk|
3
M2
≤ f (xk)
2
3
3
H
M2 . (3.35)
By aggregating (3.34) and (3.35), we have that at each xk for which the
condition (3.32) does not hold, we attain a decrease in the objective of at least
min 2
g
2L,
2
3
3
H
M2

.
Using the lower bound f¯ on the objective f , we see that the number of
iterations K required to meet the condition (3.32) must satisfy the condition
K min 2
g
2L,
2
3
3
H
M2

≤ f (x0) f,
from which we conclude that
K ≤ max
2L−2
g ,
3
2
M2 3
H

f (x0) f

.
Note that the maximum number of iterations required to identify a point for
which just the approximate stationarity condition ∇f (xk) ≤ g holds is
at most 2L−2
g (f (x0) f ). (We can just omit the second order part of the
algorithm to obtain this result.) Note, too, that it is easy to devise approximate
versions of this algorithm with similar complexity. For example, the negative
curvature direction pk in step (ii) can be replaced by an approximation to the
direction of most negative curvature, obtained by the Lanczos iteration with
random initialization.
3.7 Mirror Descent
The steps of the steepest-descent method (3.2) can also be obtained from the
solution of simple quadratic problems:
xk+1 = arg min f (xk) + ∇f (xk)
T (x xk) +
1
2αk
x xk2. (3.36)3.7 Mirror Descent 45
Thus, we can think of the new iterate being obtained from a first-order Taylor
series model with a quadratic penalty term, based on the Euclidean norm, that
penalizes our move away from the current iterate. Moreover, as αk decreases,
the penalty becomes more severe, so the step becomes shorter. (This viewpoint
is useful in later chapters, where we consider constrained and regularized
problems.)
In this section, we consider a framework like (3.36) but with the final term
replaced by a general class of distance measures called Bregman divergences
and denoted by Dh(·,·). The steps have the form
xk+1 = arg min f (xk) + ∇f (xk)
T (x − xk) +
1
αk
Dh(x,xk). (3.37)
The subscript h refers to a function that is smooth and strongly convex in some
norm. That is, it satisfies (2.19) for some m > 0, but the norm in the final
term (m/2)y − x2 of this definition can be any norm, not necessarily the
Euclidean norm that we use elsewhere in this book. This function h is said to
generate the Bregman divergence Dh(·,·) by means of the following formula:
Dh(x,z) := h(x) h(z) ∇ h(z)T (x z), (3.38)
which is the difference between h(x) and the first order Taylor series approxi
mation of h at z, evaluated at x. See the illustration in Figure 3.2.
Figure 3.2 Illustration of how to compute a Bregman divergence Dh(x,z).46 3 Descent Methods
Since h is strongly convex, Dh(x,z) is nonnegative and strongly convex
in the first argument. It may not satisfy other familiar properties of squared
norms, but it does satisfy a “three-point property.” This property holds for the
squared Euclidean norm, where it is known as the “law of cosines”: For any
three points x, y, z in Rn, we have
x y2 = x z2 + z y2 2(x z)T (y z)
= x − z2 + z − y2 − 2x − zy − z cos γ,
where γ is the angle made at z by the vectors (x − z) and (y − z). When γ is
π/2, then x, y, and z form a right-angled triangle, and this law reduces to the
Pythagorean theorem.
Bregman divergences share the “three-point property.” We can show that
Dh(x,y) = Dh(x,z) − (x − z)T (∇h(y) − ∇h(z)) + Dh(z,y). (3.39)
The proof is just algebra (see the Exercises). Remarkably, this property is
all we need to “mirror” the analysis of our standard convergence proofs for
steepest descent.
Example 3.7 (Squared Euclidean Norm) For h(x) = 1
2 x2, we have
Dh(x,z) = 1
2
x2 1
2
z2 zT (x z) = 1
2
x z2
,
so that (3.36) is a special case of (3.37) when the generating function is the
squared Euclidean norm.
Example 3.8 (Negative Entropy) Consider the n-simplex of probability
distributions, defined by n := {p ∈ Rn | p ≥ 0, n
i=1 pi = 0}. Take
h(p) = n
i=1 pi log pi to be the negative entropy of the distribution p. This
function is convex, and for any p,q ∈ n, we have
Dh(p,q) = n
i=1
pi log pi −n
i=1
qi log qi −n
i=1
(log qi − 1)(pi − qi)
= n
i=1
pi log pi
n
i=1
pi log qi +n
i=1
(pi qi)
= n
i=1
pi logpi
qi

.
This measure is the Kullback–Liebler divergence, or KL divergence, between
p and q. The function h of this example is strongly convex with respect to the
norm ·1 on the interior of n, with modulus 1. That is, we have3.7 Mirror Descent 47
h(p) ≥ h(q) + ∇h(q)T (p q) +
1
2
p q2
1, for all p,q ∈ int n.
This bound is known as Pinsker’s inequality.
We now consider the mirror descent algorithm, which defines its iterates by
(3.37). Because, from (3.38), we have that
∇xDh(x,z) = ∇h(x) ∇ h(z),
the optimality conditions for (3.37) are
∇f (xk) +
1
αk
∇h(xk+1) − 1
αk
∇h(xk) = 0.
We can thus write the next iterate xk+1 explicitly as
xk+1 = (∇h)−1

∇h(xk) αk∇f (xk)

,
where (∇h)−1 is the inverse function of h. In fact, this inverse function is
rarely computable, but for our special cases of Examples 3.7 and 3.8, it can
be computed explicitly. For h(x) = 1
2 x2, we have (∇h) 1(v) = v. For
h(p) = n
i=1 pi log pi, we can show that
(∇h)−1(v)i = evi
n
j=1 evj , i = 1,2,...,n.
Examples 3.7 and 3.8 cover almost the full range of applications of mirror
descent. There are not many other strongly convex functions whose gradient
maps have simple inverses. But in principle, any such function h would define
its own Bregman divergence and hence its own mirror-descent algorithm.
Mirror-Descent Analysis. Because one of the key applications of mirror
descent (Example 3.8) restricts the iterates to a subset of Rn, we are more
careful than usual here in setting up and analyzing the method over something
less than the whole space Rn.
Let X ⊆ D ⊆ Rn be convex sets, and suppose that h: X → R is
continuously differentiable. Let   be some arbitrary norm (not necessarily
Euclidean), and assume that h is strongly convex with modulus m with respect
to this norm; that is,
h(x) ≥ h(z) + ∇h(z),x z + m
2 x z2
, for all x,z ∈ X .
Also recall that a function f is L-Lipschitz with respect to · if and only if
g∗ ≤ L for all g ∈ ∂f (x), where ·∗ denotes the dual norm of ·.48 3 Descent Methods
Consider the mirror-descent algorithm (3.37), modified slightly to confine
its iterates to the set X :
xk+1 = arg min x∈X f (xk) + ∇f (xk)
T (x xk)+
1
αk
Dh(x,xk), k = 0,1,2,... .
From Theorem 7.2 and the definition of the normal cone, we see that the
optimality conditions for this subproblem are

∇f (xk) +
1
αk
∇h(xk+1)
1
αk
∇h(xk)
T
(x xk+1) ≥ 0, for all x ∈ X .
(3.40)
Here (and also for later algorithms), we analyze the behavior of a weighted
average of the iterates rather than the iterates xk themselves. We define
λk = 
k
j=0
αj , xk = λ−1
k

k
j=0
αj xj . (3.41)
We have the following result, the proof of which is due to Beck and Teboulle
(2003).
Theorem 3.9 Let · be an arbitrary norm on X , and suppose that h is a m
strongly convex function with respect to · on X . Suppose that f is convex
and L-Lipschitz with respect to · and that a solution x∗ to the problem
minx∈X f (x) exists, with objective f ∗ = f (x∗). Then for any integer T ≥ 1,
we have
f (x¯T ) − f ∗ ≤
Dh(x∗,x0) + L2
2m
T
t=0 α2
t T
t=0 αt
,
where xT is defined by (3.41).
Proof By adding and subtracting terms, we have
αk∇f (xk)
T (xk x∗)
= ( αk∇f (xk) ∇ h(xk+1) + ∇h(xk))T (x∗ xk+1)
+ (∇h(xk+1) ∇ h(xk))T (x∗ xk+1) + (αk∇f (xk))T (xk xk+1).
The first term on the right hand side is nonpositive because of the optimality
conditions (3.40). The second term can be rewritten using the three-point
property (3.39) as follows:
(∇h(xk+1) − ∇h(xk))T (x∗ − xk+1)
= −Dh(x∗
,xk+1) − Dh(xk+1
,xk) + Dh(x∗
,xk).3.7 Mirror Descent 49
The final term can be bounded as
αk∇f (xk)
T (xk xk+1) ≤ αk∇f (xk)xk xk+1
≤
α2
k
2m∇f (xk)2
 + m
2 xk − xk+12
,
where we used the bound ab ≤ 1
2 a2 + 1
2 b2 for any scalars a and b. Finally,
note that since h is strongly convex with parameter m, we have
− Dh(xk+1
,xk) + m
2 xk − xk+12
= h(xk+1) + h(xk) + ∇h(xk)
T (xk+1 xk) + m
2 xk xk+12 ≤ 0.
By assembling all these inequalities and substituting into the original expres￾sion, we obtain
αk∇f (xk)
T (xk x∗) ≤ Dh(x∗
,xk+1) + Dh(x∗
,xk) +
α2
k
2m∇f (xk)2

.
(3.42)
We now proceed with a telescoping sum argument. We first use convexity of f
and then (3.42) to obtain
f (x¯T ) f ∗ ≤ λ−1
T

T
k=0
αk(f (xk) f (x∗))
≤ λ−1
T

T
k=0
αk∇f (xk)
T (xk − x∗)
≤ λ−1
T

T
k=0

Dh(x∗
,xk) Dh(x∗
,xk+1) +
α2
k
2m∇f (xk)2


≤
Dh(x∗,x0) + 1
2m
T
k=0 α2
k∇f (xk)2

λT
,
where we used Dh(x∗,xT +1) ≥ 0 in the final inequality. Since ∇f (xk) ≤ L
by assumption, the proof is complete. 
We can use this result to make various choices of steplengths αk. Suppose
that we have a bound R on Dh(x∗,x0) (this may be easy to obtain if the set X
is compact, for example) and knowledge of the constants L associated with f
and m associated with h. Then choosing the number of iterations T in advance,
the “optimal” choice of fixed steplength will be the value α that minimizes50 3 Descent Methods
R + L2
2m
T
k=0 α2
T
k=0 α = R + L2(T +1)
2m α2
(T + 1)α .
A short calculation shows that the minimizing value is
α =
√2mR
L
1
√T + 1
, (3.43)
which yields the following estimate:
f (xT ) f ∗ ≤
L √2R
√m
1
√T + 1
. (3.44)
Note that this rate of 1/
√T is asymptotically slower than the 1/T rate
achieved for convex functions in Section 3.2.2. However, we note two points.
First, mirror descent is not particularly sensitive to variations in the steplength.
For example, if the choice of fixed steplength in (3.43) is scaled by a constant
θ > 0 (because of misestimation of the constants L and R, for example),
the effect on the convergence expression (3.44) is modest; the right hand side
increases, but only by a factor related to θ and θ−1. Averaging of the iterates
results in slower convergence but greater robustness to choice of steplength. (If
the steplength in the regular steepest descent method of Section 3.2.2 is chosen
to be too long, the method may not converge at all.)
The second point is that the constants L, R, and m may be smaller for a
certain choice of Bregman divergence and norm than for the usual Euclidean
norm. Returning to Example 3.8, where X is the unit simplex n, we have, by
choosing x0 to be the midpoint (1/n)1 of the simplex, that
R = sup
p∈n
Dh(p, 1
n 1) ≤ sup
p∈n
n
i=1
(pi log pi pi log 1/n) ≤ log n.
We noted already that in Example 3.8, the function h is strongly convex with
respect to norm   1 with modulus m = 1. Moreover, using the dual norm
 ∞, the constant L bounds the supremum of ∇f (x)∞ over X , rather than
∇f (x)2, which may be larger by a factor of n. The advantage of this setup
can be observed in practice. Mirror descent with the KL divergence is often
considerably faster for optimizing a function over the simplex than the mirror￾descent variant based on the Euclidean norm, particularly when the gradients
∇f (x) are dense vectors.3.8 The KL and PL Properties 51
3.8 The KL and PL Properties
Some functions that are convex but not strongly convex have a property
that allows convergence results to be proved with rates similar to those for
strongly convex functions. The Polyak–Łojasiewicz (PL) condition (Polyak,
1963; Karimi et al., 2016) holds when there exists m > 0 such that (3.10)
holds; that is,
∇f (x)2 ≥ 2m[f (x) f (x∗)], (3.45)
where x∗ is any minimizer of f . This condition can be combined with a bound
of the form (3.19) on the per-iterate decrease to obtain linear convergence rates
of the form (3.15).
An example of a function satisfying PL but not strong convexity is the
quadratic function f (x) = 1
2 xT Ax, where A  0 but A is singular. Then
f ∗ = 0 and the condition (3.45) holds where m is the smallest nonzero
eigenvalue of A. (See Section A.7 in the Appendix for a proof of this claim.)
The PL condition is a special case of the Kurdyka–Łojasiewicz (KL)
condition (Łojasiewicz, 1963; Kurdyka, 1998), which again requires ∇f (x)
to grow at a rate that depends on f (x) − f (x∗) as x moves away from the
solution set. The nature of this growth rate and of the algorithm for generating
{xk} allows local convergence of {f (xk)} to f (x∗) at various rates to be
proved.
Notes and References
The proof of Theorem 3.3 is from the notes of L. Vandenberghe, and
Theorem 3.6 is from Nesterov (2004, theorem 2.1.14).
Additional information about line-search algorithms can be found in
Nocedal and Wright (2006, chapter 3).
Exercises
1. Verify that if f is twice continuously differentiable with the Hessian
satisfying mI  ∇2f (x) for all x ∈ dom (f ), for some m > 0, then the
strong convexity condition (2.18) is satisfied.
2. Show as a corollary of Theorem 3.5 that if the sequence {xk} described in
this theorem is bounded and if f is strongly convex, we have
limk→∞ xk = x∗.52 3 Descent Methods
3. How is the analysis of Section 3.2 affected if we take an even shorter
constant steplength than 1/L – that is, α ∈ (0,1/L)? Show that we can
still attain a “1/k” sublinear convergence rate for {f (xk)} but that the
rate involves a constant that depends on the choice of α.
4. Find positive values of , γ1, and γ2 such that the Gauss–Southwell
choice dk = [∇f (xk)]ik eik , where ik = arg mini=1,2,...,n |[∇f (xk)]i|
and eik is the vector containing all zeros except for a 1 in position ik,
satisfies conditions (3.22).
5. Suppose that f: Rn → R is a strongly convex function with modulus m,
an L Lipschitz gradient, and (unique) minimizer x∗ with function value
f ∗ = f (x∗). Use the co coercivity property (2.21) and the fact that
∇f (x∗) = 0 to prove that the kth iterate of the steepest-descent method
applied to f with steplength 2
m+L satisfies
xk x∗ ≤ κ 1
κ + 1
k
x0 x∗,
where κ = L/m.
6. Let f be a convex function with L Lipschitz gradients. Assume that we
know that the minimizer lies in a ball of radius R about zero. In this
exercise, we show that minimizing a nearby strongly convex function will
yield an approximate minimizer of f with good complexity. Consider
running the steepest-descent method on the strongly convex function
f (x) = f (x) +

2R2 x2
,
where 0 <  	 L, initialized at some x0 with x0 ≤ R. Let x∗
 denote
the (unique) minimizer of f .
(a) Prove that f (z) f (x∗) ≤ f (z) f (x∗
 ) + 
2 , for any z with
z ≤ R.
(b) Prove that for an appropriately chosen steplength, the
steepest descent method applied to f will find a solution such that
f (z) f (x∗
 ) ≤

2
in at most approximately
R2L

log8R2L


iterations.
Find a precise estimate of this rate, and write the fixed steplength that
yields this convergence rate.3.8 The KL and PL Properties 53
7. Let A be an N × d matrix with N<d and rank(A) = N, and consider
the least-squares optimization problem
min
x f (x) := 1
N Ax b2. (3.46)
(a) Assume there exists a z such that Az = b. Characterize the solution
space of the system Ax = b.
(b) Write down the Lipschitz constant for the gradient of the function
(3.46) in terms of A.
(c) If you run the steepest-descent method on (3.46) starting at x0 = 0,
with appropriate choice of steplength, how many iterations are
required to find a solution with 1
n Ax b2 ≤ ?
(d) Consider the regularized problem
min fμ(x) := 1
n
Ax − b2 + μx2 (3.47)
for some μ > 0. Express the minimizer xμ of (3.47) in closed form.
(e) If you run the steepest-descent method on (3.47) starting at x0 = 0,
how many iterations are required to find a solution with
fμ(x) − fμ(xμ) ≤ ?
(f) Suppose xˆ satisfies fμ(x)ˆ fμ(xμ) ≤ . Find a tight upper bound on
f (x)ˆ .
(g) From Section 3.8, for f defined in (3.46), find the value of m that
satisfies (3.45), in terms of the minimum eigenvalue of AT A (and
possibly other quantities).
(h) Referring to Section 3.2.3, define an appropriate choice of steplength
for the steepest-descent method applied to (3.46), and write down the
linear convergence expression for the resulting method.
8. Modify the Extrapolation Bisection Line Search (Algorithm 3.1) so that
it terminates at a point satisfying strong Wolfe conditions, which are
f (xk + αdk) ≤ f (xk) + c1α∇f (xk)
T dk
, (3.48a)
|∇f (xk + αdk)
T dk| ≤ c2|∇f (xk)
T dk|, (3.48b)
where c1 and c2 are constants that satisfy 0 < c1 < c2 < 1. (The
difference between the strong Wolfe conditions and the weak Wolfe
conditions (3.26) is that in the strong Wolfe conditions the directional
derivative ∇f (xk+αdk)Tdk is not only bounded below by c2|∇f (xk)Tdk|
but also bounded above by this same quantity. That is, it cannot be too
positive. (Hint: You should test separately for the two ways in which
(3.48b) is violated; that is, ∇f (xk + αdk)T dk< − c2|∇f (xk)T dk| and54 3 Descent Methods
∇f (xk + αdk)T dk > c2|∇f (xk)T dk|. Different adjustments of L, α, and
U are required in these two cases.)
9. Consider the following function f: Rn → R:
f (x) = 1
4
n−1
l=1
cos(xl xl+1) +n
l=1
lx2
l .
(a) Compute a fixed steplength for which the steepest-descent method is
guaranteed to converge.
(b) Characterize the stationary points x (the points for which
∇f (x) = 0). For each such point, determine if it is a local minima,
local maxima, or a global minimum.
(c) Consider the steepest-descent method with the fixed steplength you
computed in part (a) and the initial point x0 = [1,1,1,...,1]T .
Determine to which stationary point the algorithm converges.
Explain your reasoning.
10. Prove the three-point property (3.39) for Bregman divergences.
11. Suppose the choice of fixed steplength α (3.43) in the mirror-descent
algorithm is scaled by some positive constant θ. Show how this modified
choice changes the bound (3.44).4
Gradient Methods Using Momentum
The steepest-descent method described in Chapter 3 always steps in the
negative gradient direction, which is orthogonal to the boundary of the level
set for f at the current iterate. This direction can change sharply from one
iteration to the next. For example, when the contours of f are narrow and
elongated, the search directions at successive iterations may point in almost
opposite directions and may be almost orthogonal to the direction in which the
minimizer lies. The method may thus take small steps that produce only slow
convergence toward the solution.
The steepest descent method is “greedy” in that it steps in the direction that
is apparently most productive at the current iterate, making no explicit use of
knowledge gained about the function f at earlier iterations. In this chapter,
we examine methods that encode knowledge of the function in several ways
and exploit this knowledge in their choice of search directions and steplengths.
One such class of techniques makes use of momentum, in which the search
direction tends to be similar to the one used on the previous step but adds a
small component from the negative gradient of f , evaluated at the current point
or a nearby point. Each search direction is thus a combination of all gradients
encountered so far during the search a compact encoding of the history of
the search. Momentum methods include the heavy-ball method, the conjugate
gradient method, and Nesterov’s accelerated gradient methods.
The analysis of momentum methods tends to be laborious and not very
intuitive. But these methods often achieve significant practical improvements
over steepest descent, so it is worthwhile to gain some theoretical under￾standing. Several approaches to the analysis have been proposed. Here, we
begin with strictly convex quadratic functions (Section 4.2) and present a
convergence analysis of Nesterov’s accelerated gradient method that uses tools
from linear algebra. We relate this analysis technique to the notion of Lyapunov
functions, which we then use as a tool to analyze first strongly convex functions
5556 4 Gradient Methods Using Momentum
(Section 4.3) and then general convex functions (Section 4.4). We make some
remarks about the conjugate gradient method in Section 4.5 and then discuss
lower bounds on global convergence rates in Section 4.6. (Lower bounds define
a “speed limit” for methods of a certain class; methods that achieve these
bounds are known as “optimal methods.”)
One way to motivate momentum methods is to relate them to techniques for
differential equations. We do this next.
4.1 Motivation from Differential Equations
One way to build intuition for momentum methods is to consider an optimiza￾tion algorithm as a dynamical system. The continuous limit of an algorithm (as
the steplength goes to zero) often traces out the solution path of a differential
equation. For instance, the gradient method is akin to moving down a potential
well, where the dynamics are driven by the gradient of f , as follows:
dx
dt
= ∇ f (x). (4.1)
This differential equation has fixed points precisely when ∇f (x) = 0, which
are minimizers of a convex smooth function f . Equation (4.1) is not the only
differential equation whose fixed points occur precisely at the points for which
∇f (x) = 0. Consider the second-order differential equation that governs a
particle with mass moving in a potential defined by the gradient of f :
μ
d2x
dt2 = −∇f (x) − b
dx
dt , (4.2)
where μ ≥ 0 governs the mass of the particle and b ≥ 0 governs the friction
dissipated during the evolution of the system. As before, the points x for which
∇f (x) = 0 are fixed points of this ODE. In the limit as the mass μ → 0, we
recover a scaled version of system (4.1). For positive values of μ, trajectories
governed by (4.2) show evidence of momentum, gradually changing their
orientations toward the direction indicated by −∇f (x).
A simple finite-difference approximation to (4.2) yields
μ
x(t + t) 2x(t) + x(t t)
(t)2 ≈ −∇f (x(t)) − b
x(t + t) x(t)
t .
(4.3)4.1 Motivation from Differential Equations 57
By rearranging terms and defining α and β appropriately (see the Exercises),
we obtain
x(t + t) = x(t) α∇f (x(t)) + β(x(t) x(t t)). (4.4)
By using this formula to generate a sequence {xk} of estimates of the vector x
along the trajectory defined by (4.2), we obtain
xk+1 = xk − α∇f (xk) + β(xk − xk−1), (4.5)
where x−1 := x0. The algorithm defined by (4.5) is heavy ball method,
described by Polyak (1964). With a small modification, we obtain a related
method known as Nesterov’s optimal method, discussed later. When applied
to a convex quadratic function f , approaches of the form (4.5) (possibly
with adaptive choices of α and β that vary between iterations) are known as
Chebyshev iterative methods.
Nesterov’s optimal method (also known as Nesterov’s accelerated gradient
method (Nesterov, 1983)) is defined by the formula
xk+1 = xk α∇f (xk + β(xk xk−1)) + β(xk xk 1). (4.6)
The only difference from (4.5) is that the gradient ∇f is evaluated at xk +
β(xk xk−1) rather than at xk. By introducing an intermediate sequence {yk}
and allowing α and β to have possibly different values at each iteration, this
method can be rewritten as follows:
yk = xk + βk(xk − xk−1) (4.7a)
xk+1 = yk − αk∇f (yk), (4.7b)
where we define x−1 = x0 as before, so that y0 = x0. Note that we
obtain yk by taking a pure momentum step based on the last two x-iterates,
while we obtain xk+1 by taking a pure gradient step from yk. In this sense,
the momentum step and the gradient step are teased apart, rather than being
combined in a single step.
Note that each of these methods has a fixed point with xk = x∗, where x∗
is a minimizer of f . (For Nesterov’s method, we also need y∗ = x∗.) The rest
of the chapter is devoted to finding conditions under which these accelerated
algorithms converge to x∗ at provable global rates. As we will see, with proper
setting of parameters, these methods converge faster than the steepest-descent
method.58 4 Gradient Methods Using Momentum
4.2 Nesterov’s Method: Convex Quadratics
We now analyze the convergence behavior of Nesterov’s optimal method (4.6)
when applied to convex quadratic objectives f and derive suitable values for
its parameters α and β. We consider
f (x) = 1
2
xT Qx bT x + c (4.8)
with positive definite Hessian Q and eigenvalues
0 < m = λn ≤ λn−1 ≤···≤ λ2 ≤ λ1 = L. (4.9)
The condition number of Q is thus
κ := L/m. (4.10)
Note that x∗ = Q−1b is the minimizer of f , and ∇f (x) = Qx b = Q(x x∗).
By applying (4.6) to (4.8) and adding and subtracting x∗ at several points
in this expression, we obtain
xk+1 x∗
= (xk x∗) αQ(xk + β(xk xk−1) x∗) + β

(xk x∗) (xk−1 x∗)

.
By concatenating the error vector xk − x∗ over two successive steps, we can
restate this expression in matrix form as follows:

xk+1 x∗
xk x∗

=

(1 + β)(I αQ) β(I αQ)
I 0
  xk x∗
xk−1 x∗

. (4.11)
By defining
wk :=

xk+1 x∗
xk x∗

, T :=

(1 + β)(I αQ) β(I αQ)
I 0

, (4.12)
we can write the iteration (4.11) as
wk = T wk 1
, k = 1,2,... . (4.13)
For later reference, we define x−1 := x0, so that
w0 =

x0 x∗
x0 − x∗

. (4.14)
Before stating a convergence result for Nesterov’s method applied to (4.8),
we recall that the spectral radius of a matrix T is defined as follows:
ρ(T ) := max{|λ|: λ is an eigenvalue of T }. (4.15)4.2 Nesterov’s Method: Convex Quadratics 59
For appropriate choices of α and β in (4.6), we have ρ(T ) < 1, which implies
convergence of the sequence {wk} to zero. We develop this theory in the
remainder of this section.
Theorem 4.1 Consider Nesterov’s optimal method (4.6) applied to the convex
quadratic (4.8) with Hessian eigenvalues satisfying (4.9). If we set
α := 1
L, β :=
√L − √m √L + √m =
√κ − 1
√κ + 1
, (4.16)
the matrix T defined in (4.12) has complex eigenvalues
νi,1 = 1
2

(1 + β)(1 αλi) + i

4β(1 αλi) (1 + β)2(1 αλi)2

,
(4.17a)
νi,2 = 1
2

(1 + β)(1 − αλi) − i

4β(1 − αλi) − (1 + β)2(1 − αλi)2

.
(4.17b)
Moreover, ρ(T ) ≤ 1 1/
√κ.
Proof We write the eigenvalue decomposition of Q as Q = UUT , where
 = diag (λ1,λ2,...,λn). By defining the permutation matrix  as
ij =
⎧
⎪⎪⎨
⎪⎪⎩
1 i odd, j = (i + 1)/2
1 i even, j = n + (i/2)
0 otherwise,
we have, by applying a similarity transformation to the matrix T , that


U 0
0 U
T 
(1 + β)(I αQ) β(I αQ)
I 0
 U 0
0 U

T
= 

(1 + β)(I α) β(I α)
I 0

T
=
⎡
⎢
⎢
⎢
⎣
T1
T2
...
Tn
⎤
⎥
⎥
⎥
⎦,
where
Ti =

(1 + β)(1 − αλi) −β(1 − αλi)
1 0 
, i = 1,2,...,n.60 4 Gradient Methods Using Momentum
The eigenvalues of T are the eigenvalues of Ti, for i = 1,2,...,n, which are
the roots of the following quadratic:
u2 − (1 + β)(1 − αλi)u + β(1 − αλi) = 0,
which are given by (4.17). Note first that for i = 1, we have from α = 1/L and
λ1 = L that ν1,1 = ν1,2 = 0. Otherwise, the roots (4.17) are distinct complex
numbers when 1 − αλi > 0 and (1 + β)2(1 − αλi) < 4β. It can be shown that
these inequalities hold when α and β are defined in (4.16) and λi ∈ (m,L).
Thus, for i = 2,3,...,n, the magnitude of both νi,1 and νi,2 is
1
2

(1 + β)2(1 αλi)2 + 4β(1 αλi) (1 + β)2(1 αλi)2
= 1
2
"
4β(1 αλi) = "
β
"
1 (λi/L).
Thus, for λi ≥ m, we have
"
β
"
1 (λi/L) ≤ "
β
"
1 (m/L)
=
 √L − √m √L + √m ·
L − m
L
1/2
=
 √L − √m √L + √m ·
(
√L − √m)( √L + √m)
L
1/2
=
√L √m √L = 1 "
m/L,
with equality in the case of λi = m (that is, i = n). We thus have
ρ(T ) = max
i=1,2, ,n max(|νi,1|,|νi,2|) = 1 1/
√κ,
as required. 
We now examine the consequence of T having a spectral radius less than 1.
A famous result in numerical linear algebra called Gelfand’s formula (Gelfand,
1941) states that
ρ(T ) = lim
k→∞

T k
1/k
. (4.18)
A consequence of this result is that for any  > 0, there is C > 1 such that
T k ≤ C (ρ(T ) + )k. (4.19)
Thus, from (4.13), we have
wk=T kw0≤T kw0 ≤ (Cw0)(ρ(T ) + )k
,4.2 Nesterov’s Method: Convex Quadratics 61
which implies R-linear convergence, provided we choose  ∈ (0,1 ρ(T )).
Thus, when ρ(T ) < 1, we have from (4.19) that the sequence {wk} (hence, also
{xk x∗}) converges R-linearly to zero, with rate arbitrarily close to ρ(T ).
Let us compare the linear convergence of Nesterov’s method against steep￾est descent on convex quadratics. Recall from (3.17) that the steepest-descent
method with constant step α = 1/L requires O((L/m)log ) iterations to
obtain a reduction of factor  in the function error f (xk) f ∗. The rate defined
by β in Theorem 4.1 suggests a complexity of O( √L/m log ) to obtain a
reduction of factor  in wk (which is obviously a different quantity from
f (xk) f ∗, but one that also shrinks to zero as xk → x∗). For problems in
which the condition number κ = L/m is moderate to large, Nesterov’s method
has a significant advantage. For example, if κ = 1,000, the improved rate
translates into an approximate factor-of 30 reduction in number of iterations
required, with similar workload per iteration (one gradient evaluation and a
few vector operations).
A similar convergence result can be obtained by using the notion of
Lyapunov functions. A Lyapunov function V : RD → R has two essential
properties:
1. V (z) > 0 for all z  z∗, for some z∗ ∈ RD
2. V (z∗) = 0.
Lyapunov functions can be used to show convergence of an iterative process.
For example, if we can show that V (zk+1)<ρ2V (zk) for the sequence {zk}
and some ρ < 1, we have demonstrated a kind of linear convergence of the
sequence to its limit z∗.
We construct a Lyapunov function for Nesterov’s optimal method by
defining a matrix P from the following theorem.
Theorem 4.2 Let A be a square real matrix. Then, for a given positive scalar
ρ, we have that ρ(A) < ρ if and only if there exists a symmetric matrix P  0
satisfying AT PA ρ2P ≺ 0.
Proof If ρ(A) < ρ, then the matrix
P := ∞
k=0
ρ 2k(Ak)
T (Ak)
is well defined, is positive definite (because the first term in the sum is a
multiple of the identity and all other terms are at least positive semidefi￾nite), and satisfies AT PA − ρ2P = − ρ2I ≺ 0, proving the “only if” part
of the result. For the converse, assume that the linear matrix inequality62 4 Gradient Methods Using Momentum
AT PA ρ2P ≺ 0 has a solution P  0, and let λ ∈ C be an eigenvalue
of A with corresponding eigenvector v ∈ CD. Then
0 > vH AH PAv − ρ2vH Pv = (|λ|
2 − ρ2)vH Pv.
But since vH Pv > 0, we must have |λ| < ρ. 
We apply this result to Nesterov’s method by setting A = T in (4.12). If
there exists a P  0 satisfying T T PT ρ2P ≺ 0, we have from (4.13) that
(wk)
T Pwk < ρ2(wk−1)
T Pwk−1. (4.20)
Iterating (4.20) down to k = 0, we see that
(wk)
T Pwk < ρ2k(w0)
T Pw0
,
where w0 is defined in (4.14). We thus have
λmin(P)xk − x∗2 ≤ λmin(P)wk2 ≤ ρ2kPw02 = 2ρ2kPx0−x∗2
,
so that
xk x∗ ≤ "
2cond(P)x0 x∗ρk
,
where cond(P) is the condition number of P. In other words, the function
V (w) := wT Pw is a Lyapunov function for the Nesterov algorithm, with
optimum at w∗ = 0. This function decreases strictly over all trajectories and
thus certifies that the algorithm is stable; that is, it converges to nominal values.
For quadratic f , we are able to construct a quadratic Lyapunov function
by doing an elementary eigenvalue analysis. This proof does not generalize to
the nonquadratic case, however. We show in the next section how to construct a
Lyapunov function for Nesterov’s optimal method that guarantees convergence
for all strongly convex functions.
4.3 Convergence for Strongly Convex Functions
We have shown that methods that use momentum are faster on convex
quadratic functions than steepest-descent methods, and the proof techniques
build some intuition for the case of general strongly convex functions. But they
do not generalize directly. In this section, we propose a different Lyapunov
function that allows us to prove convergence of Nesterov’s method for the case
of strongly convex smooth functions, satisfying (2.18) (with m > 0) and the
L-smooth property (2.7).4.3 Convergence for Strongly Convex Functions 63
It follows from the analysis of Section 3.2 that V (x) := f (x) f ∗ is
actually a Lyapunov function for the steepest-descent method (see (3.14)). For
Nesterov’s method, we need to define a specially adapted Lyapunov function.
First, for any variable v, we define v˜k := vk v∗ for any sequence {vk} that
converges to v∗. Next, we define the Lyapunov function as follows:
Vk = f (xk) f ∗ +
L
2 ˜xk ρ2x˜k−12. (4.21)
(We have omitted the dependence of Vk on xk and xk−1 for clarity.) We will
show that
Vk+1 ≤ ρ2Vk for some ρ < 1, (4.22)
provided that αk and βk are chosen as in (4.16); that is,
αk ≡ 1
L, βk ≡
√κ 1
√κ + 1
. (4.23)
To do so, we only make use of the standard chain of inequalities for
strongly convex functions with Lipschitz gradients that we used extensively
in Chapter 3 for studying the gradient methods. Namely, we use inequalities
(2.9) and (2.19), restated here for convenience:
f (z) + ∇f (z)T (w − z) + m
2 w − z2
≤ f (w)
≤ f (z) + ∇f (z)T (w z) +
L
2 w z2
2, for all w and z. (4.24)
For compactness of notation, we define uk := 1
L∇f (yk). (Since u∗ = 0,
we have u˜k = uk.) The decrease in the Lyapunov function at iteration k is
developed as follows:
Vk+1 = f (xk+1) f ∗ +
L
2 ˜xk+1 ρ2x˜k2
≤ f (yk) f ∗ L
2 ˜uk2 +
L
2 ˜xk+1 ρ2x˜k2 (4.25a)
= ρ2
#
f (yk) − f ∗ + L(u˜k)
T (x˜k − ˜yk)
$
− ρ2L(u˜k)
T (x˜k − ˜yk)
(4.25b)
+ (1 ρ2)(f (yk) f ∗ L(u˜k)
T y˜
k) + (1 ρ2)L(u˜k)
T y˜
k
− L
2 ˜uk2 +
L
2 ˜xk+1 − ρ2x˜k2.
Here, formula (4.25a) follows from the right-hand inequality in (4.24), with
w = xk+1 and z = yk, and (4.25b) is obtained by adding and subtracting64 4 Gradient Methods Using Momentum
the same term several times. We now invoke the left-hand inequality in (4.24)
twice. By setting w = yk and z = xk and using u˜k = uk = 1
L∇f (yk), we
obtain
f (yk) ≤ f (xk) − ∇f (yk)
T (xk − yk) − m
2 xk − yk2
= f (xk) L(u˜k)
T (x˜k ˜yk) m
2 ˜xk ˜yk2.
By setting w = x∗ and z = yk in this same bound, we obtain
f (x∗) ≥ f (yk) + ∇f (yk)
T (x∗ yk) + m
2 yk x∗2
= f (yk) − L(u˜k)
T y˜
k + m
2 ˜yk2.
By substituting these bounds into (4.25b), we obtain
Vk+1 ≤ ρ2
#
f (xk) f ∗ m
2 ˜xk ˜yk2
$ m(1 ρ2)
2 ˜yk2
ρ2L(u˜k)
T (x˜k ˜yk) + (1 ρ2)L(u˜k)
T y˜
k
− L
2 ˜uk2 +
L
2 ˜xk+1 − ρ2x˜k2
= ρ2

f (xk) f ∗ +
L
2 ˜xk ρ2x˜k−12

mρ2
2 ˜xk ˜yk2 m(1 ρ2)
2 ˜yk2
+ L(u˜k)
T (y˜
k ρ2x˜k)
L
2 ˜uk2
+
L
2 ˜xk+1 ρ2x˜k2 ρ2L
2 ˜xk ρ2x˜k−12 (4.26a)
= ρ2Vk + Rk, (4.26b)
where
Rk := mρ2
2 ˜xk ˜yk2 m(1 ρ2)
2 ˜yk2+L(u˜
k)
T (y˜
k ρ2x˜k)
L
2 ˜uk2
+
L
2 ˜xk+1 ρ2x˜k2 ρ2L
2 ˜xk ρ2x˜k−12. (4.27)
The bound (4.26b) suffices to prove (4.21), provided we can show that Rk
is negative. We state the result formally as follows.
Proposition 4.3 For Nesterov’s optimal method (4.7) applied to a strongly
convex function, with αk and βk defined in (4.23), and setting ρ2 = (1 −
1/
√κ), we have for Rk defined in (4.27) that4.3 Convergence for Strongly Convex Functions 65
Rk = 1
2
Lρ2
1
κ
+
1
√κ

˜xk ˜yk2.
This result is proved purely by algebraic manipulation, using the specifi
cation of Nesterov’s optimal method along with the definitions of the various
quantities and the steplength settings (4.23). We leave it as an Exercise. Note
that any choice of ρ and βk that make this quantity negative would suffice. It
is possible that one could derive a faster bound (that is, a lower value of ρ) by
making other choices of the parameters that lead to a nonpositive value of Rk.
Proposition 4.3 asserts that Rk is a negative square for appropriately chosen
parameters. Hence, we can conclude that Vk+1 ≤ ρ2Vk. We summarize the
convergence result in the following theorem.
Theorem 4.4 For Nesterov’s optimal method (4.7) applied to a strongly convex
function, with αk and βk defined in (4.23), and setting ρ2 = (1 1/
√κ), we
have
f (xk) − f ∗ ≤

1 − 1
√κ
k 
f (x0) − f ∗ + m
2 x0 − x∗2

.
Proof We have from Vk+1 ≤ ρ2Vk and the definition of Vk in (4.22) that
f (xk) f ∗ ≤ Vk ≤ ρ2kV0 =

1
1
√κ
k
V0.
Recalling that x−1 := x0, we have from (4.22) that
V0 = f (x0) − f ∗ +
L
2 (1 − ρ2)x˜02
= f (x0) f ∗ +
L
2
 1
√κ
2
x0 x∗2
= f (x0) f ∗ + m
2 x0 x∗2
,
giving the result. 
We note that the provable convergence rate is slightly worse for Nesterov’s
method than for the heavy-ball method applied to quadratics: 1 1/
√κ for
Nesterov and approximately 1 2/
√κ for heavy ball. (We prove the latter
rate in the Exercises, using a similar technique to the one in Section 4.2.) This
worst-case bound suggests that Nesterov’s method may require about twice as
many iterates to reach a given tolerance threshold . This discrepancy is rarely
observed in practice. Moreover, Nesterov’s method can be adapted to a wider
class of functions, as we show now.66 4 Gradient Methods Using Momentum
4.4 Convergence for Weakly Convex Functions
We can prove convergence of Nesterov’s optimal method (4.7) for weakly
convex functions by modifying the analysis of Section 4.3. We need to allow
βk to vary with k (and, hence, ρk also) while maintaining a constant value for
the α parameter: αk ≡ 1/L.
We start by redefining Vk to use a variable value of ρ, as follows:
Vk = f (xk) f ∗ +
L
2 ˜xk ρ2
k 1x˜k−12. (4.28)
We can now proceed with the derivation of the previous section, substituting
this modified definition of Vk into (4.25) and (4.26) and replacing ρ with ρk in
the addition/subtraction steps. By setting m = 0 in (4.26a), we obtain
Vk+1 ≤ ρ2
k

f (xk) − f ∗ +
L
2 ˜xk − ρ2
k−1x˜k−12

+ L(u˜k)
T (y˜
k − ρ2
k x˜k) − L
2 ˜uk2
+
L
2 ˜xk+1 ρ2
k x˜k2 ρ2
kL
2 ˜xk ρ2
k−1x˜k 12
= ρ2
k

f (xk) f ∗ +
L
2 ˜xk ρ2
k−1x˜k−12

+
L
2 ˜yk ρ2
k x˜k2 ρ2
kL
2 ˜xk ρ2
k−1x˜k−12 (4.29a)
= ρ2
kVk + Wk, (4.29b)
where
Wk := L
2 ˜yk ρ2
k x˜k2 ρ2
kL
2 ˜xk ρ2
k−1x˜k−12. (4.30)
Formula (4.29a) follows by using the identity x˜k+1 = xk+1 x∗ = yk uk
x∗ = ˜yk ˜uk, from (4.7b), and by completing the square.
We now choose ρk to force Wk = 0 for k ≥ 1. From the definition (4.30),
this will be true, provided
y˜
k − ρ2
k x˜k = ρkx˜k − ρkρ2
k−1x˜k−1. (4.31)
By substituting y˜k = (1 + βk)x˜k − βkx˜k−1 (from (4.7b)) and setting the
coefficients of x˜k and x˜k−1 to zero, we find that the following conditions ensure
(4.31):
1 + βk − ρ2
k = ρk, βk = ρkρ2
k−1. (4.32)4.4 Convergence for Weakly Convex Functions 67
From an arbitrary choice of ρ0 (more information about this is given in what
follows), we can use these formulas to define subsequent values of βk and ρk,
for k = 1,2,.... By substituting for βk, we obtain the following relationship
between two successive values of ρ:
1 + ρk(ρ2
k−1 1) ρ2
k = 0, (4.33)
which yields
ρ2
k = (1 ρ2
k )2
(1 ρ2
k−1)2
, k = 1,2,... . (4.34)
Using the fact that Vk ≤ ρ2
k−1Vk−1 for k = 1,2,... (from (4.29b) and Wk = 0
for k = 1,2,... ), we obtain
Vk ≤ ρ2
k−1ρ2
k−2 ...ρ2
1V1 =
⎧
⎨
⎩
k
%−1
j=1
ρ2
j
⎫
⎬
⎭
V1 = (1 ρ2
k−1)2
(1 ρ2
0 )2 V1. (4.35)
For a bound on V1, we make the choices ρ0 = 0 and ρ 1 = 0, use (4.29b) and
(4.30), and recall that y0 = x0 to obtain
V1 ≤ W0 = L
2 ˜y02 = L
2 x0 − x∗2
,
which by substitution into (4.35) (setting ρ0 = 0 again) yields
Vk ≤ (1 − ρ2
k−1)
2L
2 x0 − x∗2. (4.36)
We now use an elementary inductive argument to show that
1 − ρ2
k ≤
2
k + 2
. (4.37)
Note first that the choice ρ0 = 0 ensures that (4.37) is satisfied for k = 0.
Supposing that it is satisfied for some k, we want to show that 1 ρ2
k+1 ≤
2/(k + 3). Suppose for contradiction that this claim is not true. We then have
1 ρ2
k+1 >
2
k + 3
, so that ρ2
k+1 <
k + 1
k + 3
and, thus,
(1 ρ2
k+1)2
ρ2
k+1
>
 2
k + 3
2 k + 3
k + 1 = 4
(k + 1)(k + 3)
.
Since (k + 1)(k + 3) < (k + 2)2 for all k, this bound together with (4.37)
contradicts (4.34). We conclude that (4.37) continues to hold when k is
replaced by k + 1, so, by induction, (4.37) holds for k = 0,1,2,... .68 4 Gradient Methods Using Momentum
By substituting (4.37) into (4.36) and using the definition (4.28), we obtain
f (xk) − f ∗ ≤ Vk ≤
2L
(k + 1)2 x0 − x∗2. (4.38)
This sublinear rate is faster than the rate proved for the steepest-descent method
in Theorem 3.3 in that 1/k convergence has become 1/k2 convergence.
We summarize Nesterov’s optimal method for the weakly convex case in
Algorithm 4.1. Note that we have defined ρk and βk to satisfy formulas (4.32)
and (4.33), for k = 1,2,... , and set αk ≡ 1/L in (4.7b).
Algorithm 4.1 Nesterov’s Optimal Algorithm: Weakly Convex f
Given x0 and constant L satisfying (2.7), set x−1=x0, β0 = 0, and ρ0 = 0;
for k = 0,1,2,... do
Set yk := xk + βk(xk xk−1);
Set xk+1 := yk (1/L)∇f (yk);
Define ρk+1 to be the root in [0,1] of the following quadratic: 1 +
ρk+1(ρ2
k 1) ρ2
k+1 = 0;
Set βk+1 = ρk+1ρ2
k ;
end for
4.5 Conjugate Gradient Methods
A problem with the version of Nesterov’s method described before is that we
need to know the parameters L and m to compute the appropriate steplengths.
(There are versions of these methods for which this prior knowledge is
not required, and adaptive estimates of L are made (see Nesterov, 2015;
Beck and Teboulle, 2009). The conjugate gradient method, developed in the
early 1950s for systems of equations involving symmetric positive definite
matrices (equivalently, minimizing strongly convex quadratic functions) does
not require knowledge of these parameters. The conjugate gradient method,
which is also a momentum method, can be extended and adapted to solve
smooth (even nonconvex) optimization problems, as shown first by Fletcher
and Reeves (1964).
Focusing for the moment on the case of strongly convex quadratic f ,
consider first the heavy-ball formula (4.5) in which α and β are allowed to
vary across iterations, as follows:
xk+1 = xk − αk∇f (xk) + βk(xk − xk−1). (4.39)4.5 Conjugate Gradient Methods 69
We now introduce a vector pk that captures the search direction, such that
xk+1 = xk + αkpk for all k. With some manipulation, we see that
pk = −∇f (xk) + βk
αk
(xk − xk−1) = −∇f (xk) + βkαk−1
αk
pk−1
= −∇f (xk) + γk−1pk−1
,
where we introduced a new scalar γk−1 to replace βkαk−1/αk. (Initially, we
set p0 = ∇ f (x0).) The conjugate gradient method also keeps track of the
residual rk = ∇f (xk) = Qxk b, where we used the notation (4.8). Note that
rk can be updated to rk+1 as follows:
rk+1 = Qxk+1 − b = Qxk − b + αkQpk = rk + αkQpk.
Thus, the conjugate gradient method for strongly convex quadratic functions
can be defined by the following three update formulas:
xk+1 ← xk + αkpk
, (4.40a)
rk+1 ← rk + αkQpk
, (4.40b)
pk+1 ← rk+1 + γkpk
, (4.40c)
together with the formulas defining the scalars γk and αk. We choose αk by
performing an exact minimization of f (xk+αpk) for α – which, for the convex
quadratic (4.8), leads to the explicit formula
αk = (pk)T rk
(pk)T Qpk . (4.41)
We choose γk to ensure that the two directions pk and pk+1 satisfy conjugacy
with respect to Q that is, (pk)T Qpk+1 = 0. By substituting from (4.40c), we
obtain
γk = (rk+1)T Qpk
(pk)T Qpk
= (rk+1)T rk+1
(rk)T rk . (4.42)
(The equality of the last two formulas is not obvious, and we leave it as an
Exercise.) Formulas (4.40), (4.41), and (4.42), along with the initial iterate x0
and search direction p0 = (Qx0 b), give a complete description of the basic
conjugate gradient method for the strongly convex quadratic function (4.8).
One remarkable property of the conjugate gradient method is that we do not
just have conjugacy of two successive search directions pk and pk+1, ensured
by formula (4.42), but, in fact, pk+1 is conjugate to all preceding search
directions pk,pk−1,...,p0! It follows that these directions form a linearly
independent set, and we can show in addition that xk+1 is the minimizer of70 4 Gradient Methods Using Momentum
f in the affine set defined by x0 + span {p0,p1,...,pk}. Thus, the conjugate
gradient method is guaranteed to terminate at an exact minimizer of a strongly
convex quadratic f in at most n iterations.
Many extensions of the conjugate gradient approach to nonquadratic and
nonconvex functions have been proposed. These typically involve choosing αk
with a (possibly inexact) line search along the direction pk and defining γk in a
way that mimics (4.42) (and usually reduces to this formula when f is convex
quadratic and αk is exact). The many variants of nonlinear CG are discussed
in Nocedal and Wright (2006, chapter 5). There are some convergence results
for these methods, but they are generally not as strong and those proved for the
accelerated gradient methods that are the main focus of this chapter. Because
these methods often perform well, we expect them to become topics of further
investigation, so stronger results can be expected in future. (In contrast, the
convergence theory for the conjugate gradient method applied to the convex
quadratic case is extraordinarily rich, as also discussed in Nocedal and Wright
(2006, chapter 5).)
4.6 Lower Bounds on Convergence Rates
The term “optimal” is used in connection with Nesterov’s method because the
convergence rate achieved by the method is the best possible (up to a constant),
among algorithms that make use of gradient information at the iterates xk and
functions with Lipschitz continuous gradients. This claim can be proved by
means of a carefully designed function for which no method that makes use
of all gradients observed up to and including iteration k (namely, ∇f (xi
),
i = 0,1,2,...,k) can produce a sequence {xk} that achieves a rate better
than (4.38). The function proposed by Nesterov (2004) is a convex quadratic
f (x) = 1
2 xT Ax eT
1 x, where
A =
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣
2 −10 0 ... ... 0
−1 2 −1 0 ... ... 0
0 −1 2 −1 0 ... 0
... ... ...
0 ... 1 2 1
0 ... 0 1 2
⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦
, e1 =
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1
0
0
.
.
.
0
⎤
⎥
⎥
⎥
⎥
⎥
⎦
.
The solution x∗ satisfies Ax∗ = e1; its components are x∗
i = 1 − i/(n + 1),
for i = 1,2,...,n. It is easy to show that A2 ≤ 4, so that this function is
L-smooth with L = 4.4.6 Lower Bounds on Convergence Rates 71
If we use x0 = 0 as the starting point and construct the iterate xk+1 as
xk+1 = xk +
k
j=0
γj∇f (xj )
for some coefficients γj , j = 0,1,...,k, an elementary inductive argument
shows that each iterate xk can have nonzero entries only in its first k
components. It follows that for any such algorithm, we have
xk − x∗2 ≥ n
j=k+1
(x∗
j )
2 = n
j=k+1

1 − j
n + 1
2
. (4.43)
A little arithmetic (see Exercises) shows that
xk − x∗2 ≥
1
8
x0 − x∗2
, k = 1,2,..., n
2 − 1. (4.44)
It can be shown further that
f (xk) f ∗ ≥
3
8(k + 1)2 x0 x∗2
, k = 1,2,..., n
2 1. (4.45)
This lower bound on f (xk) x∗ is within a constant factor of the upper bound
(4.38) when we recall that L = 4 for this function.
The restriction k < n/2 in the preceding argument is not fully satisfying.
A more compelling example would show that the lower bound (4.45) holds
for all k.
Notes and References
A description of Chebyshev iterative methods for convex quadratics can be
found in Golub and Van Loan (1996, chapter 10).
The use of ODE methodology to study continuous-time limits of momen￾tum methods dates to the paper of Su et al. (2014). Many other papers that
pursue this line of work have appeared in subsequent years; the following
references give some idea of the scope of this work: Wibisono et al. (2016);
Attouch et al. (2018); Maddison et al. (2018); Shi et al. (2018).
The heavy-ball method was described by Polyak (1964). Nesterov’s method
was described originally in Nesterov (1983). Convergence proofs based on
bounding functions were given in the text (Nesterov, 2004). Our description of
Lyapunov functions follows that of Lessard et al. (2016). The FISTA algorithm
(Beck and Teboulle, 2009) extends a similar approach to problems in which the
objective is a smooth convex function added to a simple (possibly nonsmooth)72 4 Gradient Methods Using Momentum
convex function. (We consider functions with this structure further in
Section 9.3.)
A momentum method whose analysis can be performed with geometric
tools is described by Bubeck et al. (2015), and an approach based on “optimal
quadratic averaging” is presented in Drusvyatskiy et al. (2018).
The conjugate gradient method was proposed by Hestenes and Steifel;
their first comprehensive description is in Hestenes and Steifel (1952). There
are many later treatments by other authors (for example, Golub and Van
Loan, 1996). This method has become a workhorse in scientific computing
for solving large systems of linear equations with symmetric positive definite
matrices. Its extension to nonlinear function minimization was first proposed
by Fletcher and Reeves (1964), and many variants followed. More information
can be found in Nocedal and Wright (2006, chapter 5) and its extensive list of
references.
Exercises
1. Define α and β in terms of b, μ, and t such that (4.4) corresponds to
(4.3). Repeat the question for the case in which the term dx/dt is
approximated by central differences:
x(t + t) − x(t − t)
2t .
2. Minimize a quadratic objective f (x) = 1
2 xT Ax with some first-order
methods, generating the problems using the following MATLAB code
fragment (or its equivalent in another language) to generate a Hessian
with eigenvalues in the range [m,L].
mu=0.01; L=1; kappa=L/mu;
n=100;
A = randn(n,n); [Q,R]=qr(A);
D=rand(n,1); D=10.ˆ{D}; Dmin=min(D); Dmax=max(D);
D=(D Dmin)/(Dmax Dmin);
D = mu + D*(L mu);
A = Q’*diag(D)*Q;
epsilon=1.e-6;
kmax=1000;
x0 = randn(n,1); % different x0 for each trial4.6 Lower Bounds on Convergence Rates 73
Run the code in each case until f (xk) ≤  for tolerance  = 10 6.
Implement the following methods.
• Steepest descent with αk ≡ 2/(m + L)
• Steepest descent with αk ≡ 1/L
• Steepest descent with exact line search
• Heavy-ball method, with α = 4/( √L + √m)2 and
β = (
√L − √m)/( √L + √m)
• Nesterov’s optimal method, with α = 1/L and
β = (
√L − √m)/( √L + √m)
(a) Tabulate the average number of iterations required, over 10 random
starts.
(b) Draw a plot of the convergence behavior on a typical run, plotting
iteration number against log10(f (xk) f (x∗)). (Use the same figure,
with different colors for each algorithm.)
(c) Discuss your results, noting in particular whether the worst-case
convergence analysis is reflected in the practical results.
3. Discuss what happens to the codes and algorithms in the previous
question when we reset m to 0 (making f weakly convex). Comment in
particular on what happens when you use the uniform steplength
αk ≡ 2/(L + m) in steepest descent. Are these observations consistent
with the convergence theory of Chapter 3?
4. Consider the function
f (x) =
⎧
⎪⎪⎨
⎪⎪⎩
25x2 x < 1
x2 + 48x 24 1 ≤ x ≤ 2
25x2 48x + 72 x > 2
.
(a) Prove f is strongly convex with parameter 2 and has L Lipschitz
gradients with L = 50.
(b) What is the global minimizer of f ? Justify your answer.
(c) Run the gradient method with steplength 1/50, Nesterov’s method
with steplength 1/50 and β = 2/3, and the heavy-ball method with
α = 1/18 and β = 4/9, starting from x0 = 3 in each case. Plot the
function value versus the iteration counter for each method. For each
method, also plot the worst-case upper bounds on the function value
as derived for the case in which f is a strongly convex quadratic with
m = 2 and L = 50. Explain how the actual performance relates to
the worst-case upper bound for quadratic functions.
5. Prove using Gelfand’s formula (4.18) that (4.19) is true for any  > 0, for
some C > 1.74 4 Gradient Methods Using Momentum
6. Show that the heavy-ball method (4.5) converges at a linear rate on the
convex quadratic (4.8) with eigenvalues (4.9), if we set
α := 4
(
√L + √m)2
, β :=
√L √m √L + √m.
You can follow the proof technique of Section 4.2 to a large extent,
proceeding in the following steps.
(a) Write the algorithm as a linear recursion wk+1 = T wk for
appropriate choice of matrix T and state variables wk.
(b) Use a transformation to express T as a block-diagonal matrix, with
2 × 2 blocks Ti on the diagonals, where each Ti depends on a single
eigenvalue λi of Q.
(c) Find the eigenvalues λ¯i,1, λ¯i,2 of each Ti as a function of λi, α, and β.
(d) Show that, for the given values of α and β, these eigenvalues are all
complex.
(e) Show that, in fact, |λi,1|=|λi,2| = √β for all i = 1,2,...,n, so that
ρ(T ) = √β ≈ 1 √κ.
7. Prove Proposition 4.3 by using (4.7); the definitions κ = L/m,
u˜k = uk = (1/L)∇f (yk), and ρ2 = (1 − 1/
√κ); and (4.23).
8. Show that if ρk−1 ∈ [0,1], the quadratic equation (4.33) has a root ρk in
[0,1].
9. For the quadratic function of Section 4.6, prove the following bounds:
x0 x∗2
2 ≤ n
3
, xk x∗2 ≥ (n k)3
3(n + 1)2 ≥ (n k)3
n(n + 1)2 x0 x∗2.
(The bound (4.44) follows by setting k = n
2 − 1 in this expression and
noting that it is decreasing in k.)
10. Show that the two formulas in (4.42) for the parameter γk in the
conjugate gradient method are, in fact, equal by making use of the
formulas (4.40) and (4.41).5
Stochastic Gradient
The stochastic gradient (SG) method is one of the most popular algorithms
in modern data analysis and machine learning. It has a long history, with
variants having been invented and reinvented several times by different
communities, under such names as “least mean squares,” “back propagation,”
“online learning,” and the “randomized Kaczmarz method.” Most attribute
the stochastic gradient approach to Robbins and Monro (1951), who were
interested in devising efficient algorithms for computing random means and
roots of scalar functions for which only noisy values are available. In this
chapter, we explore some of the properties and implementation details of SG.
As in much of this book, our goal is to minimize the multivariate convex
function f: Rn → R, which we assume to be smooth for the purposes of
this discussion. Extension of SG to the case of nonsmooth convex functions is
straightforward and left as an Exercise in the chapter on nonsmooth methods.
SG differs from methods of Chapters 3 and 4 in the kind of information that
is available about f . In place of an exact value of ∇f (x), we assume that we
can compute or acquire a vector g(x,ξ) ∈ Rn, which is a function of a random
variable ξ and x such that
∇f (x) = Eξ [g(x,ξ)]. (5.1)
We assume that ξ belongs to some space  with probability distribution P,
and Eξ denotes the expectation taken over ξ ∈  according to distribution
P. Equation (5.1) asserts that g(x,ξ) is an unbiased estimate of ∇f (x). SG
proceeds by substituting g(x,ξ) for the true gradient ∇f in the steepest￾descent update formula, so each iteration is as follows:
xk+1 = xk αkg(xk
,ξ k), (5.2)
where the random variable ξ k is chosen according to the distribution P
(independently of the choices at other iterations) and αk > 0 is the steplength.
7576 5 Stochastic Gradient
The method steps in a direction that in expectation equals the steepest-descent
direction. Although g(xk,ξ k) may differ substantially from ∇f (xk) – it may
contain a lot of “noise” – it also contains enough “signal” to make progress
toward the optimal value of f over the long term. In typical applications, com￾putation of the gradient estimate g(xk,ξ k) is much cheaper than computation
of the true gradient ∇f (xk).
The choice of steplength αk is critical to the theoretical and practical
behavior of SG. We cannot expect to match the performance of the steepest
descent method, in which we move along the true negative gradient direction
∇f (xk) rather than its noisy approximation g(xk,ξ k). In the steepest
descent method, the fixed steplength αk ≡ 1/L (where L is the Lipschitz
constant for ∇f ) yields convergence; see Chapter 3. We can show that this
fixed steplength choice will not yield the same convergence properties in the
stochastic gradient context by considering what happens if we initialize the
method at the minimizer of f that is, x0 = x∗. Since ∇f (x∗) = 0, there
are no descent directions, and the methods of Chapter 3 will generate a zero
step – as they should, since we are already at a solution. The stochastic gradient
direction g(x0,ξ 0) may, however, be nonzero, causing SG to step away from
the solution (and increase the objective). But we can show that for judicious
choice of the steplength sequence {αk}, the sequence {xk} converges to x∗, or
at least to a neighborhood of x∗, at rates that are typically slower than those
achieved by (true) gradient descent.
5.1 Examples and Motivation
There are many situations in which SG is a powerful tool. Here we discuss a
few motivating examples that drive our subsequent implementation details and
theoretical analyses.
5.1.1 Noisy Gradients
The simplest application of SG is to the case when the gradient estimate g(x,ξ)
is the true gradient with additive noise; that is,
g(x,ξ) = ∇f (x) + ξ, (5.3)
where ξ is some noise process. The unbiasedness property (5.1) will hold,
provided that E(ξ) = 0. Our analysis in what follows reveals a protocol for
choosing step sizes αk so that SG (5.2) converges. Formula (5.2) reduces in
this case to5.1 Examples and Motivation 77
xk+1 = xk αk(∇f (xk) + ξ k), (5.4)
which is the steepest-descent step with an additive noise term αkξ k.
5.1.2 Incremental Gradient Method
The incremental gradient method, also known as the perceptron or back
propagation, is one of the most common variants of SG. Here we assume that
f has the form of a finite sum; that is,
f (x) = 1
N

N
i=1
fi(x), (5.5)
where N is usually very large. Computing a full gradient ∇f generally requires
computation of ∇fi, i = 1,2,...,N – a computation that scales proportionally
to N in general. Iteration k of the incremental gradient procedure selects some
index ik from {1,2,...,N} and sets
xk+1 = xk αk∇fik (xk).
That is, we choose one of the functions fi and follow its negative gradient.
The standard incremental gradient method chooses ik to cycle through the
components {1,2,...,N} in order; that is, ik = (k mod N) + 1 for k =
0,1,2,....
Alternatively, we could choose ik according to some random procedure
at each iteration, which would be an SG approach. We see this by defining
the random variable space  to be the set of indices {1,2,...,N}, and
the choice of random variable ξ k is the index ik ∈ {1,2,...,N}, so that
g(xk,ξ k) = ∇fik (xk). Here, the distribution P is such that P(i) = 1/N for all
i = 1,2,...,N. The unbiasedness property (5.1) holds, since
Eξ (g(x,ξ)) = 1
N

N
i=1
∇fi(x) = ∇f (x).
The convergence analysis of this method is straightforward, as we will see.
Surprisingly, analysis of standard incremental gradient, with the cyclic choice
of indices ik, is more challenging, and the convergence guarantees are weaker.
5.1.3 Classification and the Perceptron
Classification is a canonical problem in machine learning, as we showed in
Chapter 1. We have data consisting of pairs (ai,yi), with feature vectors78 5 Stochastic Gradient
ai ∈ Rn and labels yi ∈ { 1,1} for i = 1,2,...,N. The goal is to find a
vector x ∈ Rn such that
xT ai > 0 for yi = 1, xT ai < 0 for yi = −1.
Any x satisfying these requirements defines a line through the origin with all
positive examples on one side and all negative examples on the other. (Often,
the division is not so clean, in that the data may allow no line to perfectly
separate the two classes, but we can still search for a w that most nearly
achieves this goal.)
A popular algorithm for finding x called the perceptron was invented in the
1950s. It uses one example at a time to generate a sequence {xk}, k = 1,2,...
from some starting point x0. At iteration k, we choose one of our data pairs
(aik ,yik ) and update according to the formula
xk+1 = (1 γ ) xk +

ηyikaik if yik (xk)T aik < 1
0 otherwise, (5.6)
for some positive parameters γ and η. If the current guess xk classifies the pair
(aik ,yik ) incorrectly, then this iteration “nudges” xk to make (xk)T aik closer
to the correct sign. If xk produces correct classification on this example, no
change is made.
This method is an instance of SG. A quick calculation shows that this
procedure is obtained by applying SG to the cost function
1
N

N
i=1
max 
1 yiaT
i x,0

+
λ
2
x2
2, (5.7)
where ξ k is the index ik of a single term from the summation. In the update
equation (5.6), we have used (5.2) with
g(xk
,ξ k) = g(xk
,ik) = λxk +
 ηyikaik if yik (xk)T aik < 1
0 otherwise, (5.8)
and γ = αkλ and η = αk. (In machine learning, the steplength is often referred
to as the learning rate.) The cost function (5.7) is often called the support
vector machine (see Section 1.4). In the parlance of our times, the perceptron
is equivalent to “training” a support vector machine using SG.
5.1.4 Empirical Risk Minimization
In machine learning, the support vector machine is one of many instances
of the class of optimization problems called empirical risk minimization5.1 Examples and Motivation 79
(ERM). Many classification, regression, and decision tasks can be evaluated
as expected values of error over the distribution from which the data is drawn
or generated. The most common example is known as statistical risk. Given a
data generating distribution P and a loss function (u,v), we define the risk as
R[f ] := E(x,y)∼P [(f (x),y)]; (5.9)
that is, the expectation is taken over the data space (x,y) according to
probability distribution P. The function  measures the cost of assigning the
value f (x) when the quantity to be estimated is y. (Typically,  becomes
larger when f (x) deviates further from y.) The quantity R is the expected
loss of the decision rule f (x) with respect to the probability distribution P.
The goal of many learning tasks is to choose the function f that minimizes
the risk. For example, the support vector machine uses a “hinge loss” as the
function , which measures the distance between the prediction wT x and the
correct half-space. In regression problems, y is a target variate, and the loss
measures the difference between f (x) and y according to the square function
(f (x),y) = 1
2 (f (x) − y)2.
Often, minimization (and even evaluation) of the risk function (5.9) is
computationally intractable and requires knowledge of the likelihood and prior
models for the data pairs (x,y). A popular alternative uses samples to provide
an estimate for the true risk. Suppose we have a process that generates indepen￾dent identically distributed (i.i.d.) samples (x1,y1),(x2,y2),... ,(xN,yN ) from
the joint distribution p(x,y). For these data points and a fixed decision rule
x(y) ˆ , we can expect the empirical risk defined by
Remp[f ] := 1
N

N
i=1
(f (yi),xi) (5.10)
to be “close” to the true risk R[f ]. Indeed, Remp[f ] is a random variable equal
to the sample mean of the loss function. If we take the expectation with respect
to our sample set, we have
E [Remp[f ]] = R[f ].
Given these samples, the empirical risk is no longer a function of the likelihood
and prior models. It yields a simpler optimization problem, in which the
objective is a finite sum of the form (5.5). Minimizing this empirical risk
corresponds to finding the best function f that minimizes the average loss
over our data sample.
SG and ERM are intimately related. One variant of ERM formulates
the problem finitely as (5.10) and then applies the randomized incremental80 5 Stochastic Gradient
gradient approach of Section 5.1.2 to this function. Another variant does not
explicitly take a finite data sample; instead it applies SG directly to (5.9). At
each step, a pair (x,y) is sampled according to the distribution P, and a step is
taken along the negative gradient of loss function  with respect to f , evaluated
at the point (f (x),y).
The perceptron is a particular instance of ERM, in which we define f (x) =
wT x (so that f is parametrized by the vector w) and (f (x),y) = max(1
yxT w,0).
5.2 Randomness and Steplength: Insights
Before turning to a rigorous analysis of SG, we give some background and
insight into how to choose the steplength parameters αk, using some simple
but informative examples.
5.2.1 Example: Computing a Mean
Consider applying an incremental gradient method to the scalar function
f (x) := 1
2N

N
i=1
(x − ωi)
2
, (5.11)
where ωi, i = 1,2,...,N are fixed scalars. This function has the form of the
finite sum (5.5) when we define fi(x) = 1
2 (x ωi)2, so that
∇fi(x) = x ωi.
We start with x0 = 0 and, as in standard incremental gradient, step through
the indices in order and use the steplength αk = 1/(k + 1). The first few
iterations are
x1 = x0 (x0 ω1) = ω1,
x2 = x1 − 1
2

x1 − ω2

= 1
2
ω1 +
1
2
ω2,
x3 = x2 1
3

x2 ω3

= 1
3
ω1 +
1
3
ω2 +
1
3
ω3,
so that
xk =
k − 1
k

xk−1 +
1
k
ωk = 1
k

k
j=1
ωj , k = 1,2,... . (5.12)5.2 Randomness and Steplength: Insights 81
The steplength αk = 1/(k + 1) was the one originally proposed by Robbins
and Monro (1951), and it makes sense for this simple example, as it produces
iterates that are the running average of all the samples ωj encountered so far.
This choice of step size has two other important features.
• Even when the gradients g(x;i) = ∇fi(x) are bounded in norm, the iterates
can traverse an arbitrary distance across the search space, because
∞
k= 0 1/(k + 1) = ∞. Thus, convergence can be obtained even when the
starting point x0 is arbitrarily far from the solution x∗.
• The steplengths shrink to zero, so that when the iterates reach a
neighborhood of the solution x∗, they tend to stay there, even though the
search directions g(x;ξ) contain noise.
For this simple example, the global minimum of f is found after N steps of
the cyclic, incremental method; there is no need for randomness. In fact, when
we choose the component function fik randomly, we are unlikely to converge
to the minimizer of (5.11) in a finite number of iterations. However, there are
other instances of finite sum objectives in which randomness produces much
better performance than cyclic schemes, as we see in the next section.
Let us consider now a “continuous” version of (5.11):
f (x) = 1
2Eω (x − ω)2
, (5.13)
where ω is some random variable with mean μ and variance σ2. At step j of
SG, we select some value ωj+1 from the distribution of ω, independently of
the choices of ω that were made at previous iterations. We take a step of length
1/(j +1) in direction xj −ωj+1. After k steps, starting from x0 = 0, we have,
as before, that xk satisfies (5.12). By plugging this value into (5.13) and taking
the expectation over ω and all the random variables ω1,ω2,...,ωk, we obtain
f (xk) = 1
2
Eω1,ω2,...,ωk,ω
⎡
⎢
⎣
⎛
⎝
1
k

k
j=1
ωj ω
⎞
⎠
2
⎤
⎥
⎦ = 1
2k
σ2 +
1
2
σ2. (5.14)
In this simple case, too, we can compute the minimizer of (5.13) exactly. We
have
f (x) = 1
2E [x2 − 2ωx + ω2] = 1
2 x2 − μx + 1
2σ2 + 1
2μ2.
Thus, the minimizer of f is x∗ = μ, with f (x∗) = 1
2σ2. By comparing this
value with (5.14), we have
f (xk) f (x∗) = 1
2k
σ2.82 5 Stochastic Gradient
Statistically speaking, it can be shown that xk is the highest-quality estimate
that can be attained for x∗ given the sequence {ω1,ω2,...,ωk}. Interestingly,
SG, which considers the samples ωj+1 one at a time and makes a step
after each iteration, is able to achieve the same quality as an estimator that
makes use of the complete set of data {ω1,ω2,...,ωk} at once. Even so, the
convergence rate for this best-possible performance is sublinear: The sequence
of differences between function values and their optimum {f (xk) f ∗} shrinks
like 1/k, rather than decreasing exponentially to zero. This rate demonstrates
a fundamental limitation of SG: Linear convergence cannot be expected in
general. Statistics, not computation or algorithm design, stands in the way of
linear convergence rates.
5.2.2 The Randomized Kaczmarz Method
The potential benefits of randomness can be seen when we consider a special
case of the following linear least squares problem:
min f (x) := 1
2N

N
i=1

aT
i x bi
2
, (5.15)
where ai = 1, i = 1,2,...,N. Assume that there exists an x∗ such that
aT
i x∗ = bi for i = 1,2,...,N. This point will be a minimizer of f , with
f (x∗) = 0. SG with steplength αk ≡ 1 known as the randomized Kaczmarz
method yields the recursion
xk+1 = xk aik

aT
ik xk bik

= xk aikaT
ik (xk x∗).
Aggregating the effects of the first k iterations, we obtain
xk+1 x∗ =

I aikaT
ik
 xk x∗

= %
k
j=0

I aij aT
ij
 x0 x∗

.
Iteration k is a projection of the current iterate xk onto the plane defined by
aT
ik
x = bik . If two successive subspaces are close to one another, then xk+1
and xk are close together, and we do not make much progress toward x∗.
The following example describes a set of vectors {a1,a2,...,aN } such that
the deterministic, cyclic choice of indices ik = (k mod N) + 1 yields slow
progress, while much faster convergence is attained by making random choices
of ik ∈ (1,2,...,N} for each k.
For N ≥ 3, set ωN := π/N and define the vectors ai as follows:
ai =
 cos(iωN )
sin (iωN )

, i = 1,2,...,N. (5.16)5.2 Randomness and Steplength: Insights 83
Define bi = 0, i = 1,2,...,N, so that the solution of (5.15) is x∗ = 0. We
have that ai = 1 for all i and, in addition, that ai,ai+1 = cos(ωN ) for
1 ≤ i ≤ N 1. The matrices Mi := I aiaT
i are positive semidefinite for all
i, and the following identity is satisfied:
Ej (Mj ) = 1
N

N
i=1
Mi = 1
2
I (5.17)
Any set of unit vectors satisfying (5.17) is called a normalized tight frame, and
the vectors (5.16) form a harmonic frame due to their trigonometric origin.
Consider the randomized version of the Kaczmarz method in which we
select the vector aik with equal likelihood from among {a1,a2,...,aN }, with
the choice made independently at each iteration. The expected decrease in error
over iteration k, conditional on the value of xk, is
Eik (xk+1 − x∗ | xk) =

Eik (I − aikaT
ik )

(xk − x∗) = 1
2 (xk − x∗), (5.18)
where we used (5.17) to obtain the fraction of 1/2. The following argument
shows exponential decrease of the expected error with rate (1/2) per iteration:
E(xk x0) = Ei0,i1,...,ik−1
k
%−1
j=0
Mij (x0 x∗)
=
⎡
⎣
k
%−1
j=0
Eij (Mij )
⎤
⎦ (x0 x∗)
= -
Eij (Mij )
.k (x0 x∗) = 2−k(x0 x∗).
(The critical step of taking the expectation inside the product is possible
because of independence of the ij , j = 0,1,...,k − 1.)
The behavior of randomized Kaczmarz is shown in the right diagram in
Figure 5.1, with the path traced by the iterations shown as a dotted line.
Why do we attain linear convergence for the randomized method, when the
example of computing means in Section 5.2.1 attained only a sublinear rate?
The answer is that this problem is rather special, in that the solution x∗ is a
fixed point of both a gradient map and a stochastic gradient step. That is, both
∇f (x) and ∇fi(x) approach zero as x → x∗, for all i = 1,2,...,N. For the
same reason, we were able to use a large fixed steplength αk ≡ 1 rather than
the usual decreasing steplength.
The fact that the vectors aik are selected randomly for k = 0,1,2,... is also
critical to the fast convergence. If we use the deterministic order ik = k + 1,
k = 0,1,2,...,N 1, the convergence analysis is quite different. Define the
vectors5.3 Key Assumptions for Convergence Analysis 85
5.3 Key Assumptions for Convergence Analysis
We now turn to convergence analysis of SG, applied to the convex function
f: Rn → R, with steps of the form (5.2) and search directions g(x,ξ)
satisfying condition (5.1). To prove convergence, we need to assume some
bounds on the sizes of the gradient estimates g(x,ξ) so that the information
they contain is not swamped by noise. We assume that there are nonnegative
constants Lg and B such that
Eξ
#
g(x;ξ)2
2
$
≤ L2
gx x∗2 + B2 for all x. (5.19)
Note that this assumption may be satisfied even when g(x;ξ) is arbitrarily large
for some combination of x and ξ ; formula (5.19) requires only boundedness in
expectation over ξ for each x. (Section 5.3.3 contains an example in which ξ
is unbounded but (5.19) still holds for suitable choices of Lg and B.)
Note that when Lg = 0 in (5.19), f cannot be strongly convex over
an unbounded domain. If f were strongly convex function with modulus of
convexity m, we would have
∇f (x) ≥ m
2 x − x∗
for all x. On the other hand, we have, by Jensen’s inequality, that
∇f (x)2 = E g(x;ξ)2 ≤ E [g(x;ξ)2].
These two bounds together imply that it is not possible to find a B for which
(5.19) holds with Lg = 0 if the domain of f is unbounded.
When f has the finite-sum form (5.5) and we have ∇fik (xk) as the gradient
estimate at iterate xk, where ik chosen uniformly at random from {1,2,...,N},
as in Section 5.1.2, the bound (5.19) specializes to
1
N

N
i=1
∇fi(x)2 ≤ L2
gx x∗2 + B2 for all x. (5.20)
The steplengths αk in the stochastic gradient iteration formula (5.2) typi
cally depend on the constants Lg and B in (5.19). Throughout, we will assume
that the sequence {ξ k}k=0,1,2,... needed to generate the gradient approximations
g(xk,ξ k) is selected i.i.d. from a fixed distribution. (It is possible to weaken the
i.i.d. assumptions, but we do not consider such extensions here.)
We now examine how the constants Lg and B appear in different problem
settings, including those described in earlier sections.86 5 Stochastic Gradient
5.3.1 Case 1: Bounded Gradients: Lg = 0
Suppose that the stochastic gradient function g(·;·) is bounded almost surely
for all x – that is, Lg = 0 in (5.19). This is true for the logistic regression
objective
f (x) = 1
N

N
i=1
yixT ai + log(1 + exp(xT ai)), (5.21)
where the data are (ai,yi) with yi ∈ {0,1}, i = 1,2,...,N. Following the
finite-sum setting (5.5), the random variable ξ is drawn uniformly from the set
{1,2,...,N}, and
g(x;i) =

yi + exp(xT ai)
1 + exp(xT ai)

ai.
Thus, (5.19) holds with Lg = 0 and B = supi=1,2, ,N ai2.
5.3.2 Case 2: Randomized Kaczmarz: B = 0, Lg > 0
Consider the least-squares objective (5.15), where we assume that ai  0 but
not necessarily ai = 1 for each i. Assume that there is x∗ for which f (x∗) =
0 that is, aT
i x∗ = bi for all i = 1,2,...,N. By substituting into (5.15), we
obtain
f (x) = 1
2N

N
i=1
(x x∗)
T aiaT
i (x x∗),
and with the random variable ξ being drawn uniformly from {1,2,...,N},
we have
g(x;i) = aiaT
i (x x∗).
For the expected norm, we have
E [g(x;i)2] = E [ai2|aT
i (x x∗)|
2] ≤ E [ai4]x x∗2
,
so that (5.19) can be satisfied by setting Lg = E[ai4]
1/2 and B = 0.
5.3.3 Case 3: Additive Gaussian Noise
Consider the additive noise model (5.3) where ξ is, from the Gaussian
distribution with mean zero and covariance, σ2I ; that is, ξ ∈ N(0,σ2I). We
have E [g(x;ξ)] = ∇f (x) and5.4 Convergence Analysis 87
E[g(x;ξ)2] = ∇f (x)2 + 2∇f (x)T E(ξ) + E(ξ2) = ∇f (x)2 + nσ2.
(5.22)
We can satisfy (5.19) by setting B = σ √n and defining Lg to the Lipschitz
constant L of the gradient of f (because ∇f (x)2 = ∇f (x) ∇ f (x∗)2 ≤
L2x x∗2).
5.3.4 Case 4: Incremental Gradient
Consider the finite-sum formulation (5.5) in which the gradient ∇fi of
each term in the sum has Lipschitz constant Li. As in Section 5.1.2, the
distribution for the random variable ξ is discrete with N equally likely choices
corresponding to the indices i = 1,2,...,N of the terms in the sum. For the
ith term fi(x), we define x∗i to be any point for which ∇fi(x∗i
) = 0. We then
have
Eξ [g(x;ξ)2] = Ei[∇fi(x)2]
≤ E[L2
i x − x∗i
2]
≤ E
#
2L2
i x x∗2 + 2L2
i x∗i x∗2
$
= 2
N

N
i=1
L2
i x x∗2 +
2
N

N
i=1
L2
i x∗i x∗2
,
where we used the bound a +b2 ≤ 2a2 +2b2. Thus, (5.19) holds if we
define
L2
g = 2
N

N
i=1
L2
i, B2 = 2
N

N
i=1
L2
i x∗i x∗2
.
There is nice intuition for this choice of B. If x∗i = x∗ for all i, then B = 0,
as in the case of the randomized Kaczmarz method (Section 5.3.2).
5.4 Convergence Analysis
Our convergence results track the decrease in certain measures of error as
a function of iteration count. These measures are of two types. The first is
an expected squared error in the point x – that is, E
-
x − x∗2
.
, where x∗
is the solution and the expectation is taken over all the random variables ξ k
encountered to that point of the algorithm. This measure is most appropriate
when the objective f is strongly convex, so that the solution x∗ is uniquely88 5 Stochastic Gradient
defined. The second measure of optimality is the gap between the current
objective value and the optimal value – that is f (x) f ∗, where f ∗ is the value
of the objective at any solution x∗. This measure can be used when f is convex
but not necessarily strongly convex (so the solution may not be unique). In the
strongly convex case, each of these two measures can be bounded in terms of
the other, with the bound depending on the Lipschitz constant for ∇f and the
modulus of convexity m.
We see that the suitable choices of steplengths αk in (5.2) depend on Lg and
B, and that the convergence rates also depend on these two quantities.
Using the formula (5.2) for updating the iterate, we expand the distance to
any solution x∗, as follows:
xk+1 x∗2 = xk αkg(xk;ξ k) x∗2
= xk x∗2 2αkg(xk;ξ k),xk x∗ + α2
kg(xk;ξ k)2.
(5.23)
We deal with each term in this expansion separately. We take the expectation
of both sides with respect to all the random variables encountered by the
algorithm up to and including iteration k, namely i0,i1,...,ik. By applying
the law of iterated expectation and noting that xk depends on ξ 0,ξ 1,...,ξ k−1
but not on ξ k, we obtain
E[g(xk;ξ k),xk x∗] = E
#
Eξ k [g(xk;ξ k),xk x∗ | ξ 0
,ξ 1
,...,ξ k−1]
$
= E
#
Eξ k [g(xk;ξ k) | ξ 0
,ξ 1
,...,ξ k−1],xk x∗
$
= E
#
∇f (xk),xk x∗
$
.
In the last step of this derivation, we used the fact that g(xk;ξ k) depends on
ξ k while xk does not, so we took the expectation of g(xk;ξ k) explicitly with
respect to ξ k, to obtain ∇f (xk).
By a similar argument, we can bound the last term in (5.23) by using (5.19):
E[g(xk;ξ k)2
2] = E
#
Eξ k [g(xk;ξ k)2
2 | ξ 0
,ξ 1
,...,ξ k−1]
$
≤ E[L2
gxk − x∗2
2 + B2].
By defining the squared expected error as
Ak := E[xk − x∗2], (5.24)5.4 Convergence Analysis 89
we obtain by taking expectations of both sides of (5.23) and substituting these
relationships that
Ak+1 ≤ (1 + α2
kL2
g)Ak 2αkE
#
∇f (xk),xk x∗
$
+ α2
kB2
. (5.25)
Our results follow from different manipulations of (5.25) for different
settings of Lg and B. We proceed through several cases.
5.4.1 Case 1: Lg = 0
When Lg = 0, the expression (5.25) reduces to
Ak+1 ≤ Ak 2αkE
#
∇f (xk),xk x∗
$
+ α2
kB2
. (5.26)
Define λk to be the sum of all steplengths up to and including iteration k,
and xk to be the average of all iterates so far, weighted by the steplengths αj ;
that is,
λk = 
k
j=0
αj , xk = λ 1
k

k
j=0
αj xj
. (5.27)
(We also made use of averaged iterates in the analysis of mirror descent in
Section 3.7.) We analyze the deviation of f (x¯k) from optimality. Given the
initial point x0 and any solution x∗, we define D0 := x0 − x∗ to be the
initial squared error. (Note from (5.24) that A0 = D2
0.) After T iterations, we
have the following estimate for x¯T:
E[f (xT ) f (x∗)] ≤ E
⎡
⎣λ 1
T

T
j=0
αj (f (xj ) f (x∗))
⎤
⎦ (5.28a)
≤ λ 1
T

T
j=0
αjE[∇f (xj ),xj x∗] (5.28b)
≤ λ−1
T

T
j=0
#
1
2 (Aj Aj+1) + 1
2α2
jB2
$
(5.28c)
= 1
2
λ−1
T
⎡
⎣A0 AT +1 + B2
T
j=0
α2
j
⎤
⎦
≤
D2
0 + B2 T
j=0 α2
j
2
T
j=0 αj
. (5.28d)90 5 Stochastic Gradient
Here, (5.28a) follows from convexity of f and the definition of x¯T ; (5.28b)
again uses convexity of f; and (5.28c) follows from (5.26).
With the bound (5.28d) in hand, we can prove the following result for the
case of fixed steplengths: αk ≡ α > 0 for all k.
Proposition 5.1 (Nemirovski et al., 2009) Suppose we run SG on a convex
function f with Lg = 0 for T steps with fixed steplength α > 0. Define
αopt = D0
B √T + 1
and θ := α
αopt
.
Then we have the bound
E[f (xT ) f ∗] ≤

1
2 θ + 1
2 θ−1
 BD0
√T + 1
. (5.29)
Proof The proof follows directly when we set αj ≡ α = θαopt = θ D0
B √T +1 in
(5.28d). We have
E
#
f

x¯T

− f ∗
$
≤
D2
0 + B2(T + 1)α2
2(T + 1)α =

1
2 θ−1 + 1
2 θ
 BD0
√T + 1
.

The tightest bound is attained when θ = 1; that is, α = αopt. The bound
degrades approximately linearly in the error factor in our choice of α. That
is, if our α differs by a factor of 2 (in either direction) from αopt, the bound
is worse by a factor of approximately 2. This means that to achieve the same
bound as with the optimal step size, we need to take about four times as many
iterations because the bound also depends on the iteration counter T through a
factor of approximately 1/
√T .
Other steplength schemes could also be selected here, including choices of
αk that decrease with k. But the fixed steplength is optimal for an upper bound
of this type.
5.4.2 Case 2: B = 0
When B = 0, we obtain a linear rate of convergence in the expected-error
measure Ak. The expression (5.25) simplifies in this case to
Ak+1 ≤ (1 + α2
kL2
g)Ak 2αkE
#
∇f (xk),xk x∗
$
. (5.30)
Supposing that f is strongly convex, with modulus of convexity m > 0, we
have that
∇f (x),x − x∗ ≥ mx − x∗2
. (5.31)5.4 Convergence Analysis 91
By substituting into (5.30), we obtain
Ak+1 ≤ (1 2mαk + L2
gα2
k )Ak. (5.32)
By choosing a fixed steplength αk ≡ α for any α in the range (0,2m/L2
g),
we obtain a linear rate of convergence. The optimal choice of α is the one
that minimizes the factor (1 2mα + L2
gα2) in the right hand side of (5.32);
that is, α = m/L2
g. For this choice, we obtain from (5.32) that Ak+1 ≤ (1
m2/L2
g)Ak, k = 0,1,2,... , so that
Ak ≤

1 − m2
L2
g
k
D2
0. (5.33)
We can use this expression to bound the number of iterations T required to
guarantee that the expected error E
-
xT − x∗2
.
= AT falls below a specified
threshold  > 0. By applying the technique in Section A.2 to (5.33), we
find that
T =
0
L2
g
m2 log
D2
0

1.
Special Case: The Kaczmarz Method. For problems with additional struc￾ture, we can obtain even faster rates of convergence. In particular, faster rates
are achievable for the randomized Kaczmarz method where we specialize
our analysis to overdetermined least-squares problems (5.15). Recall that we
assume that each vector ai has unit norm and that there exists an x∗ (possibly
nonunique) such that aT
i x∗ = bi for all i. Consider the stochastic gradient
method with steplength 1:
xk+1 = xk aik (aT
ik xk bik ),
where ik is chosen uniformly at random each iteration. We have
xk+1 x∗2 =


xk aik (aT
ik xk bik ) x∗



2
= xk x∗2 2

aT
ik (xk x∗)(aT
ik xk bik )

+(aT
ik xk bik )
2
= xk x∗2 (aT
ik xk bik )
2
,
where we used aT
ik
(xk x∗) = aT
ik
xk bik . Let A be the matrix whose rows
are the vectors ai, and let λmin,nz denote the minimum nonzero eigenvalue of
AT A. We choose x∗ to be the specific point that minimizes xk − x∗ among
all points satisfying Ax∗ = b (see Section A.7). By taking expectations, we
obtain for this value of x∗ that92 5 Stochastic Gradient
E
#
xk+1 x∗2|xk
$
≤ xk x∗2 Eik
#
(aT
ik xk bik )
2
$
= xk x∗2 1
n
Axk b2
≤

1
λmin,nz
n

xk x∗2.
Defining Dk := minx : Ax=b xk x2, we have, from Dk+1 ≤ xk+1 x∗2
and Dk = xk x∗2 (because of the way that x∗ is defined earlier), that
E [Dk+1] ≤ E xk+1 x∗2 ≤

1
λmin,nz
n

E [Dk].
This is a faster rate of convergence than the one we derived for the general case
where B = 0.
5.4.3 Case 3: B and Lg Both Nonzero
In the general case in which both B and Lg are nonzero but f is strongly
convex, we have, by using (5.31) in (5.25), that
Ak+1 ≤ (1 2mαk + α2
kL2
g)Ak + α2
kB2. (5.34)
Fixed Steplength. First, consider the case of a fixed steplength. Assuming
that α ∈ (0,2m/L2
g), we can roll out the recursion (5.34) to obtain
Ak ≤ (1 2mα + α2L2
g)
kD2
0 +
αB2
2m αL2
g
. (5.35)
No matter how many iterations k are taken, the bound on the right-hand side
never falls below the threshold value
αB2
2m − αL2
g
. (5.36)
This behavior can be observed in practice. The iterates converge to a ball
around the optimal solution, whose radius is bounded by (5.36), but from that
point forward, they bounce around inside this ball. We can reduce the radius of
the ball by decreasing α, but this strategy has the effect of slowing the linear
rate of convergence indicated by the first term in the right-hand side of (5.35):
The quantity 1 − 2mα + α2L2
g moves closer to 1.
One way to balance these two effects is to make use of epochs, as we discuss
in Section 5.5.1.5.5 Implementation Aspects 93
Decreasing Steplength. The scheme just described suggests another
approach, one in which we decrease the steplength αk at a rate approximately
proportional to 1/k. (The epoch-doubling scheme of Section 5.5.1 is a
piecewise constant approximation to this strategy. At the last iterate of epoch S,
we will have taken about (2S 1)T total iterations, and the current steplength
will be α/2S−1.)
Suppose we choose the steplength to satisfy
αk = γ
k0 + k
,
where γ and k0 are constants (hyperparameters) to be determined. We will
show that suitable choices of these constants lead to an error bound of the
form
Ak ≤ Q
k0 + k
for some Q. The following proposition can be proved by induction.
Proposition 5.2 Suppose f is strongly convex with modulus of convexity m.
If we run SG with steplength
αk = 1
2m(L2
g/2m2 + k), k = 0,1,2,...,
then we have, for some numerical constant c0,
E[xk x∗2] ≤ c0B2
2m(L2
g/2m2 + k), k = 0,1,2,... .
5.5 Implementation Aspects
We mention here several techniques that are important elements of many
practical implementations of SG.
5.5.1 Epochs
As mentioned in Section 5.4.3, epochs are a central concept in SG. In an epoch,
some number of iterations are run, and then a choice is made about whether
to change the steplength. A common strategy is to run with a fixed step size
for some specified number of iterations T and then reduce the steplength by
a factor γ ∈ (0,1). Thus, if our initial steplength is α, on the kth epoch, the
steplength is αγ k 1. This method is often more robust in practice than the94 5 Stochastic Gradient
diminishing steplength rule. For this steplength rule, a reasonable heuristic is
to choose γ in the range [0.8,0.9]. (Tuning of “hyperparameters” such as γ
and the lengths of the epochs is one of the most important issues in practical
implementation of SG.)
Another popular rule is called epoch doubling. In this scheme, we run for
T steps with steplength α, then run 2T steps with steplength α/2, and then
4T , steps with steplength α/4 and so on. Note that this scheme provides a
piecewise constant approximation to the function α/k.
5.5.2 Minibatching
When applying SG to the finite sum objective (5.5), steps are often not based
just on the gradient on a single term in this sum but rather on a minibatch of
terms, usually of a given size (say p). That is, at iteration k, we select a subset
Sk ⊂ {1,2,...,n}, with |Sk| = p, and set
xk+1 = xk αk
1
p

i∈Sk
∇fi(xk).
If the subset Sk is chosen uniformly at random among the set of all subsets of
size p from {1,2,...,n} and is i.i.d. across the iterations k, the convergence
theory outlined before can be applied. The idea is that the minibatch has lower
variance as an estimate of ∇f (xk) than does the estimate based on a single
term, namely ∇fik (xk), so more rapid convergence can be expected. Of course,
it is also typically p times more expensive to obtain this estimate! Still, when
we account for the cost of performing the update to the vector x and possibly
communicating this update to the nodes in a parallel processing architecture,
the minibatching approach makes sense. It is used almost universally in
practical implementations of SG. The choice of minibatch size p is another
“hyperparameter” that can influence significantly the practical performance of
the approach.
5.5.3 Acceleration Using Momentum
A popular variant of SG makes use of momentum, replacing the basic step (5.2)
with one of the form
xk+1 = xk αkg(xk
,ξ k) + βk(xk xk−1). (5.37)
The inspiration for this approach comes, of course, from the accelerated
gradient methods of Chapter 4. In practice, these variants are highly successful,
with popular choices for βk often falling in the range [0.8,0.95].5.5 Implementation Aspects 95
In the case when B = 0, as in the randomized Kaczmarz method, the
use of momentum can yield speedups comparable to those seen in the
accelerated gradient methods of Chapter 4. The overhead of computing and
maintaining the momentum term can cancel out the gains in speedup. (See
further discussion in this chapter’s Notes and References.)
In the general case, the theoretical guarantees for momentum methods
demonstrate only meager gains over standard SG. Essentially, we know that
the function value will converge at a rate of 1/k, but for certain instances, one
can reduce the constant in front of the 1/k using momentum or acceleration.
Regardless of the theoretical guarantees, one should always keep in mind that
momentum can provide significant practical accelerations, and it should be
considered an option in any implementation of SG.
Notes and References
The foundational paper for SG is by Robbins and Monro (1951). As we men￾tioned, similar ideas were proposed independently in other contexts. Among
these, we can count Rosenblatt’s perceptron (Rosenblatt, 1958), discussed
in Section 5.1.3. Application of SG to problems in machine learning were
described by first by Zhang (2004) and later by the authors of the Pegasos paper
(Shalev-Shwartz et al., 2011), who described a minibatched SG approach for
linear SVM.
Analysis of SG for the case of Lg = 0, for both weakly and strongly convex
cases, appears in Nemirovski et al. (2009). (This paper did much to popularize
the SG approach in the optimization community.)
The algorithm of Kaczmarz (1937) was used as the standard method in
image reconstruction from tomographic data for many years. The description
of a randomized variant by Strohmer and Vershynin (2009) generated a new
wave of interest in the approach, leading to the development of many new
variants with interesting properties.
The ideas behind empirical risk minimization for learning were described
by Vapnik (1992) and in a classic text by the same author (Vapnik, 2013).
Incremental gradient was described in the context of least squares by
Bertsekas (1997), who also wrote a survey (in a more general context) in
Bertsekas (2011). Another interesting contribution on this topic is by Blatt
et al. (2007).
The past few years have seen a number of principled approaches emerge for
using acceleration in conjunction with stochastic gradient. Accelerated SG has
been described for least squares in Jain et al. (2018). A general (but complex)96 5 Stochastic Gradient
approach called Katyusha is described by Allen-Zhu (2017). (A convergence
analysis of Katyusha and other SG methods based on dissipativity theory and
semindefinite programs appears in Hu et al., 2018.)
Another set of techniques that has been explored to enhance the perfor￾mance of SG in the finite-sum setting involves hybridization of SG with
steepest descent. For example, the SVRG method (Johnson and Zhang, 2013)
occasionally calculates a full gradient, and moves along directions in which
this gradient is gradually modified by using gradient information from a single
function in the finite sum, evaluated at the latest iterate. Other methods in this
vein include SAG (Le Roux et al., 2012) and SAGA (Defazio et al., 2014).
Exercises
1. Consider the kth iteration (5.12) of the cyclic incremental gradient
method applied to the function (5.11). Show that the minimizer is found
after exactly N steps (that is, xN = x∗) and that f (x∗) is one half of the
variance of the set {ω1,ω2,...,ωN }.
2. Verify the formula (5.14), given that the mean of the random variable ω is
μ and its variance is σ2. (The random variables ωi, i = 1,2,...,k follow
the same distribution, and all the random variables in this expression are
independent.)
3. We showed that the unregularized support vector machine (5.21) admits a
bound of the form (5.19) with Lg = 0. Find values of Lg and B such that
the regularized support vector machine (5.7) (with g(wk,ξ k) defined by
(5.8)) satisfies (5.19). (Hint: Use the inequality
a + b2 ≤ 2a2 + 2b2.)
4. (a) Consider the finite-sum objective (5.5) with additive Gaussian noise
model on the component functions fi; that is,
[∇fi(x)]j =[∇f (x)]j +ij , for all i =1,2,...,N and j =1,2,...,n,
where ij ∼ N(0,σ2) for all i,j . Show that when we estimate the
gradient using a minibatch S ⊂ {1,2,...,N}, that is,
g = 1
|S|

i∈S
∇fi(x),
then we have
E(g − ∇f (x)2) = n
|S|
σ2
, E(g2) = ∇f (x)2 + n
|S|
σ2.5.5 Implementation Aspects 97
(b) Consider a minibatch strategy for the additive Gaussian noise model
(5.3) for the general formulation (5.1). That is, the gradient
estimate is
g(x;ξ1,ξ2,...,ξs) := ∇f (x) +
1
s
s
j=1
ξj ,
where each ξj is i.i.d. with distribution N(0,σ2I), and s ≥ 1. Show
that
Eξ1,ξ2, ,ξs (g(x;ξ1,ξ2,...,ξs)2) = ∇f (x)2 + n
s
σ2.
5. A popular heuristic in training neural networks is called dropout.
Suppose we are running stochastic gradient descent on a function on Rn.
In each iteration of stochastic gradient descent, a subset S ⊂ {1,2,...,n}
of variables is chosen at random. A stochastic gradient is computed with
those coordinates in S set to 0. Then only the coordinates in the
complement Sc are updated. Suppose we are minimizing the
least-squares cost
f (x) = 1
2N

N
i=1
(aT
i x bi)
2
.
Find a function f (x) ˆ such that each iteration of dropout SGD
corresponds to taking a valid step of the incremental gradient method
applied to fˆ. Qualitatively, how does changing the cardinality S change
the solution to which dropout SGD converges?
6. Let f (x) = E[F(x;ξ)] be a strongly convex function with parameter m.
Assume that
E[∇F(x;ξ)2] ≤ L2
gx − x∗2 + B2
,
where x∗ denotes the minimizer of f , and Lg and B are constants.
Suppose we run the stochastic gradient method on f by sampling ξ and
taking steps along ∇F(x;ξ) using an epoch doubling approach. That is,
we run for T steps with steplength α, and then 2T steps with steplength
α/2, and then 4T steps with steplength α/4, and so on. Let xˆt be the
average of all of the iterates in the tth epoch. How many epochs are
required to guarantee that E [ˆxt x∗2] ≤ ?
7. Let f: Rn → R be a strongly convex function with L-Lipschitz gradients
and strong convexity parameter m. Consider an algorithm that performs
exact line searches along random search directions. Each iterate uses the
following scheme to move from current iterate x to the next iterate x+.98 5 Stochastic Gradient
(a) Choose a direction v randomly from N(0,σ2I) (independently of the
search directions at all previous iterations).
(b) Set tmin = arg mint f (x + tv).
(c) Set x+ = x + tminv.
Prove that E[f (xT ) − f (x∗)] ≤ , provided
T ≥
CnL
m logf (x0) f (x∗)


for some constant C. What is the most appropriate value for C? (Hint:
Use Lemma 2.2 to deduce that
f (x + tv) ≤ f (x) + tvT ∇f (x) +
L
2
t
2v2.
Use that if v ∼ N(0,σ2I), then for any component vj of v,
j = 1,2,...,n, we have Ev v2
j /v2 = 1/n. Also use the bound (3.10).)
8. Consider applying stochastic gradient with fixed steplength α ∈ (0,1) to
(5.11), so that each iteration has the form
xk+1 = xk α(xk ωik )
for ik drawn uniformly at random from {1,2,...,N}. Assuming that the
initial point is x0 = 0, write down an explicit expression for xk, and find
Ei0,i1,...,ik 1 (xk).
9. Let f: Rn → R be a convex, differentiable function, and let g(x,ξ) be a
continuous function, satisfying (5.1), where ξ is a random variable from
set  with distribution P. Consider a projected SG approach on a
compact convex set , whose iterates are defined as
xk+1 = P

xk αg(xk
,ξ k)

, k = 0,1,2,...,
where ξ k is chosen randomly with distribution P and α is a fixed step
size. Defining x¯T := T
t=0 xt
/(T + 1) (consistently with (5.27)), prove
that this algorithm converges at the following rate:
E f (xT ) min
x∈ f (x) ≤ c √T + 1
,
where c is a problem-specific constant.
10. Consider the convex quadratic function defined in (5.11), where x ∈ Rn
and ωi ∈ Rn, i = 1,2,...,N. The vectors ωi have the following
additional properties:

N
i=1
ωi = 0, ωi = 1, for all i = 1,2,...,N.5.5 Implementation Aspects 99
Consider the SG iteration defined by xk+1 = xk αk(xk ωik ), where ik
is selected i.i.d. uniformly from {1,2,...,N}, for some steplength
αk > 0, where x0 is any initial point.
(a) Show that the minimizer of f is x∗ = 0.
(b) Express the conditional expectation Eik (xk+12 | xk) in terms of
xk2 and αk.
(c) By applying the bound in (b) recursively and using the notation
AK := E(xK2), find a bound for AK for any K = 1,2,... in
terms of A0 = x02 and α0,α1,...,αK−1, where E denotes the
expectation with respect to all random variables i0,i1,i2,.... (Hint:
Derive the formula for the first few values of K – that is,
A1,A2,A3 ... – until you see the patterm emerge.)
(d) Simplify the bound in (c) for the case in which all steplengths are the
same – that is, αk ≡ α for all k = 0,1,2,....
(e) Do you expect the iterates {xk} generated from the fixed-steplength
variant in (d) to converge to the solution x∗ = 0? Do you expect them
to converge to a ball around the solution? If so, what is the
approximate radius of this ball?
(f) Consider choosing the steplengths αk = 1/(k + 2), k = 0,1,2,....
From your answer in part (c), can you say that E(xK2) → 0 as
K → ∞ for this choice of steplengths? Explain.6
Coordinate Descent
Coordinate descent (CD) methods minimize a multivariate function by chang￾ing one of the variables (or sometimes a “block” of variables) to decrease the
objective function while holding the others fixed. Such methods have a certain
intuitive appeal, as they replace the multivariate optimization problem by a
sequence of scalar (or lower-dimensional) problems, for which steps can be
taken more cheaply. There are many variants and extensions of the basic CD
approach that have gone in and out of style over the years. The latest wave of
interest is driven largely by the usefulness of CD methods in machine learning
and data analysis problems.
To describe the approach, we focus on the basic method in which a single
coordinate is chosen for updating at each iteration of coordinate descent. When
applied to a function f: Rn → R, the kth iteration chooses some index ik ∈
{1,2,...,n} and takes a step of the form
xk+1 ← xk + γkeik , (6.1)
where eik is the ik unit vector and γk is the step. In one variant of CD
(also known as the Gauss Seidel method), γk is chosen to minimize f along
direction eik
:
γk := arg min
γ
f (xk + γeik ).
More practical variants do not minimize exactly along the coordinate directions
but rather choose γk to be a negative multiple of the partial derivative ∂f/∂xik
(also denoted by ∇ikf ):
xk+1 ← xk − αk∇ikf (xk)eik , (6.2)
for some αk > 0. Different variants of CD are distinguished by different
techniques for choosing ik and αk. In this chapter, we focus mainly on methods
1006.1 Coordinate Descent in Machine Learning 101
of type (6.2) with fixed values of αk that are defined in terms of Lipschitz
constants for the gradients, as for the full gradient methods of Section 3.2.
Section 6.1 illustrates two important optimization formulations in machine
learning in which the per-iteration cost of CD is much lower (possibly by a
factor of n) than the per-iteration cost of a full gradient method, making CD
an potentially competitive approach. In Section 6.2, we describe complexity
results for CD applied to convex functions for two variants of CD. The worst
case analysis for one of these approaches the one in which the index ik
is chosen randomly and independently of previous iterations for all k can
be stronger than that of full gradient descent, when factor of n per-iteration
savings are realized for CD iterations. (Section 9.4 extends the result for
randomized CD to functions that are strongly convex and that contain separable
convex regularization terms.) Practical variants of CD often take steps in blocks
of variables at a time rather than in a single variable. The analysis of such cases
is not vastly different from single-variable CD, and we discuss these block-CD
variants in Section 6.3.
6.1 Coordinate Descent in Machine Learning
In deciding whether CD is a plausible approach for minimizing f , relative
to such alternatives as the gradient methods of Chapters 3 and 4, we need to
consider how the properties and structure of f impact the economics of the
approach. Since CD typically requires more steps than full gradient methods,
they make sense only if the cost of computing them is correspondingly lower.
That is, the cost of computing partial gradient information needs to be cheaper
than computing the full gradient, and the computation and bookkeeping
required to take the step also should be relatively inexpensive. We describe
two examples from machine learning in which these properties hold, making
them good candidates for CD.
Coordinate Descent for Empirical Risk Minimization. Consider the objec￾tive that arises in regularized regression, classification, and ERM problems:
f (x) = 1
N

N
j=1
φj (Aj ·x) + λ
n
i=1
i(xi),
where each φj is a convex loss, Aj · denotes the j th row of the N ×n matrix A,
the functions i, i = 1,2,...,n are convex regularization functions, and λ ≥ 0
is a regularization parameter. (We assume for the present that the functions φj102 6 Coordinate Descent
and i are all differentiable) Although computing the ith component of the
gradient (∇if ) is expensive, it is easy to store and update information to lower
this cost greatly. The trick is to store the vector g = Ax for the current x, along
with the scalars ∇φj (gj ), j = 1,2,...,N. We then have
∇if (x) = 1
N

N
j=1
Aj,i∇φj (gj ) + λ∇i(xi),
where Aj,i denotes the (j,i) element of the matrix A. Note that the terms in
the summation need be evaluated only for those indices j for which Aj,i is
nonzero; that is,
∇if (x) = 1
N

j :Aj,i0
Aj,i∇φj (gj ) + λ∇i(xi).
This computation costs O(|A·i|) operations, where A·i is the ith column of
A. (The number of operations required to compute the full gradient would be
proportional to the number of nonzeros in the full matrix A.) Additionally,
the cost of updating the quantities gj := Aj ·x and ∇φj (gj ), j = 1,2,...,N
following a step γi along the coordinate direction xi is also reasonable. The
update formulas for the components of g are
gj ← gj + Aj,iγi, j = 1,2,...,N,
so it is necessary to update only those gj (and ∇φj (gj )) for which Aj,i  0 –
a total workload of O(|A·i|) operations. Considering all possible choices of
components i = 1,2,...,n, we see that the expected cost per iteration of CD
is about O(|A|/n), where |A| is the number of nonzeros in A. The cost per
iteration of a gradient method would be O(|A|). This is a large advantage for
CD methods a factor of 1/n that makes CD potentially appealing relative
to full gradient methods.
The least squares problem min 1
2N AT x b2
2 is a special case of this
example, as we see by defining φj (gj ) = 1
2 (gj bj )2.
Graph-Structured Objectives. Many optimization can be written as a sum
of functions, each of which involves only two components of the vector of
variables. For example, problems in image segmentation might couple pixels
only when they are adjacent. In topic modeling, terms may be coupled only
when they appear in the same document.
We can express the structure of such a function as an undirected graph
G = (V,E), where each edge (j,l) ∈ E connects two vertices j and l from
V = {1,2,...,n}. The objective has the form6.2 Coordinate Descent for Smooth Convex Functions 103
f (x) = 
(j,l)∈E
fjl(xj ,xl) + λ
n
j=1
j (xj ).
(We assume that each fjl and each regularization function j is differen￾tiable.) If we assume that evaluation of each gradient ∇fjl and ∇j is an O(1)
operation, the cost of a full gradient ∇f would be O(|E|+n). To implement a
CD method efficiently, we would store the values of fjl and ∇fjl at the current
x, for all (j,l) ∈ E. To compute the ith gradient component ∇if (x), we need
to sum components from the terms ∇fjl(x) for which j = i or l = i (at a total
cost proportional to the number of edges incident on vertex i) and evaluate
the term ∇i(xi). In updating the values of fjl and ∇fjl after the step in xi,
we need again only change those components for which j = i or l = i. The
“expected” cost of one CD iteration is thus O(|E|/n). We see once again the
desired 1/n relationship between the cost per iteration of CD and the cost per
iteration of a gradient method.
In both of these cases, the amount of computation required to update f
is similar to that required to update the full gradient vector, and some of the
actual operations are the same (for example, the update of gj terms in the
ERM example). This observation suggests that we can perform line searches
along the coordinate directions efficiently, using information about changes in
function and directional derivative information to find near exact minima along
each search direction.
If we are using a naive finite difference scheme to estimate derivatives,
based on a formula such as
∇if (x) ≈ f (x + δei) f (x)
δ ,
then n function evaluations are required to evaluate a full gradient, compared
with 1 to estimate a single component. However, we note in this connection that
automatic differentiation techniques (Griewank and Walther, 2008), imple￾mented in many software packages, can compute ∇f for a modest multiple
(independent of n) of the cost of evaluating f . (Note that this observation is
not really relevant to the examples of this section, since for these objectives,
the cost of evaluating f is itself too high.)
6.2 Coordinate Descent for Smooth Convex Functions
We again develop most of the ideas with reference to the familiar smooth
convex minimization problem defined by
min
x∈Rn f (x), (6.3)104 6 Coordinate Descent
where f is smooth and convex, with modulus of convexity m and a bound
L on the Lipschitz constant of the gradient for all points x in some region
of interest; see (2.19) and (2.7). We showed in Lemmas 2.3 and 2.9 that,
in the case of f twice continuously differentiable, these conditions are a
consequence of uniform bounds on the eigenvalues of the Hessian (2.10) –
that is, mI  ∇2f (x)  LI . Because the variants we consider here are mostly
descent methods, it is enough to restrict our attention in these definitions to an
open neighborhood O0 of the level set of f for the starting point x0, which is
L0 := {x | f (x) ≤ f (x0)}.
6.2.1 Lipschitz Constants
We introduce other partial Lipschitz constants for the gradient ∇f . Each
componentwise Lipschitz constant Li, i = 1,2,...,n satisfies the bound
|∇if (x + γei) ∇ if (x)| ≤ Li|γ |, i = 1,2, . . . ,n, (6.4)
for all x, γ such that x ∈ O0 and x + γei ∈ O0, while we define Lmax to be
the maximum of these constants:
Lmax := max
i=1,2,...,nLi. (6.5)
These Lipschitz constants play important roles both in implementing
variants of CD and in analyzing its convergence rates and in comparing these
rates with those of full gradient methods. We can obtain some bounds on the
difference between L and Lmax by considering the convex quadratic function
f (x) = (1/2)xT Ax where A is symmetric positive semidefinite. We have that
L = A2 = λmax(A), Lmax = max
i=1,2,...,n Aii.
It is clear from definition of matrix norm that
L ≥ Aei/ei =
2334n
j=1
A2
ji ≥ Aii,
from which it follows that L ≥ Lmax. (Equality holds for any nonnegative
diagonal matrix.) On the other hand, we have by the relationship between
trace and sum of eigenvalues (A.4) that
L = λmax(A) ≤ n
i=1
λi(A) = n
i=1
Aii ≤ nLmax.6.2 Coordinate Descent for Smooth Convex Functions 105
(Equality holds for the matrix A = eeT, where e = (1,1,...,1)T .) Thus,
we have
Lmax ≤ L ≤ nLmax. (6.6)
6.2.2 Randomized CD: Sampling with Replacement
In the basic randomized coordinate descent (RCD) approach, the index ik to be
updated is selected uniformly at random from {1,2,...,n}, and the iterations
have the form (6.2) for some αk > 0. For short-step variants, in which αk is
determined by the Lipschitz constants rather than by exact minimization or
a line-search process, sublinear convergence rates can be attained for convex
functions and linear convergence rates for strongly convex functions (m > 0
in (2.19)). Later, we discuss how this rate relates to the rates obtained for the
full gradient steepest-descent method of Chapter 3.
For precision, we make the following assumption for the remainder of this
section. We make use here of the level set L0 and its open neighborhood O0
defined earlier.
Assumption 1 The function f is convex and uniformly Lipschitz continu￾ously differentiable on the set O0 defined earlier, and attains its minimum on a
set S ⊂ L0. There is a finite R0 > 0 for which the following bound is satisfied:
max
x∈L0 min
x∗∈S x x∗ ≤ R0.
In the analysis that follows, we denote expectation with respect to a single
random index ik by Eik (·), while E(·) denotes expectation with respect to all
random variables i0,i1,i2,... encountered during the algorithm.
Our main result shows convergence of randomized CD for the fixed
steplength αk ≡ 1/Lmax.
Theorem 6.1 Suppose that Assumption 1 holds, that each index ik in the
iteration (6.2) is selected uniformly at random from {1,2,...,n}, and that
αk ≡ 1/Lmax. Then for all k > 0, we have
E(f (xk)) − f ∗ ≤
2nLmaxR2
0
k . (6.7)
When m > 0 in (2.19), we have, in addition, that
E

f (xk)

f ∗ ≤

1 m
nLmax k
(f (x0) f ∗). (6.8)106 6 Coordinate Descent
Proof By application of Taylor’s theorem, and using (6.4) and (6.5), we have
f (xk+1) = f

xk αk∇ikf (xk)eik

≤ f (xk) αk[∇ikf (xk)]
2 +
1
2
α2
kLik [∇ikf (xk)]
2
≤ f (xk) αk

1
Lmax
2
αk

[∇ikf (xk)]
2
= f (xk)
1
2Lmax
[∇ikf (xk)]
2
, (6.9)
where we substituted the choice αk = 1/Lmax in the last equality. Taking the
expectation of both sides of this expression over the random index ik, we have
Eikf (xk+1) ≤ f (xk) − 1
2Lmax
1
n
n
i=1
[∇if (xk)]
2
= f (xk)
1
2nLmax
∇f (xk)2. (6.10)
(We used here the facts that xk does not depend on ik and that ik was chosen
from among {1,2,...,n} with equal probability.) We now subtract f (x∗) from
both sides of this expression and take expectation of both sides with respect to
all random variables i0,i1,... , using the notation
φk := E(f (xk)) f ∗
, (6.11)
to obtain
φk+1 ≤ φk
1
2nLmax
E

∇f (xk)2

≤ φk
1
2nLmax
#
E(∇f (xk))
$2
.
(6.12)
(We used Jensen’s inequality in the second inequality.) We see already from
this last inequality that {φk} is a nonincreasing sequence. By convexity of f ,
we have for any x∗ ∈ S that
f (xk) f ∗ ≤ ∇f (xk)
T (xk x∗) ≤ ∇f (xk)xk x∗ ≤ R0∇f (xk),
where the final inequality is obtained from Assumption 1, because f (xk) ≤
f (x0), so that xk ∈ L0. By taking expectations of both sides, we have
E(∇f (xk)) ≥
1
R0
φk.
When we substitute this bound into (6.12) and rearrange, we obtain
φk φk+1 ≥
1
2nLmax
1
R2
0
φ2
k .6.2 Coordinate Descent for Smooth Convex Functions 107
We thus have
1
φk+1
1
φk
= φk φk+1
φkφk+1
≥ φk φk+1
φ2
k
≥
1
2nLmaxR2
0
.
By applying this formula recursively, we obtain
1
φk
≥
1
φ0
+
k
2nLmaxR2
0
≥
k
2nLmaxR2
0
,
from which (6.7) follows.
In the case of f strongly convex with modulus m > 0, we have by taking
the minimum of both sides with respect to y in (2.19), and setting x = xk, that
f ∗ ≥ f (xk)
1
2m∇f (xk)2.
By using this expression to bound ∇f (xk)2 in (6.12), we obtain
φk+1 ≤ φk
m
nLmax
φk =

1 m
nLmax 
φk.
Recursive application of this formula leads to (6.8). 
The same convergence expressions can be obtained for more refined choices
of steplength αk by making minor adjustments to the logic in (6.9). For
example, the (usually longer) steplength αk = 1/Lik leads to the same bounds
(6.7) and (6.8). The same bounds hold too when αk is the exact minimizer of
f along the coordinate search direction; we modify the logic in (6.9) for this
case by taking the minimum of all expressions with respect to αk and use the
fact that αk = 1/Lmax is, in general, a suboptimal choice.
We prove a second convergence result, with a bound different from (6.7),
for the weakly convex case. This variant, from Lu and Xiao (2015, theorem 1),
is also of interest because it uses a different proof technique.
Theorem 6.2 Suppose that Assumption 1 holds, that each index ik in the
iteration (6.2) is selected uniformly at random from {1,2,...,n}, and that
αk ≡ 1/Lmax. Then for all k > 0, we have
E(f (xk)) − f ∗ ≤
nLmaxR2
0
2k + n(f (x0) − f (x∗))
k ≤
n(Lmax + L)R2
0
2k .
(6.13)
Proof Define φk as in (6.11) and
ak := E(xk − x∗2) (6.14)108 6 Coordinate Descent
for some minimizer x∗ of f , where the expectation E is taken over all random
indices i0,i1,.... For any iteration T , we have
xT +1 − x∗2
=



xT 1
Lmax
∇iT f (xT )eiT x∗




2
= xT x∗2 2
Lmax
∇iT f (xT )(xT x∗)iT +
1
L2
max
#
∇iT f (xT )
$2
≤ xT x∗2 2
Lmax
∇iT f (xT )(xT x∗)iT +
2
Lmax
#
f (xT ) f (xT +1)
$
,
where the last inequality is obtained by applying (6.9) to the last term. By
taking the expectations of both sides with respect to the random index iT , and
using the fact that f (x∗) ≥ f (xT ) + ∇f (xT )T (x∗ xT ) (by convexity), we
have
EiT xT +1 x∗2 ≤ xT x∗2 2
nLmax
∇f (xT )
T (xT x∗)
+
2
Lmax
#
f (xT ) − EiT f (xT +1)
$
≤ xT x∗2 +
2
nLmax
(f (x∗) f (xT ))
+
2
Lmax
#
f (xT ) EiT f (xT +1)
$
,
which, by rearrangement, yields
2
nLmax
(f (xT ) f (x∗)) ≤ xT x∗2 EiT xT +1 x∗2
+
2
Lmax
[f (xT ) EiT f (xT +1)].
By taking expectations of both sides over all random indices i0,i1,... , and
using the definitions (6.11) and (6.14), we obtain
2
nLmax
φT ≤ aT aT +1 +
2
Lmax
(φT φT +1).
By summing both sides over T = 0,1,...,k, we obtain
2
nLmax

k
T =0
φT ≤ a0 − ak+1 +
2
Lmax
(φ0 − φk+1)
≤ x0 x∗2 + 2[f (x0) f (x∗)]
Lmax
, (6.15)6.2 Coordinate Descent for Smooth Convex Functions 109
where for the last inequality, we used a0 = x0 x∗2 and φ0 = f (x0)
f (x∗) along with ak+1 ≥ 0 and φk+1 ≥ 0. Since {f (xT )} is a monotonically
decreasing sequence, we can bound the left-hand side of (6.15) below by (k+1)
2
nLmax φk, and by substituting this expression into (6.15), we obtain
φk = Ef (xk) − f ∗ ≤
nLmaxx0 − x∗2
2(k + 1) + n(f (x0) − f ∗)
k + 1 ,
and we simply replace k + 1 with k on the right-hand side to obtain the result.
The final bound in the theorem is obtained by using convexity and
∇f (x∗) = 0 to obtain
f (x0)≤f (x∗)+∇f (x∗)
T (x0 x∗)+
L
2 x0 x∗2 =f (x∗)+
L
2 x0 x∗2.

The convergence rates in Theorem 6.1 make interesting comparisons with
the corresponding rates for full gradient short step methods from Section 3.2.
In comparing (6.7) with the corresponding result for the (full gradient)
steepest-descent method with constant steplength αk = 1/L (where L is from
(2.7)). We showed in Theorem 3.3 that the iteration
xk+1 = xk 1
L
∇f (xk)
leads to a convergence expression
f (xk) − f ∗ ≤
LR2
0
2k . (6.16)
Since, for problems of interest in this chapter, there is roughly a factor-of-n
difference between one iteration of CD and one iteration of a full gradient
method, the bounds (6.16) and (6.7) would be comparable (to within a factor
of 4) if L and Lmax are approximately the same. The bounds (6.6) suggest that
Lmax can be significantly less than L for some problems, and by comparing
the two worst-case convergence expressions, we see that randomized CD may
have an advantage in such cases.
A similar conclusion is reached when we compare the convergence rates on
the strongly convex case. We have for the steepest-descent method with line
search α ≡ 2/(L + m) (see Section 3.2) that
xk+1 x∗ ≤ 
1
2
(L/m) + 1

xk x∗. (6.17)
Because of Lemma 3.4, the quantities f (xk)−f (x∗) and xk −x∗2 converge
at similar rates, so we get a more apt comparison with (6.8) by squaring both110 6 Coordinate Descent
sides of (6.17). By using the approximation (1 )r ≈ 1 r for any constants
r and  with r 	 1, we estimate that the rate constant for convergence of
{f (xk)} in the short-step steepest-descent method would be about
1
4m
L + m ≈ 1
4m
L , (6.18)
because we can assume that L + m ≈ L for all but the most well-conditioned
problems. Apart from the extra factor of 4 in (6.18), and the expected factor-of￾n difference between the key terms, we note again that the main difference is
the replacement of Lmax in (6.8) by L in (6.18). Again, we note the possibility
of a faster overall rate for CD when Lmax is significantly less than L.
These observations make intuitive sense. CD methods are able to take
longer steps in general while still guaranteeing significant decrease in f .
Moreover, they make incremental improvements to x using fresh gradient
information at every step, whereas full gradient methods update all compo￾nents of x at once using information from all components of the gradient at a
single point.
Complexity results for the case of strongly convex f appear in Section 9.4.
In fact, we consider there the more general situation in which convex separable
regularization functions are added to f , and a proximal-gradient framework
(which generalizes gradient descent to regularized objective functions) is used
to minimize them.
6.2.3 Cyclic CD
The cyclic variant of CD updates the coordinates in sequential order 1,2,...,n,
then repeats the cycle until convergence is declared. This is perhaps the most
intuitive form of the algorithm. The classical Gauss–Seidel method, popular
also for linear systems of equations, has this form, with the steplengths chosen
to minimize f exactly along each search direction. Other variants do not
minimize exactly but rather take steps of the form (6.2), with αk chosen
according to estimates of the Lipschitz properties of the function, and other
considerations.
In the general CD framework (6.1), the choice of index ik in cyclic CD is
ik = (k mod n) + 1, k = 0,1,2,..., (6.19)
giving the sequence 1,2,3, . . . ,n,1,2,3, . . . ,n,1,2,3,....
Surprisingly, results concerning the convergence of cyclic variants for
smooth convex f have emerged only recently. See, for example, Beck and
Tetruashvili (2013) from which the results below are extracted; Sun and Hong6.2 Coordinate Descent for Smooth Convex Functions 111
(2015); and Li et al. (2018). (Results for the special case of the Gauss–Seidel
method applied to a convex quadratic, f , and its important symmetric over￾relaxation (SOR) variant, have been standard results in numerical linear alge￾bra for many years.) We describe a result with a flavor similar to Theorem 6.1,
assuming a fixed steplength α at every iteration, where α ≤ 1/Lmax.
Theorem 6.3 Suppose that Assumption 1 holds and that the iteration (6.2) is
applied with the index ik at iteration k chosen according to the cyclic ordering
(6.19) and αk ≡ α ≤ 1/Lmax. Then, for k = n,2n,3n,... , we have
f (xk) − f ∗ ≤
(4n/α)(1 + nL2α2)R2
0
k + 8 . (6.20)
When f is strongly convex with modulus m, we have, in addition, for k =
n,2n,3n,..., that
f (xk) f ∗ ≤

1 m
(2/α)(1 + nL2α2)
k/n
(f (x0) f ∗). (6.21)
Proof The results follow from Beck and Tetruashvili (2013, theorems 3.6
and 3.9). We note that (i) each iteration of Algorithm BCGD in Beck and
Tetruashvili (2013) corresponds to a “cycle” of n iterations of (6.2); (ii) we
update coordinates rather than blocks, so that the parameter p in Beck and
Tetruashvili (2013) is equal to n; (iii) we set Lmax and Lmin in Beck and
Tetruashvili (2013) both to 1/α, which is greater than or equal to Lmax, as
required by the proofs in that paper. 
The cyclic CD approach would seem to have an intuitive advantage over the
full gradient steepest-descent method, if we compare a single cycle of cyclic
CD to one step of the steepest-descent method. Cyclic CD is making use of the
most current gradient information whenever it takes a step along a coordinate
direction, whereas the steepest-descent method evaluates the moves along all
n coordinates at the same value of x. This advantage is not reflected in the
worst case analysis of Theorem 6.3, which suggests slower convergence than
the full gradient steepest descent method, even when we assume that the cost
per iteration differs by O(n) between the two approaches (see details in what
follows). Indeed, the proof of Beck and Tetruashvili (2013) treats the cyclic CD
method as a kind of perturbed steepest descent method, bounding the change
in objective value over one cycle in terms of the gradient at the start of the
cycle.
The bounds (6.20) and (6.21) are generally worse than the corresponding
bounds (6.7) and (6.8) obtained for the randomized algorithm, as we explain
in a moment. Computational comparisons between randomized and cyclic112 6 Coordinate Descent
methods show similar performance on many problems, but as a comparison of
the bounds suggests, cyclic methods perform worse (sometimes much worse)
when the ratio L/Lmax significantly exceeds its lower bound of 1. We note also
that the bounds (6.20) and (6.21) are deterministic, whereas (6.7) and (6.8) are
bounds on expected suboptimality.
We illustrate the results of Theorem 6.3 with three possible choices for α.
Setting α to its upper bound of 1/Lmax, we have for (6.20) that
f (xk) f ∗ ≤
4nLmax(1 + nL2/L2
max)R2
0
k + 8 ≈ 4n2L2R2
0
kLmax
.
The numerator here is worse than the corresponding result (6.7) by a factor of
approximately 2nL2/L2
max ∈ [2n,2n3], suggesting better performance for the
randomized method, with a larger advantage on problems for which Lmax 	 L.
If we set α = 1/L (a valid choice, since L ≥ Lmax), (6.20) becomes
4n(n + 1)LR2
0
k + 8 ≈ 4n2LR2
0
k ,
which is worse by a factor of approximately 2n2 than the bound (6.16) for the
full-step gradient descent approach. For α = 1/( √nL), we obtain
8n3/2LR2
0
k + 8 ,
which still trails (6.16) by a factor of 4n3/2. If we take into account the factor￾of-n difference in cost between iterations of CD and full gradient methods
for problems of interest, these differences shrink to factors of n and n1/2,
respectively.
A different analysis (due to Sun and Hong, 2015, section 3) for weakly
convex f yields a 1/k sublinear rate, like (6.20), but the constant has
different dependences on the various Lipschitz constants. The constant can be
significantly smaller in some cases, when L/Lmax near its upper bound of n,
but larger in other cases.
6.2.4 Random Permutations CD: Sampling without Replacement
The random permutations variant of CD is a kind of hybrid of the randomized
and cyclic approaches. As in the cyclic approach, the computation are divided
into epochs of n iterations each, where within each epoch, every coordinate is
updated exactly once. Unlike the cyclic approach, however, the coordinates are
shuffled at the start of each epoch. (Equivalently, we can think of the iterations6.3 Block-Coordinate Descent 113
within each epoch as sampling the coordinates from the set {1,2,...,n}
without replacement.)
The convergence properties proved for cyclic CD in Theorem 6.3 continue
to hold for random-permutations CD; the proofs in Beck and Tetruashvili
(2013) need no modification. Curiously, however, computational experience
shows that the random-permutations variant avoids the poor behavior of the
purely cyclic variant in cases for which the ratio L/Lmax is large. In all
cases, performance is quite similar to that of “sampling with replacement”
the randomized CD approach of Section 6.2.2. This behavior is explained
analytically in some special cases, a particular strongly convex quadratic in Lee
and Wright (2018) and for a more general class of strongly convex quadratics
in Wright and Lee (2020). Even in these special cases, the analysis of random￾permutations CD is much more complex than for either randomized CD or
cyclic CD.
6.3 Block-Coordinate Descent
All methods described in this chapter can be extended to the case in which the
coordinates are partitioned into blocks, each of which contains one or more
components. After possible rearrangement of the components of x, we can
partition it as
x = (x(1),x(2),...,x(p)),
where x(i) ∈ Rni , i = 1,2,...,p and p
i=1 ni = n. We use Ui to denote those
columns of the n×n identity matrix that correspond to the components in x(i).
Generalizing (6.2), step k of the block-coordinate descent method can thus be
defined as follows:
xk+1 ← xk − αkUik∇ikf (xk),
for some αk > 0, where ∇if (x) is the vector of partial derivatives of f with
respect to the components in x(i). Definitions of the componentwise Lipschitz
constants (6.4) can be extended trivially to blocks, as follows:
∇if (x + Uivi) ∇ if (x) ≤ Livi, any vi ∈ Rni , all i = 1,2,...,p.
(6.22)
Some algorithms also make use of moduli of convexity mi on the blocks, where
mi satisfies miI  UT
i ∇2f (x)Ui ∈ Rni×ni , for all x in the domain of interest.
The block-coordinate structure allows many algorithms to be extended to
the case when block-separable regularizers are present. These problems have
objectives of the form114 6 Coordinate Descent
f (x) +
p
i=1
i(x(i)), (6.23)
where each i is convex and often nonsmooth.
Generalization of the analysis of randomized CD and cyclic CD methods is
straightforward; in fact, several of our sources for this chapter describe the
results in the block-coordinate framework rather than the single-coordinate
setting that we adopted here (see, for example, Beck and Tetruashvili, 2013;
Nesterov, 2012; Nesterov and Stich, 2017; Lu and Xiao, 2015).
Block coordinate descent is a natural technique to apply in several applica
tions. For example, in low-rank matrix completion, given observations of the
(i,j) elements of a matrix M ∈ Rp×q , where (i,j) ∈ O ⊂ {1,2,...,p} ×
{1,2,...,q}, we seek matrices U ∈ Rp×r and V ∈ Rq×r (for some t ≤
min(p,q)) to minimize the objective
f (U,V ) := 
(i,j)∈O

[UV T M]i,j2
.
A natural approach is to define two blocks of variables – U and V – and
minimize successively with each of these blocks. Since f (U,V ) is a least￾squares problem in U for fixed V , and a least-squares problem in V for
fixed U, standard methods are available for minimizing over the blocks.
Tensor completion problems can be handled in a similar way; once again, the
subproblems are least-squares problems.
In nonnegative matrix factorization, we further constrain U and V to have
only nonnegative elements. The objective for the problem can be stated in the
form (6.23) that is,

(i,j)∈O

[UV T M]i,j2
+ I +
p,r(U) + I +
q,r(V ),
where I + are indicator functions that have the value 0 if all elements of the
matrix are nonnegative and ∞ otherwise. The subproblems in this formulation
are bound-constrained least-squares problems, which can be solved with
projected gradient or active set methods.
Notes and References
A notable early paper on block-coordinate descent with block-separable
regularization is by Tseng and Yun (2010), who proved convergence and
complexity rates for several settings (including nonconvex f ) and different6.3 Block-Coordinate Descent 115
variants. This paper assumes that the block of variables chosen for updating
at each step satisfies a generalized Gauss–Southwell condition, which ensures
that the improvement in f by considering this block is a nontrivial fraction of
the improvement available from a full gradient step. Some extensions of this
paper are considered by Wright (2012), together with a discussion of several
applications and local convergence results.
The proof of Theorem 6.1 is a simplified version of the analysis in
Nesterov (2012, section 2). The proof of Theorem 6.2 is from Lu and Xiao
(2015, theorem 1). Another analysis (extendable to problems with separable
regularizers) is given by Richtarik and Takac (2014).
Variants of the randomized CD approach that make use of Nesterov
acceleration were proposed first by Nesterov (2012), with a version that can be
implemented efficiently in some applications proposed later in Lee and Sidford
(2013). A more generally applicable version is described by Nesterov and
Stich (2017). When the sampling probability for component i is chosen to be
L1/2
i /(n
j=1 L1/2
j ) (where the Li are the componentwise Lipschitz constants
defined in (6.4)), Nesterov and Stich (2017, theorem 1) proves the bound
E(f (xk) f (x∗)) ≤
2R2
0
n
i=1 L1/2
i
2
k2 .
(Note that the 1/k rate of Theorems 6.1 and 6.2 has been replaced by the 1/k2
rate that is typical of accelerated methods.)
Analysis of the cyclic method in Section 6.2.3 is from Beck and Tetruashvili
(2013). A later work (Li et al., 2018) uses techniques similar to those of
Sun and Hong (2015) to analyze a version of cyclic CD with a particular
choice of step sizes, related to the componentwise Lipschitz constants, for the
strongly convex case. Some improvements to the complexity results of Beck
and Tetruashvili (2013) are obtained for these cases. (The different setting and
assumptions make it difficult to compare the results directly, but the bound on
the number of iterates required to attain a specified accuracy in the objective
function is approximately a factor of L/Lmax better in Li et al., 2018.) The
techniques of Li et al. (2018) apply to the case in which separable nonsmooth
regularization terms also appear in the objective; we consider the extension of
CD methods to such problems in Section 9.4.
When the function f satisfies the Polyak–Łojasiewicz (PL) condition
of Section 3.8, Karimi et al. (2016) shows linear convergence rates for
algorithms of the form (6.2) and its extensions to problems with separable
regularization terms. Here, as before, the PL condition yields results similar
to those obtained for strongly convex functions. Chouzenoux et al. (2016)116 6 Coordinate Descent
consider block-coordinate descent for the separable regularized case, using
variable metrics to modify the gradient at each iteration, and proves global
convergence as well as local convergence rates under the Kurdyka–Łojasiewicz
(KL) condition (which is also described in Section 3.8).
The justification for using CD methods as opposed to full gradient methods
is perhaps seen best in asynchronous implementations on parallel computers.
Multiple cores can, of course, share the workload of evaluating a full gradient,
but there is inevitably a synchronization point the computation must wait
for all cores to complete their share of the work before it can proceed with
computing and taking the step. Asynchronous implementations of CD methods
are easy to design, especially for multicore, shared memory computers in
which all cores have access to a shared version of the variable x (and
possibly other quantities involved in the evaluation of gradient information).
Strong results about the convergence of asynchronous algorithms under weak
assumptions were obtained by Bertsekas and Tsitsiklis (1989, section 7.5).
More recently, several papers (Liu et al., 2015; Liu and Wright, 2015) showed
that convergence rates of the serial CD methods are largely inherited by
multicore implementations provided that the number of cores is not too
large. Other parallel implementations have also been devised, analyzed, and
implemented in Richtarik and Takac (2016b), Fercoq and Richtarik (2015), and
Richtarik and Takac (2016a). Parallel CD remains an active area of research.
We note that the analysis in the papers cited in this chapter defines the
constant R0 differently from in Assumption 1, to be maxx∈L0 maxx∗∈S x
x∗ rather than maxx∈L0 minx∗∈S x x∗. This alternative definition results
of course in a larger value, which has the disadvantage of being infinite when
the solution set is unbounded. A careful look at the analysis of these papers
shows that the definition that we use here suffices.
Exercises
1. In the ERM example of Section 6.1, assume that the objective function f
is known at the current point x, along with the quantity g = Ax. Show that
the cost of computing f (x + γiei) for some i = 1,2,...,n is O(|A·i|) (the
number of elements in column i of A) the same order as the cost of
updating the gradient ∇f . Show that a similar observation holds for the
graph example in Section 6.1.
2. Consider the convex quadratic f (x) = 1
2 xT Ax with A = eeT , where
e = (1,1,...,1)T , for which L = n and Lmax = Li = 1 for
i = 1,2,...,n. Show that any variant of CD with α = 1/Lmax or α = 1/Li6.3 Block-Coordinate Descent 117
converges in one iteration. Show that the steepest-descent method (with
either exact line search or steplength α = 1/L) also converges in one step.
3. Implement the following variants of coordinate descent:
• Randomized CD (the method of Section 6.2.2) with exact line search
and with constant steplength 1/Lmax
• Cyclic CD (Section 6.2.3) with exact line search and with constant
steplengths 1/Lmax, 1/L, and 1/( √nL)
• Random-permutations CD (Section 6.2.4) with exact steps and and with
constant steplength 1/Lmax.
Compare the performance of these methods on convex quadratic problems
f (x) = 1
2 xT Ax, where A is an n × n positive semidefinite matrix
constructed randomly in the manner described in what follows. (Note that
x∗ = 0 with f (x∗) = 0.) Terminate when f (x) ≤ 10−6f (x0). Use a
random starting point x0 whose components are uniformly distributed in
[0,1]. Compute and print the values of L and Lmax for each instance.
Test your code on the following matrices A.
(i) A = QT DQT , where Q is random orthogonal and D is a positive
diagonal matrix whose here each diagonal Dii has the form 10−ζi ,
where each ζi is drawn uniformly i.i.d. from [0,1].
(ii) The same as in (i), but with each ζi drawn uniformly i.i.d. from [0,2].
(iii) Generate the matrix A as in (i), then replace it by A + 10eeT , where
e = (1,1,...,1)T .
Discuss the relative performance of the methods on these different
problems. How is your computational experience consistent (or
inconsistent) with the convergence expressions obtained in Theorems 6.1
and 6.3?
4. Compare the linear convergence bounds (6.8) and (6.21) for randomized
CD and cyclic CD, for various choices of steplength in the cyclic method,
including α = 1/Lmax, α = 1/L, and α = 1/( √nL). (In making these
comparisons, note that, for small , we have (1 )1/n ≈ 1 /n.)
Which of these choices of fixed steplength α in the cyclic method is
optimal, in the sense of approximately minimizing the factor on the
right hand side of (6.21)?7
First Order Methods for Constrained
Optimization
In constrained optimization, we seek the point x∗ in a specified set  that
attains the smallest value of the objective function f in . The set  is called
the feasible set, and it is often defined via a number of algebraic equalities
and inequalities, called constraints. The constraints can simply be bounds on
the values of the variables, or they can be more complex formulas that capture
temporal dependencies, resource usage, or statistical models. In this chapter,
we focus on case in which  is a simple closed convex set. Later chapters
consider setups in which the feasible set is more complicated.
7.1 Optimality Conditions
We consider problem (2.1), restated here as
min
x∈ f (x), (7.1)
where  ⊂ Rn is closed and convex and f is smooth (at least differentiable).
We refer to earlier definitions of local and global solutions in Section 2.1 and
convexity of sets and functions in Sections 2.4 and 2.5.
To characterize optimality for minimizing a smooth function f over a
closed convex set , we need to generalize beyond the optimality theory
of Section 2.3, which was for unconstrained optimization. Typically, the
unconstrained first-order conditions ∇f (x) = 0 are not satisfied at the solution
of (7.1). To define optimality conditions for this constrained problem, we need
the notion of a normal cone to a closed convex set  at a point x ∈ .
Definition 7.1 Let  ⊂ Rn be a closed convex set. At any x ∈ , the normal
cone N(x) is defined as
N(x) = {d ∈ Rn : dT (y − x) ≤ 0 for all y ∈ }.
1187.1 Optimality Conditions 119
Figure 7.1 Normal Cone
(Note that N(x) satisfies trivially the definition of a cone C ∈ Rn, which
is that z ∈ C ⇒ tz ∈ C for all t > 0.) See Figure 7.1 for an example of a
normal cone.
The following result is a first-order necessary condition for x∗ to be a
solution of (7.1). When f is convex, the condition is also sufficient.
Theorem 7.2 Consider (7.1), where  ⊂ Rn is closed and convex and f
is continuously differentiable. If x∗ ∈  is a local solution of (7.1), then
−∇f (x∗) ∈ N(x∗). If f is also convex, then the condition −∇f (x∗) ∈
N(x∗) implies that x∗ is a global solution of (7.1).
Proof Suppose that x∗ is a local solution, and let z be any point in . We have
that x∗+α(z x∗) ∈  for all α ∈ [0,1], and, by Taylor’s theorem (specifically
(2.3)), we have
f (x∗ + α(z − x∗)) = f (x∗) + α∇f (x∗)
T (z − x∗)
+ α
-
∇f (x∗ + γαα(z x∗)) ∇ f (x∗)
.T (z x∗)
= f (x∗) + α∇f (x∗)
T (z x∗) + o(α)
for some γα ∈ (0,1). Since x∗ is a local solution, we have that f (x∗ + α(z
x∗)) ≥ f (x∗) for all α > 0 sufficiently small. By substituting this inequality
into the previous expression and letting α ↓ 0, we have that ∇f (x∗)T (z
x∗) ≤ 0. Since the choice of z ∈  was arbitrary, we conclude that ∇f (x∗) ∈
N(x∗), as required.
Suppose now that f is also convex, and that ∇f (x∗) ∈ N(x∗). Then
∇f (x∗)T (z − x∗) ≤ 0 for all z ∈ . By convexity of f , we have
f (z) ≥ f (x∗) + ∇f (x∗)
T (z − x∗) ≥ f (x∗),
verifying that x∗ minimizes f over , proving the second claim. 120 7 First-Order Methods for Constrained Optimization
When f is strongly convex (see (2.19)), problem (7.1) has a unique solution.
Theorem 7.3 Suppose that in the problem (7.1), f is differentiable and
strongly convex, while  is closed, convex, and nonempty. Then (7.1) has a
unique solution x∗, characterized by ∇f (x∗) ∈ N(x∗).
Proof Given any z ∈ , it follows immediately from (2.19) that f is globally
bounded below by a quadratic function that is,
f (x) ≥ f (z) + ∇f (z)T (x z) + m
2 x z2
,
with m > 0. Thus, the set  ∩ {x | f (x) ≤ f (z)} is closed and bounded, hence
compact, so f attains its minimum value on this set at some point x∗, which is
thus a solution of (7.1).
For uniqueness of this solution x∗, we note that, for any point x ∈ , we
have, from (2.19) again, together with the property ∇f (x∗) ∈ N(x∗) from
Theorem 7.2, that
f (x) ≥ f (x∗) + ∇f (x∗)
T (x x∗) + m
2 x x∗2 > f (x∗),
since ∇f (x∗)T (x − x∗) ≥ 0, m > 0, and x  x∗. 
7.2 Euclidean Projection
Let  be a closed, convex set. The Euclidean projection of a point x onto  is
the closest point in  to x, measured by the Euclidean norm (which we denote
by ·). Denoting this point by P(x), we see that it solves the following
constrained optimization problem:
P(x) = arg min{z x| z ∈ },
or, equivalently,
P(x) = arg min z∈
1
2 z x2
2. (7.2)
Since the cost function of this problem is strongly convex, Theorem 7.3 tells
us that P(x) exists and is unique, so well defined. The same theorem gives us
the following characterization of P(x):
x P(x) ∈ N(P(x));
that is, from the Definition 7.1,
(x − P(x))T (z − P(x)) ≤ 0, for all z ∈ . (7.3)7.2 Euclidean Projection 121
In fact, this inequality characterizes P(x); there is no other point x¯ ∈  such
that (x ¯x)T (z ¯x) ≤ 0 for all z ∈ , since if such a point existed, it would
also be a solution of the projection subproblem.
We refer to (7.3) as a minimum principle. We can use it to compute a variety
of projections onto simple sets .
Example 7.4 (Nonnegative Orthant) Consider the set of vectors whose
components are all nonnegative:  = {x | xi ≥ 0, i = 1,2,...,n}. Note
that  is a closed, convex cone. We have
P(x) = max(x,0);
that is, the ith component of P(x) is xi if xi ≥ 0, and 0 otherwise. We prove
this claim by referring to the minimum principle (7.3). We have
(x P(x))T (z P(x))
=
xi<0
(xi [P(x)]i)(zi [P(x)]i) +

xi≥0
(xi [P(x)]i)(zi [P(x)]i)
= 
xi<0
xizi ≤ 0,
since zi ≥ 0 for all i.
Example 7.5 (Unit Norm Ball) Defining  = {x | x ≤ 1}, we have
P(x) =

x if x ≤ 1,
x/x otherwise.
We leave the proof as an Exercise.
The following result is an immediate consequence of (7.3).
Lemma 7.6 Let  be closed and convex. Then (P(y) z)T (y z) ≥ 0 for
all z ∈ , with equality if and only if z = P(y).
Proof
(P(y) − z)T (y − z) = (P(y) − z)T (y − P(y) + P(y) − z)
= (P(y) − z)T (y − P(y)) + P(y) − z2
≥ (P(y) z)T (y P(y)) ≥ 0,
where the final inequality follows from (7.3). When (P(y) z)T (y z) = 0,
we have from the same reasoning that P(y) z = 0, proving the final
claim. 
Euclidean projections are nonexpansive operators, as we show now.122 7 First-Order Methods for Constrained Optimization
Proposition 7.7 Let  be a closed convex set. Then P(·) is a nonexpansive
operator – that is,
P(x) P(y)≤x y, for all x,y ∈ Rn.
Proof We have
x y2
= (x P(x)) (y P(y)) + P(x) P(y)2
=


(x − P(x)) − (y − P(y))2 + P(x) − P(y)



2
− 2 [x − P(x)]
T -
P(y) − P(x).
− 2
-
y − P(y).T -
P(x) − P(y).
≥ (x − P(x)) − (y − P(y))2 + P(x) − P(y)2
≥ P(x) P(y)2
,
where the first inequality follows from (7.3). 
7.3 The Projected Gradient Algorithm
We consider (7.1) in which f is Lipschitz continuously differentiable with
constant L (see (2.7)) and  is closed and convex. Iteration k of the projected
gradient algorithm consists of a step along the negative gradient direction
∇f (xk), followed by projection onto the feasible set . The steplength is
chosen to ensure descent in f at each iteration. This approach is most useful
when the projection operation P(·) is inexpensive to compute, no greater than
the same order as the cost of evaluating a gradient ∇f .
Given a feasible starting point x0 ∈ , the projected gradient algorithm is
defined by the formula
xk+1 = P

xk αk∇f (xk)

, (7.4)
where αk > 0 is a steplength. Figure 7.2 shows the path traced by P(x tg)
for given x,g ∈ Rn and scalar t > 0 for a box shaped set . In this case, the
path is piecewise linear.
The following proposition shows that if xk is a point satisfying first order
conditions (see Theorem 7.2), then the projected gradient algorithm will not
move away from xk that is, xk+1 = xk, regardless of the value αk > 0
chosen for the steplength.124 7 First-Order Methods for Constrained Optimization
This expression confirms that within the first T iterations, we will find a point
x such that
P(x (1/L)∇f (x)) x ≤ .
To verify the bound (7.6), we have from Lemma 2.2 that for any x ∈ ,
f (x) ≤ qk(x) := f (xk) + ∇f (xk)
T (x − xk) +
L
2 x − xk2. (7.7)
The minimizer of qk(x) over x ∈  is simply P(xk (1/L)∇f (xk)) (see the
Exercises), which is xk+1 by (7.5). We thus have, from Theorem 7.2 applied to
minx∈ qk(x), that
∇qk(xk+1) = ∇ f (xk) L(xk+1 xk) ∈ N(xk+1).
Thus, by Definition 7.1, it follows that
[ ∇f (xk) L(xk+1 xk)]
T (xk xk+1) ≤ 0
⇒ ∇f (xk)
T (xk xk+1) ≥ Lxk xk+12.
Since f (xk) = qk(xk) and f (xk+1) ≤ qk(xk+1), we have
f (xk) f (xk+1) ≥ qk(xk) qk(xk+1)
= −∇f (xk)
T (xk+1 − xk) − L
2 xk+1 − xk2
≥
L
2 xk+1 xk2.
By summing these inequalities up for k = 0,1,...,T − 1, we have
T
−1
k=0
xk+1 xk2 ≤
2
L(f (x0) f (xT )) ≤
2
L(f (x0) f ),
from which the result follows, in a similar fashion to Section 3.2.1.
7.3.2 General Case: Backtracking
We now describe a backtracking version of the projected gradient method,
which does not require knowledge of the Lipschitz constant L. We fol
low the backtracking approach for unconstrained optimization described in
Section 3.5, but include the projection operator to ensure that all iterates xk
are feasible.
The scheme is shown in Algorithm 7.1. At each iteration, we choose some
initial guess of the steplength α¯ k > 0. (This could be either some constant,
such as α¯ k = 1 for all k, or a slight increase on the successful steplength7.3 The Projected Gradient Algorithm 125
Algorithm 7.1 Projected Gradient with Backtracking
Given 0 < c1 < 1
2 , β ∈ (0,1); Choose x0;
for k = 0,1,2, do
Set αk = αk, for some initial guess of steplength αk > 0;
while f (P(xk αk∇f (xk)))>f (xk)+c1∇f (xk)T (P(xk αk∇f (xk)) xk)
do
αk ← βαk;
end while
Set xk+1 = P(xk − αk∇f (xk));
end for
from the previous iteration, such as αk = 1.2αk−1.) We then test a sufficient
decrease condition, similar to (3.26a). This condition asks whether the actual
improvement in f obtained with this value of αk is at least a fraction c1 of
the improvement expected from the first order Taylor series expansion of f
around the current iterate xk. If this condition is not satisfied, we decrease
αk by a factor β ∈ (0,1), repeating the process until the sufficient decrease
condition holds.
Provided the initial guess α¯ k is chosen larger than 1/L, the steps that are
accepted by this backtracking approach are typically larger than the 1/L steps
of the previous section, and convergence is often faster in practice. We derive
convergence results for Algorithm 7.1 in the Exercises.
7.3.3 Smooth Strongly Convex Case
We now consider f that is strongly convex with modulus of convexity m (see
(2.19)), as well as having L Lipschitz gradients (2.7). Moreover, we assume
that f is twice continuously differentiable so that (2.4) from Theorem 2.1
applies. We have from the latter result that for any y,z ∈ Rn and any α ≥ 0,
(y α∇f (y)) (z α∇f (z))
=





 1
0
#
I − α∇2f (z + t(y − z))$
(y − z)dt





≤
 1
0



I α∇2f (z + t(y z))



dt y z
≤ sup
t∈[0,1]



I α∇2f (z + t(y z))


 y z
≤ max (|1 − αm|,|1 − αL|) y − z, (7.8)126 7 First-Order Methods for Constrained Optimization
where the second inequality follows from the fact that the spectrum of ∇2f (·)
is contained in the interval [m,L]. The right-hand side is minimized by setting
α = 2/(L + m) (see the Exercises), for which value we have
α = 2
L + m ⇒ (y − α∇f (y)) − (z − ∇f (z)) ≤
L − m
L + my − z.
We set y = xk, z = x∗, and αk ≡ 2/(L + m) in (7.8) and use the
characterization of x∗ in Proposition 7.8 and the nonexpansive property
(Proposition 7.7) to obtain
xk+1 − x∗ =



P(xk − αk∇f (xk)) − P(x∗ − αk∇f (x∗))



≤ (xk x∗) αk(∇f (xk) ∇ f (x∗))
≤
L m
L + mxk x∗,
which indicates linear convergence of {xk} to the optimal x∗ for a fixed
steplength version of projected gradient. Note that when 0 < m 	 L, the
linear rate constant is approximately (1 2m/L).
The projected gradient method analyzed here is a special case of the
proximal-gradient algorithm described in Section 9.3. We refer to that section
for analysis of cases other than those analyzed here – for example, the case in
which f is convex but not strongly convex.
7.3.4 Momentum Variants
There are versions of the projected gradient method that make use of the
momentum ideas of Chapter 4. Following (4.7), Nesterov’s method can be
adapted to (7.1) as follows:
yk = xk + βk(xk − xk−1) (7.9a)
xk+1 = P(yk αk∇f (yk)), (7.9b)
where we define x−1 = x0 as before, so that y0 = x0. (When βk ≡ 0, we
recover the projected gradient method (7.4).) Note that the sequence {xk} is
feasible, whereas the yk are not necessarily feasible. With appropriate choices
of αk and βk, and when applied to strongly convex f , the iterations (7.9) will
converge at an approximate linear rate of (1 √m/L).
7.3.5 Alternative Search Directions
Recall that in Section 3.1, we show that search directions dk other than the
negative gradient could be used in conjunction with line searches in algorithms128 7 First-Order Methods for Constrained Optimization
set requires (naively) a sorting of the elements of y. The conditional gradient
method, the first variant of which was proposed by Frank and Wolfe (1956),
provides an effective algorithm for constrained optimization that requires only
linear minimization rather than Euclidean projection.
The conditional gradient method replaces the objective in (7.1) by a linear
Taylor series approximation around the current iterate xk and solves the
following subproblem:
x¯k := arg min x¯∈ f (xk) + ∇f (xk)
T (x¯ xk) = arg min x¯∈
∇f (xk)
T x¯. (7.10)
The next iterate is obtained by stepping toward x¯k from xk as follows:
xk+1 = xk + αk(xk xk), for some αk ∈ (0,1]. (7.11)
Note that if the initial iterate x0 is feasible (that is, x0 ∈ ), all subsequent
iterates xk, k = 1,2,... are also feasible, as are all the subproblem solutions
xk, k = 0,1,2.... The method is usually applied only when  is compact (that
is, closed and bounded) and convex, so that xk in (7.10) is well defined for
all k. The conditional gradient method is practical only when the linearized
subproblem (7.10) is much easier to solve than the original problem (7.1). As
we have discussed, such is the case for various interesting choices of .
The original approach of Frank and Wolfe makes the particular choice of
steplength αk = 2/(k + 2), k = 0,1,2,.... The resulting method converges
at a sublinear rate, as we show now. Again assume that  ⊂ Rn is a closed,
bounded convex set and f is a smooth convex function. We define the diameter
D of  as follows:
D := max
x,y∈
x − y. (7.12)
We have the following result.
Theorem 7.9 Suppose that f is a convex function whose gradient is Lipschitz
continuously differentiable with constant L on an open neighborhood of ,
where  is a closed bounded convex set with diameter D, and let x∗ be the
solution to (7.1). Then if algorithm (7.10)–(7.11) is applied from some x0 ∈ 
with steplength αk = 2/(k + 2), we have
f (xk) − f (x∗) ≤
2LD2
k + 2
, k = 1,2,... .7.4 The Conditional Gradient (Frank–Wolfe) Method 129
Proof Since f has L-Lipschitz gradients, we have
f (xk+1) ≤ f (xk) + αk∇f (xk)
T (xk xk) +
1
2
α2
kLxk xk2
≤ f (xk) + αk∇f (xk)
T (x¯k − xk) +
1
2
α2
kLD2
, (7.13)
where the second inequality comes from the definition of D. For the first-order
term, we have by definition of xk in (7.10) and feasibility of x∗ that
∇f (xk)
T (x¯k xk) ≤ ∇f (xk)
T (x∗ xk) ≤ f (x∗) f (xk).
By substituting this bound into (7.13) and subtracting f (x∗) from both sides,
we have
f (xk+1) f (x∗) ≤ (1 αk)[f (xk) f (x∗)] +
1
2
α2
kLD2.
We now demonstrate the required bound by induction. By setting k = 0 and
substituting α0 = 1, we have
f (x1) f (x∗) ≤
1
2
LD2 <
2
3
LD2
,
as required. For the inductive step, we suppose that the claim holds for some
k, and demonstrate that it still holds for k + 1. We have
f (xk+1) − f (x∗) ≤

1 − 2
k + 2

[f (xk) − f (x∗)] +
1
2
4
(k + 2)2LD2
= LD2
 2k
(k + 2)2 +
2
(k + 2)2

= 2LD2 (k + 1)
(k + 2)2
= 2LD2 k + 1
k + 2
1
k + 2
≤ 2LD2 k + 2
k + 3
1
k + 2 = 2LD2
k + 3
,
as required. 
Note that the same result holds if we choose αk to exactly minimize f along
the line from xk to x¯k; only minimal changes to the proof are needed.130 7 First-Order Methods for Constrained Optimization
Notes and References
The projected gradient method originated with Goldstein (1964) and Levitin
and Polyak (1966). Goldstein proposed the steplength acceptance condition
used in Algorithm 7.1 in Goldstein, 1974. Convergence properties of projected
gradient were developed further by Bertsekas (1976) and Dunn (1981).
The conditional gradient approach was described first for the case of convex
quadratic programming by Frank and Wolfe (1956). Extensions to more
general problems of the type (7.1) are described by Dem’yanov and Rubinov
(1967) (which is difficult to read) and in Dem’yanov and Rubinov (1970).
Dunn (1980) presents comprehensive results for various line-search proce
dures, including linear convergence results for problems that satisfy a condition
akin to second-order sufficiency, and results for nonconvex problems. The
revival of interest in the conditional gradient approach in the machine learning
community is due largely to Jaggi (2013).
Exercises
1. Prove that the formula for P(x) in Example 7.5 is correct.
2. Prove that (7.8) is minimized by setting α = 2/(L + m), when
0 <m<L. Prove that the alternative choice of steplength α = 1/L leads
to a linear convergence rate of (1 m/L) in xk x∗ (similar to the rate
obtained for the unconstrained case in Section 3.2.3). How do these two
different choices compare in terms of the number of iterations T required
to guarantee xT x∗ ≤  for some tolerance  > 0?
3. By adapting the analysis of Section 4.3 to the projected version of
Nesterov’s method for the constrained case (7.9) and for the choice of
parameters αk and βk shown in (4.23), prove linear convergence of this
method, and find the constant for the linear rate.
4. Find the minimizer of cT x (for c ∈ Rn, a constant vector, and for x ∈ Rn,
a variable) over , where  is each of the following sets:
(a) The unit ball: {x | x2 ≤ 1}
(b) The unit simplex: 5
x ∈ Rn | x ≥ 0, n
i=1 xi = 1
6
(c) A box: {x | 0 ≤ xi ≤ 1, i = 1,2,...,n}
5. Show that Theorem 7.9 continues to hold if αk is chosen in (7.11) to
minimize f (xk + αk(x¯k − xk)) for αk ∈ [0,1], rather than from the
formula αk = 2/(k + 2).7.4 The Conditional Gradient (Frank–Wolfe) Method 131
6. Prove that for any αk > 0 and for xk+1 defined by (7.4), we have
xk+1 = arg min x∈ f (xk) + ∇f (xk)
T (x xk) +
1
2αk
x xk2
and
P(xk αk∇f (xk)) xk2 ≤ αk∇f (xk)
T [xk P(xk αk∇f (xk))].
(Note that with αk = 1/L, it follows that, for qk defined in (7.7), we have
xk+1 = minx∈ qk(x).)
7. Show by using arguments similar to those of Section 7.3.1 that when f is
L-smooth, the sufficient decrease condition in Algorithm 7.1 will be
satisfied whenever αk ≤ 1/L – that is,
f (P(xk αk∇f (xk)))≤f (xk) + c1∇f (xk)
T (P(xk αk∇f (xk)) xk),
(7.14)
where c1 ∈ (0,1/2). Deduce that, provided αk ≥ 1/L, the inner loop in
Algorithm 7.1 terminates with αk ≥ β/L.
8. Show by combining with the results of the previous two questions that for
any αk > 0 such that (7.14) is satisfied, we have, using (7.4), that
f (xk+1) ≤ f (xk) c1
1
αk
xk+1 xk2 ≤ f (xk) c1
1
αk
xk+1 xk2.
Hence, taking αk = 1/M for some M > 0 and all k, derive a convergence
bound similar to (7.6) for Algorithm 7.1.8
Nonsmooth Functions and Subgradients
Most of our discussion so far has focused on functions f: Rn → R that are
smooth, at least differentiable. But there are many interesting optimization
problems in data analysis that involve nonsmooth functions. When these
functions are convex, it is not difficult to generalize the concept of a gradient.
These generalizations, known as subgradients and subdifferentials, are the
subject of this chapter. We show in the next chapter and beyond how they can
be used to construct algorithms, related to those of earlier chapters, but with
their own convergence and complexity analysis.
We start with a few examples of interesting nonsmooth functions. In
Section 1.4, we introduced the “hinge loss” function, which appears often in
support vector machines and deep learning. This function h: R → R has the
form
h(t) = max(t,0).
It is obviously differentiable at every nonzero value of t, since h
(t) = 0 for
t < 0 and h
(t) = 1 for t > 0. As t moves through 0, the gradient switches
instantly from 0 to 1. We may be tempted to think of both these values as a
kind of derivative for h at t = 0, and we would be right! Both values are
“subgradients” of h. In fact, any value between 0 and 1 is also a subgradient.
The collection of all subgradients at t = 0 the closed interval [0,1] is the
“subdifferential” of h at t = 0.
A similar example is the absolute value function h(t) = |t| that has
derivative −1 for t < 0 and +1 for t > 0. At t = 0, the subdifferential of
h is the interval [−1,1], and as always, each point in the subdifferential is a
subgradient.
Consider next the multivariate function f (x) = max(aT
1 x + b1,aT
2 x + b2),
where a1 and a2 are (distinct) vectors in Rn and b1 and b2 are scalars. It is
easy to verify that f is convex and piecewise linear. In fact, there are just
1328 Nonsmooth Functions and Subgradients 133
two pieces: a region in which aT
1 x + b1 ≥ aT
2 x + b2 and another in which
aT
1 x + b1 ≤ aT
2 x + b2. These regions both include the hyperplane defined by
aT
1 x + b1 = aT
2 x + b2. In the interior of each region, the gradient ∇f (x) is
defined uniquely; we have
aT
1 x + b1 > aT
2 x + b2 ⇒ ∇f (x) = a1,
aT
1 x + b1 < aT
2 x + b2 ⇒ ∇f (x) = a2.
Along the hypeplane aT
1 x + b1 = aT
2 x + b2, and similarly to the hinge loss
function, the appropriate definition of subdifferential is the line joining a1 and
a2 in Rn space that is, {αa1 + (1 α)a2 | α ∈ [0,1]}.
Other nonsmooth functions include norms, which are always nondifferen￾tiable at 0 (see the Exercises). More exotically, the maximum eigenvalue of a
symmetric matrix is a convex, but not differentiable, function of its elements.
We can see this by considering the special case of diagonal 2 × 2 matrices

a11 0
0 a22
,
whose maximum eigenvalue is max(a11,a22), a nonsmooth (in fact, piecewise
linear) function of its entries.1
Besides being of interest in their own right as a way to formulate important
applications, nonsmooth convex functions play a major role in constrained
optimization, where they can be used both to derive optimality conditions and
to construct useful algorithms.
In Section 8.1, we define terms and discuss some key properties of
subgradients and subdifferentials. Subdifferentials are related to the directional
derivatives of a function, as we describe in Section 8.2. We give elements of
a calculus of subdifferentials in Section 8.3, stating in the process a useful
result known as Danskin’s theorem. We examine the indicator function of a
convex set in Section 8.4 and show that the subdifferential of this function is
identical to the normal cone to this set. This fact has several consequences for
the way we formulate optimization problems over convex sets. In Section 8.5,
we examine functions that are the sum of a smooth function and a convex
(possibly nonsmooth) function and investigate optimality conditions for such
functions which are common in data analysis. Finally, in Section 8.6, we
define the proximal operator and the Moreau envelope, concepts that are
important in defining and analyzing the fundamental algorithms discussed in
Chapter 9.
1 The maximum eigenvalue is a convex function because it can be defined by maxv:v2=1 vT Av,
which is a supremum over an infinite number of functions that are linear in the elements of A.134 8 Nonsmooth Functions and Subgradients
8.1 Subgradients and Subdifferentials
In this section, we allow the convex function f to be an extended real-valued
convex function, by which we mean that it is allowed to take infinite values at
some points. (In some later discussions, we will restrict f to have finite values
at all x.) We state some useful definitions.
• The effective domain of f , denoted by dom f , is defined to be the set of
points x ∈ Rn for which f (x) < ∞.
• The epigraph of f is the convex subset of Rn+1 defined by
epi f := {(x,t) ∈  × R: t ≥ f (x)}. (8.1)
• f is a proper convex function if f (x) < +∞ for some x ∈ Rn and
f (x) > ∞ for all x ∈ Rn. All convex functions of practical interest are
proper.
• f is a closed proper convex function if it is a proper convex function and
the set {x ∈ Rn : f (x) ≤ t
¯} is a closed set for all t
¯ ∈ R.
• f is lower semicontinuous at x if for all sequences {yk} such that yk → x
we have lim infk→∞ f (yk) ≥ f (x).
We define the subgradient and subdifferential as follows.
Definition 8.1 Given x ∈ dom f , we say that g ∈ Rn is a subgradient of f
at x if
f (z) ≥ f (x) + gT (z − x), for all z ∈ dom f .
The subdifferential of f at x, denoted by ∂f (x), is the set of all subgradients
of f at x.
It follows immediately from this definition that ∂f (x) is closed and convex,
for all x (see the Exercises). Note that if z is outside the effective domain of f ,
we have f (z) = ∞, and the inequality in Definition 8.1 is satisfied trivially.
Thus, there is no need to restrict z to dom f in the preceding definition, and
we can use instead the following requirement:
f (z) ≥ f (x) + gT (z x), for all z ∈ Rn. (8.2)
Definition 8.1 leads immediately to a characterization of the minimizer of a
convex function.
Theorem 8.2 (Optimality Conditions for Convex Function) The point x∗ is a
minimizer of the convex function f if and only if 0 ∈ ∂f (x∗).136 8 Nonsmooth Functions and Subgradients
Lemma 8.4 A subgradient g of f exists at x if x is in the interior of the
effective domain of f .
Proof The assumption implies that there is  > 0 such that all f (x + w) < ∞
for all w with w ≤ . Since (x,f (x)) is on the boundary of the convex
set epi f , the supporting hyperplane result, Theorem A.15, implies that there
exists a vector c ∈ Rn and a scalar β ∈ R – where at least one of c and β must
be nonzero – such that

c
β
T z
t
  x
f (x) ≤ 0, for all (z,t) ∈ epi f . (8.3)
We cannot have β > 0, since we are free to drive t to +∞, and (8.3) will fail
to hold for sufficiently large t. If β = 0, we must have that c  0. But then if
we set z = x + c/c in (8.3), we would have c2 ≤ 0, which does not
hold. Thus, we must have β < 0. By setting t = f (z) in (8.3), rearranging,
and dividing both sides by −β, we obtain
cT (z − x) ≤ −β(f (z) − f (x)) ⇒ f (z) ≥ f (x) + (−c/β)T (z − x),
which implies that −c/β is a subgradient of f at x. 
Lemma 8.4 shows that when x is in the interior of the effective domain, the
subdifferential ∂f (x) is nonempty. The same condition implies that ∂f (x) is
bounded and, in fact, compact, as we show next.
Lemma 8.5 If x is in the interior of the effective domain of f , the subdifferen￾tial ∂f (x) is compact.
Proof As in the proof of Lemma 8.4, there exists  > 0 such that f (x + w) <
∞ for all w ≤ . Suppose, for contradiction, that ∂f (x) is unbounded. Then
we can choose a sequence {gk} with gk ∈ ∂f (x) for all k = 1,2,... and
gk→∞. Since all normalized vectors gk/gk are in the unit ball, which is
compact, we can assume by taking a subsequence if necessary that gk/gk →
g¯ for some g¯ with ¯g = 1. Note that gT
k g/¯ gk → 1, from which it follows
that gT
k g¯ → ∞. From the definition of subgradient, we have
f (x + g)¯ ≥ f (x) + gT
k g, k ¯ = 1,2,...,
so by driving k → ∞, we deduce that f (x + g)¯ = ∞, yielding the
contradiction.
We have proved boundedness. Since, as we remarked earlier, ∂f (x) is
closed, compactness follows, completing the proof. 8.2 The Subdifferential and Directional Derivatives 137
If f is convex and differentiable at x, the subgradient coincides with the
gradient.
Theorem 8.6 If f is convex and differentiable at x, then ∂f (x) = {∇f (x)}.
Proof Differentiability of f implies that for all vectors d ∈ Rn with d = 1,
we have f (x + td) = f (x) + t∇f (x)T d + o(|t|) (see (2.6)). In particular,
f is finite at all points in the neighborhood of x, so x is in the interior of the
effective domain of f , so it follows from Lemma 8.4 that ∂f (x) is nonempty.
Let v be an arbitrary vector in ∂f (x). From Definition 8.1, we have for any
d  0 that
f (x + td) = f (x) + t∇f (x)T d + o(t)
≥ f (x) + tvT d ⇒ (∇f (x) v)T d ≥ o(t)/t,
and it follows by taking t ↓ 0 that (∇f (x) v)T d ≥ 0. By setting d =
v ∇f (x), we have d2 ≥ 0, which implies that d = 0, so that v = ∇f (x),
proving the result. 
A converse of this result is also true: If the subdifferential of a convex
function f at x contains a single subgradient, then f is differentiable with
gradient equal to this subgradient (see Rockafellar, 1970, theorem 25.1).
8.2 The Subdifferential and Directional Derivatives
We turn now to directional derivatives. Given a function f: Rn → R, the
directional derivative of f at x ∈ dom f in the direction v  0 is denoted
by f 
(x;v) and defined by
f 
(x;v) := lim
α↓0
f (x + αv) f (x)
α
. (8.4)
This definition holds for any function f , but our focus here is again on convex
functions.2 The definition suggests that a direction v for which f 
(x;v) < 0
is a descent direction for f and, thus, a useful direction when our goal is to
minimize f . In this context, note that directional derivatives in all directions
are nonnegative if and only if x∗ is a minimizer of f .
Theorem 8.7 Suppose that f is a convex function. Then, for some x∗ ∈ dom f ,
f 
(x∗;v) ≥ 0 for all v if and only if x∗ is a minimizer of f .
2 Note that the limit can be infinite when f is an extended-value convex function. For example,
the convex function f: R → R that has f (0) = 0 and f (t) = +∞ for t  0 has f 
(0,v) = +∞
for all v  0.138 8 Nonsmooth Functions and Subgradients
Proof If x∗ is a minimizer of f , then f (x∗ + αv) ≥ f (x∗) for all α > 0
and all v, so it follows directly from the definition (8.4) that f 
(x∗;v) ≥ 0.
Conversely, suppose that x∗ is not a minimizer of f . Then there exists some
z∗ ∈ dom f with f (z∗) < f (x∗), and for any α ∈ (0,1), we have
f (x∗ + α(z∗ x∗)) ≤ (1 α)f (x∗) + αf (z∗),
and so
f (x∗ + α(z∗ x∗)) f (x∗)
α
≤ f (z∗) f (x∗) < 0, for all α ∈ (0,1).
By taking limits as α ↓ 0 and using (8.4), we have f 
(x∗;z∗ x∗) ≤ f (z∗)
f (x∗) < 0, completing the proof. 
In the remainder of this section, we explore the relationship between
directional derivatives and subgradients, showing that knowledge of the subd￾ifferential makes it possible to compute descent directions for f .
For f convex, we have that the ratio in (8.4) is a nondecreasing function of
α; that is,
0 < α1 < α2 ⇒ f (x + α1v) f (x)
α1
≤ f (x + α2v) f (x)
α2
. (8.5)
(The proof is a consequence of the definition of convexity; see the Exercises.)
We can thus replace the definition (8.4) with
f 
(x;v) := inf
α>0
f (x + αv) f (x)
α
. (8.6)
It follows from these definitions that the directional derivative is additive; that
is, for two convex functions f1 and f2, we have
(f1 + f2)

(x;v) = f 
1(x;v) + f 
2(x;v). (8.7)
Moreover, it is homogeneous with respect to the direction; that is,
f 
(x;λv) = λf 
(x;v), for all λ ≥ 0. (8.8)
(We leave proofs of these results as Exercises.) Moreover, f 
(x;v), regarded
as a function of v, for fixed x, is a convex function. We see this from the
following elementary argument: Given v1 and v2 and γ ∈ (0,1), consider
f 
(x;γv1 + (1 − γ)v2), for which we have8.2 The Subdifferential and Directional Derivatives 139
f 
(x;γv1 + (1 γ)v2)
= lim
α↓0
f (x + αγv1 + α(1 γ)v2) f (x)
α
= lim
α↓0
f (γ(x + αv1) + (1 γ)(x + αv2)) γf (x) (1 γ)f (x)
α
≤ lim
α↓0
γ(f (x + αv1) − f (x)) + (1 − γ)(f (x + αv2) − f (x))
α
= γ lim
α↓0
f (x + αv1) f (x)
α
+ (1 γ)lim
α↓0
f (x + αv2) f (x)
α
= γf 
(x;v1) + (1 γ)f 
(x;v2).
It follows from the definition (8.4) and Taylor’s theorem (specifically, (2.3))
that if f is differentiable at x, we have, for any v ∈ Rn, that
f 
(x;v) = lim
α↓0
f (x + αv) − f (x)
α
= lim
α↓0
∇f (x + γαv)T (αv)
α
for some γ ∈ (0,1)
= ∇f (x)T v.
Thus, in particular, we have
f 
(x;v) = f 
(x; v) when f is differentiable at x. (8.9)
This equality is not true for nonsmooth functions at points of nondifferentiabil
ity. For example, the hinge loss function h(t) = max(t,0) has h
(0;1) = 1 but
h
(0; 1) = 0. Similarly, the absolute value funtion h(t) = |t| has h(0;1) = 1
and h
(0, 1) = 1. We give a generalization of (8.9) in Corollary 8.9.
It follows from the second definition (8.6) that for convex f , the directional
derivative f 
(x;v) has a property reminiscent of the subgradient (compare with
Definition 8.1):
f (x + αv) ≥ f (x) + αf 
(x;v), for all α ≥ 0. (8.10)
Related to this observation, we can prove the following result.
Theorem 8.8 Suppose that x is in the interior of the effective domain of the
convex function f . Then, for any v ∈ Rn, we have that
f 
(x;v) = sup
g∈∂f (x)
gT v. (8.11)140 8 Nonsmooth Functions and Subgradients
Proof From (8.6), we have for any g ∈ ∂f (x) that
f 
(x;v) = inf
α>0
f (x + αv) f (x)
α
≥ inf
α>0
αgT v
α = gT v,
so that
f 
(x;v) ≥ gT v for all g ∈ ∂f (x). (8.12)
Because ∂f (x) is closed, we obtain equality in (8.11) if we can find gˆ ∈
∂f (x) such that f 
(x;v) = ˆgT v. For this, we use the convexity of f 
(x;y) with
respect to its second argument y for all y ∈ Rn (proved earlier). By Lemma 8.4,
there exists a subgradient of f 
(x;·) at v; let us call it gˆ. By the definition of
subgradient, together with (8.8), we have, for all λ ≥ 0 and all y, that
λf 
(x;y) = f 
(x;λy) ≥ f 
(x;v) + ˆgT (λy − v). (8.13)
Letting λ ↑ ∞, we have that gˆT y ≤ f 
(x;y). By the definition (8.6), we have
f (x + y) −f (x)≥ inf
α>0
f (x + αy) − f (x)
α =f 
(x;y)≥ ˆgT y, for all y ∈ Rn,
which implies that gˆ ∈ ∂f (x), so by (8.12), we have that f 
(x;v) ≥ ˆgT v. On
the other hand, by taking λ = 0 in (8.13), we have that f 
(x;v) ≤ ˆgT v. There
fore, f 
(x;v) = ˆgT v for this particular gˆ ∈ ∂f (x), completing the proof. 
An immediate corollary of this result leads to a generalization of (8.9).
Corollary 8.9 Suppose that x is in the interior of the effective domain of the
convex function f . Then, for any v ∈ Rn, we have
f 
(x;v) ≥ f 
(x; v).
Proof From Theorem 8.8, we have
f 
(x; v) = sup
g∈∂f (x)
gT ( v) = inf
g∈∂f (x) gT v ≥ sup
g∈∂f (x)
gT v = f 
(x;v).

We conclude with another result that relates subgradients to directional
derivatives.
Theorem 8.10 Suppose that x ∈ dom f for the convex function f , and that
there is some vector g ∈ Rn such that for all v ∈ Rn, we have
f 
(x;v) ≥ gT v.
Then g ∈ ∂f (x).8.3 Calculus of Subdifferentials 141
Proof By setting α2 = 1 and α1 ↓ 0 in (8.5) and using the definition (8.4), we
have
gT v ≤ f 
(x;v) ≤ f (x + v) − f (x), for all v ∈ Rn.
Thus, g satisfies the definition (8.2), so that g ∈ ∂f (x). 
8.3 Calculus of Subdifferentials
In this section, we describe the properties of subdifferentials that are the key to
calculating subgradients. Unlike for differentiable functions, there are only a
few rules that are commonly used in practice, involving positive combinations,
combinations with linear mapping, and partial maximization. We collect these
rules here in Theorems 8.11, 8.12, and 8.13. (Proofs of Theorems 8.11 and 8.12
appear at the end of this section.)
We start with some elementary rules of subdifferential calculus.
Theorem 8.11 Supposing that f , f1, and f2 are convex functions and α is a
positive scalar, the following is true.
∂(f1 + f2)(x) ⊃ ∂f1(x) + ∂f2(x), (8.14)
∂(αf )(x) = α∂f (x). (8.15)
If, in addition, x is in the interior of the effective domain for both f1 and f2,
then equality holds in (8.14); that is, ∂(f1 + f2)(x) = ∂f1(x) + ∂f2(x). In
particular, if f1 and f2 are finite-valued convex functions, then ∂(f1+f2)(x) =
∂f1(x) + ∂f2(x) for all x.
We emphasize that the relationship in (8.14) is not an equality in general.
We will see an example of strict inclusion in the next section. However,
equality holds in some interesting special cases (see, for example, Burachik
and Jeyakumar, 2005).
The next result allows us to compute the subdifferential under affine
transformations.
Theorem 8.12 (Bertsekas et al., 2003, Theorem 4.2.5(a)) Suppose that
f: Rm → R is a convex function and defines h(x) := f (Ax + b) for some
matrix A ∈ Rm×n and vector b ∈ Rm. Suppose that Ax + b is in the interior
of dom f . Then ∂h(x) = AT ∂f (Ax + b).
The third result is Theorem 8.13, known as Danskin’s theorem, which
shows us how to compute the subdifferential of a function that is defined as the142 8 Nonsmooth Functions and Subgradients
pointwise maximum of a possibly infinite set of functions. Such functions are
ubiquitous objects in optimization, particularly in data analysis applications.
The setup is as follows. Let I ⊂ Rn be a compact set. (Sets I with finite
cardinality are a useful special case.) Let ϕ : Rd × I → R be a family of
functions, continuous in (x,i), and assume that each ϕ(·,i), i ∈ I , is convex.
We define
f (x) := max
i∈I ϕ(x,i). (8.16)
Note that f is convex, because it is the pointwise maximum of convex
functions (see the Exercises). Following Section 8.2, we denote the directional
derivative of ϕ( ,i) at x in direction y by ϕ
(x,i;y). For each x, we define
Imax(x) to be the subset of I for which the maximum is achieved in (8.16);
that is,
Imax(x) := arg max j∈I ϕ(x,j) = {j : f (x) = ϕ(x,j)}. (8.17)
Note that Imax(x) is nonempty (by compactness of I ) and compact for all x,
by continuity of ϕ(x, ) with respect to its second argument. Danskin’s theorem
describes the directional derivatives and subdifferentials of f . (Interestingly,
this theorem first arose out of cold war research by Danskin and appeared
in a 1967 monograph called The Theory of Max-Min and Its Applications to
Weapons Allocation Problems (Danskin, 1967).)
Theorem 8.13 (Danskin’s Theorem) (a) The directional derivative of f
defined by (8.16) at x in direction y is given by
f 
(x,y) = max
i∈Imax(x) ϕ
(x,i;y).
(b) If, in addition to the conditions on the function family ϕ stated earlier, we
have that ϕ(·,i) is a differentiable function of x for all i ∈ I , with
∇xϕ(x, ) continuous on I for all x, then
∂f (x) = conv{∇xϕ(x,i): i ∈ Imax(x)}.
We refer to Bertsekas (1999, section B.5) and Bertsekas et al. (2003,
proposition 4.5.1) for proofs of Theorem 8.13, which are quite technical.
We now provide proofs of Theorems 8.11 and 8.12. Generally speaking,
one direction of the inclusion is quite straightforward, while the other direction
requires a separating hyperplane argument. Practitioners can skip these proofs,
but we note that the arguments are of interest in that they highlight some
important structural aspects of convex optimization.8.3 Calculus of Subdifferentials 143
Proof of Theorem 8.11 The proofs of (8.14) and (8.15) are immediate conse￾quences of the definitions of subgradients. For the case in which x is in the
interior of both dom f1 and dom f2, Lemmas 8.4 and 8.5 show that ∂f1(x),
∂f2(x), and ∂(f1+f2)(x) are all nonempty, convex, and compact sets. Suppose
for contradiction that the inclusion (8.14) is strict in this case; that is, there
exists g ∈ ∂(f1+f2)(x) such that g  ∂f1(x)+∂f2(x). By the strict separation
result Lemma A.12, setting X = (∂f1(x) + ∂f2(x)) { g}, there is a vector
t ∈ Rn and a scalar α > 0 such that
t
¯
T (g1 + g2) ≤ t
¯
T g − α, for all g1 ∈ ∂f1(x) and all g2 ∈ ∂f2(x).
From results about the relationship between subdifferentials and directional
derivatives (8.7) and Theorem 8.8 we have
(f1 + f2)

(x;t)¯ = f 
1(x;t)¯ + f 
2(x;t)¯
= sup
g1∈∂f1(x)
gT
1 t
¯ + sup
g2∈∂f2(x)
gT
2 t
¯ ≤ t
¯
T g α<gT t
¯.
Thus, by Theorem 8.8, we have g  ∂(f1 + f2)(x), a contradiction.
When f1 and f2 are finite valued, we have dom f1 = dom f2 = Rn, so that
the effective domain condition holds for all x ∈ Rn, and the result follows. 
Proof of Theorem 8.12 Since Ax + b is in the interior of dom f , x is in the
interior of dom h. Thus, by Lemmas 8.4 and 8.5, the subdifferentials ∂h(x) and
∂f (Ax+b) are nonempty and compact. From the definition (8.4) of directional
derivatives, it follows that
h
(x;y) = f 
(Ax + b;Ay), for any y ∈ Rn.
From Theorem 8.8, we have, for any z ∈ Rm, that
gT z ≤ f 
(Ax + b;z) for all g ∈ ∂f (Ax + b).
By setting z = Ay, we have
(AT g)T y = gT (Ay) ≤ f 
(Ax + b;Ay) = h
(x;y), for any y ∈ Rn.
It follows from Theorem 8.10 that AT g ∈ ∂h(x), and since this result holds
for all g ∈ ∂f (Ax + b), we have that AT ∂f (Ax + b) ⊂ ∂h(x).
To prove equality, suppose for contradiction that there is a vector v ∈ ∂h(x)
such that v  AT ∂f (Ax + b). Since the set  := AT ∂f (Ax + b) is compact,
we invoke the strict separation result Theorem A.14 to deduce the existence of
a vector y and a scalar β such that
yT (AT g) < β < yT v, for all g ∈ ∂f (Ax + b).144 8 Nonsmooth Functions and Subgradients
It follows by compactness that
sup
g∈∂f (Ax+b)
(Ay)T g<yT v.
Theorem 8.8 then implies that
h
(x,y) = f 
(Ax + b;Ay) < yT v,
contradicting the assumption that v ∈ ∂h(x) and completing the proof. 
8.4 Convex Sets and Convex Constrained Optimization
In this section, we study the connections between closed convex sets and the
indicator functions for those sets (which are extended-value convex functions
that is, they may take infinite values at some points).
Let  ⊂ Rn be a convex set (see (2.14) for the definition of convexity). The
indicator function I for a convex set  is defined by
I(x) =

0 if x ∈ ,
∞ if x  .
This function is convex, extended valued (except for the trivial case  =
Rn), and has dom I = . When  is a closed set, I(x) is also lower
semicontinuous. We have the following result.
Theorem 8.14 For a closed convex set  ⊂ Rn, we have that N(x) = ∂I(x)
for all x ∈ .
Proof Given v ∈ N(x), we have
I(y) I(x) = 0 0 = 0 ≥ vT (y x), for all y ∈  = dom I,
which implies that v ∈ ∂I(x), by Definition 8.1. For the converse, suppose
that v ∈ ∂I(x), so we have
0 = I(y) ≥ I(x) + vT (y x) = vT (y x), for all y ∈ ,
which implies that v ∈ N(x), completing the proof. 
In optimization, we often deal with sets that are intersections of closed con￾vex sets. We have the following result for normal cones of such intersections.146 8 Nonsmooth Functions and Subgradients
point in question needs to capture the essential geometry of the set itself in a
neighborhood of the point. This is not true of the preceding example, where the
tangents (linear approximations) to both 1 and 2 at x = 0 are half-planes
bounded by the vertical axis, so the intersection of their linear approximations
is also the vertical axis. On the other hand, the actual intersection of the two
sets is the single point {0}, which is a set with entirely different geometry.
Recall from Chapter 7 the problem of minimizing a smooth convex function
f over a closed convex set  (7.1). We showed in Theorem 7.2 that first
order necessary condition for optimality of x∗ is ∇f (x∗) ∈ N(x∗). Because
of the identity in Theorem 8.14, we can write this condition alternatively as
follows:
0 ∈ ∇f (x∗) + ∂I(x∗). (8.20)
Moreover, by (8.14) and Theorem 8.6, it is a consequence of (8.20) that
0 ∈ ∂

f (x∗) + I(x∗)

. (8.21)
From Theorem 8.2, this condition in turn is true if and only if x∗ is a minimizer
of the “unconstrained” problem
min
x f (x) + I(x),
and we can see easily that this problem is equivalent to (7.1).
8.5 Optimality Conditions for Composite
Nonsmooth Functions
We now consider first-order optimality conditions for functions of the form
φ(x) := f (x) + ψ(x), (8.22)
where f is a smooth function and ψ is (possibly) nonsmooth, convex, and
finite valued. (Because of the latter property, the effective domain of ψ is the
entire space Rn, so we can apply such results as Theorem 8.11 for all x.) We
encounter this type of objective often in machine learning applications; see
Chapter 1.
We deal first with the case in which f is convex.
Theorem 8.16 When f is convex and differentiable and ψ is convex and finite
valued, the point x∗ is a minimizer of φ defined in (8.22) if and only if 0 ∈
∇f (x∗) + ∂ψ(x∗).8.5 Optimality Conditions for Composite Nonsmooth Functions 147
Proof By Theorem 8.6, we have that ∂f (x) = {∇f (x)}, so by using the fact
that ψ(x) has effective domain Rn and applying Theorem 8.11, we have
∂φ(x) = ∇f (x) + ∂ψ(x).
The result follows immediately from Theorem 8.2. 
When f is strongly convex, the problem (8.22) has a minimizer and it is
unique.
Theorem 8.17 Suppose that the conditions of Theorem 8.16 hold and, in
addition, that f is strongly convex. Then the function (8.22) has a unique
minimizer.
Proof We show first that for any point x0 in the domain of φ, the level set
{x | φ(x) ≤ φ(x0)} is closed and bounded, and hence compact. Suppose for
contradiction that there is a sequence {x} such that x→∞ and
f (x
) + ψ(x
) ≤ f (x0) + ψ(x0). (8.23)
By convexity of ψ, we have that ψ(x) ≥ ψ(x0) + gT (x x0) for any
g ∈ ∂ψ(x0). By strong convexity of f , we have for some m > 0 that
f (x
) ≥ f (x0) + ∇f (x0)
T (x x0) + m
2 x x02.
By substituting these relationships in (8.23), and rearranging slightly, we
obtain
m
2 x x02 ≤ (∇f (x0) + g)T (x x0) ≤ ∇f (x0) + gx x0.
By dividing both sides by (m/2)x x0, we obtain x x0 ≤
(2/m)∇f (x0) + g for all , which contradicts unboundedness of {x}.
Thus, the level set is bounded.
Since φ is continuous, it attains its minimum on the level set, which is also
the solution of minx φ(x), and we denote it by x∗. By Theorem 8.16, we have
that there is g ∈ ∂φ(x∗) such that 0 = ∇f (x∗) + g = 0. By strong convexity
of f , we have for any x  x∗ that
f (x) + ψ(x) ≥ f (x∗) + ψ(x∗) + (∇f (x∗) + g)T (x x∗)
+ m
2 x − x∗2 > f (x∗) + ψ(x∗),
proving that x∗ is the unique minimizer. 
For the more general case in which f is possibly nonconvex, we have a
first-order necessary condition.148 8 Nonsmooth Functions and Subgradients
Theorem 8.18 Suppose that f is continuously differentiable and ψ is convex
and finite valued, and let φ be defined by (8.22). Then if x∗ is a local minimizer
of φ, we have that 0 ∈ ∇f (x∗) + ∂ψ(x∗).
Proof Supposing that 0  ∇f (x∗) + ∂ψ(x∗), we show that x∗ cannot be a
local minimizer. We define the following convex approximation to φ(x + d):
φ(d) := f (x∗) + ∇f (x∗)
T d + ψ(x∗ + d),
By continuous differentiability of f , we have that, for all α ∈ [0,1] and for
any d, φ(αd) ¯ = φ(x + αd) + o(αd). Since, by assumption, 0  ∂φ(¯ 0) =
∇f (x∗)+∂ψ(x∗), we have from Theorem 8.2 that 0 is not a minimizer of φ(d) ¯ .
Hence, there exists d¯ with φ(¯ d) < ¯ φ(¯ 0), so that the quantity c := φ(¯ 0) φ(¯ d)¯
is strictly positive. By convexity of φ¯, we have for all α ∈ [0,1] that
φ(αd)¯ ≤ φ(0) α(φ(0) φ(d)) ¯ = φ(x∗) αc,
and, therefore,
φ(x∗ + αd)¯ ≤ φ(x∗) − αc + o(αd).
Therefore, φ(x∗ + αd) < φ(x∗) for all α > 0 sufficiently small, so x∗ is not a
local minimizer of φ. 
8.6 Proximal Operators and the Moreau Envelope
We define here the proximal operator that is a key component of algorithms for
regularized optimization, and we analyze some of its properties in preparation
for convergence analysis of proximal-gradient algorithms in Section 9.3. The
proximal operator is a powerful generalization of Euclidean projection and it
enhances our nonsmooth optimization toolbox considerably.
For a closed proper convex function h, we define the proximal operator, or
prox operator, of the function h as
prox h(x) := arg minu
7
h(u) +
1
2
u x2
8
. (8.24)
Note that this is a well-defined function because of the strong convexity of the
Euclidean norm.
When h(x) = I(x), the indicator function for a closed convex set ,
prox I (x) is simply the Euclidean projection of x onto the set , as we see
from the following argument:8.6 Proximal Operators and the Moreau Envelope 149
prox I (x) = arg minu
7
I(u) +
1
2
u x2
8
= arg min u∈
1
2
u x2.
Proximal operators are more general than Euclidean projections, but they
satisfy a similar nonexpansiveness property.
Proposition 8.19 Suppose h is a convex function. Then
prox h(x) prox h(y)≤x y.
Proof From optimality properties, we have from (8.24) that
0 ∈ ∂h(prox h(x)) + (prox h(x) x). (8.25)
Rearranging these expressions, at two points x and y, we have
x − prox h(x) ∈ ∂(prox h(x)), y − prox h(y) ∈ ∂(prox h(y)).
Now, for a convex function f , it follows from the definition of subgradients
that if a ∈ ∂f (x) and b ∈ ∂f (y), we have (a b)T (x y) ≥ 0. By applying
this inequality, we have

(x prox h(x)) (y prox h(y))T (prox h(x) prox h(y)) ≥ 0,
which, by rearrangement and application of the Cauchy–Schwartz inequality,
yields
prox h(x) − prox h(y)2 ≤ (x − y)T (prox h(x) − prox h(y))
≤ x − yprox h(x) − prox h(y),
from which we obtain the proposition. 
We note several special cases of the prox-operator that are useful in later
chapters.
• h(x) = 0 for all x, for which we have prox h(x) = x. Though trivial, this
observation is useful in proving that the proximal-gradient method of
Chapter 9 reduces to the familiar steepest-descent method when the
objective contains no regularization term.
• h(x) = λx1. By substituting it into definition (8.24), we see that the
minimization separates into its n separate components and that the ith
component of prox λ·1 is
[prox λ·1 ]i = arg min ui
7
λ|ui| +
1
2
(ui xi)
2
8
.150 8 Nonsmooth Functions and Subgradients
It is straightforward to verify that
[prox λ·1 (x)]i =
⎧
⎪⎪⎨
⎪⎪⎩
xi − λ if xi > λ
0 if xi ∈ [ λ,λ]
xi + λ if xi < −λ,
(8.26)
an operator that is known as soft thresholding.
• h(x) = λx0, where x0 denotes the cardinality of the vector x, its
number of nonzero components. Although this h is not a convex function
(as we can see by considering convex combinations of the vectors (0,1)T
and (1,0)T in R2), its prox-operator is well defined to be the hard
thresholding operation:
[prox λ·0 (x)]i =

xi if |xi| ≥ √2λ;
0 if |xi| < √2λ.
For the cardinality function, the definition (8.24) separates into n individual
components, and the fixed price of λ for allowing ui to be nonzero is not
worth paying unless |xi| ≥ √2λ.
The proximity operator is closely related to smooth approximations of
convex functions. For a closed proper convex function h and a positive scalar
λ, we define the Moreau envelope as
Mλ,h(x) := inf
u
7
h(u) +
1
2λ
u − x2
8
= 1
λ
inf
u
7
λh(u) +
1
2
u − x2
8
.
(8.27)
The Moreau envelope can be seen as a smoothing or regularization of the
function h. It has a finite value for all x, even when h takes on infinite values
for some x ∈ Rn. In fact, it is differentiable everywhere: Its gradient is
∇Mλ,h(x) = 1
λ
(x prox λh(x)).
Moreover, x∗ is a minimizer of h if and only if it is a minimizer of Mλ,h, for
any λ > 0.8.6 Proximal Operators and the Moreau Envelope 151
Notes and References
Some material of this chapter is from the slides of Vandenberghe (2016) on
“Subgradients.”
Further background on Moreau envelopes and the proximal mapping is
given by Parikh and Boyd (2013).
The classical reference on convex analysis is the book of Rockafellar
(1970), which contains much of the fundamental material on subdifferentials
and their calculus (along with a great deal else). A more recent treatment with
an emphasis on optimization is Bertsekas et al. (2003); we make use of two
results from this text in Section 8.3. A proof of Danskin’s theorem can also be
found in Bertsekas et al. (2003, proposition 4.5.1).
Exercises
1. Prove that if f is convex and x ∈ dom f , the subdifferential ∂f (x) is
closed and convex.
2. Prove (8.5) by applying the definition (2.15) of a convex function.
3. Show that any norm f (x) := x has 0 as a subgradient at x = 0; that is,
0 ∈ ∂f (0). Show that f (x) is not differentiable at x = 0. (Reminder: A
norm · has the properties that (a) x = 0 if and only if x = 0; (b)
αx=|α|x for all scalars α and vectors x; (c) x + y≤x+y
for all x,y.)
4. Prove the additivity property (8.7) and the homogeneity property (8.8) of
directional derivatives.
5. For the following norm functions f over the vector space Rn, find ∂f (x)
and f 
(x;v) for all x and v:
(a) The 1 norm: f (x) = x1
(b) The ∞ norm: f (x) = x∞
(c) The 2 (Euclidean) norm: f (x) = x2
6. Show that the pointwise maximum function f defined by (8.16) is
convex, under the stated conditions on ϕ(x,i) for x ∈ Rd and i ∈ I ,
where I is a compact set.
7. Find the subdifferential of the piecewise linear convex function
f: Rn → R defined by
f (x) = max
i=1,2,...,m aT
i x + bi,
where ai ∈ Rn and bi ∈ R, i = 1,2,...,n.152 8 Nonsmooth Functions and Subgradients
8. Suppose that f is defined as a maximum of m convex functions; that is,
f (x) := maxi=1,2, ,m fi(x), where each fi is convex. Show that, for any
x in the interior of the effective domain of f , we have
∂f (x) =
⎧
⎨
⎩

i : fi(x)=f (x)
λivi : vi ∈ ∂fi(x), λi ≥ 0, 
i : fi(x)=f (x)
λi = 1
⎫
⎬
⎭
.
(Hint: The technique in the proof of Theorem 8.11 may be useful.)
9. (a) Show that I is a convex function if and only if  is a convex set.
(b) Show that  is a nonempty closed convex set if and only if I(x) is a
closed proper convex function.
10. Show that a closed proper convex function h and its Moreau envelope
Mλ,h have identical minimizers.
11. Calculate prox λh(x) and Mλ,h(x) for h(x) = 1
2 x2
2.9
Nonsmooth Optimization Methods
The steepest-descent method for smooth functions f , described in Chapter 3,
is intuitive in that it follows the negative gradient direction at each iteration,
which is a guaranteed direction of descent for f . Generalizing this method to
nonsmooth functions f is not straighforward, as the “gradient” is not unique
in general, even for convex f , as we saw in Chapter 8. A natural idea would
be to choose the search direction to be the negative of a vector from the
subdifferential ∂f , but such a direction may not give descent in f .
Consider the absolute value function f (x) = |x|, where x ∈ R. At the
minimizing value x = 0, the subdifferential is ∂|0| = [ 1,1], and any vector
drawn from this interval (except for the very special choice g = 0) will step
away from 0 and thus increase the function value. The situation is similar
in higher dimensions. Consider the two-dimensional function f: R2 → R
defined by
f (x1,x2) = |x1| + 2|x2|,
whose optimum is (0,0). At the point (1,0), the subdifferential is the
compact set
∂f (1,0) = {(1,z)| |z| ≤ 2}.
For the particular subgradient g = (1,2), the directional derivative in the
negative of this direction is
f 
((1,0);(−1, − 2)) = sup
g∈∂f (1,0)
−g1 − 2g2 = −1 + 4 = 3,
showing that the function increases along this direction. These trivial exam￾ples, and the example f (x) = max(aT
1 x + b1,aT
2 x + b2) for x ∈ R2 illustrated
in Figure 9.1, show that it is not obvious how to design a method that follows
subgradients.
1539.1 Subgradient Descent 155
Throughout this chapter, we focus on convex objectives, although some of
the techniques can also be applied in nonconvex settings.
9.1 Subgradient Descent
When x is not a minimizer of f , the subdifferential ∂f (x) always contains
a vector g such that g is a descent direction for f . The vector gmin with
minimum norm in ∂f (x) has this property, and, in fact, gmin is the direction
of steepest descent. We define
gmin := arg min z∈∂f (x)
z2. (9.1)
Note that gmin exists and is uniquely defined when ∂f (x) is nonempty, since
∂f (x) is always closed and convex.
Proposition 9.1 For a convex function f , and x ∈ dom f that is not a
minimizer of f , the vector −gmin defined from (9.1) is the direction of steepest
descent for f at x.
Proof Note that for all gˆ ∈ ∂f (x) and all t ∈ [0,1], we have
gmin + t(gˆ − gmin)2 ≥ gmin2.
We have by expanding the left hand side of this expression that
gmin,gˆ gmin ≥ 0, for all gˆ ∈ ∂f (x).
It follows that ˆg,gmin≥gmin2
2 for all gˆ ∈ ∂f (x), so that
f 
(x; gmin) = sup
g∈∂f (x)
 gmin,g = inf
g∈∂f (x)
gmin,g=  gmin2
2,
proving that gmin is a descent direction whenever it is nonzero. To see that
gmin is the steepest descent direction, we use a min-max argument. Note that
inf
v≤1 f 
(x;v) = inf
v≤1 sup
g∈∂f (x)
v,g
≥ sup
g∈∂f (x)
inf
v≤1
v,g = sup
g∈∂f (x)
−g = −gmin. (9.2)
The inequality in this expression follows from weak duality, which says that
for any function ϕ(x,z), we have
inf
x sup
z
ϕ(x,z) ≥ sup
z
inf
x ϕ(x,z).
(See Proposition 10.1.) In fact, we attain equality in (9.2) by setting v =
gmin/gmin. 156 9 Nonsmooth Optimization Methods
Example 9.2 Consider the function f (x) = x1, whose minimizer is x = 0.
At any nonzero x, the subdifferential ∂x1 consists of vectors g such that
gi ∈
⎧
⎪⎪⎨
⎪⎪⎩
{+1} if xi > 0
{ 1} if xi < 0
[−1,1] if xi = 0.
The minimum norm subgradient is thus gmin, where
(gmin)i =
⎧
⎪⎪⎨
⎪⎪⎩
+1 if xi > 0
1 if xi < 0
0 if xi = 0.
Proposition 9.1 suggests a natural algorithm for minimizing convex, nons
mooth functions: Compute the minimum norm element of the subdifferential
and search along the negative of this direction. The problem with this approach
is that the process of finding the full subdifferential and computing its
minimum-norm element might be prohibitively expensive. Bundle methods are
algorithms that are inspired by this approach. Typically, these methods assume
that a single subgradient is obtained at each iteration, and they approximate
the subdifferential by the convex hull of subgradients gathered at recent iter￾ations. This “bundle” of subgradients needs to be curated carefully, removing
elements when they appear to be too far from the current subdifferential. (We
give some references for these methods at the end of the chapter.)
In the next section, we show that a naive algorithm that simply follows
arbitrary subgradients at each iteration can converge, under appropriate choices
of steplengths.
9.2 The Subgradient Method
At each step k of the subgradient method, we simply choose any element of
the subdifferential gk ∈ ∂f (xk) and set
xk+1 = xk αkgk
.
Though we have already pointed out that this method may take steps that
increase f , the weighted average of all iterates encountered so far, defined by
xT = λ 1
T

T
k=1
αkxk
, where λT := 
T
j=1
αj , (9.3)
is well behaved and may even converge to a minimizer of f .9.2 The Subgradient Method 157
The analysis of this method is nearly identical to the proof of convergence
of the stochastic gradient method for convex functions with bounded stochastic
gradients. We assume that
g2 ≤ G, for all g ∈ ∂f (x) and all x.
Note that this assumption implies that f must be Lipschitz with constant G
(why?). We also denote by x∗ a minimizer of f and define
D0 := x1 − x∗, (9.4)
which is the distance of the initial point x1 to a minimizer of f .
To proceed with our analysis of the behavior of the weighted-average iterate
x¯T , we expand the distance to an optimal solution of iterate xk+1:
xk+1 − x∗2 = xk − αkgk − x∗2
= xk − x∗2 − 2αk(gk)
T (xk − x∗) + α2
kgk2
≤ xk x∗2 2αk(gk)
T (xk x∗) + α2
kG2
. (9.5)
This expression looks the same as the basic inequality for the subgradient
method (5.26), except there are no expected values here. We can rearrange
(9.5) to obtain
αk(gk)
T (xk x∗) ≤
1
2
xk x∗2 1
2
xk+1 x∗2 +
1
2
G2α2
k . (9.6)
Since gk ∈ ∂f (xk), we have, by the definition of subgradient, that
f (xk) f (x∗) ≤ (gk)
T (xk x∗). (9.7)
By multiplying both sides of (9.7) by αk > 0, combining with (9.6), summing
both sides from k = 1 to k = T , and using convexity of f , we obtain
f (x¯T ) − f (x∗) ≤ λ−1
T

T
k=1
αk(f (xk) − f (x∗))
≤ λ−1
T
1
2

T
k=1

xk x∗2  xk+1 x∗2

+
1
2
λ−1
T G2
T
k=1
α2
k
≤ λ 1
T
1
2

x1 x∗2  xT +1 x∗2)

+
1
2
λ 1
T G2
T
k=1
α2
k
≤
D2
0 + G2 T
k=1 α2
k
2
T
k=1 αk
. (9.8)158 9 Nonsmooth Optimization Methods
We also immediately have the bound
min
t≤T f (xt
) f (x∗) ≤ λ−1
T

T
k=1
αk(f (xk) f (x∗)),
so our analysis works for both the weighted average of the first T iterates and
the best of these iterates.
9.2.1 Steplengths
Let us look at different possibilities for the steplengths αk, k = 1,2,....
Fixed Steplength. First, we can just pick αk = α for all k. In this case, we
know from (9.8) that
f (xT ) f (x∗) ≤
D2
0 + T G2α2
2T α .
The choice α = θD0
G √T for some parameter θ > 0 yields
f (x¯T ) f (x∗) ≤ 1
2

θ + θ−1
 D0G
√T , (9.9)
and the bound is minimized when we set θ = 1.
Constant Step Norm. An alternative is to choose αk = α
gk , so that the norm
of each step αkgk is constant. A slight modification of the previous analysis
yields the bound
f

x¯T

− f (x∗) ≤
D2
0 + T α2
2T α/G .
Setting α = θD
√ 0
T , we obtain (9.9) again, matching the bound for fixed
steplength. Note that this choice of step depends only D0 (distance of x1 to
optimality) and not the maximal subgradient norm G.
An interesting feature of both choices discussed so far is that the conver￾gence rate bound is not very sensitive to errors in the estimates of D0 and G.
Such errors can be captured in the parameter θ, and we see that the bound
increases by only the modest factor 1
2 (θ + θ−1) when θ moves away from its
optimal value of 1.
Decreasing Steplength. The preceding fixed steplengths required us to make
a prior choice of T , the number of iterates to be taken. We now consider making
choices of αk that depend on k and that decrease as k increases. Such choices9.2 The Subgradient Method 159
do not require us to choose T in advance, and they guarantee convergence to
the optimal value of f as the number of iterates goes to ∞.
From (9.8), we see that for any sequence αk > 0 such that αk → 0, but
T
k=1 αk ↑ ∞ as T → ∞, then
lim
T →∞ f (xT ) = f (x∗).
This is particularly easy to see if 
k α2
k = M < ∞, because we have, from
(9.8), that
f (xT ) f ∗ ≤
D2
0 + G2 T
j=1 α2
j
2
T
t=1 αt
≤
D2
0 + G2M
2
T
j=1 αj
,
and the left-hand side clearly tends to zero as T → ∞. To see that this
approach works for general decreasing steplengths, we need to prove that
T
j=1 α2
j
T
j=1 αj
→ 0, as T → ∞,
whenever αk tends to zero but T
k=1 αk diverges. We leave the proof of this
limit as an Exercise.
We close this section by deriving more quantitative bounds for an explicit
choice of steplength. Setting αk = √
θ
k
, we have
f (x¯T ) − f ∗ ≤
D2
0 + G2θ 2 T
j=1 j−1
2θ
T
j=1 j−1/2 ≤
D2
0 + G2θ 2(log T + 1)
2θ
√T
. (9.10)
The upper bound in the numerator comes from the Riemann sum bound

T
j=1
j−1 ≤ 1 +
 T
t=1
1
t
dt ≤ log T + 1,
while the lower bound in the denominator comes from

T
j=1
j−1/2 ≥ 
T
j=1
T −1/2 = T 1/2.
Note that this bound tends to zero at a rate of log(T )/√T . This is slightly
slower than the 1/
√T rate of a constant steplength, but we are guaranteed
asymptotic convergence to zero and can continue to iterate well beyond a fixed
number of iterations.
The alternative decreasing steplength choice αk ∝ k−p for p ∈ (0,1) yields
a worse convergence bound than for p = 1/2 (see the Exercises).160 9 Nonsmooth Optimization Methods
More sophisticated schemes for choosing steplengths involve a combination
of fixed and decreasing lengths. The steplength is fixed for a number of
consecutive iterations (sometimes called an epoch) and then decreased to a
smaller value, which again is fixed for a number of consecutive iterations.
9.3 Proximal-Gradient Algorithms for
Regularized Optimization
While provably correct, the 1/
√T rate of the subgradient method is con￾siderably slower than the rates achievable for smooth functions. In this
section, we explore how to exploit the structure of the composite nonsmooth
objective function to accelerate convergence rates. In particular, we describe
an elementary but powerful approach for solving the problem
min
x∈Rn φ(x) := f (x) + τψ(x), (9.11)
where f is a smooth convex function, ψ is a convex regularization function
(often known simply as the “regularizer”), and τ ≥ 0 is a regularization
parameter. The technique we describe here is a natural extension of the
steepest descent approach, in that it reduces to the steepest descent method
analyzed in Theorem 3.3 applied to f when the regularization term is not
present (τ = 0). The approach is useful when the regularizer ψ has a
simple structure that is easy to account for explicitly. Such is true for many
regularizers that arise in data analysis, including the 1 function (ψ(x) =
x1) and the indicator function for a simple set  (ψ(x) = I(x)), such
as a box  = [l1,u1] ⊗ [l2,u2] ⊗···⊗ [ln,un]. Moreover, as we will see,
the convergence rate will be dictated by the smooth part of the decomposition
in (9.11), even though the function φ is not smooth.
Each step of the algorithm is defined as follows:
xk+1 := prox αk τψ (xk αk∇f (xk)), (9.12)
for some steplength αk > 0, and the prox operator defined in (8.24). By
substituting into this definition, we can verify that xk+1 is the solution of an
approximation to the objective φ of (9.11), namely
xk+1 := arg minz ∇f (xk)
T (z xk) +
1
2αk
z xk2 + τψ(z). (9.13)9.3 Proximal-Gradient Algorithms for Regularized Optimization 161
One way to verify this equivalence is to note that the objective in (9.13) can be
written as
1
αk
71
2


z (xk αk∇f (xk))



2
+ αkτψ(x)8
,
(modulo a term αk∇f (xk)2 that does not involve z and thus does not
affect the minimizer of (9.13)). The subproblem objective in (9.13) consists
of a linear term ∇f (xk)T (z − xk) (the first-order term in a Taylor series
expansion), a proximality term 1
2αk
z − xk2 that becomes stricter as αk ↓ 0,
and the regularization term τψ(x) in unaltered form. When τ = 0, we have
xk+1 = xk − αk∇f (xk), so the iteration (9.12) (or (9.13)) reduces to the usual
steepest-descent approach discussed in Chapter 3 in this case. It is useful to
continue thinking of αk as playing the role of a steplength parameter, though
here the line search is expressed implicitly through a proximal term.
The key idea behind the proximal-gradient algorithm is summarized in
the following proposition, which shows that every fixed point of (9.12) is a
minimizer of φ.
Proposition 9.3 Let f be differentiable and convex, and let ψ be convex. x∗ is
a solution of (9.11) if and only if x∗ = prox ατψ (x∗ −α∇f (x∗)) for all α > 0.
Proof x∗ is a solution if and only if ∇f (x∗) ∈ ∂τψ(x∗). This condition is
equivalent to
(x∗ α∇f (x∗)) x∗ ∈ α∂τψ(x∗),
which is, in turn, equivalent to x∗ = prox ατψ (x∗ α∇f (x∗)). 
Linear convergence of the proximal gradient method when f is strongly
convex can be derived in a similar way to that of the projected gradient method.
Indeed, we need only to invoke the nonexpansive property of the proximal
operator (See Proposition 8.19) and then follow the argument in Section 7.3.3
to obtain the following result.
Proposition 9.4 Let f have L-Lipschitz gradients and strong convexity
modulus m > 0, and let ψ be convex. Let x∗ be the unique minimizer of
φ = f +τψ. Then the iterates of the proximal gradient method with steplength
2
m+L satisfy
xk x∗ ≤ κ 1
κ + 1
k
x0 x∗, (9.14)
where κ = L/m.162 9 Nonsmooth Optimization Methods
The analysis of convergence for general convex functions is more delicate.
We show next that a rate of 1/T can be attained, just as in the case of smooth
convex functions.
9.3.1 Convergence Rate for Convex f
We will demonstrate convergence of the method (9.12) at a sublinear rate for
convex functions f whose gradients satisfy a Lipschitz continuity property
with Lipschitz constant L (see (2.7)) and for the fixed steplength choice
αk = 1/L.
The proof makes use of a “gradient map” defined by
Gα(x) := 1
α

x prox ατψ (x α∇f (x))
. (9.15)
By comparing with (9.12), we see that this map defines the step taken at
iteration k:
xk+1 = xk − αkGαk (xk) ⇔ Gαk = 1
αk
(xk − xk+1). (9.16)
The following technical lemma reveals some useful properties of Gα(x).
Lemma 9.5 Suppose that, in problem (9.11), ψ is a closed convex function
and that f is is convex with Lipschitz continuous gradient on Rn, with Lipschitz
constant L. Then for the definition (9.15) with α > 0, the following claims are
true.
(a) Gα(x) ∈ ∇f (x) + τ∂ψ(x αGα(x)).
(b) For any z and any α ∈ (0,1/L], we have that
φ(x αGα(x)) ≤ φ(z) + Gα(x)T (x z)
α
2
Gα(x)2.
Proof For part (a), we use the following optimality property of the prox
operator:
0 ∈ λ∂h(prox λh(x)) + (prox λh(x) x).
We make the substitutions: x − α∇f (x) for x, α for λ, and τψ for h to obtain
0 ∈ ατ∂ψ(prox ατψ (x α∇f (x)))+(prox ατψ (x α∇f (x)) (x α∇f (x)).
We use definition (9.15) to make the substitution prox ατψ (x α∇f (x)) =
x αGα(x) to obtain
0 ∈ ατ∂ψ(x αGα(x)) α(Gα(x) ∇ f (x)).
The result follows when we divide by α.9.3 Proximal-Gradient Algorithms for Regularized Optimization 163
For (b), we start with the following consequence of Lipschitz continuity of
∇f , from Lemma 2.2:
f (y) ≤ f (x) + ∇f (x)T (y x) +
L
2 y x2.
By setting y = x − αGα(x), for any α ∈ (0,1/L], we have
f (x − αGα(x)) ≤ f (x) − αGα(x)T ∇f (x) +
Lα2
2 Gα(x)2
≤ f (x) αGα(x)T ∇f (x) +
α
2
Gα(x)2. (9.17)
(The second inequality uses α ∈ (0,1/L].) We also have by convexity of f and
ψ that, for any z and any v ∈ ∂ψ(x αGα(x), the following are true:
f (z) ≥ f (x) + ∇f (x)T (z x),
ψ(z) ≥ ψ(x − αGα(x)) + vT (z − (x − αGα(x))). (9.18)
We have, from part (a), that v = (Gα(x) ∇ f (x))/τ ∈ ∂ψ(x αGα(x)),
so, by making this choice of v in (9.18) and also using (9.17), we have for any
α ∈ (0,1/L] that
φ(x αGα(x))
= f (x αGα(x)) + τψ(x αGα(x))
≤ f (x) αGα(x)T ∇f (x) +
α
2
Gα(x)2 + τψ(x αGα(x))
≤ f (z) + ∇f (x)T (x − z) − αGα(x)T ∇f (x) +
α
2
Gα(x)2
+ τψ(z) + (Gα(x) − ∇f (x))T (x − αGα(x) − z)
= f (z) + τψ(z) + Gα(x)T (x z)
α
2
Gα(x)2
,
where the first inequality follows from (9.17), the second inequality from
(9.18), and the last equality from cancellation of several terms in the previous
line. Thus, (b) is proved. 
Theorem 9.6 Suppose that in problem (9.11), ψ is a closed convex function
and that f is is convex with Lipschitz continuous gradient on Rn, with Lipschitz
constant L. Suppose that (9.11) attains a minimizer x∗ (not necessarily unique)
with optimal objective value φ∗. Then if αk = 1/L for all k in (9.12), we have
that {φ(xk)} is a decreasing sequence and that
φ(xk) φ∗ ≤
Lx0 x∗2
2k , k = 1,2,... .164 9 Nonsmooth Optimization Methods
Proof Since αk = 1/L satisfies the conditions of Lemma 9.5, we can use part
(b) of this result to show that the sequence {φ(xk)} is decreasing and that the
distance to the optimum x∗ also decreases at each iteration. Setting x = z = xk
and α = αk in Lemma 9.5, and recalling (9.16), we have
φ(xk+1) = φ(xk αkGαk (xk)) ≤ φ(xk)
αk
2 Gαk (xk)2
,
justifying the first claim. For the second claim, we have by setting x = xk,
α = αk, and z = x∗ in Lemma 9.5 that
0 ≤ φ(xk+1) φ∗ = φ(xk αkGαk (xk)) φ∗
≤ Gαk (xk)
T (xk − x∗) − αk
2 Gαk (xk)2
= 1
2αk

xk x∗2  xk x∗ αkGαk (xk)2

= 1
2αk

xk x∗2  xk+1 x∗2

, (9.19)
from which xk+1 x∗≤xk x∗ follows.
By setting αk = 1/L in (9.19), and summing over k = 0,1,2,...,K 1,
we obtain from a telescoping sum on the right hand side that
K
−1
k=0
(φ(xk+1) φ∗) ≤
L
2

x0 x∗2  xK x∗2

≤
L
2 x0 x∗2.
Since {φ(xk)} is monotonically decreasing, we have
K(φ(xK) φ∗) ≤
K
−1
k=0
(φ(xk+1) φ∗).
The result follows immediately by combining these last two expressions. 
9.4 Proximal Coordinate Descent for Structured
Nonsmooth Functions
Coordinate descent methods and proximal gradient methods can be combined
and applied in a fairly straightforward way to separable regularized objectives
of the form
min
x∈Rn h(x) := f (x) + λ
n
i=1
i(xi), (9.20)9.4 Proximal Coordinate Descent for Structured Nonsmooth Functions 165
where f is convex, as before, and each regularization term i : R → R is
convex but possibly nonsmooth. Mirroring the proximal-gradient method, in
place of the step (6.2) along coordinate ik, we obtain the next iteration by
solving the following scalar subproblem:
χk := arg min
χ
(χ xk
ik
)
T ∇ikf (xk) +
1
2αk
|χ xk
ik
|
2 + λik (χ), (9.21)
which we recognize as
xk+1
i = prox αλik
(xk
i αk∇ikf (xk)). (9.22)
In this section, we prove a result for the randomized CD method, which
applies the step (9.21), (9.22) to a component ik selected randomly and
uniformly from {1,2,...,n} at each iteration. We prove the result for the case
of strongly convex f , using a simplified version of the analysis from Richtarik
and Takac (2014). It makes use of the following assumption.
Assumption 2 The function f in (9.20) is uniformly Lipschitz continuously
differentiable and strongly convex with modulus m > 0 (see (2.18)). The
functions i, i = 1,2,...,n are convex.
Under this assumption, h attains its minimum value h∗ at a unique point x∗.
Our result uses the coordinate Lipschitz constant Lmax for ∇f defined in
(6.5). Note that the modulus of convexity m for f is also the modulus of
convexity for h. By elementary results for convex functions, we have that
h(αx + (1 α)y) ≤ αh(x) + (1 α)h(y)
1
2
mα(1 α)x y2. (9.23)
Theorem 9.7 Suppose that Assumption 2 holds. Suppose that the indices ik
in (9.21) are chosen independently for each k with uniform probability from
{1,2,...,n}, and that αk ≡ 1/Lmax. Then for all k ≥ 0, we have
E

h(xk)

h∗ ≤

1 m
nLmax k
(h(x0) h∗). (9.24)
Proof Define the function
H(xk
,z) := f (xk) + ∇f (xk)
T (z xk) +
1
2
Lmaxz xk2 + λ(z),
and note that this function is separable in the components of z and attains its
minimum over z at the vector zk whose ik component is defined in (9.21). Note,
by strong convexity (2.18), we have that166 9 Nonsmooth Optimization Methods
H(xk
,z) ≤ f (z)
1
2
mz xk2 +
1
2
Lmaxz xk2 + λ(z)
= h(z) +
1
2
(Lmax m)z xk2. (9.25)
We have, by minimizing both sides over z in this expression, that
H(xk
,zk) = minz H(xk
,z)
≤ minz h(z) +
1
2
(Lmax − m)z − xk2
≤ min
α∈[0,1]
h(αx∗ + (1 α)xk) +
1
2
(Lmax m)α2xk x∗2
≤ min
α∈[0,1]
αh∗ + (1 α)h(xk)
+
1
2
#
(Lmax m)α2 mα(1 α)$
xk x∗2
≤ m
Lmax
h∗ +

1 m
Lmax 
h(xk), (9.26)
where we used (9.25) for the first inequality, (9.23) for the third inequality, and
the particular value α = m/Lmax for the fourth inequality (for which value the
coefficient of xk x∗2 vanishes). By taking the expected value of h(xk+1)
over the index ik, we have
Eikh(xk+1) = 1
n
n
i=1
⎡
⎣f (xk + (zk
i xk
i )ei) + λi(zk
i ) + λ

ji
j (xk
j )
⎤
⎦
≤
1
n
n
i=1
7
f (xk) + [∇f (xk)]i(zk
i xk
i ) +
1
2
Lmax(zk
i xk
i )
2
+ λi(zk
i ) + λ

ji
j (xk
j )
8
= n − 1
n h(xk) +
1
n
#
f (xk) + ∇f (xk)
T (zk − xk)
+
1
2
Lmaxzk xk2 + λ(zk)
$
= n 1
n h(xk) +
1
n
H(xk
,zk).
By subtracting h∗ from both sides of this expression, and using (9.26) to
substitute for H(xk,zk), we obtain
Eikh(xk+1) h∗ ≤

1 m
nLmax 
(h(xk) h∗).9.5 Proximal Point Method 167
By taking expectations of both sides of this expression with respect to the
random indices i0,i1,i2,...,ik−1, we obtain
E(h(xk+1)) h∗ ≤

1 m
nLmax 
(E(h(xk)) h∗).
The result follows from a recursive application of this formula. 
A result similar to (6.7) can be proved for the case in which f is convex but
not strongly convex, but there are a few technical complications. We refer to
Richtarik and Takac (2014) for details.
9.5 Proximal Point Method
The proximal point method of Rockafellar (1976b) is a fundamental method
for solving the problem
min
x∈Rn ψ(x), (9.27)
where ψ is a convex function. The iterates are obtained from
xk+1 := arg minz ψ(z) +
1
2αk
z xk2 = prox αkψ (xk), (9.28)
where αk > 0 is a steplength parameter. Note that smoothness of ψ is not
required. The problem (9.27) is a special case of (9.11) in which we set f = 0
and τ = 1. We can thus state convergence results as corollaries of the results
in Section 9.3.
The subproblem to be solved in (9.28) for the proximal point method
contains the original objective ψ and, thus, would appear to be as difficult
to solve as the original problem. However, the quadratic regularization term in
(9.28) plays an important stabilizing role. In important special cases (such as
the augmented Lagrangian methods described in Section 10.5), its presence
can make solving the proximal subproblem (9.28) easier than solving the
original problem (9.27).
Because there is no smooth part f in (9.27) (when we compare the
objectives in (9.11) and (9.27)), there are no restrictions on the steplengths
αk. In a constant-steplength variant of (9.28), we can fix αk ≡ α for any α > 0
and set L = 1/α in Theorem 9.6 to obtain the following convergence result.
Theorem 9.8 Suppose that ψ is a closed convex function and that (9.27)
attains a minimizer x∗ (not necessarily unique) with optimal objective value
ψ∗. Then if αk = α > 0 for all k in (9.28), we have168 9 Nonsmooth Optimization Methods
ψ(xk) ψ∗ ≤ x0 x∗2
2αk , k = 1,2,... .
We observe again a sublinear 1/k rate of convergence, with a constant term
depending inversely on α. The dependence on α makes intuitive sense. If α
is chosen to be large, the quadratic regularization in (9.28) is mild, and the
constant factor x0 − x∗2/(2α) in the convergence expression is small. (In
the extreme case, as α → ∞, the effect of regularization vanishes, and the
approach (9.28) almost converges in one step. This is not surprising, as (9.28)
is close to the original problem (9.27) in this case.) When α is smaller, and the
quadratic regularization is more significant, the constant in the convergence
experession is correspondingly larger, so overall convergence is slower, when
measured in terms of iterations. However, in the latter case, each subproblem
may be easier to solve, as we may be able to use the approximate solution of
one subproblem as a “warm start” for the following subproblem and exploit the
strong convexity of the subproblems. Overall, the optimal choice of parameter
α will depend very much on the structure of ψ.
Notes and References
Bundle methods were proposed by Lemarechal ( ´ 1975) and Wolfe (1975).
They underwent much development in the years that followed; some key
contributions include Kiwiel (1990) and Lemarechal et al. ( ´ 1995). Applications
to regularized optimization problems in machine learning are described by Teo
et al. (2010).
Our proof of convergence of the proximal-gradient method in the convex
case in Section 9.3.1 is from the lecture on “Proximal Gradient Methods” in
the slides of Vandenberghe (2016).
Application of a version of the proximal-gradient approach to compressed
sensing was described by Wright et al. (2009). An accelerated version of
the proximal-gradient method was famously described by Beck and Teboulle
(2009).
Exercises
1. Let {αk}k=1,2,... be a sequence of positive numbers such that αk ↓ 0 but
T
k=1 αk ↑ ∞ as T → ∞. Show that
T
j=1 α2
j
T
j=1 αj
→ 0, as T → ∞.9.5 Proximal Point Method 169
2. Consider the subgradient method with decreasing steplength of the form
αk = θ/kp for some fixed value of p in the range (0,1). Using the
techniques of Section 9.2, find a bound on f (x¯T ) f (x∗) that generalizes
the bound (9.10). Verify that p = 1/2 yields the tightest bound for
p ∈ (0,1).
3. Define f (x,y) := |x y| + 0.1(x2 + y2).
(a) Show that f is convex.
(b) Compute the subdifferential of f at any point (x,y).
(c) Consider the coordinate descent method starting at the point
(x0,y0) = (1,1). Determine to which point the algorithm converges.
Explain your reasoning. What can you conclude about the coordinate
descent method for nonsmooth functions from this example?
4. Let f be strongly convex with modulus of convexity m and L Lipschitz
gradients. Define the function
fm(x) := f (x) m
2 x2
2.
(a) Prove that fm is convex with L m-Lipschitz gradients.
(b) Write down the proximal-gradient algorithm for the function
fm(x) + m
2 x2
,
where we take fm to be the “smooth” part and m
2 ·2 to be the
“convex but possibly nonsmooth” part.
(c) Does there exist a steplength α such that this proximal-gradient
algorithm has the same iterates as gradient descent applied to f for
some (possibly different) constant steplength? Explain.
(d) Find a steplength for the proximal-gradient method such that
xk − x∗ ≤ 
1 − m
L

xk−1 − x∗,
where x∗ is the unique minimizer of f .10
Duality and Algorithms
To this point, we have considered optimization over simple sets – sets over
which it is easy to minimize a linear objective or to compute a Euclidean
projection. The methods we described have strong theory and often good
performance, but in many cases, they do not extend well to cases in which the
feasible set has more complicated structure – for example, when it is defined
as the intersection of several sets or implicitly via algebraic equalities or
inequalities. In this chapter, we explore the use of duality to obtain a different
class of optimization methods that may perform better in such cases. For
any constrained optimization problem, duality defines an associated concave
maximization problem the dual problem whose solutions lower-bound the
optimal value of the original problem. In fact, under mild assumptions, we
can solve the original problem (also referred to as the primal problem in this
context) by first solving the dual problem. While there is a vast literature on
general techniques for constrained optimization, we highlight a few methods
that exploit duality and build on the algorithms studied in earlier chapters.
We begin by discussing how duality arises in problems in which the
feasible set  is the intersection of a hyperplane and a closed convex set X .
We introduce the Lagrangian function and discuss optimality conditions for
constrained problems of this form. We then present two methods based on the
Lagrangian function for problems of this type. Finally, we mention several
interesting problems to which these algorithms are particularly well suited.
10.1 Quadratic Penalty Function
Consider the following formulation for an optimization problem with both a
set inclusion constraint x ∈ X and a linear equality constraint Ax = b:
min
x f (x) subject to Ax = b, x ∈ X . (10.1)
17010.1 Quadratic Penalty Function 171
Here, X is a closed convex set, f: Rn → R is differentiable, and A ∈ Rm×n has
full row rank m (thus, m ≤ n). We described in Chapter 7 first-order methods
for the case in which only the set inclusion constraint is present. The addition
of an equality constraint complicates matters.
One approach to dealing with the equality constraint is to move it into the
objective function via a penalty. That is, we add a positive term to the objective
when the constraint is violated, with larger penalties being incurred for larger
violations. One simple type of penalty is a quadratic penalty, which leads to
the following approximation to (10.1):
min
x∈X f (x) +
1
2α
Ax b2
, (10.2)
where α > 0 is the penalty parameter. As α tends to zero, the penalty for
violating the constraint Ax = b become more severe, so the solution of (10.2)
will more nearly satisfy this constraint.
An intuitive approach to solving (10.1) would be to solve (10.2) with a
large value of α to yield a minimizer x∗(α). Then decrease the value of α (by a
factor of 2 or 5, say) and solve (10.2) again, “warm-starting” from the solution
obtained at the previous value of α. Generally, we have that Ax∗(α) b → 0
as α ↓ 0. In the limit, as α ↓ 0, we hope that x(α) approaches the solution
of (10.1).
We can make the relationship between (10.2) and (10.1) more crisp by
considering the following penalized min-max problem (also known as a saddle
point problem)
min
x∈X
max
λ∈Rm f (x) λT (Ax b)
α
2
λ2 . (10.3)
To see that this problem is equivalent to (10.2), note that we can carry out
the maximization with respect to λ explicitly, because the function is strongly
concave in λ with a simple Hessian. The optimal value is λ = −(Ax − b)/α.
By substituting this value into (10.3), we obtain (10.2).
Note too that (10.3) is well defined even for α = 0. In this case, we have
min
x∈X
max
λ∈Rm f (x) − λT (Ax − b), (10.4)
and this problem is equivalent to (10.1). To see this, note that if Ax  b, then
the maximization with respect to λ is infinite. On the other hand, if Ax = b,
then f (x)−λT (Ax b) = f (x) for all values of λ, so inner maximization with
respect to λ in (10.4) yields f (x) in this case. Hence, the outer minimization
in (10.4) considers only points in X with Ax = b, and it minimizes f over the
set of such points. The problem (10.4) is the starting point for our discussion
of duality.172 10 Duality and Algorithms
10.2 Lagrangians and Duality
The function L: Rn × Rm → R defined by
L(x,λ) := f (x) λT (Ax b) (10.5)
is called the Lagrangian function (often abbreviated to simply Lagrangian)
associated with the constrained optimization problem (10.1). This function
appears frequently in theory and algorithms for constrained optimization, both
convex and nonconvex. The vector λ is known as a Lagrange multiplier,
specifically, the Lagrange multiplier associated with the constraint Ax = b.
As we saw in (10.4), the problem
min
x∈X
max
λ∈Rn L(x,λ) (10.6)
is equivalent to (10.1). When we switch the order of the minimization and
maximization, we obtain the following dual problem associated with (10.1):
max
λ∈Rn q(λ), where q(λ) := min
x∈X
L(x,λ) (10.7)
In discussing duality, we often refer to the original formulation (10.1) as the
primal problem.
Note that the function q(λ) defined in (10.7) is always a concave function,
as can be proved from first principles. Thus, the dual problem is a concave
maximization problem, regardless of whether f is a convex function and
whether X is a convex set. We now show that the solution of (10.7) always
lower bounds the optimal objective of the primal problem (10.1).
Proposition 10.1 For any function ϕ(x,z), we have
min
x maxz ϕ(x,z) ≥ maxz min
x ϕ(x,z). (10.8)
Proof The proof is essentially tautological. Note that we always have
ϕ(x,z) ≥ min
x ϕ(x,z).
By taking the maximization with respect to the second argument, we obtain
maxz ϕ(x,z) ≥ maxz min
x ϕ(x,z) for all x.
Minimizing the left-hand side of this expression with respect to x yields our
assertion (10.8). 
When applied to (10.6) and (10.7), Proposition 10.1 yields a result known
as weak duality: The maximum value of q gives a lower bound on the optimal
objective value from (10.1). (The gap between the these two values is known10.2 Lagrangians and Duality 173
as a duality gap.) This result would be especially useful if the inequality (10.8)
were to be replaced by an equality – that is, the duality gap is zero. In this
case, knowledge of the dual maximum value would tell us the optimal value
of the primal (10.1), so we would know when to terminate an algorithm for
solving the latter problem. However, the inequality in (10.8) can be strict, as
the following example shows.
Example 10.2 (Bertsekas et al., 2003, p. 203) For x ∈ R2 and z ∈ R, define
ϕ(x,z) := exp(− √x1x2) + zx1 + IX(x) + IZ(z),
where IX and IZ are the indicator functions for the sets X and Z (respectively)
defined by X = {x ∈ R2 | x ≥ 0} and Z = {z ∈ R| z ≥ 0}. We have that
1 = min
x maxz ϕ(x,z) > maxz min
x ϕ(x,z) = 0. (10.9)
We will see in the sequel that if the minimization problem is convex, the
primal and dual problems usually attain equal optimal values (that is, the
duality gap is zero), and we are able to reconstruct minimizers of the primal
problem from the solution of the dual problem. Even in the convex case,
though, there are exceptions: The inequality can still be strict.
Example 10.3 (Todd, 2001) In semidefinite programming, we work with
matrix variables that are required to be symmetric positive semidefinite. We
also work with an inner product operation  ,  defined on two n×n symmetric
matrices X and Y as follows: X,Y  = n
i=1
n
j=1 XijYij . Consider the
following Lagrangian for a semidefinite program:
ϕ(X,λ) = C,X λ1(A1,X b1) λ2(A2,X b2) + IX0, (10.10)
where
X =
⎡
⎣
X11 X12 X13
X12 X22 X23
X13 X23 X33
⎤
⎦, C =
⎡
⎣
000
000
001
⎤
⎦,
A1 =
⎡
⎣
100
000
000
⎤
⎦, A2 =
⎡
⎣
010
100
002
⎤
⎦,
and b1 = 0, b2 = 2, where X ∈ R3×3 and λ ∈ R2. The last term in (10.10) is
an indicator function for the positive semidefinite cone; that is, it is zero when
X is positive semidefinite and ∞ otherwise. By substituting the definitions of
C, A1, etc., into (10.10), we obtain
ϕ(X,λ) = X33 − λ1X11 − λ2(2X12 + 2X33 − 2) + IX0. (10.11)174 10 Duality and Algorithms
In considering maxλ ϕ(X,λ), we note that this value will be infinite if the
coefficients of λ1 or λ2 are nonero. (If, for example X11 < 0, we can drive
λ1 to +∞ to make maxλ ϕ(X,λ) = ∞.) Thus, in seeking (X,λ) that achieve
finite values of ϕ(X,λ), we need only consider X for which X11 = 0 and
X12 + X33 = 1, and also X positive semidefinite. These conditions on X are
satisfied only when X11 = X12 = X13 = 0 and X33 = 1. Thus, we have that
minX maxλ ϕ(X,λ) = 1.
In considering maxλ minX ϕ(X,λ), we rewrite (10.11) as
ϕ(X,λ) = X,S + IX0, where S =
⎡
⎣
−λ1 −λ2 0
−λ2 0 0
0 01 − 2λ2
⎤
⎦ .
If S were to have a negative eigenvalue μ with corresponding eigenvector v,
we have vvT ,S = μv2, so by setting X = βvvT for β > 0, we have that
ϕ(βvvT ,λ) = μβv2 ↓ ∞ as β ↑ ∞. Thus, the maximum with respect
to λ of minX ϕ(X,λ) cannot be attained by any λ for which S has a negative
eigenvalue. We therefore have
S =
⎡
⎣
λ1 λ2 0
−λ2 0 0
0 01 − 2λ2
⎤
⎦  0,
which is satisfied only when λ2 = 0 and λ1 ≤ 0, for which values we have
ϕ(X,λ) = X • S + IX0 = λ1X11 + X33 + IX0.
The minimum over X is achieved at X = 0, so we have maxλ minX ϕ(X,λ) = 0.
In conclusion, we have
1 = min
X
max
λ ϕ(X,λ) > max
λ
min
X ϕ(X,λ) = 0,
so for this choice of ϕ, the inequality in Proposition 10.1 is strict.
In the next section, we identify conditions under which (10.8) holds with
equality, when ϕ obtained from the constrained optimization problem (10.1).
10.3 First-Order Optimality Conditions
In this section, we describe algebraic and geometric conditions that are
satisfied by the solutions of constrained optimization problems of the form
(10.1). Such problems admit “checkable” conditions that allow us to recognize
solutions as being solutions and allow practical algorithms to be constructed.10.3 First-Order Optimality Conditions 175
These conditions are related to stationary points of the Lagrangian (10.5).
In the next section, we describe algorithms that seek points at which these
optimality conditions are satisfied.
We will build on fundamental first-order optimality conditions, like the
one proved in Theorem 7.2 for the problem minx∈ f (x), for the case of
 closed and convex namely, that ∇f (x∗) ∈ N(x∗). The normal cone
has a particular structure for the case in which  = X ∩ {x | Ax = b} (as
in (10.1)), which, when characterized, yields the optimality conditions. This
characterization is described in the following result, which uses the definition
of the relative interior of a set C (denoted by ri(C)) from (A.3).
Theorem 10.4 Suppose that X ∈ Rn is a closed convex set and that A :=
{x | Ax = b} for some A ∈ Rm×n and b ∈ Rm, and define  := X ∩ A. Then
for any x ∈ , we have
N(x) ⊃ NX (x) + {AT λ | λ ∈ Rm}. (10.12)
If, in addition, the set ri(X ) ∩ A is nonempty, then this result holds with
equality; that is,
N(x) = NX (x) + {AT λ | λ ∈ Rm}. (10.13)
This result is proved in the Appendix (see Theorem A.18). We demonstrate
the need for the assumption ri(X ) ∩ A  ∅ for the “⊂” inclusion in (10.13)
with an example. Consider
X = {x ∈ R2 | x2 ≤ 1}, A = -
0 1.
, b = [1],
for which ri(X ) ∩ A = ∅ and  = X ∩ A = (0,1)T . We have that
N((0,1)T ) = R2, whereas
NX ((0,1)
T ) + {AT λ | λ ∈ R}={(0,τ)T | τ ∈ R},
so the left hand set in (10.13) is a superset of the right-hand set.
The condition ri(X ) ∩ A  ∅ is an example of a constraint qualification.
These conditions appear often in the theory of constrained optimization,
particularly in the definition of optimality conditions. Broadly speaking,
constraint qualifications are conditions under which the local geometry of a
set in particular, its normal cone at a point is captured accurately by some
alternative representation, usually more convenient and more “arithmetic” than
geometric. In the case of the set  defined before, the representation of the
normal cone on the right-hand side of (10.13) can be much easier to use when
determining membership of N(x) than when directly checking this condition.176 10 Duality and Algorithms
Using Theorem 10.4, we can now write the first-order optimality conditions
for (10.1) as follows.
Theorem 10.5 Consider the problem (10.1) in which f is continuously
differentiable and X is a closed convex set, with ri(X ) ∩ A  ∅, where
A = {x | Ax = b}. If x∗ is a local solution of (10.1), then there exists λ∗ ∈ Rm
such that
x∗ ∈  = X ∩ A, − ∇f (x∗) + AT λ∗ ∈ NX (x∗). (10.14)
Proof The proof follows immediately by combining Theorems 7.2 and 10.4,
noting that  is a closed convex set. 
We next show that a converse of this result holds when we assume
additionally that f is convex. Note that the assumption ri(X ) ∩ A  ∅ is
not needed for this result.
Theorem 10.6 Consider the problem (10.1) in which f is continuously
differentiable and convex, and X is a closed convex set. If there exists λ∗ ∈ Rm
such that the conditions (10.14) are satisfied at some x∗, then x∗ is a solution
of (10.1).
Proof Note from the first part of Theorem 10.4 that (10.14) implies that
∇f (x∗) ∈ N(x∗). The second part of Theorem 7.2 can then be applied
to obtain the result. 
The following is an immediate corollary of the last two results, which
applies to constrained convex optimization problems of the form (10.1).
Corollary 10.7 Consider the problem (10.1) in which f is continuously
differentiable and convex, and X is a closed convex set, with ri(X ) ∩ A  ∅,
where A = {x | Ax = b}. Then the conditions (10.14) are necessary and
sufficient for x∗ to be a solution of (10.1).
Example 10.8 Consider the problem
min
x∈Rn
n
i=1
xi subject to x2 ≤ 1, x1 = 1/2, x2 = 1/2, (10.15)
for some n ≥ 3. Note that we can eliminate the variables x1 and x2, and write
the problem equivalently as
min x3,x4,...,xn
n
i=3
xi subject to
2334n
i=3
x2
i ≤
1
√2
. (10.16)10.3 First-Order Optimality Conditions 177
By using Theorem 7.2, we can check that the point
(x3,x4,...,xn)
T = −1
√2(n 2)
(1,1,...,1)
T (10.17)
is the global solution of (10.16). It follows that the solution of (10.15) is
x∗ =
1
2
,
1
2
, 1
√2(n 2)
, 1
√2(n 2)
,..., 1
√2(n 2)

. (10.18)
We can use Corollary 10.7 to verify optimality of this point directly, by noting
that (10.15) has the form of (10.1) with X = {x ∈ Rn | x2 ≤ 1} and
A =

100 ... 0
010 ... 0

, b =

1/2
1/2

.
Note that the condition ri(X ) ∩ A  ∅ is satisfied, because ri(X ) =
{x ∈ Rn|x2 < 1}, and we have, for example, that (1/2,1/2,0,0,...,0)T ∈
ri(X ) ∩ A. For any x with x = 1, we have that NX (x) = αx for any α ≥ 0.
Thus, the optimality condition (10.14) at x∗ defined by (10.18) is
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1
1
1
.
.
.
1
⎤
⎥
⎥
⎥
⎥
⎥
⎦
+
⎡
⎢
⎢
⎢
⎢
⎢
⎣
1 0
0 1
0 0
.
.
. .
.
.
0 0
⎤
⎥
⎥
⎥
⎥
⎥
⎦

λ1
λ2

= α
⎡
⎢
⎢
⎢
⎢
⎣
1/2
1/2
√ −1
2(n−2)
.
.
. √
1
2(n−2)
⎤
⎥
⎥
⎥
⎥
⎦
,
for some α ≥ 0, λ1 ∈ R, and λ2 ∈ R. It is easy to check that this equality holds
when we set
α = "
2(n 2), λ1 = λ2 = 1 +
9n 2
2 .
Example 10.9 Consider the following problem, which has a combination of
nonnegativity bound constraints and equality constraints:
min f (x) subject to Ax = b, x ≥ 0.
By defining X := {x | x ≥ 0}, we have for any x ∈ X that
NX (x) = {v | vi ∈ ( ∞,0] if xi = 0, vi = 0 if xi > 0}.
Thus, the first-order optimality condition (10.14) becomes that there exists
λ∗ ∈ Rm such that Ax∗ = b, x∗ ≥ 0, and
#
∇f (x∗)+ATλ∗
$
i
≤0 when x∗
i = 0,
#
∇f (x∗)+ATλ∗
$
i
=0 when x∗
i > 0.
Note that since ri(X ) = {x | x > 0}, the constraint qualification ri(X ) ∩ A
requires existence of and x with x > 0 (all positive components) for which178 10 Duality and Algorithms
Ax = b. In fact, it can be shown that in this particular case, the characteri￾zation (10.13) holds even when this condition does not hold, because all the
constraints (equalities and inequalities) are linear functions of x.
10.4 Strong Duality
Having characterized optimality conditions, we now return to proving that the
primal problem (10.1) and the dual problem (10.7) attain the same optimal
objective values for many convex optimization problems. The following
theorem also shows that if we know a solution to the dual problem, we can
extract a solution to the primal via a simpler optimization problem.
Theorem 10.10 (Strong Duality) Suppose that f in (10.1) is continuously
differentiable and convex, that X is closed and convex, and that the condition
ri(X ) ∩ A  ∅ holds, where A = {x | Ax = b}. We then have the following.
1. If (10.1) has a solution x∗, then the dual problem (10.7) also has an
optimal solution λ∗, and the primal and dual optimal objective values are
equal.
2. For x∗ to be optimal for the primal and λ∗ optimal for the dual, it is
necessary and sufficient that Ax∗ = b , x∗ ∈ X , and
x∗ ∈ arg min x∈X
L(x,λ∗) = f (x) (λ∗)
T (Ax b).
Proof For all λ ∈ Rn and all x feasible for (10.1), we have, from (10.7), that
q(λ) ≤ f (x) λT (Ax b) = f (x),
where the equality holds because Ax = b. By Corollary 10.7, x∗ ∈  is
optimal if and only if there exists a λ∗ ∈ Rm such that
(∇f (x∗) − AT λ∗)
T (x − x∗) ≥ 0, for all x ∈ X . (10.19)
But since L(·,λ∗) is convex as a function of its first argument, and
∇xL(x,λ∗) = ∇f (x) AT λ∗, condition (10.19) shows that x∗ minimizes
L(x,λ∗) over x ∈ X . It now follows that
q(λ∗) = inf
x∈X
L(x,λ∗) = L(x∗
,λ∗) = f (x∗) (λ∗)
T (Ax∗ b) = f (x∗),
completing the proof of Part 1. The proof of Part 2 is left as an Exercise. 
Note that even if λ is only approximately dual optimal, minimizing the
Lagrangian with respect to x gives a reasonable approximation to the original
optimization problem. This claim follows from the calculation10.5 Dual Algorithms 179
f (x∗) = q(λ∗) ≤ q(λ) +  = inf
x∈
L(x,λ) +  ≤ L(x,λ) + 
= f (x) − λT (Ax − b) + .
Hence, if Ax b is small and our dual optimal value λ is accurate to
within an objective margin of , then f (x) is a reasonable approximation to
the optimal function value f (x∗) = q(λ∗).
10.5 Dual Algorithms
Though the dual objective function q is concave, it is typically nonsmooth,
so minimization may not be a straightforward operation. In this section, we
review how the algorithms derived earlier for nonsmooth optimization can be
leveraged to solve dual problems.
10.5.1 Dual Subgradient
Since the concave dual objective q defined by (10.7) is a minimum of
linear functions parametrized by the primal variable x, we can compute a
subgradient by finding the minimizing x and then applying Danskin’s theorem
(Theorem 8.13). Since q is a convex function, we have
∂( q)(λ) :=
7
Az b | z ∈ arg min x∈X
{f (x) λT (Ax b)}
8
.
Starting from some initial guess λ1 of the optimum, step k of the subgradient
method of Section 9.2 applied to q thus has the form
xk ← arg min x∈X
L(x,λk), λk+1 ← λk − sk(Axk − b),
where sk ∈ R+ is a steplength. To analyze this method, note that the maximum
norm of any subgradient of q is bounded by the maximal infeasibility of the
equality constraints over the set X . If we set
M = sup
x∈X
Ax − b,
we can apply our analysis of the subgradient method from Section 9.2 to obtain
q

1
T
k=1 sk

T
k=1
skλk

q∗ ≥
λ1 − λ∗2 + M2 T
k=1 s2
k
2
T
k=1 sk
.180 10 Duality and Algorithms
Hence, a convergence rate of O(T 1/2) is attainable, for the choices of
steplength sk discussed in Section 9.2. We can achieve a faster rate of conver￾gence by appealing to the proximal point method rather than the subgradient
method, as we show next.
10.5.2 Augmented Lagrangian Method
Application of the proximal point method of Section 9.5 to the problem of
maximizing the dual objective q(λ) leads to the following iteration:
λk+1 ← arg maxλ q(λ)
1
2αk
λ λk2
= arg maxλ
inf
x∈X
7
f (x) λT (Ax b)
1
2αk
λ λk2
8
,
where αk is the proximality parameter. This is a saddle point problem in (x,λ).
Since the objective is convex in x and strongly convex in λ, we can swap the
infimum and supremum by Sion’s minimax theorem (Sion, 1958) to obtain the
equivalent problem
inf
x∈X
7
max
λ f (x) λT (Ax b)
1
2αk
λ λk2
8
. (10.20)
The inner problem is quadratic in λ and has the trivial solution λ = λk
αk(Ax b), which we can substitute into (10.20), to obtain
min
x∈X f (x) (λk)
T (Ax b) +
αk
2 Ax b2 =: Lαk (x,λk).
The function Lα(x,λ) is called the augmented Lagrangian. It consists of the
ordinary Lagrangian function added to a quadratic penalty term that penalizes
violation of the equality constraint Ax = b. Iteration k of this overall approach
can be summarized as follows:
xk ← arg min x∈X
Lαk (x,λk), λk+1 ← λk αk(Axk b).
This algorithm was historically referred to as the method of multipliers in the
optimization literature but more recently has been known as the augmented
Lagrangian method.
For a fixed parameter αk (that is, αk ≡ α), we have, from the convergence
rate of the proximal point method (Theorem 9.8), that
q∗ q(λT ) ≤ λ∗ λ12
2αT , T = 1,2,... ;
that is, the dual objective converges at a rate of O(1/T).10.5 Dual Algorithms 181
The only difference between the augmented Lagrangian approach and
the dual subgradient method is that we have to minimize the augmented
Lagrangian for the x-step instead of the original Lagrangian. This may
add algorithmic difficulty, but in many cases, it does not; the augmented
Lagrangian can be as inexpensive to minimize as its non-augmented coun￾terpart. We give several examples in what follows.
Although the proximal point method is guaranteed to converge even for
a constant step size αk, the use of some heuristics frequently improves its
practical performance. In particular, the following approach is suggested by
Conn et al. (1992).
Algorithm 10.1 Augmented Lagrangian
Choose initial point λ1, initial parameter α1 > 0, δ1 = ∞, and parameters
η ∈ (0,1) and γ > 1;
for k = 1,2,... do
Set xk = arg minx∈X Lαk (x,λk);
Set δ = Axk − b2;
if δ < ηδk then
λk+1 ← λk αk(Axk b); αk+1 ← αk; δk+1 ← δ; {Improvement in
feasibility of x is acceptable; take step in λ.}
else
λk+1 ← λk; αk+1 ← γαk; δk+1 ← δk; {Insufficient improvement
in feasibilty; don’t update λ but increase penalty parameter α for next
iteration.}
end if
end for
Typical values of the parameters are η = 1/4 and γ = 10.
10.5.3 Alternating Direction Method of Multipliers
The alternating direction method of multipliers (ADMM) is a powerful
extension of the method of multipliers that is well suited to a variety of
interesting problems in data analysis and elsewhere. ADMM is targeted to
problems of the form
min
x,z f (x) + g(z) subject to Ax + Bz = c, x ∈ X, z ∈ Z, (10.21)
where X and Z are closed convex sets. The augmented Lagrangian for this
problem is182 10 Duality and Algorithms
Lα(x,z,λ) = f (x) + g(z) λT (Ax + Bz c) +
α
2
Ax + Bz c2.
ADMM essentially performs one step of block coordinate descent on the
primal problem and then updates the Lagrange multiplier, as follows:
xk = arg min x∈X
Lαk (x,zk−1
,λk) (10.22a)
zk = arg min z∈Z
Lαk (xk
,z,λk) (10.22b)
λk+1 = λk αk(Axk + Bzk c). (10.22c)
Note that if we were to loop on the first two update steps until Lαk (x,z,λk)
were minimized with respect to the primal variables (x,z), this approach would
become a particular implementation of the ordinary method of multipliers.
But the fact that only one round of block coordinate descent steps is taken
before updating λ is what distinguishes ADMM. In practice, taking multiple
coordinate descent steps may be advantageous in some contexts, but traditional
convergence proofs for ADMM have an “operator splitting” character that does
not exploit their relationship to the augmented Lagrangian method. The paper
of Eckstein and Yao (2015) explores this point and also gives computational
comparisons of ADMM with variants that more closely approximate the
augmented Lagrangian method. A proof of convergence of ADMM (10.22)
for the case of convex f and g is given in (Boyd et al., 2011, section 3.2 and
appendix A).
10.6 Some Applications of Dual Algorithms
Here we describe several applications for which the duality-based methods of
this chapter may be a good fit.
10.6.1 Consensus Optimization
Let G = (V,E) be a graph with vertex set V and edge set E. Consider the
following optimization problem in unknowns [xv]v∈V , where each xv ∈ Rnv
and the functions fv: Rnv → R are convex:
min
x

v∈V
fv(xv) subject to xu = xv for all (u,v) ∈ E. (10.23)10.6 Some Applications of Dual Algorithms 183
The Lagrangian for this problem is
L(x,λ) = 
v∈V
fv(xv) − 
(u,v)∈E
λT
u,v(xu − xv)
= 
v∈V
⎧
⎪⎨
⎪⎩
fv(xv)
⎛
⎝ 
(v,w)∈E
λv,w 
(u,v)∈E
λu,v
⎞
⎠
T
xv
⎫
⎪⎬
⎪⎭ .
Note that this function is separable in the components of x, so we can minimize
with respect to each xv, v ∈ V separately, even in a distributed fashion. The
λ-step of the dual subgradient method is
λk+1 u,v = λk
u,v − sk(xk
u − xk
v ), for all (u,v) ∈ E.
Many problems can be stated in the form (10.23). For instance, the case in
which we wish to minimizing a finite-sum objective with a shared variable:
min
x
m
i=1
fi(x), (10.24)
(where some of the fi may even be indicator functions for convex sets) can
be stated in the form (10.23) by defining V := {1,2,...,m}, giving each node
its own version of the variable x and defining an edge set E so that the graph
G = (V,E) is completely connected.
The augmented Lagrangian for (10.23) does not yield a problem that
is separable in the xv, because the quadratic penalty term couples the xv
at different nodes. We can, however, devise an equivalent formulation that
enables a convenient splitting with ADMM. Introducing new “edge variables”
zu,v for all (u,v) ∈ E, we rewrite (10.23) as follows:
min 
v∈V
fv(xv) subject to xu = zu,v, xv = zu,v, for all (u,v) ∈ E.
(10.25)
The augmented Lagrangian for this formulation is
Lα(x,z,λ,β) = 
v∈V
fv(xv) − 
(u,v)∈E
λT
u,v(xu − zu,v) − 
(u,v)∈E
βT
u,v(xv − zu,v)
+ 
(u,v)∈E
α
2
(xu − zu,v)
2 + 
(u,v)∈E
α
2
(xv − zu,v)
2 .
This function is separable in the components xv, v ∈ V , so the x-update step
in ADMM can be performed in a separated manner, possibly on a distributed
computational platform. Similarly, it is separable in the zu,v variables, and184 10 Duality and Algorithms
also in the dual variables λu,v and βu,v. Note that distributed implementations
would require information to be passed between nodes, or to a central server,
between updates of the various components.
A particular method for the finite-sum problem (10.24) is to allow each
function fi to have its own variable xi and then define a “master variable” x
and constraints that ensure that all xi are identical to x. We thus obtain the
following formulation, equivalent to (10.24):
min x,x1,x2, ,xm
m
i=1
fi(xi) subject to xi = x, i = 1,2,...,m. (10.26)
The augmented Lagrangian for this problem is
Lα(x,z,λ) = m
i=1
fi(xi) −m
i=1
λT
i (xi − x) +
α
2
m
i=1
xi − x2
,
where we defined z := (x1,x2,...,xm). The z-update step in ADMM is
separable in the replicates xi, i = 1,2,...,m; the step (10.22b) can be
performed as m separate optimization problems of the form
xk
i = arg minxi
fi(xi) (λk
i )
T xi +
α
2
xi xk2.
The x-update step (10.22a) can be performed explicitly, since the augmented
Lagrangian is a simple convex quadratic in x. We have
xk = 1
m
m
i=1

xk−1
i − 1
αk
λk
i

.
This example illustrates the flexibility that is possible with dual algorithms.
Different problem formulations play to the strengths of different algorithms,
and sometimes the algorithms with better worst-case complexities are not the
most appropriate, due to issues surrounding overhead and communication in
distributed implementations.
10.6.2 Utility Maximization
The general utility maximization problem is
max n
i=1
Ui(xi) subject to Rx ≤ c,
where R is a p×n matrix. Each utility function Ui represents some measure of
well-being for the ith agent as a function of the amount of resource xi available
to it. The inequalities are resource constraints, coupling the amount of utility10.6 Some Applications of Dual Algorithms 185
available to each user. Rewriting in our form (10.1), using minimization and
slack variables s, we have
min
(x,s)
n
i=1
Ui(xi) subject to Rx + c s = 0, s ≥ 0.
The Lagrangian is
L(x,s,λ) = n
i=1
Ui(xi) λT ( Rx + c s).
The dual subgradient method requires us to minimize this function over (x,s)
for s ≥ 0. Note that this minimization is unbounded below if any components
of λ are negative: If λi < 0, we can drive si to +∞ to force L(x,s,λ) to ∞.
Thus, the dual problem (10.7) is equivalent to
max
λ≥0
min
(x,s):s≥0
n
i=1
−Ui(xi) − λT (−Rx + c − s)
= max
λ≥0
min
x
n
i=1
Ui(xi) λT ( Rx + c),
where we can eliminate s because when λ ≥ 0, the optimal value of s is
clearly s = 0. The x step of the dual subgradient method is separable; agent i
maximizes
U(xi)
⎡
⎣

p
j=1
Rjiλk
j
⎤
⎦ xi.
The λ-update step is the projection of a subgradient step onto the nonnegative
orthant defined by λ ≥ 0; that is,
λk+1 ←
#
λk αk( Rxk + c)$
+ .
The dynamics of this model are interesting. Component j of λ can be
interpreted as a price for the resources represented by j th row of R and c.
If the prices are high, users incur a negative cost for acquiring more of their
quantities x. When the resource constraints are loose, the prices go down.
When they are violated, the prices go up.
10.6.3 Linear and Quadratic Programming
Consider the bound constrained convex quadratic program,
min
x cT x + 1
2 xT Qx, subject to Ax = b,  ≤ x ≤ u, (10.27)186 10 Duality and Algorithms
where Q  0 and  and u represent vectors of lower and upper bounds on
the components of x, respectively. (Some or all components of  and u may
be infinite.) When  = 0, the components of u are all +∞, and Q = 0,
then (10.27) is a linear program – the fundamental problem in constrained
optimization. The augmented Lagrangian for (10.27) is
Lαk (x,λ) = cT x + 1
2 xT Qx λT (Ax b) +
αk
2 Ax b2
,
so the x-step of the augmented Lagrangian method reduces to the following
bound-constrained quadratic problem:
xk = min
≤x≤u cT x + 1
2 xT Qx (λk)
T (Ax b) +
αk
2 Ax b2.
This problem can be solved via first-order methods, such as the projected
gradient or conditional gradient methods of Chapter 7.
To apply ADMM to this problem, we could formulate (10.27) equiva
lently as
min
(x,z) cT x +
1
2
xT Qx subject to Ax = b,  ≤ z ≤ u, z = x. (10.28)
The augmented Lagrangian for this problem is
Lα(x,z,λ) = cT x + 1
2 xT Qx − λT (x − z) +
α
2
z − x2
,
where we choose to enforce the constraints Ax = b and  ≤ z ≤ u explicitly.
The ADMM updates are therefore
xk+1 = arg minx Lαk (x,zk
,λk) subject to Ax = b, (10.29a)
zk+1 = arg minz Lαk (xk+1
,z,λk) subject to  ≤ z ≤ u, (10.29b)
λk+1 = λk αk(xk+1 zk+1). (10.29c)
The x update can be solved by solving an equality constrained quadratic
program, which reduces to solving a system of linear equations, as follows:

Q + αkI AT
A 0
 x
ν

=
 c + λk + αkzk
b

.
Note that if αk is constant, only the right-hand side changes from iteration to
iteration, so a factorization of the left-hand side can be precomputed and reused
at every iteration. A closed-form solution is available for the z update (see the
Exercises). This strategy for solving QPs is the main algorithmic idea behind
the OSQP quadratic programming solver (Stellato et al., 2020).10.6 Some Applications of Dual Algorithms 187
Notes and References
Several further examples of duality gaps (gaps between the primal and dual
optimal objective values) in convex problems appear in (Luo et al., 2000;
Vandenberghe and Boyd, 1996).
The method of multipliers (a.k.a. the augmented Lagrangian method) was
invented in the late 1960s by Hestenes (1969) and Powell (1969). It was
developed further by Rockafellar (1973, 1976a) and Bertsekas (1982) and
made into the practical general software package Lancelot for nonlinear
programming by Conn et al. (1992).
The alternating direction method of multipliers is described in the classic
review paper of Boyd et al. (2011). The approach was first proposed in the
1970s in Glowinski and Marrocco (1975) and Gabay and Mercier (1976), while
Eckstein and Bertsekas (1992) is an important early reference.
Exercises
1. Consider minimization of a smooth function f: Rn → R over the
polyhedral set defined by a combination of linear equalities and
inequalities as follows:
{x | Ex = g, Cx ≥ d},
where E ∈ Rm×n and C ∈ Rp×n. Show that the first-order necessary
conditions for x∗ to be a solution of this problem are that there exist
vectors λ ∈ Rm and μ ∈ Rp such that
∇f (x∗) − ET λ − CT μ = 0, Ex∗ = g, 0 ≤ μ ⊥ Cx∗ − d ≥ 0,
where 0 ≤ u ⊥ v ≥ 0 for two vectors u,v ∈ Rp indicates that for all
i = 1,2,...,p, we have ui ≥ 0, vi ≥ 0, and uivi = 0. (Hint: Introduce
slack variables s ∈ Rp, and reformulate the problem equivalently as
follows:
min
(x,s)∈Rn+p f (x) s.t. Ex = g, Cx s = d, s ≥ 0.
Now, by defining X , A, and b appropriately, use Theorem 10.5 to find
optimality conditions for the reformulated problem; then eliminate s to
obtain the aforementioned conditions.)
2. Prove the strict duality gap (10.9) for the function in Example 10.2.
3. Prove Part 2 of Theorem 10.10.
4. Verify by checking the condition ∇f (x∗) ∈ N(x∗) that the point x∗
defined by (10.17) is the solution of (10.16).
5. Write down a closed-form solution for the step (10.29b).11
Differentiation and Adjoints
In this chapter, we describe efficient calculation of gradients for certain
structured functions, particularly those that arise in the training of deep neural
networks (DNNs). Such functions share some features with objective functions
that arise in such applications as data assimilation and control, in both of which
the optimization problem is integrated with a model of a dynamic process, one
that evolves in time or proceeds by stages. In deep learning, the progressive
transformation of each item of data as it moves through the layers of the
network is akin to a dynamic process.
11.1 The Chain Rule for a Nested Composition
of Vector Functions
We start by introducing some notational conventions for derivatives of vector￾valued functions. For a function h: Rp × Rq → Rr, we denote the partial
gradient with respect to w at a point (w,y) ∈ Rp × Rq by ∇wh(w,y). This is
the p × r matrix whose ith column is the gradient of hi with respect to w, for
i = 1,2,...,r. Note that this matrix is the transpose of the Jacobian, which is
the r × p matrix whose rows are (∇whi)T .
We now consider the chain rule for differentiation of a function that is a
nested composition of vector functions. Given a vector x ∈ Rn of variables,
suppose that the objective f: Rn → R has the following nested form:
f (x) = (φ ◦ φl ◦ φl−1 ◦ ... ◦ φ1)(x) = φ(φl(φl−1(... (φ1(x))... ), (11.1)
where
φ1 : Rn → Rm1, φi : Rmi 1 → Rmi (i = 2,3, . . . ,l), and φ : Rml → R.
18811.1 The Chain Rule for a Nested Composition of Vector Functions 189
The chain rule for calculating ∇f (x) yields the following formula:
∇f (x) = (∇xφ1)

∇φ1φ2
 ∇φ2φ3

... 
∇φl−1φl
 ∇φlφ

, (11.2)
where all partial derivatives are evaluated at the current point x and all
consistent values of φ1,φ2,...,φl. Since f: Rn → R, the left-hand side ∇f (x)
of (11.2) is a (column) vector in Rn. Following the convention on derivative
notation, we have the following shapes for the terms on the right hand side of
(11.2):
∇xφ1 is a matrix of dimensions n × m1;
∇φiφi+1 is a matrix of dimensions mi × mi+1, for i = 1,2,...,l 1;
∇φlφ is a column vector of length ml.
The matrix multiplications on the right-hand side of the formula (11.2) are all
valid, and the product is a vector in Rn.
The function evaluation formula (11.1) and the derivative formula (11.2)
suggest the following scheme for evaluating the function f and its gradient.
Algorithm 11.1 Evaluation of a nested function and its gradient using the chain
rule.
Given x ∈ Rn;
Define x1 := φ1(x) and A1 := ∇φ1(x);
for i = 1,2,...,l 1 do
Evaluate
xi+1 := φi+1(xi) and Ai+1 := ∇φiφi+1(xi);
end for
Evaluate f := φ(xl) and pl := ∇φlφ(xl);
for i = l,l 1,...,2 do
Define pi−1 := Aipi;
end for
Define g := A1p1;
Output f = f (x) and g = ∇f (x).
This scheme consists of a function evaluation loop that makes a forward
pass through the succession of functions, and the derivative calculation loop
that implements a reverse pass. During the forward pass, we store partial
derivative information in the matrices Ai, i = 1,2,...,l, that are subse￾quently applied during the reverse pass to accumulate the product in (11.2).
The scheme is efficient because it requires only matrix-vector multiplications.190 11 Differentiation and Adjoints
The total cost of the algorithm is approximately nm1+l 1
i=1 mimi+1 multipli￾cations and additions, plus the cost of evaluating the functions and gradients.
(A more naive scheme for evaluating ∇f (x) from the formula (11.2) might
involve numerous matrix-matrix multiplications, not just the matrix-vector
multiplications required by this scheme.)
11.2 The Method of Adjoints
We now consider a more general function evaluation model that captures those
seen in simple DNNs and in other applications such as data assimilation. In this
model, the variables do not all appear at the innermost level of nesting, as in
(11.1). Rather, they are introduced progressively, at each stage of the function
evaluation. We use the term progressive functions to denote functions with this
structure. Despite the greater generality of this model, the process of evaluating
the function and its gradient still contains a forward pass and a reverse pass
and requires only a slight modification of Algorithm 11.1.
We consider a partition of the variable vector x as follows:
x = (x1,x2,...,xl), where xi ∈ Rni , i = 1,2,...,l, (11.3)
so that x ∈ Rn with n = n1 + n2 +···+ nl. A progressive function has the
following form
f (x) = φ(φl(xl,φl−1(xl−1,φl−2(xl−2 ... (x2,φ2(x2,φ1(x1))... ), (11.4)
where
φ1 : Rn1 →Rm1, φi : Rni ×Rmi−1 →Rmi (i = 2,3, . . . ,l), and φ : Rml → R.
Stage i of the evaluation requires the variable subvector xi together with
the value of the function φi−1, which depends on the previous variables
xi−1,xi−2,...,x1.
The dependence of f on the final subvector xl is straightforward, but the
chain rule is needed to recover derivatives with respect to other subvectors xi,
with more and more factors in the product as i decreases toward 1. Writing the
gradients with respect to the last few subvectors xl, xl−1, xl−2, xl−3, we obtain
∇xlf (x) = 
∇xlφl
 ∇φlφ

∇xl 1f (x) = 
∇xl 1φl−1
 ∇φl 1φl
 ∇φlφ

∇xl−2f (x) = 
∇xl 2φl−2
 ∇φl 2φl−1
 ∇φl 1φl
 ∇φlφ

∇xl 3f (x) = 
∇xl 3φl−3
 ∇xl 3φl−2
 ∇φl−2φl−1
 ∇φl 1φl
 ∇φlφ

.11.3 Adjoints in Deep Learning 191
A pattern emerges. We see that in the expression for each ∇xi f , i = l,l 1,
l 2,... , the last factor is ∇φlφ and the first factor is the partial derivative of
φi with respect xi. The intermediate terms are partial derivatives of one of the
nested functions with respect to the next function in the sequence. The general
formula is as follows:
∇xi f (x) = 
∇xiφi
 ∇φiφi+1
 ∇φi+1φi+2

... 
∇φl−2φl 1
 ∇φl 1φl
 ∇φlφ

.
(11.5)
(Note the similarity in the middle terms for different i the same derivative
matrices appear repeatedly in multiple expressions.) By extending Algorithm
11.1, we derive the efficient procedure shown in Algorithm 11.2 for computing
∇f (x). Algorithm 11.2 is efficient because it requires only matrix vector
multiplications and exploits fully the repeated structures seen in the middle
terms of (11.5).
Algorithm 11.2 Efficient evaluation of a progressive function and its gradient
using the chain rule.
Given x = (x1,x2,...,xl) ∈ Rn1+n2+. +nl ;
Evaluate s1 := φ1(x1) and B1 := ∇φ1(x1);
for i = 1,2,...,l − 1 do
Evaluate
si+1 := φi+1(xi+1,si), Ai+1 := ∇φiφi+1(xi+1,si),
Bi+1 := ∇xi+1φi+1(xi+1,si);
end for
Evaluate f := φ(sl) and pl := ∇φlφ(sl);
for i = l,l 1,...,2 do
Define pi 1 := Aipi, gi := Bipi;
end for
Define g1 := B1p1;
Output f = f (x) and g = (g1,g2,...,gl) = ∇f (x).
11.3 Adjoints in Deep Learning
The objective functions that arise in the training of the neural networks
described in Section 1.6 have the progressive form (11.4) (albeit with
different notation). Consider the supervised multiclass classification problem192 11 Differentiation and Adjoints
of Section 1.6, and suppose we are given m training examples (aj ,yj ),
j = 1,2,...,m, where each aj is a feature vector and yj ∈ RM is a label vector
that indicates membership of aj in one of M classes (see (1.20)). The loss
function for training the neural network is a finite summation of m functions
of the form (11.4), one for each training input. To be precise, given feature
vector aj , we can define s
(j)
1 = φ1(x1;aj ) in (11.4) to be the output of the first
layer of the DNN with aj denoting the input and x1 denoting the parameters in
the first layer. We can define s
(j)
i+1 = φi+1(xi+1,s(j)
i ), i = 1,2,...,l − 1 as in
Algorithm 11.2, ending with the outputs of the final layer, which is the vector
s
(j)
l . Note that ml = M; that is, the number of outputs from the final layer
equals the number of classes M. To define the loss function for this example,
we set
φ(j)(s(j)
l ) = − 

M
c=1
(yj )c(s(j)
l )c − log
M
c=1
e(s(j)
l )c

, (11.6)
while the overall loss function is
f (x) = 1
m
m
j=1
φ(j)(s(j)
l ). (11.7)
This is a slight generalization of our framework (11.4) in that this f is defined
as the average of the loss functions over m training examples (not as a function
based on a single training example), but note that the variables x are the same
in each of these m terms, as are the functions φl,φl−1,...,φ2. However, φ1 is
different for each of the m terms, because the feature vector aj that is input to
the first layer differs between terms. The function φ also differs between terms,
because it is based on the label vector yj for training example j . Algorithm
11.2 can, in principle, be applied to each function φ(j), j = 1,2,...,m to
obtain ∇f (x). In practice, training is usually done with some variant of a
stochastic gradient algorithm from Chapter 5 and we obtain an approximate
gradient needed by these methods by taking a single term or a minibatch of
terms from the summation in (11.7) and apply Algorithm 11.2 only to these
terms.
11.4 Automatic Differentiation
Consider now a generalization of the approach in Section 11.2 in which the
variables are not necessarily introduced progressively, stage by stage, and in
which each stage may depend not just on the previous stage but on many prior
stages. We use only the observation that the computation of the function f can194 11 Differentiation and Adjoints
each j ∈ P(i). The extra computation required for these partial derivatives is
often minimal, no more than a few floating-point operations per arc.
Equipped with the partial derivative information, we can find ∇f (x) by
performing a reverse sweep through the computation graph. At the conclusion
of this sweep, node i of the graph will contain the partial gradient ∂f/∂xi, so
that, in particular, the first n nodes will contain ∂f/∂xi, i = 1,2,...,n, which
are the components of the gradient ∇f (x).
To do the reverse sweep, we introduce variables zi at each node i =
1,2,...,N, initializing them to zi = 0 for i = 1,2,...,N 1, and zN = 1.
At the end of the computation, each zi will contain the partial derivative of f
with respect to xi. Since f (x) = xN , we have, in fact, that ∂f/∂xN = 1, so
zN = 1 already contains the correct value. The sweep now proceeds through
nodes i = N,N 1,N 2,...,1, as follows: At the start of step i, we have
that zi = ∂f/∂xi. We then update the variables zj in the nodes j in the parent
set P(i) as follows:
zj ← zj + zi
∂φi
∂xj
, for all j ∈ P(i). (11.9)
When the time comes to process a node i, the variable zi contains the sum of
the contributions from all of its of its children, since nodes i + 1,i + 2,...,N
have been processed already. We thus have
zi = 
l : i∈P(l)
zl
∂φl
∂xi
= 
l : i∈P(l)
∂f
∂φl
∂φl
∂xi
, (11.10)
the second equality being due to the fact that zl contains the value ∂f/∂xl at the
time that zi is updated, since i ∈ P(l). Since the formula (11.10) captures the
total dependence of f on xj , and since j ∈ P(i) only when i>j , we have by
an inductive argument that when zj has gathered all the contributions from its
child nodes i, it too contains the partial derivative ∂f/∂xj . The formula (11.10)
is essentially the chain rule for ∂f/∂xj .
Returning to the example in Figure 11.1, we see that the partial function
evaluations at nodes 4,5,...,10 can be carried out in numerical order, and the
partial derivatives can be evaluated in the reverse order 10,9,8,...,4,3,2,1.
What we have described in this section is the reverse mode of automatic
differentiation (also known as “computational differentiation” and “algorith
mic differentiation”). (The technique is often called “back-propagation” in
the machine learning community.) This technique and many other issues in
automatic differentiation are explored in the monograph of Griewank and
Walther (2008). The reverse mode has the remarkable property that the
computational cost of obtaining the gradient ∇f is bounded by a small11.5 Derivations via the Lagrangian and Implicit Function Theorem 195
multiple of the cost of evaluating f . This fact can be deduced readily from
the facts that (a) each arc in the computation graph corresponds to just one
or a few floating-point operations during the evaluation of f ; (b) labeling
the arc from j to i with the partial derivative ∂xi/∂xj requires just one or
a few additional floating-point operations; (c) the reverse sweep visits each
arc exactly once, and the update formula (11.9) shows that in general two
operations (one addition and one multiplication) are associated with each arc.
The chief drawback of the reverse mode is its space complexity. The proce￾dure, as previously described, requires storage of the complete computation
graph, including storage for xi and zi, i = 1,2,...,N and the arc labels
∂φi/∂xj , so that the storage requirements grow linearly with the time required
to evaluate f . Such requirements can be prohibitive for some functions. This
issue can be solved with the use of “checkpointing,” which is essentially a
process of trading storage for extra computation. At a checkpoint, we save
only those nodes of the computation graph that will be needed in subsequent
evaluations and discard the rest. When the reverse sweep reaches this point, we
recalculate the discarded nodes, allowing the reverse sweep to continue to an
earlier checkpoint. Details are given in Griewank and Walther (2008).
11.5 Derivations via the Lagrangian and
Implicit Function Theorem
We examine here an alternative viewpoint on the progressive function (11.4)
based on a reformulation as an equality constrained optimization problem.
We also discuss algorithmic consequences of this reformulation, and several
extensions.
11.5.1 A Constrained Optimization Formulation
of the Progressive Function
Returning to the progressive functions defined in (11.4), suppose that our
task is to find a stationary point for this function – that is, a point where
∇f (x) = 0. By introducing variables si to store intermediate evaluation
results, we can formulate this problem as the following equality-constrained
optimization problem:
min
x,s f (x,s) := φ(sl) s.t. s1 = φ1(x1), si = φi(xi,si 1), i = 2,3,...,l,
(11.11)196 11 Differentiation and Adjoints
where s = (s1,s2,...,sl) and x = (x1,x2,...,xl). By introducing Lagrange
multiplier vectors p1,p2,...,pl for the constraints in (11.11), we can write the
Lagrangian for this problem as
L(x,s,p) = φ(sl) 
l
i=2
pT
i (si φi(xi,si 1)) pT
1 (s1 φ1(x1)). (11.12)
First-order conditions for (x,s) to be a solution of this problem are obtained
by taking partial derivatives of the Lagrangian with respect to x, s, and p and
setting them all to zero. These partial derivatives are as follows:
∇x1L = B1p1, where B1 = ∇φ1(x1), (11.13a)
∇xiL = Bipi, where Bi := ∇xiφi(xi,si−1), i = 2,3,...,l, (11.13b)
∇piL = −si + φi(xi,si−1), i = 2,3,...,l, (11.13c)
∇p1L = −s1 + φ1(x1), (11.13d)
∇siL = −pi + Aipi+1, where Ai := ∇siφi+1(xi+1,si), i = 1,2,...,l − 1,
(11.13e)
∇slL = pl + ∇φ(sl). (11.13f)
Note the close relationship between this nonlinear system of equations and
Algorithm 11.2. By setting the partial derivatives w.r.t. pi to zero, we obtain
the equality constraints in (11.11), which are satisfied when the si are defined
by the forward pass in Algorithm 11.2. By setting the partial derivatives w.r.t. si
to zero, we obtain the so-called adjoint equation that defines the pi, which are
identical to those obtained from the reverse sweep in Algorithm 11.2. Finally,
the partial derivatives of L w.r.t. xi yield the same formulas as for the gradient
expressions in Algorithm 11.2. Thus, all terms in (11.13) are zero when, in the
notation of (11.4), we have ∇f (x) = 0 that is, x is a stationary point for f .
The constrained optimization perspective can have an advantage when
the formulation is complicated by the presence of constraints (equalities and
inequalities) involving s as well as x, or by slightly more general structure
than is present in (11.4). In such situations, the first order conditions (11.13)
contain complementarity conditions, which can be handled by an interior-point
framework while still retaining the advantages of sparsity and structure in the
Jacobian of the nonlinear equations (11.13) that lead to efficient calculation
of steps. In the unconstrained formulation, reduction of the constraints to
formulas involving x alone, even when this is possible, can lead to loss of
structure in the constraints and thus loss of efficiency in algorithms based
on (11.4).11.5 Derivations via the Lagrangian and Implicit Function Theorem 197
11.5.2 A General Perspective on Unconstrained
and Constrained Formulations
We generalize the technique of the previous subsection by considering an
unconstrained problem
min f (x) (11.14)
(for x ∈ Rn) that can be rewritten equivalently as the following constrained
formulation, as follows:
min
x,s F(x,s) s.t. h(x,s) = 0, (11.15)
where s ∈ Rp, and h: Rn × Rp → Rp uniquely defines s in terms of x.
Because of the latter property, we can write s = s(x), where h(x(s),s) = 0 for
all s, so that the objective in (11.15) becomes F(x,s(x)), and
f (x) = F(x,s(x)). (11.16)
Under appropriate assumptions of smoothness and nonsingularity of the
p × p matrix ∇sh(x,s(x)), we have from the implicit function theorem (see
Theorem A.2 in the Appendix) that
∇x s(x) = ∇ xh(x,s(x))[∇sh(x,s(x))]
−1. (11.17)
(This can be see by taking the total derivative of h with respect to x and setting
it to zero; that is, 0 = ∇xh(x,s(x)) + ∇x s(x)∇sh(x,s(x)).) By substituting
(11.17) into (11.16), we obtain
∇f (x) = ∇xF(x,s) + ∇x s(x)∇sF(x,s)
= ∇xF(x,s) − ∇xh(x,s(x))[∇sh(x,s(x))]
−1∇sF(x,s), (11.18)
The problem (11.11) is a special case of (11.15), in which ∇sh(x,s) is
a block-bidiagonal matrix with identity matrices on the diagonal. Thus, the
inverse [∇sh(x,s(x))] 1 is guaranteed to exist. We can show that (11.18) leads
to the same formula for ∇f (x) as was obtained by Algorithm 11.2 for (11.4).
Details are left as an Exercise.
11.5.3 Extension: Control
By a slight extension to the framework (11.11), we can define discrete-time
optimal control, an important class of problems in engineering and, more
recently, in machine learning. The only essential difference is that the objective198 11 Differentiation and Adjoints
depends not just on the sl but possibly on all variables xi, i = 1,2,...,l and
all intermediate variables si, i = 1,2,...,l, so we have
min
x,s f (x,s) := φ(x,s) (11.19a)
subject to s1 = φ1(x1), si = φi(xi,si−1), i = 2,3,...,l. (11.19b)
In the language of control, the variables xi are referred to as controls or
inputs, whose purpose is to influence the evolution of a dynamical system
that is described by the functions φi. The variables si are called the states
of this system. This is usually some known initial state s0 (not included in the
formulation above because it is fixed), and other states are fully determined by
the equations in (11.19).
The problem (11.19) has the form (11.15), where again the Jacobian
∇sh(x,s) has block-bidiagonal structure with identity matrices on the diag￾onal, so that it is structurally nonsingular. Algorithms for solving (11.19)
can thus make use either of the unconstrained perspective or the constrained
perspective. The latter is often more useful in the case of control, as many
problems have constraints on the states si as well as the controls xi, and these
can be handled more efficiently in the constrained formulations. (Even bound
constraints on si would convert to complicated constraints on the controls xi,
and elimination of the si, as is done in the unconstrained formulation, would
cause the stagewise structure to be lost.)
Notes and References
The monograph of Griewank and Walther (2008) is the standard reference on
computational differentiation. The widespread use of ReLU activations in neu
ral networks introduces some complications into the derivative computation, as
the functions are not smooth! The same ideas as described in Sections 11.2
and 11.3 obtain, but the concept of “derivative” needs to be generalized
considerably. Generalizations are discussed in Griewank and Walther (2008,
chapter 14), focusing on the Clarke subdifferential. The latter generalization,
used also by David et al. (2020), analyzes the convergence of a stochastic
subgradient method based on this generalization in a framework that can
be applied to neural networks with ReLU activations. Later work (Bolte
and Pauwels, 2020) makes use of “conservative fields” as generalizations of
derivatives (the Clarke subdifferential is a “minimal conservative field,” a
kind of special case). The latter paper gives details of the generalization of11.5 Derivations via the Lagrangian and Implicit Function Theorem 199
the reverse mode of automatic differentiation and of the convergence of a
minibatch variant of the stochastic gradient method.
Efficient solution of optimal control problems that exploit the stagewise
structure are discussed in (Rao et al., 1998), including variants in which
additional constraints are present at each stage of the problem.
Exercises
1. By expressing (11.11) in the form (11.15), show that the formula (11.18)
leads to the same gradient of f as is calculated in Algorithm 11.2. Explain
in particular why the matrix ∇sh(x,s) is nonsingular for the particular
function h from (11.11).
2. Sketch the computation graphs for the nested function (11.1) and the
progressive function (11.4), in a format similar to Figure 11.1.
3. By expressing (11.19) in the form (11.15), derive an expression for the
gradients with respect to x1,x2,...,xl of the version of this problem in
which the states si are eliminated.
4. Consider a DNN with ResNet structure, in which there are connections not
just between adjacent layers but also connections that skip one layer,
connecting the transformed output of the neurons at layer i to the input at
layer i + 2. Write down the extension of the constrained formulation
(11.11) to this case. By working with the implicit function theorem
techniques of Section 11.5.2, derive expressions for the total derivative of
the objective function with respect to the parameters of the DNN.Appendix
We gather in this appendix some background information for the analysis in the book,
including definitions, proofs of results stated in the chapters, and some foundational
results, such as linear programming duality and separation of convex sets.
A.1 Definitions and Basic Concepts
Sets. We assume familiarity with the ideas of open, closed, and compact sets. The
distance of a point x to a set C is
dist(x,C) = inf
y∈C
x − y. (A.1)
The closure of a set C, denoted by cl(C), is the set of all points x such that dist(x,C) =
0 The interior of a set C, denoted by int(C), is the largest open set contained in C
A set C is convex if x ∈ C,y ∈ C ⇒ αx + (1 α)y ∈ C for all α ∈ [0,1] A set
C is affine if x ∈ C,y ∈ C ⇒ αx + (1 α)y ∈ C for all α ∈ R
The affine hull of a set C, denoted aff(C), is the smallest affine set containing C
An explicit definition is
aff(C) :=
⎧
⎨
⎩
m
i=1
αixi |
m
i=1
αi = 1, xi ∈ C, i = 1,2,...,m
⎫
⎬
⎭
. (A.2)
The relative interior of C, denoted ri(C), is the interior of C when regarded as a subset
of its affine hull. Explicitly:
ri(C) := {x ∈ aff(C)| ∃  > 0 such that y − x < 
and y ∈ aff(C) ⇒ y ∈ C}. (A.3)
As examples, the set C := [0,1] × (0,1] × {1} ⊂ R3 has affine hull aff(C) = R2 × {1}
and relative interior ri(C) = (0,1) × (0,1) × {1}.
When  is a convex set, we define multiplication by a nonnegative scalar α as
follows:
α := {αv : v ∈ }.
200A.1 Definitions and Basic Concepts 201
We define set addition for convex sets i, i = 1,2,...,m, as follows:
m
i=1
i :=
⎧
⎨
⎩
m
i=1
vi : vi ∈ i, i = 1,2, ,m
⎫
⎬
⎭
The set C ∈ Rn is a cone if x ∈ C ⇒ αx ∈ C for all α > 0. The polar C◦ of a
cone C is defined by C◦ := {y | yT x ≤ 0 for all x ∈ C}
Order Notation. Given two sequences of nonnegative scalars {ηk} and {ζk}, with
ζk → ∞, we write ηk = O(ζk) if there exists a constant M such that ηk ≤ Mζk
for all k sufficiently large. The same definition holds if ζk → 0.
For sequences {ηk} and {ζk}, as before, we write ηk = o(ζk) if ηk/ζk → 0 as
k → ∞ We write ηk = (ζk) if both ηk = O(ζk) and ζk = O(ηk)
For a nonnegative sequence {ηk}, we write ηk = o(1) if ηk → 0
We sometimes (as in Section 2 2) use order notation without explicitly defining
sequences like {ηk} and {ζk} Consider, for example, the expression (2.6), which is
f (x + p) = f (x) + ∇f (x)T p + o(p).
This usage can be reconciled with our previous definition by considering a sequence of
vectors {pk} with pk → 0. We then have
f (x + pk) = f (x) + ∇f (x)T pk + o(pk),
where the notation o(·) is defined as before. Even more specifically, if we define
rk := f (x + pk) − f (x) − ∇f (x)T pk,
we have rk = o(pk).
Convergence of Sequences. Given a sequence of points {xk}k=0,1,2,... with xk ∈ Rn
for all k, we say that x is the limit of this sequence if for any  > 0, there is k such that
xk x ≤  for all k>k We denote this by x = limk→∞ xk
We say that x¯ is an accumulation point of the sequence {xk} if, for any index K and
any  > 0, there exists k>K such that xk − x ≤ . When this condition holds, we
can define an infinite index set S ⊂ {1,2,... } such that limk→∞,k∈S xk = x.
When x = limk→∞ xk, we say that the sequence {xk} converges Q-linearly to x if
there is ρ ∈ (0,1) such that for all k sufficiently large, we have
xk+1 x
xk x ≤ ρ
We say that {xk} converges R-linearly to x if there is a sequence of positive scalars {ηk}
such that {ηk} converges Q-linearly to zero, and xk − x ≤ ηk for all k.
Linear Algebra. A symmetric matrix A ∈ SRn×n admits the eigenvalue decomposi￾tion A = n
i=1 λiui(ui)T , where {u1,u2,...,un} is an orthonormal set of eigenvec￾tors and λi = λi(A) are the (real) eigenvalues, usually arranged in nonincreasing order.202 Appendix
We define λmax(A) = maxi=1,2,...,n λi(A) and λmin(A) = mini=1,2,...,n λi(A). For
such matrices, the trace equals the sum of eigenvalues; that is,
trace (A) = n
i=1
Aii = n
i=1
λi(A) (A 4)
Jensen’s Inequality and an Integral-Norm Inequality. Jensen’s inequality can be
stated in several forms, one of which is the following: Let (,A,μ) be a probability
space, so that μ() = 1 Suppose that g is a real valued function that is μ integrable,
and that ϕ is a convex function on the real line. Then we have
ϕ


g(s)dμ(s)
≤


ϕ(g(s))dμ(s) (A 5)
Noting that the integral represents an expected value, then by relabeling the function g
as a random variable X, we can rewrite this result as follows:
ϕ(E(X)) ≤ E(ϕ(X)) (A 6)
A closely related result from analysis is the following: Let (S,A,μ) be a measure
space, f: S → X be integrable, where X is a Banach space equipped with norm ·.
We then have





S
f (s)dμ(s)




≤

S
f (s) dμ(s). (A.7)
Taylor’s Theorem For Vector Functions. Taylor’s theorem is a foundational result for
smooth optimization, as it enables us to use derivative information about a function f
at a particular point to estimate its behavior at nearby points. We included a discussion
of Taylor’s theorem for a smooth function f: Rn → R in Chapter 2 Here we introduce
a variant for vector functions F : Rn → Rn, that is useful in analyzing systems of
nonlinear equations
Theorem A.1 Let F : Rn → Rn be a system of nonlinear equations with continuously
differentiable Jacobian J(x). We then have for any x,p ∈ Rn that
F(x + p) − F(x) =
 1
0
J(x + tp)p dt.
Implicit Function Theorem. The implicit function theorem describes the sensitivity
of a vector function s(x) ∈ Rp to its vector argument x ∈ Rn, where there is an implicit
relationship between function and argument that is defined in terms of another vector
function h(x,s(x)) = 0, where h ∈ Rp has the same dimension as s
We state the result rigorously as follows. (For a proof, see Lang, 1983, p. 131.)
Theorem A.2 Let h: Rn × Rp → Rp be a function such that the following three
conditions hold.
(i) h(x∗,s∗) = 0 for some s∗ ∈ Rp and x∗ ∈ Rn
(ii) h(·,·) is continuously differentiable in some neighborhood of (x∗,s∗)
(iii) ∇sh(x∗,s∗) ∈ Rp×p is nonsingularA.2 Convergence Rates and Iteration Complexity 203
Then there exist open sets Ns ∈ Rp and Nx ∈ Rn containing s∗ and x∗, respectively,
and a continuous function s(·): Rn → Rp, uniquely defined, such that s(x∗) = s∗, and
h(x,s(x)) = 0 for all x ∈ Nx . The gradient of the function s is defined by
∇s(x) = ∇ xh(x,s(x))[∇sh(x,s(x))]
−1
If h is r ≥ 1 times continuously differentiable with respect to both its arguments, then
s(x) is also r times continuously differentiable with respect to x.
A.2 Convergence Rates and Iteration Complexity
We show here how convergence rate expressions, both linear and sublinear, can be used
to obtain a lower bound on the number of iterations required to reduce the quantity of
interest below a certain given threshold  > 0 This bound is often called the iteration
complexity of the algorithm
Denote by {τk} the sequence of nonnegative scalar quantities of interest, with
τk → 0. We could have τk = f (xk) f ∗ (the difference between the function
value at iteration k and its optimal value), or τk = ∇f (xk) (gradient norm), or
τk = dist(xk,S) (distance between current iterate xk and the solution set), to mention
three examples We denote the target value for τk by  > 0 and obtain expressions for
the number of iterations k require to guarantee τk ≤ 
Suppose that we can prove sublinear convergence of the form
τk ≤
A
k + B, k = 1,2,...,
for some scalars A > 0 and B ≥ 0. Simple manipulation shows that we have τk ≤ 
whenever k ≥ (A/) B.
Suppose instead that we have a slower form of sublinear convergence, namely,
τk ≤
A
√k + B
, k = 1,2,
In this case, we can guarantee τk ≤  for all k ≥ (A/)2 − B.
Suppose that we are able to prove Q-linear convergence of {τk} to zero – that is,
τk+1 ≤ (1 − φ)τk, for some φ ∈ (0,1). (A.8)
By applying the bound (A 8) recursively, we have
τk ≤ (1 − φ)k 1τ1, k = 1,2,... .
Thus, we can guarantee τT ≤  when
(1 − φ)T −1τ1 ≤ .
When τ1 ≤ , we don’t need to look further T = 1 will suffice Otherwise, divide
both sides by τ1 and take logs, to obtain the equivalent condition
(T − 1)log(1 − φ) ≤ log(/τ1).204 Appendix
Now, using the fact that log(1+t) ≤ t for all t > −1, we find that a sufficient condition
for this inequality is that
−(T − 1)φ ≤ log(/τ1),
or, equivalently,
T ≥
1
φ | log(/τ1)| + 1. (A.9)
Note that the threshold  enters only logarithmically into the estimate of K The more
important term involves the value φ, which captures the rate of linear convergence
A.3 Algorithm 3.1 Is an Effective Line-Search Technique
We prove here that Algorithm 3 1 succeeds in identifying a value of α that satisfies the
weak Wolfe conditions, unless the function f is unbounded below along the direction
d (This proof is adapted from Burke and Engle, 2018, lemma 4 2 )
Theorem A.3 Suppose that f: Rn → R is continuously differentiable and that x,d ∈
Rn are such that ∇f (x)T d < 0. Then one of the following two possibilities must occur
in Algorithm 3 1
(i) The algorithm terminates at a finite value of α for which the weak Wolfe
conditions (3 26) are satisfied
(ii) The algorithm does not terminate finitely, in which case U is never set to a finite
value, L is set to 1 on the first iteration and is doubled at every subsequent
iteration, and f (x + αd) → ∞ for the sequence of α values generated by the
algorithm.
Proof Suppose that finite termination does not occur. If, indeed, U is never finite, then
L is set to 1 on the first iteration (since otherwise the algorithm would have terminated)
and α is set to 2. In fact, α is doubled at every subsequent iteration, and moreover, the
condition f (x + αd) ≤ f (x) + c1α∇f (x)T d holds for all such α, which implies that
f (x + αd) approaches ∞ for some sequence of values of α approaching ∞. Hence,
we are in case (ii).
Suppose now that finite termination does not occur but that U is set to a finite value
at some iteration Using l to denote the iterations of Algorithm 3 1, and Ll, αl, and
Ul denote the values of the parameters at the start of iteration l, we have initial values
L0 = 0, α0 = 1, and U0 = ∞ Note too that Ll < αl < Ul Since Ul is eventually
finite for some l, we have that Ll < αl < Ul for all l, and since the length of the interval
[Ll,Ul] is halved at each iteration after Ul becomes finite, there is a value α such that
Ll ↑ α, αl → α, Ul ↓ α (A 10)
If Ll = 0 for all l, then we have α = 0 and
f (x + αld) f (x)
αl
> c1∇f (x)T d, l = 0,1,2,...,A.4 Linear Programming Duality, Theorems of the Alternative 205
so by taking limits as l → ∞, we have ∇f (x)T d ≥ c1∇f (x)T d, which is a
contradiction since c1 ∈ (0,1) and ∇f (x)T d < 0. Thus, there exists an index l0 such
that Ll > 0 for all l ≥ l0.
Consider now all indices l>l0. We have the following three conditions:
f (x + Lld) ≤ f (x) + c1Ll∇f (x)T d, (A.11a)
f (x + Uld) > f (x) + c1Ul∇f (x)T d, (A.11b)
∇f (x + Lld)T d<c2∇f (x)T d. (A.11c)
Condition (A.11b) holds because each value of Ul is defined to be a value of α for which
the first “if” test is satisfied – that is, f (x + αd) > f (x) + c1α∇f (x)T d. Similarly,
condition (A.11a) holds because each Ll is defined to be a value of α for which the first
“if” test fails – that is, f (x + αd) ≤ f (x) + c1α∇f (x)T d. Condition (A.11c) holds
because each Ll is defined to be a value of α for which the “else if” condition holds –
that is, ∇f (x + αd)T d<c2∇f (x)T d.
By taking limits in (A.11c) as l → ∞, we have
∇f (x + αd)T d ≤ c2∇f (x)T d. (A.12)
By combining (A.11a) and (A.11b) and using the mean value theorem, we have
c1(Ul − Ll)∇f (x)T d ≤ f (x + Uld) − f (x + Lld) = (Ul − Ll)∇f (x + ˆαld)T d,
for some αˆl ∈ (Ll,Ul), for all l>l0. By dividing by Ul − Ll and taking limits in
this expression, we obtain that c1∇f (x)T d ≤ ∇f (x +αd)T d. This contradicts (A.12),
since ∇f (x)T d < 0 and 0 < c1 < c2. We conclude that if U is set to a finite value on
some iteration, finite termination must occur. But when finite termination occurs, the
final value of α satisfies the weak Wolfe conditions (3.26), so we are in case (i). 
A.4 Linear Programming Duality, Theorems
of the Alternative
Linear programming duality results are important in proving optimality conditions for
constrained optimization, as well as being of vital interest in their own right. We start
by discussing weak and strong duality theorems, then discuss the use of these theorems
in proving so-called theorems of the alternative. (The celebrated Farkas lemma used in
constrained optimization theory is one such theorem.)
Consider the following linear program in standard form:
min
x cT x subject to Ax = b, x ≥ 0, (A.13)
where A ∈ Rm×n, c ∈ Rn, b ∈ Rm, and x ∈ Rn. This problem is said to be infeasible
if there is no x ∈ Rn that satisfies the constraints Ax = b, x ≥ 0, and unbounded if
there is a sequence of vectors {xk}k=1,2, that is feasible (that is, Axk = b, xk ≥ 0),
with cT xk → −∞.206 Appendix
The dual linear program for (A.13) is
max
λ,s
bT λ subject to AT λ + s = c, s ≥ 0. (A.14)
Sometimes, for compactness of expression, the “dual slack” variables s are eliminated
from the dual formulation, and it is written equivalently as
max
λ
bT λ subject to AT λ ≤ c (A 15)
Two fundamental theorems in linear programming relate the primal and dual
problems. The first, called weak duality, has a trivial proof.
Theorem A.4 Suppose that x is feasible for (A 13) and (λ,s) is feasible for (A 14)
Then bT λ ≤ cT x
Proof
cT x = (AT λ + s)T x = λT (Ax) + sT x ≥ bT λ.
(The inequality follows from primal feasibility Ax = b and the fact that s ≥ 0 and
x ≥ 0 imply sT x ≥ 0.) 
The second duality result, called strong duality, is much more difficult to prove.
Theorem A.5 Considering the primal-dual pair (A 13)–(A 14), exactly one of the
following three statements is true.
(i) Both (A.13) and (A.14) are feasible, both have solutions, and the objective values
of the two problems are equal at the optimal points.
(ii) Exactly one of (A.13) and (A.14) is feasible, and the other is unbounded.
(iii) Both (A.13) and (A.14) are infeasible.
This result has several interesting consequences It tells us, for example, that we
cannot have a situation where one of the primal dual pair has an optimal solution while
the other is infeasible or unbounded. It also tells us that if one of the pair is unbounded,
the other is infeasible.
A common proof methodology (omitted here) is via the properties of the simplex
method. Using the traditional exposition of simplex via tableaus and pivot rules, it can
be shown that the method terminates in one of the three states above, when appropriate
anti-cycling rules are applied.
Strong duality can be used to prove theorems of the alternative, which are typically
a pair of conditions, each involving linear equalities and inequalities, of which
exactly one holds. One such theorem, known as the Farkas lemma, is instrumental in
proving the Karush–Kuhn–Tucker (KKT) conditions, which are first-order optimality
conditions for constrained optimization.
Lemma A.6 (Farkas Lemma) Given a set of vectors {ai ∈ Rn | i = 1,2, ,K } and a
vector b ∈ Rn, exactly one of the following two statements is true
I. There exist nonnegative coefficients λi ≥ 0, i = 1,2,...,K, such that
b = K
i=1 λiai. That is, b is in the cone defined by {ai ∈ Rn | i = 1,2,...,K}.
II. There exists s ∈ Rn such that bT s < 0 and (ai)T s ≥ 0 for all i = 1,2,...,K.A.4 Linear Programming Duality, Theorems of the Alternative 207
Proof Assembling the vectors ai into an n × K matrix A := [a1,a2,...,aK], we
consider the following linear program:
min
λ
0T λ subject to Aλ = b, λ ≥ 0, (A.16)
which has the form of (A 13) with c = 0 The dual is
maxt bT t subject to AT t ≤ 0. (A.17)
Since the dual is always feasible (t = 0 satisfies the constraints), we have from
Theorem A.5 that there are only two possible outcomes: Either (A.16) is infeasible and
(A.17) is unbounded (case (ii) of Theorem A.5) or both (A.16) and (A.17) both have
solutions, with equal objectives. The first of these alternatives corresponds to case II:
There is no vector λ ≥ 0 such that Aλ = b, but because of unboundedness of (A.17),
we can identify t such that bT t > 0 and (ai)T t ≤ 0 for all i = 1,2,...,K. We set
s = −t to obtain case II. The second alternative corresponds to case I: Feasibility of
(A.16) means existence of λ ≥ 0 such that b = K
i=1 aiλi. 
A second theorem of the alternative called Gordan’s theorem is useful in proving
results about separating hyperplanes between convex sets We make use of this result
in Section A 6
Theorem A.7 (Gordan’s Theorem) Given a matrix A, exactly one of the following two
statements is true.
AT y > 0 for some vector y; (I)
Ax = 0, x ≥ 0, x  0 for some vector x (II)
Proof Defining 1 to be the vector (1,1,...,1) with the same number of elements as
there are columns in A, we note that statement (I) is equivalent to the following linear
program having a solution:
min
y
0T y subject to AT y ≥ 1 ( P)
The dual of (P) is
maxx 1T x subject to Ax = 0, x ≥ 0. (D)
We now argue from strong duality. Suppose first that (I) is true. Then, by scaling y by
a positive scalar as needed, we can say that (P) is feasible and, thus, has a solution with
objective 0. Thus, from strong duality, (D) also has a solution with zero objective. But
this means that (II) cannot be true, because if any x were to satisfy (II), it would be
feasible for (D) with a strictly positive objective – greater than the maximum value.
Hence, we have shown that if (I) is true, (II) must be false.
Suppose now that (I) is false. Then there can be no feasible point for (P) (since if
there were, it would satisfy (I)) Thus, from strong duality, (D) is either infeasible or
unbounded Since it is clearly not infeasible (the vector x = 0 is a feasible point), it
must be unbounded. In particular, there must be a vector x such that Ax = 0, x ≥ 0,
with 1T x > 0, and from the latter, we can infer than x  0 Thus, (II) holds. A.6 Separation Results 209
A.6 Separation Results
Here we discuss separation results, which are classical results about the existence of
hyperplanes that separate two convex sets X and Y , such that X is on one side of
the hyperplane and Y is on the other. These results are vital to deriving optimality
conditions for convex optimization problems; we rely on them in Chapter 10.
We start with a technical result about compact sets.
Lemma A.10 Suppose that  is a compact set. Let x , x ∈ X be a collection of
subsets of , all closed in , for some index set X. If for every finite collection of
points x1,x2,...,xm ∈ X, we have that ∩m
i=1xi  ∅, then ∩x∈Xx  ∅.
Proof We prove the result by contradiction Since each x is closed in , its
complement c
x is open in  If ∩x∈Xx = ∅, then {c
x | x ∈ X} is an open cover
of  Thus, by the Heine–Borel theorem, there is a finite subcover that is, a set of
points x1,x2, ,x m ∈ X such that ∪m
i=1c
xi =  It follows that ∩m
i=1xi = ∅, a
contradiction 
Using this result, we show that any convex set not containing the origin can be
contained in a half-space passing through the origin.
Lemma A.11 Let X be any nonempty convex set such that 0  X Then there is a
nonzero vector t ∈ Rn such that t
¯T x ≤ 0 for all x ∈ X
Proof Define  := {v ∈ Rn | v2 = 1}, and for all x ∈ X, define
x := {v ∈  | vT x ≤ 0}
Clearly, x is compact for all x ∈ X. Now let x1,x2,...,xm be any finite set of vectors
in X. Since 0  X, then 0 is not in the convex hull of the vectors x1,x2,...,xm. That
is, defining A to be the matrix whose columns are x1,x2,...,xm, there is no vector
p ∈ Rm such that Ap = 0, p ≥ 0, 1T p = 1 (where 1 is the vector containing m
elements, all of which are 1). Thus, there is no p such that
Ap = 0, p ≥ 0, p  0,
since if there were, then p = p/(1T p) would have the forbidden properties It
follows from Gordan’s theorem (Theorem A.7) that there must be a vector t such
that AT t > 0, that is, (xi)T t > 0, i = 1,2,...,m. Therefore, we have that
−t/t2 ∈ ∩i=1,2,...,mxi . Thus, the conditions of Lemma A.10 are satisfied, so there
must exist a vector t such that t2 = 1 and t
¯T x ≤ 0 for all x ∈ X 
The inequality t
¯T x ≤ 0 need not be strict. An example is when X ⊂ R2 is the
convex set consisting of the entire left half-plane {(x1,x2)T : x1 ≤ 0} with the exception
of the half-line {(0,x2)T : x2 ≤ 0}. The only possible choices for t here are t = (β,0)T
for any β > 0, and all these choices have t
¯T x = 0 for some x ∈ X. However, with the
additional assumption of closedness of X, we can obtain strict separation.
Lemma A.12 Let X be a nonempty, convex, and closed set with 0  X. Then there is
t ∈ Rn and α > 0 such that tT x ≤ −α for all x ∈ X.210 Appendix
Proof Recalling the projection operator defined in (7.2), we have, by assumption, that
PX(0)  0. (If PX(0) were zero, we would have 0 ∈ cl(X) = X, which is false
by assumption.) We have by setting y = 0 in the minimum principle (7.3) that (0 −
PX(0))T (z−PX(0)) ≤ 0 for all z ∈ X, which implies PX(0)T z ≥ PX(0)2
2 > 0. We
obtain the result by taking t = P(0) and α = P(0)2
2. 
Having understood the issue of separation between a point and a convex set, we turn
to separation between two closed convex sets. It turns out that separation is possible,
but strict separation requires the additional condition of compactness of one of the sets.
We show these facts in the next two results.
Theorem A.13 (Separation of Closed Convex Sets) Let X and Y be two nonempty
disjoint closed convex sets Then these sets can be separated; that is, there is c ∈ Rn
with c  0, and α ∈ R such that cT x α ≤ 0 for all x ∈ X and cT y α ≥ 0 for all
y ∈ Y
Proof We first define the set X − Y as follows:
X Y := {x y : x ∈ X, y ∈ Y } (A 18)
An elementary argument shows that X Y is convex Since X and Y are disjoint, we
have that 0  X Y We can thus apply Lemma A 11 to deduce that there is c  0
such that cT (x y) ≤ 0 for all x ∈ X, y ∈ Y By choosing an arbitrary xˆ ∈ X, we
have that cT y is bounded below by cT xˆ for all y ∈ Y . Hence, the infimum of cT y over
y ∈ Y exists; we denote it by α and note that cT y ≥ α for all y ∈ Y . Moreover, since
cT x ≤ cT y for all x ∈ X, y ∈ Y , we must have cT x ≤ α too. We conclude that for
these definitions of c and α, the required inequalities are satisfied. 
We investigate further the properties of the set X Y defined in (A 18), where X
and Y are closed convex sets We noted above that X Y is convex, but it may not be
closed. Consider the following example of two closed convex sets in R2:
X = {(x1,x2)| x1 > 0,x2 ≥ 1/x1}, Y = {(y1,y2)| y1 > 0,y2 ≤ −1/y1},
and define the sequences {xk} and {yk} by xk := (k,1/k)T ∈ X for all k ≥ 1, and
yk := (k, 1/k) ∈ Y for all k ≥ 1 The sequence zk := xk yk = (0,2/k) ∈ X Y ,
by definition, and zk → (0,0)T , but (0,0)T  X Y Thus, X Y is not closed in
this example However, by adding a compactness assumption, we obtain closedness of
X Y , and thus a strict separation result
Theorem A.14 (Strict Separation) Let X and Y be two disjoint closed convex
nonempty sets with X compact. Then these sets can be strictly separated, that is, there
is c ∈ Rn, α ∈ R, and  > 0 such that cT x − α ≤ − for all x ∈ X and cT y − α ≥ 
for all y ∈ Y .
Proof We first show closedness of X − Y . Let zk be any sequence in X − Y such
that zk → z for some z. Closedness will follow if we can show that z ∈ X − Y . By
definition of X − Y , we can find two sequences {xk} in X and {yk} in Y such that
zk := xk − yk. Since X is compact, we have by taking a subsequence if necessary that
xk = zk + yk → x for some x ∈ X. Thus, we have that yk = xk − zk → x − z, andA.6 Separation Results 211
by closedness of Y , we have x − z ∈ Y . Thus, z = x − (x − z) ∈ X − Y , proving our
claim that X − Y is closed, as well as being nonempty and convex.
Since 0  X − Y , we use Lemma A.12 to choose a nonzero t ∈ Rn and β > 0
such that t
¯T (x − y) ≤ −β for all x ∈ X and y ∈ Y . Fixing some y ∈ Y , we have that
t
¯T x ≤ −β + t
¯T y for all x ∈ X. Hence, t
¯T x is bounded above for all x ∈ X, so there is
a supremal value γ such that t
¯T x ≤ γ . A similar argument shows that t
¯T y is bounded
below for all y ∈ Y , and has an infimal value δ. Moreover, we have that γ + β ≤ δ.
Thus, for all x ∈ X and y ∈ Y , we have that
t
T x ≤ γ<γ + β/2 < γ + β ≤ t
¯
T y.
We obtain the result by setting c = t, α = γ + β/2, and  = β/2. 
Supporting Hyperplane for Convex Sets. We now prove an almost immediate
consequence of the separating hyperplane theorem, a result called the supporting
hyperplane theorem that is used in the discussion of existence of subgradients in Section
8 1 We first need the following definition. Given a set X ⊂ Rn, we say that x ∈ X is
a boundary point of X if it is not in int(X) – that is, x ∈ X and for any  > 0, there
exists y  X with y − x < .
Theorem A.15 (Supporting Hyperplane Theorem) Let X be a nonempty convex set,
and let x be any boundary point of X Then there exists a nonzero c ∈ Rn and α ∈ R
such that cT x = α but cT z ≤ α for all z ∈ X (We call the plane defined by cT x = α
the supporting hyperplane.)
Proof If X has an interior in Rn, then x  int(X), and we apply Lemma A.11 to
separate 0 from int(X) − {x}. This result says that there is nonzero t ∈ Rn such that
t
¯T (z − x) ≤ 0 for all z ∈ int(X). Thus, tT (z − x) ≤ 0 for all z ∈ cl(X), and since
X ⊂ cl(X), we obtain the result by setting c = t and α = t
¯T x.
if X does not have an interior, it is contained in a hyperplane. That is, there exist
nonzero c ∈ Rn and α such that X ⊂ {z | cT z = α}. These c and α satisfy our claim
(rather trivially). 
Separating a Convex Set from a Hyperplane. Two sets C1 and C2 are said to be
properly separated if there is a separating hyperplane defined by cT x = α such that
it is not the case that both C1 and C2 are contained in the hyperplane. Recalling
the definition of relative interior of a set C from (A 3), we have the following result
concerning proper separation
Theorem A.16 (Rockafellar, 1970, Theorem 11.3) Let C1 and C2 be nonempty convex
sets. These sets can be properly separated if and only if their relative interiors ri(C1)
and ri(C2) are disjoint
We refer to Rockafellar (1970) for the proof, which depends on a number of other
technical results. We have the following corollary.212 Appendix
Corollary A.17 Suppose that C1 is a nonempty convex set and C2 is a subspace, with
ri(C1) disjoint from C2. Then there is a vector c such that cT x = 0 for all x ∈ C2 and
cT x ≤ 0 for all x ∈ C1, with the inequality being strict for some x ∈ C1.
Proof Since C2 is a subspace, we have C2 = aff(C2) = ri(C2) Thus ri(C1) and
ri(C2) are disjoint, so we can apply Theorem A 16 to deduce that C1 and C2 are
properly separable. Let (c,α) define a properly separating hyperplane, with cT x ≤ α
for all x ∈ C1 and cT x ≥ α for all x ∈ C2. Since C2 is a subspace, we have 0 ∈ C2
and thus α ≤ 0. In fact, we must have cT x = 0 for all x ∈ C2. (If this were not
true – that is, cT x > 0 for some x ∈ C2 – we have from βx ∈ C2 for all β ∈ R that
{cT x | x ∈ C2} = (−∞,∞), contradicting the existence of α.) If α < 0, the claim
follows immediately, from nonemptiness of C1. If α = 0, we have that the separating
hyperplane cT x = 0 contains C2. Thus, since C1 and C2 are properly separated by this
hyperplane, the hyperplane cannot contain C1 as well as C2. Thus, cT x < 0 for some
x ∈ C1, as claimed. 
Normal Cone of the Intersection of an Affine Space and a Convex Set. We now
restate Theorem 10.4, the critical result concerning the normal cone of the feasible set
for the problem (10.1), and provide a proof.
Theorem A.18 Suppose that X ∈ Rn is a closed convex set and that A := {x | Ax = b}
for some A ∈ Rm×n and b ∈ Rm, and define  := X ∩ A. Then for any x ∈ , we
have
N(x) ⊃ NX (x) + {AT λ | λ ∈ Rm} (A 19)
If, in addition, the set ri(X )∩A is nonempty, then this result holds with equality; that is,
N(x) = NX (x) + {AT λ | λ ∈ Rm} (A 20)
Proof To show (A.19), take any z ∈ , and note that z − x ∈ null(A), so that (z −
x)T AT λ = λT A(z−x) = 0 for all λ ∈ Rm. For any u ∈ NX (x), we have (z−x)T u ≤
0, by definition of NX (x). It follows that
(z x)T (u + AT λ) ≤ 0,
and so u + AT λ ∈ N(x) for any u ∈ NX (x) and any λ ∈ Rm
For the assertion “⊂” in (A 20), we choose an arbitrary v ∈ N(x), and aim to
show that v ∈ NX (x) + NA(x) By choice of v, we have vT (z x) ≤ 0 for all
z ∈  = X ∩ A. We define the following sets:
C1 = {(y,μ) ∈ Rn+1 | y = z x for some z ∈ X and μ ≤ vT y},
C2 = {(y,μ) ∈ Rn+1 | y ∈ null(A), μ = 0}.
Note that C2 is a subspace and that C1 is closed, convex, and nonempty. Note too that
ri(C1) and C2 are disjoint, because if there were a vector (y,ˆ μ)ˆ ∈ ri(C1) ∩ C2, we
would have zˆ = x + ˆy ∈ X and Azˆ = Ax = b, so that zˆ ∈ . Moreover, we would
have vT y >ˆ μˆ = 0 and, thus, vT (zˆ − x) > 0, contradicting v ∈ N(x). We can now
apply Corollary A.17 to deduce the existence of a vector (w,γ) ∈ Rn × R such thatA.7 Bounds for Degenerate Quadratic Functions 213
inf
(y,μ)∈C1
wT y + γμ< sup
(y,μ)∈C1
wT y + γμ ≤ 0, (A.21)
while
wT u = 0 for all u ∈ null(A). (A.22)
This latter equality implies that w = AT λ for some λ ∈ Rm
We note next that γ ≥ 0, since otherwise we obtain sup(y,μ)∈C1 wT y + γμ = ∞
by letting μ tend to ∞. We also cannot have γ = 0, as we argue now If γ = 0, we
would have from (A 21) that inf(y,μ)∈C1 wT y < sup(y,μ)∈C1 wT y ≤ 0, and so, in
particular, wT (z x) < 0 for some z ∈ X For any point x˜ ∈ ri(X ), we claim that
wT (x˜ x) < 0. If (for contradiction) we were to have wT (x˜ x) ≥ 0, we would find
that for small positive α and the fact that z x ∈ aff(C1) that x˜ α(z x) ∈ C1,
and hence, from (A 21), that wT (x˜ α(z x) x) ≤ 0 On the other hand, we have
wT (x˜ α(z x) x) = wT (x˜ x) αwT (z x) > 0, a contradiction Thus,
wT (x˜ − x) < 0 for all x˜ ∈ ri(X ). It follows from (A.22) that x˜ − x  null(A) and,
thus, Ax˜  Ax = b. Thus, there exists no point x˜ ∈ ri(C)∩A, so γ = 0 is not possible.
We thus have that γ in (A.21) is strictly positive. Taking any z ∈ X , we have from
(A.21), by setting μ = vT y = vT (z − x) in the definition of C1, that
wT (z − x) + γμ = wT (z − x) + γvT (z − x) = (w + γv)T (z − x) ≤ 0.
Therefore, we have w +γv ∈ NX (x) and so (1/γ)w +v = (1/γ)(w +γv) ∈ NX (x).
Since we already observed following (A.22) that w = AT λ for some λ ∈ Rm, we have
v = ((1/γ)w + v) (1/γ)w ∈ NX (x) + NA(x),
as required. 
A.7 Bounds for Degenerate Quadratic Functions
We prove here some claims concerning convex quadratic functions that may not be
strongly convex. We show here that such functions satisfy the PL property (3.45). Thus,
algorithms applied to these problems have similar performance as when applied to
strongly convex functions. The modulus of convexity m in the standard convergence
analysis can be replaced by the the minimum nonzero eigenvalue of the Hessian of the
quadratic function.
Consider first the function f (x) = 1
2 xT Ax arising in Section 3.8, where A is
positive semidefinite n × n matrix with rank r ≤ n and eigenvalues λ1 ≥ λ2 ≥ ... ≥
λr > 0. We claim that f satisfies (3.45) with m = λr. To prove the claim, we write the
eigenvalue decomposition of A as follows:
A = r
i=1
λiui
(ui
)
T ,214 Appendix
where {u1,u2,...,ur} is the orthnormal set of eigenvectors. We then have that
∇f (x)2 = Ax2 =






r
i=1
ui
λi(ui
)
T x






2
= r
i=1
λ2
i
#
(ui
)
T x
$2
Meanwhile, we have
f (x) − f (x∗) = 1
2
xT Ax = 1
2
r
i=1
λi
#
(ui
)
T x
$2
,
so that
2λr(f (x) − f (x∗)) = λr
r
i=1
λi
#
(ui
)
T x
$2
≤ r
i=1
λ2
i
#
(ui
)
T x
$2
= ∇f (x)2,
as required.
Next, we recall from Section 5.2.2 the Kaczmarz method, which is a type of
stochastic gradient algorithm applied to the function
f (x) = 1
2N Ax − b2,
where A ∈ RN×n, and there exists x∗ (possibly not unique) such that f (x∗) = 0, that
is, Ax∗ = b. (Let us assume for simplicity of exposition that N ≥ n ) We claimed in
Section 5.4 2 that for any x, there exists x∗ such that Ax∗ = b in which
Ax − b2 ≥ λmin,nzx − x∗2,
where λmin,nz is the smallest nonzero eigenvalue of AT A We prove this statement by
writing the singular value decomposition of A as
A = n
i=1
σiuivT
i ,
where the singular values σi satisfy
σ1 ≥ σ2 ≥ σ r > σr+1 =···= σn = 0,
so that r is the rank of A. The left singular vectors {u1,u2,...,un} form an orthonormal
set in RN , and the right singular vectors {v1,v2,...,vn} form an orthonormal set in Rn.
The eigenvalues of AT A are σ2
i , i = 1,2,...,n, so that the rank of AT A is r and the
smallest nonzero eigenvalue is λmin,nz = σ2
r .
Solutions x∗ of Ax∗ = b have the form
x∗ = r
i=1
uT
i b
σi
vi + n
i=r+1
τivi,
where τr+1,...,τd are arbitrary coefficients. Given x, we set τi = vT
i x, i = r +
1,...,n. (We leave it as an Exercise to show that this choice minimizes the distance
x − x∗.) We then haveA.7 Bounds for Degenerate Quadratic Functions 215
Ax − b2 = A(x − x∗)2
=






n
i=1
σiuivT
i (x − x∗)






2
=






r
i=1
σiuivT
i (x x∗)






2
≥ σ2
r
r
i=1
[vT
i (x − x∗)]
2
= λmin,nzn
i=1
[vT
i (x x∗)]
2
= λmin,nzx − x∗2,
where the last step follows from the fact that [v1,v2, ,v n] is a n × n orthogonal
matrixBibliography
Allen Zhu, Z 2017 Katyusha: The first direct acceleration of stochastic gradient
methods Journal of Machine Learning Research, 18(1), 8194–8244.
Attouch, H , Chbani, Z., Peypouquet, J , and Redont, P 2018 Fast convergence of iner
tial dynamics and algorithms with asymptotic vanishing viscosity. Mathematical
Programming, 168(1–2), 123–175.
Beck, A., and Teboulle, M. 2003. Mirror descent and nonlinear projected subgradient
methods for convex optimization. Operations Research Letters, 31, 167–175.
Beck, A., and Teboulle, M. 2009. A Fast iterative shrinkage-threshold algorithm for
linear inverse problems. SIAM Journal on Imaging Sciences, 2(1), 183–202.
Beck, A., and Tetruashvili, L. 2013. On the convergence of block coordinate descent
type methods. SIAM Journal on Optimization, 23(4), 2037–2060.
Bertsekas, D. P. 1976. On the Goldstein-Levitin-Polyak gradient projection method.
IEEE Transactions on Automatic Control, AC-21, 174–184.
Bertsekas, D. P. 1982. Constrained Optimization and Lagrange Multiplier Methods.
New York: Academic Press.
Bertsekas, D. P. 1997. A new class of incremental gradient methods for least squares
problems. SIAM Journal on Optimization, 7(4), 913–926.
Bertsekas, D. P. 1999. Nonlinear Programming. Second edition. Belmont, MA: Athena
Scientific
Bertsekas, D. P 2011 Incremental gradient, subgradient, and proximal methods for
convex optimization: A survey Pages 85–119 of: Sra, S , Nowozin, S , and
Wright, S J (eds), Optimization for Machine Learning NIPS Workshop Series
Cambridge, MA: MIT Press.
Bertsekas, D P , and Tsitsiklis, J N 1989. Parallel and Distributed Computation:
Numerical Methods Englewood Cliffs, NJ: Prentice Hall
Bertsekas, D. P., Nedic, A , and Ozdaglar, A E 2003 ´ Convex Analysis and Optimiza
tion. Optimization and Computation Series. Belmont, MA: Athena Scientific.
Blatt, D , Hero, A O , and Gauchman, H 2007 A convergent incremental gradient
method with a constant step size. SIAM Journal on Optimization, 18(1), 29–51.
Bolte, J., and Pauwels, E. 2021. Conservative set valued fields, automatic differentia￾tion, stochastic gradient methods, and deep learning. Mathematical Programming,
188(1), 19–51.
216Bibliography 217
Boser, B. E., Guyon, I. M., and Vapnik, V. N. 1992. A training algorithm for optimal
margin classifiers. Pages 144–152 of: Proceedings of the Fifth Annual Workshop
on Computational Learning Theory. Pittsburgh, PA: ACM Press.
Boyd, S., and Vandenberghe, L. 2003. Convex Optimization. Cambridge: Cambridge
University Press.
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. 2011. Distributed optimiza￾tion and statistical learning via the alternating direction methods of multipliers.
Foundations and Trends in Machine Learning, 3(1), 1–122.
Bubeck, S , Lee, Y T , and Singh, M 2015 A geometric alternative to Nesterov’s accel￾erated gradient descent Technical Report arXiv:1506.08187 Microsoft Research
Burachik, R S , and Jeyakumar, V 2005 A Simple closure condition for the normal
cone intersection formula Transactions of the American Mathematical Society,
133(6), 1741 1748
Burer, S , and Monteiro, R D. C 2003 A nonlinear programming algorithm for solving
semidefinite programs via low-rank factorizations Mathematical Programming,
Series B, 95, 329–257
Burke, J V, and Engle, A 2018 Line search methods for convex composite optimiza
tion Technical Report arXiv:1806.05218 Department of Mathematics, University
of Washington.
Candes, E., and Recht, B. 2009. Exact matrix completion via convex optimization. `
Foundations of Computational Mathematics, 9, 717–772.
Chouzenoux, E., Pesquet, J.-C., and Repetti, A. 2016. A block coordinate vari￾able metric forward-backward algorithm. Journal of Global Optimization, 66,
457–485.
Conn, A. R., Gould, N. I. M., and Toint, P. L. 1992. LANCELOT: A Fortran Package
for Large-Scale Nonlinear Optimization. Springer Series in Computational Math￾ematics, vol. 17. Heidelberg: Springer-Verlag.
Cortes, C., and Vapnik, V. N. 1995. Support-vector networks. Machine Learning, 20,
273–297.
Danskin, J. M. 1967. The Theory of Max-Min and Its Application to Weapons Allocation
Problems. Springer.
Davis, D., Drusvyatskiy, D., Kakade, S., and Lee, J. D. 2020. Stochastic subgradient
method converges on tame functions. Foundations of Computational Mathematics,
20(1), 119–154.
Defazio, A , Bach, F., and Lacoste-Julien, S 2014 SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives. Pages 1646–
1654 of: Advances in Neural Information Processing Systems, November 2014,
Montreal, Canada
Dem’yanov, V F , and Rubinov, A M 1967 The minimization of a smooth convex
functional on a convex set SIAM Journal on Control, 5(2), 280–294
Dem’yanov, V F , and Rubinov, A M 1970. Approximate Methods in Optimization
Problems Vol 32. New York: Elsevier
Drusvyatskiy, D., Fazel, M., and Roy, S. 2018. An optimal first order method based on
optimal quadratic averaging. SIAM Journal on Optimization, 28(1), 251–271.
Dunn, J. C. 1980. Convergence rates for conditional gradient sequences generated
by implicit step length rules. SIAM Journal on Control and Optimization, 18(5),
473–487.218 Bibliography
Dunn, J. C. 1981. Global and asymptotic convergence rate estimates for a class of
projected gradient processes. SIAM Journal on Control and Optimization, 19(3),
368–400.
Eckstein, J., and Bertsekas, D. P. 1992. On the Douglas-Rachford splitting method
and the proximal point algorithm for maximal monotone operators. Mathematical
Programming, 55, 293–318.
Eckstein, J., and Yao, W. 2015. Understanding the convergence of the alternating
direction method of multipliers: Theoretical and computational perspectives.
Pacific Journal of Optimization, 11(4), 619–644.
Fercoq, O , and Richtarik, P. 2015 Accelerated, parallel, and proximal coordinate
descent SIAM Journal on Optimization, 25, 1997–2023.
Fletcher, R , and Reeves, C M 1964. Function minimization by conjugate gradients
Computer Journal, 7, 149–154
Frank, M , and Wolfe, P. 1956. An algorithm for Quadratic Programming Naval
Research Logistics Quarterly, 3, 95–110
Gabay, D , and Mercier, B 1976. A dual algorithm for the solution of nonlinear vari
ational problems via finite element approximations. Computers and Mathematics
with Applications, 2, 17–40.
Gelfand, I. 1941. Normierte ringe. Recueil Math´ematique [Matematicheskii Sbornik],
9, 3–24.
Glowinski, R., and Marrocco, A. 1975. Sur l’approximation, par elements finis d’ordre
un, en al resolution, par penalisation-dualite, d’une classe dre problems de Dirich- ´
let non lineares. Revue Francaise d’Automatique, Informatique, et Recherche
Operationelle, 9, 41–76.
Goldstein, A. A. 1964. Convex programming in Hilbert space. Bulletin of the American
Mathematical Society, 70, 709–710.
Goldstein, A. A. 1974. On gradient projection. Pages 38–40 of: Proceedings of the 12th
Allerton Conference on Circuit and System Theory, Allerton Park, Illinois.
Golub, G. H., and van Loan, C. F. 1996. Matrix Computations. Third edition. Baltimore:
The Johns Hopkins University Press.
Griewank, A., and Walther, A. 2008. Evaluating Derivatives: Principles and Tech￾niques of Algorithmic Differentiation. Second edition. Frontiers in Applied Math￾ematics. Philadelphia, PA: SIAM.
Hestenes, M R 1969 Multiplier and gradient methods Journal of Optimization Theory
and Applications, 4, 303–320.
Hestenes, M , and Steifel, E. 1952. Methods of conjugate gradients for solving
linear systems Journal of Research of the National Bureau of Standards, 49(6),
409–436
Hu, B , Wright, S. J , and Lessard, L. 2018 Dissipativity theory for accelerat
ing stochastic variance reduction: A unified analysis of SVRG and Katyusha
using semidefinite programs Pages 2038–2047 of: International Conference on
Machine Learning (ICML).
Jaggi, M. 2013. Revisiting Frank-Wolfe: Projection-free sparse convex optimization.
Pages 427–435 of: International Conference on Machine Learning (ICML).
Jain, P., Netrapalli, P., Kakade, S. M., Kidambi, R., and Sidford, A. 2018. Accelerating
stochastic gradient descent for least squares regression. Pages 545–604 of: Con￾ference on Learning Theory (COLT).Bibliography 219
Johnson, R., and Zhang, T. 2013. Accelerating stochastic gradient descent using
predictive variance reduction. Pages 315–323 of: Advances in Neural Information
Processing Systems.
Kaczmarz, S. 1937. Angenaherte Aufl ¨ osung von Systemen linearer Gleichungen. ¨
Bulletin International de l’Acad´emie Polonaise des Sciences et des Lettres. Classe
des Sciences Math´ematiques et Naturelles. S´erie A, Sciences Math´ematiques, 35,
355–357.
Karimi, H., Nutini, J., and Schmidt, M. 2016. Linear convergence of gradient and
proximal-gradient methods under the Polyak-Łojasiewicz condition Pages 795–
811 of: Joint European Conference on Machine Learning and Knowledge Discov￾ery in Databases Springer
Kiwiel, K C. 1990 Proximity control in bundle methods for convex nondifferentiable
minimization Mathematical Programming, 46(1 3), 105–122
Kurdyka, K 1998 On gradients of functions definable in o-minimal structures Annales
de l’Institut Fourier, 48, 769–783
Lang, S 1983 Real Analysis Second edition Reading, MA: Addison-Wesley
Le Roux, N , Schmidt, M , and Bach, F 2012 A stochastic gradient method with
an exponential convergence rate for finite training sets Advances in Neural
Information Processing Systems, 25, 2663–2671.
Lee, C.-P., and Wright, S. J. 2018. Random permutations fix a worst case for cyclic
coordinate descent. IMA Journal of Numerical Analysis, 39, 1246–1275.
Lee, Y. T., and Sidford, A. 2013. Efficient accelerated coordinate descent methods and
faster algorithms for solving linear systems. Pages 147–156 of: 2013 IEEE 54th
Annual Symposium on Foundations of Computer Science. IEEE.
Lemarechal, C. 1975. An extension of Davidon methods to non differentiable problems. ´
Pages 95–109 of: Nondifferentiable Optimization. Springer.
Lemarechal, C., Nemirovskii, A., and Nesterov, Y. 1995. New variants of bundle ´
methods. Mathematical Programming, 69(1–3), 111–147.
Lessard, L., Recht, B., and Packard, A. 2016. Analysis and design of optimization
algorithms via integral quadratic constraints. SIAM Journal on Optimization,
26(1), 57–95.
Levitin, E. S., and Polyak, B. T. 1966. Constrained minimization problems. USSR
Journal of Computational Mathematics and Mathematical Physics, 6, 1–50.
Li, X , Zhao, T , Arora, R , Liu, H , and Hong, M 2018 On Faster convergence of cyclic
block coordinate descent-type methods for strongly convex minimization Journal
of Machine Learning Research, 18, 1 24
Liu, J , and Wright, S J 2015 Asynchronous stochastic coordinate descent: Parallelism
and convergence properties SIAM Journal on Optimization, 25(1), 351 376
Liu, J , Wright, S J , Re, C., Bittorf, V, and Sridhar, S 2015 An asynchronous parallel ´
stochastic coordinate descent algorithm Journal of Machine Learning Research,
16, 285–322
Łojasiewicz, S 1963 Une propriet´ e topologique des sous ensembles analytiques r ´ eels ´
Les Equations aus D´ ´ eriv´ees Partielles, 117, 87–89.
Lu, Z., and Xiao, L. 2015. On the complexity analysis of randomized block-coordinate
descent methods. Mathematical Programming, Series A, 152, 615–642.
Luo, Z.-Q., Sturm, J. F., and Zhang, S. 2000. Conic convex programming and self-dual
embedding. Optimization Methods and Software, 14, 169–218.220 Bibliography
Maddison, C. J., Paulin, D., Teh, Y. W., O’Donoghue, B., and Doucet, A. 2018.
Hamiltonian descent methods. arXiv preprint arXiv:1809.05042.
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. 2009. Robust stochastic approx￾imation approach to stochastic programming. SIAM Journal on Optimization,
19(4), 1574–1609.
Nesterov, Y. 1983. A method for unconstrained convex problem with the rate of
convergence O(1/k2). Doklady AN SSSR, 269, 543–547.
Nesterov, Y. 2004. Introductory Lectures on Convex Optimization: A Basic Course.
Boston: Kluwer Academic Publishers.
Nesterov, Y 2012. Efficiency of coordinate descent methods on huge-scale optimization
problems SIAM Journal on Optimization, 22(January), 341 362
Nesterov, Y 2015 Universal gradient methods for convex optimization problems
Mathematical Programming, 152(1 2), 381 404.
Nesterov, Y, and Nemirovskii, A S 1994 Interior Point Polynomial Methods in
Convex Programming Philadelphia, PA: SIAM
Nesterov, Y, and Stich, S U 2017 Efficiency of the accelerated coordinate descent
method on structured optimization problems SIAM Journal on Optimization,
27(1), 110–123
Nocedal, J., and Wright, S. J. 2006. Numerical Optimization. Second edition. New
York: Springer.
Parikh, N., and Boyd, S. 2013. Proximal algorithms. Foundations and Trends in
Optimization, 1(3), 123–231.
Polyak, B. T. 1963. Gradient methods for minimizing functionals (in Russian). Zhurnal
Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 643–653.
Polyak, B. T. 1964. Some methods of speeding up the convergence of iteration methods.
USSR Computational Mathematics and Mathematical Physics, 4, 1–17.
Powell, M. J. D. 1969. A method for nonlinear constraints in minimization prob￾lems. Pages 283–298 of: Fletcher, R. (ed), Optimization. New York: Academic
Press.
Rao, C. V., Wright, S. J., and Rawlings, J. B. 1998. Application of interior-point meth￾ods to model predictive control. Journal of Optimization Theory and Applications,
99, 723–757.
Recht, B., Fazel, M., and Parrilo, P. 2010. Guaranteed Minimum-rank solutions to linear
matrix equations via nuclear norm minimization SIAM Review, 52(3), 471–501
Richtarik, P , and Takac, M 2014 Iteration complexity of a randomized block￾coordinate descent methods for minimizing a composite function Mathematical
Programming, Series A, 144(1), 1 38
Richtarik, P., and Takac, M 2016a Distributed coordinate descent method for learning
with big data Journal of Machine Learning Research, 17, 1 25
Richtarik, P., and Takac, M 2016b. Parallel coordinate descent methods for big data
optimization Mathematical Programming, Series A, 156, 433–484
Robbins, H , and Monro, S 1951 A stochastic approximation method Annals of
Mathematical Statistics, 22(3), 400–407.
Rockafellar, R. T. 1970. Convex Analysis. Princeton, NJ: Princeton University Press.
Rockafellar, R. T. 1973. The multiplier method of Hestenes and Powell applied to
convex programming. Journal of Optimization Theory and Applications, 12(6),
555–562.Bibliography 221
Rockafellar, R. T. 1976a. Augmented Lagrangians and applications of the proximal
point algorithm in convex programming. Mathematics of Operations Research, 1,
97–116.
Rockafellar, R. T. 1976b. Monotone operators and the proximal point algorithm. SIAM
Journal on Control and Optimization, 14, 877–898.
Rosenblatt, F. 1958. The perceptron: A probabilistic model for information storage and
organization in the brain. Psychological Review, 65(6), 386.
Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter, A. 2011. Pegasos: Primal
estimated sub-gradient solver for SVM Mathematical Programming, 127(1),
3–30
Shi, B , Du, S S , Jordan, M I , and Su, W J 2018 Understanding the accel￾eration phenomenon via high-resolution differential equations. arXiv preprint
arXiv:1810.08907
Sion, M 1958 On general minimax theorems. Pacific Journal of Mathematics, 8(1),
171 176.
Stellato, B , Banjac, G., Goulart, P , Bemporad, A , and Boyd, S 2020. OSQP: An
operator splitting solver for quadratic programs Mathematical Programming
Computation, 12(4), 637–672
Strohmer, T., and Vershynin, R. 2009. A randomized Kaczmarz algorithm with
exponential convergence. Journal of Fourier Analysis and Applications, 15(2),
262.
Su, W., Boyd, S., and Candes, E. 2014. A differential equation for modeling Nesterov’s `
accelerated gradient method: Theory and insights. Pages 2510–2518 of: Advances
in Neural Information Processing Systems.
Sun, R., and Hong, M. 2015. Improved iteration complexity bounds of cyclic block
coordinate descent for convex problems. Pages 1306–1314 of: Advances in Neural
Information Processing Systems.
Teo, C. H., Vishwanathan, S. V. N., Smola, A., and Le, Q. V. 2010. Bundle methods
for regularized risk minimization. Journal of Machine Learning Research, 11(1),
311–365.
Tibshirani, R. 1996. Regression shrinkage and selection via the LASSO. Journal of the
Royal Statistical Society B, 58, 267–288.
Todd, M. J. 2001. Semidefinite optimization. Acta Numerica, 10, 515–560.
Tseng, P., and Yun, S 2010 A coordinate gradient descent method for linearly
constrained smooth optimization and support vector machines training Compu￾tational Optimization and Applications, 47(2), 179 206.
Vandenberghe, L 2016. Slides for EE236C: Optimization Methods for Large-Scale
Systems
Vandenberghe, L., and Boyd, S 1996 Semidefinite programming SIAM Review, 38,
49–95.
Vapnik, V 1992. Principles of risk minimization for learning theory Pages 831–838 of:
Advances in Neural Information Processing Systems
Vapnik, V. 2013. The Nature of Statistical Learning Theory. Berlin: Springer Science
& Business Media.
Wibisono, A., Wilson, A. C., and Jordan, M. I. 2016. A variational perspective on
accelerated methods in optimization. Proceedings of the National Academy of
Sciences, 113(47), E7351–E7358.222 Bibliography
Wolfe, P. 1975. A method of conjugate subgradients for minimizing nondifferentiable
functions. Pages 145–173 of: Nondifferentiable Optimization. Springer.
Wright, S. J. 1997. Primal-Dual Interior-Point Methods. Philadelphia, PA: SIAM.
Wright, S. J. 2012. Accelerated block-coordinate relaxation for regularized optimiza￾tion. SIAM Journal on Optimization, 22(1), 159–186.
Wright, S. J. 2018. Optimization algorithms for data analysis. Pages 49–97 of:
Mahoney, M., Duchi, J. C., and Gilbert, A. (eds), The Mathematics of Data.
IAS/Park City Mathematics Series, vol. 25. AMS.
Wright, S J , and Lee, C.-P 2020. Analyzing random permutations for cyclic coordinate
descent Mathematics of Computation, 89, 2217–2248.
Wright, S J , Nowak, R D , and Figueiredo, M A T 2009 Sparse reconstruction by
separable approximation IEEE Transactions on Signal Processing, 57(August),
2479 2493
Zhang, T 2004. Solving large scale linear prediction problems using stochastic gradient
descent algorithms Page 116 of: Proceedings of the Twenty-First International
Conference on Machine LearningIndex
accelerated gradient methods, 55–70, 94
for composite nonsmooth optimization, 71,
168
for constrained optimization, 126
accumulation point, 34, 201
active-set method, 114
adjoint method, 190–192
application to neural networks, 191 192
forward pass, 190
relationship to chain rule, 190
reverse pass, 190
algorithmic differentiation, see automatic
differentiation
alternating direction method of multipliers
(ADMM), 181 184, 186, 187
augmented Lagrangian method, 167, 180–182,
186, 187
comparison with dual subgradient method,
181
software, 187
specification of, 181
automatic differentiation, 103, 192–195
checkpointing, 195
computation graph, 193
reverse mode, 194
reverse sweep, 194, 196
averaging of iterates
in dual subgradient method, 180
in mirror descent, 48, 50
in the stochastic gradient method, 89
in subgradient method, 156, 157
back-propagation, 75, 194
boundary point of a set, 211
bounds, 26, 114, 118, 122, 185, 186, 198
Bregman divergence, 45–47, 50
generating function for, 45
bundle methods, 156, 168
cardinality of vector, 150
Cauchy-Schwartz inequality, 31, 149
chain rule, 188–190
efficiency of, 190
forward pass, 189
reverse pass, 189
Chebyshev iterative method, 57, 71
classification, 2, 12, 14, 77–78, 101, 191
clustering, 3, 6
co-coercivity property, 25, 52
complementarity condition, 196
complexity, 13, 14, 32, 42, 115, 203–204
lower bounds, 56, 70–71
of gradient methods, 61
of second-order methods, 42–44
composite nonsmooth function, 146–150, 154,
160
first-order necessary conditions, 147
first-order optimality conditions, 146–148
strongly convex, 147
compressed sensing, 168
computational differentiation, see automatic
differentiation
condition number, 61
conditional gradient method (Frank-Wolfe),
127 130, 186
definition of, 128
cone, 119, 201
polar of, 201
of positive semidefinite matrices, 173
conjugacy, 69
223224 Index
conjugate gradient method, 55, 68 70
linear, 68–70, 72
nonlinear, 70–72
consensus optimization, 182–184
constrained optimization, 15, 21, 118 129,
133, 146, 170, 172, 196
convex, 144 146
equality constraints, 170–171, 177, 186,
195, 197
statement of, 118, 170
constraint qualification, 145, 175, 177
convergence rate
linear, 30, 33, 35, 105, 126
Q-linear, 201, 203
R-linear, 61, 201
sublinear, 33, 68, 82, 105, 124, 128, 203
convex hull, 156, 206, 209
convexity
of function, 21
modulus of, 22, 30, 34, 38, 50, 85, 88, 90,
93, 104, 107, 111, 113, 161, 165, 213
in non-Euclidean norm, 45, 50
of quadratic function, 31, 55, 58
of set, 21, 144, 200, 208
strong, 21 24, 30–32, 34, 45, 47, 88, 93,
107, 109, 115, 120, 148, 165
weak, 21, 107, 112
coordinate descent methods, 39, 100–114
accelerated, 115
block, 100, 101, 113–114, 116, 182
comparison with steepest-descent method,
109–111
cyclic, 110–113, 115
for empirical risk minimization, 101 102
for graph-structured objective, 102–103
in machine learning, 101
parallel implementation, 116
proximal, 154, 164–167
random-permutations, 112
randomized, 37, 101, 105–111, 115, 165
for regularized optimization, 113
Danskin’s Theorem, 133, 141 142, 151, 179
data analysis, 1 3, 100
data assimilation, 188, 190
deep learning, see neural networks
descent direction, 27 155
definition of, 27
Gauss-Southwell, 37, 115
in line-search methods, 36–38
randomized, 37
differential equation limits of gradient
methods, 56–57, 71
dissipation term, 56
directed acyclic graph (DAG), 193
directional derivatives, 40, 137 141, 153
additivity of, 138
definition of, 137
homogeneity of, 138
at minimizer, 137
distributed computing, 183, 184
dual problem, 170, 172, 178, 185
for linear programming, 206
dual variable, see Lagrange multiplier
duality, 170, 171
for linear programming, 200, 205 206
strong, 178–179, 206, 207
weak, 155, 172 174, 206
duality gap, 173
example of positive gap, 173–174, 187
effective domain, 134, 136, 139, 143, 146
eigenvalue decomposition of symmetric
matrix, 202
empirical model, 3
empirical risk minimization (ERM), 78–80,
95, 101 102
and finite-sum objective, 79
entropy function, 46
epigraph, 21, 134, 135
epoch, 160
Euclidean projection, see projection operator
extended-value function, 134, 144
Farkas Lemma, 205–207
feasible set, 118
feature selection, 2, 5
feature vector, 1, 192
finite differences, 103
finite-sum objective, 2, 12, 77, 80, 81, 85–87,
94, 96, 183, 184, 192
frame, 83
Gauss-Seidel method, 100, 110, 111
Gelfand’s formula, 60
generalizability, 7, 13
global minimizer, 27
Gordan’s Theorem, 207, 209
gradient descent method, see steepest-descent
method
gradient map, 162Index 225
gradient methods with momentum, see
accelerated gradient methods
graph, 102, 182
objective function based on, 103, 182
heavy-ball method, 55, 57, 65, 68, 71
Heine-Borel theorem, 209
image segmentation, 102
implicit function theorem, 197, 202–203
incremental gradient method, 77, 95
cyclic, 80–81
randomized, 77, 80, 87
indicator function, 114, 133, 144, 160, 183
definition of, 144
proximal operator of, 148
subdifferential of, 144, 145
iterate averaging, see averaging of iterates
Jacobian matrix, 188, 196, 198, 202
Jensen’s inequality, 85, 106, 202
Kaczmarz method
deterministic, 82 84
linear convergence of, 83
randomized, 75, 82–84, 86–87, 91–92, 95
Karush-Kuhn-Tucker (KKT) conditions, 206
Kullback-Liebler (KL) divergence, 46
Kurdyka-Łojasiewicz (KL) condition, 51, 116
label, 2, 3, 10, 11, 192
Lagrange multiplier, 172, 182, 184, 196
Lagrangian, see Lagrangian function
Lagrangian function, 170, 172, 175, 196
augmented, 180, 181, 183, 184, 186
for semidefinite program, 173
Lanczos method, 44
law of iterated expectation, 88
learning rate, see steplength
least squares, 4–5, 75, 102, 114
with zero loss, 82, 91
level set, 35, 104, 105, 147
limiting feasible directions, 208–209
line search, 105
backtracking, 41–42, 124 125
exact, 39, 107, 110
extrapolation-bisection, 40–41, 204–205
linear independence, 69
linear programming, 186, 205–206
simplex method, 206
Lipschitz constant for gradient, 17, 23, 28, 33,
38, 76, 87, 88, 101, 104, 122, 123, 125,
128, 161 163
componentwise, 104, 113, 115, 165
componentwise, for quadratic functions,
104
for quadratic functions, 104
Lipschitz constant for Hessian, 43
Lipschitz continuity, 17
logistic regression, 9–10, 86
binary, 9
multiclass, 10, 12, 192
loss function, 2, 79, 101
hinge, 79, 132, 139
low-dimensional subspace, 2, 3
lower-semicontinuous function, 134, 144
Lyapunov function, 55
for Nesterov’s method, 61–68, 71
matrix optimization, 2, 5–6
low-rank matrix completion, 5, 114
nonnegative matrix factorization, 6, 114
maximum likelihood, 4, 9, 10, 13
method of multipliers, see augmented
Lagrangian method
min-max problem, see saddle point problem
minimizer
global, 15, 29
isolated local, 15
local, 15, 148
strict local, 15, 20
unique, 15, 147
minimum principle, 121, 210
mirror descent, 44 50, 89
convergence of, 47–50
missing data, 3
momentum, 55, 72, 94
Moreau envelope, 133, 150–151
gradient of, 150
relationship to proximal operator, 150
negative-curvature direction, 43, 44
nested composition of functions, 188
Nesterov’s method, 55, 57, 70
convergence on strongly convex functions,
62–65
convergence on strongly convex quadratics,
58–62
convergence on weakly convex functions,
66–68226 Index
neural networks, 11 13, 132, 188, 191 192
activation function, 11, 198
classification, 12
layer, 11
parameters, 12
training of, 12
Newton’s method, 37
nonlinear equations, 196, 202
nonnegative orthant, 121, 177, 185
nonsmooth function, 75, 132 150
eigenvalues of symmetric matrix, 133
norms, 133
normal cone, 48, 133, 144, 175, 208, 212–213
definition of, 118
illustration of, 119
of intersection of closed convex sets,
144 146
nuclear norm, 5
operator splitting, 182
optimal control, 188, 197–199
optimality conditions, 133, 209
for composite nonsmooth function, 146–148
for convex functions, 134
examples of, 176–178
first-order, 196
first-order necessary, 18–20, 27, 118, 119,
174 178
first-order sufficient, 22, 34, 119, 123, 146,
176, 208
geometric (for constrained optimization),
48, 118 120, 123, 146, 174–178
second-order necessary, 18–20, 42
second-order sufficient, 20
order notation, 16, 201
overfitting, 3
penalty function, 4
quadratic, 45, 170–171
penalty parameter, 171
perceptron, 78, 80, 95
as stochastic gradient method, 78
Polyak-Łojasiewicz (PL) condition, 51, 115,
213
prediction, 2
primal problem, 170, 173, 178
probability distribution, 75, 79, 202
progressive function, 190, 191, 195–196
projected gradient method, 114, 122–127, 130,
161, 186
alternative search directions, 126–127
with backtracking, 124 125
definition of, 122
short-step, 123–124
for strongly convex function, 125–126
projection operator, 120–122, 128, 148, 170,
185, 210
nonexpansivity of, 121, 126
proper convex function, 134
closed, 134, 148
prox-operator, see proximal operator
proximal operator, 133, 148–150, 160, 162
of indicator function, 148
nonexpansivity of, 149, 161
of zero function, 149
proximal point method, 154, 167–168, 180
and augmented Lagrangian, 180
definition of, 167
sublinear convergence of, 167 168
proximal-gradient method, 110, 126, 148, 149,
154, 160–164, 168
linear convergence of, 161 162
sublinear convergence of, 162
quadratic programming, 185–186
OSQP solver, 186
regression, 2, 79, 101
regularization, 3
1, 4, 9
2, 4, 168
group-sparse, 10
regularization function, 3, 13, 26, 101, 103,
149, 160, 161
block-separable, 113, 114
separable, 101, 110, 115, 154, 165
regularization functions
block-separable, 116
regularization parameter, 3, 7, 9, 101, 160
regularized optimization, see composite
nonsmooth function
regularizer, see regularization function
restricted isometry property, 6
robustness, 7
saddle point problem, 171, 180
sampling, 79
with replacement, 113
without replacement, 113
semidefinite programming, 173
separable function, 183, 184
separating hyperplane, 7, 200, 207, 209, 211,
212
separation, 200, 209–212
of closed convex sets, 210–211Index 227
of hyperplane from convex set, 211 212
of point from convex set, 209–210
proper, 211, 212
strict, 143, 209–211
set
affine, 200
affine hull of, 200
closure of, 200
interior of, 200
multipliction by scalar, 200
relative interior of, 175, 200, 211
Sion’s minimax theorem, 180
slack variables, 185
softmax, 10–12, 14
solution
global, 16, 21, 118, 119
local, 16, 21, 118, 119
spectral radius, 58
stationary point, 20, 27, 29, 34, 36, 195, 196
steepest-descent method, 27 33, 43, 44, 55,
62, 68, 76, 77, 101, 105, 111, 149, 153,
155, 160, 161
short-step, 28–30, 38, 109, 110
steplength, 27, 28, 33, 38–42, 78, 110, 122,
161
constant step norm, 158
decreasing, 93, 158–160
exact, 39
fixed, 28, 38, 92, 105, 107, 111, 158,
161 163, 167
in mirror descent, 49–50
for steepest-descent method, 28
for subgradient method, 158 160, 180
Wolfe conditions and, 39–42
stochastic gradient descent (SGD), see
stochastic gradient method
stochastic gradient method, 38, 75–95, 157,
192, 214
accelerated, 96
additive noise model, 76, 86
basic step, 75 76
bounded variance assumption, 85
contrast with steepest-descent method, 76
convergence analysis of, 87–93
epochs, 92–94
hyperparameters, 93, 94
linear convergence of, 90–92
minibatches, 94–95, 192, 199
momentum, 94–95
parallel implementation, 94
SAG, 96
SAGA, 96
steplength, 81, 85, 88, 90–93
sublinear convergence of, 82
SVRG, 96
variance reduction, 94
subdifferential, 132 144, 153
calculus of, 141 144
Clarke, 198
closedness and convexity of, 134
compactness of, 136, 143
definition of, 134
and directional derivatives, 138 141
subgradient, 132–144, 153, 211
definition of, 134
existence of, 135
minimum-norm, 154–156
of smooth function, 137
and supporting hyperplane of epigraph, 135
subgradient descent method, 155–156
subgradient method, 154, 156 160, 179, 198
with constant step norm, 158
with decreasing steplength, 158 160
dual, 179 181, 183, 185
with fixed steplength, 158
sublinear convergence of, 157–160
sufficient decrease condition, 39, 41, 125
support vector machines, 6–9, 78, 79, 132
kernel, 9
maximum-margin, 7
supporting hyperplane, 135, 211
symmetric over-relaxation, 111
Taylor series, see Taylor’s theorem
Taylor’s theorem, 15–18, 20, 22 24, 27, 28,
36, 40, 42, 43, 45, 106, 119, 125, 128,
139, 161
statement of, 16
for vector functions, 202
telescoping sum, 49, 164
theorems of the alternative, 205–207
three-point property, 46, 48
thresholding
hard, 150
soft, 150
topic modeling, 102
training, 1, 192
unbiased gradient estimate, 75, 77
utility maximization, 184 185
warm start, 168, 171
Wolfe conditions
strong, 53
weak, 39–40, 204 205
